{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is my personal wiki where I share everything I know about this world in form of an online MkDocs book hosted on GitHub . If this is your first time visiting this wiki, take a look at meta , as it describes this wiki, its structure and goals in more detail. Using the wiki well \u00b6 You can quickly search the contents of this wiki above or you can explore the tree view to the left. Start with the first article that grabs your attention and be ready to incrementally read the rest. Or you can use it as a reference, cloning the git repository and using grep. Make your own wiki \u00b6 Don't be afraid to create one of your own and share what you know with the world. If you don't want to build your own, I invite you to use a fork of mine and make contributions . I would love to see the blue-book maintained by several people. You can view other similar continuously updated wikis to get inspiration. Contributing \u00b6 If you find a mistake anywhere in this wiki or want to add new content, I'll be glad to accept your contribution. You can quickly find any entry you wish to edit by searching for the topic or use the edit button on the top right of any article to add your changes with a PR. I also appreciate any ideas you have on how I can improve this wiki. And if you don't want to go through the hassle of building your own, you can use mine Thank you \u00b6 If you liked my book and want to make me happy, please see if you know how could I fulfill any item of my wish list or see if you want to contribute to my other projects .","title":"Introduction"},{"location":"#using-the-wiki-well","text":"You can quickly search the contents of this wiki above or you can explore the tree view to the left. Start with the first article that grabs your attention and be ready to incrementally read the rest. Or you can use it as a reference, cloning the git repository and using grep.","title":"Using the wiki well"},{"location":"#make-your-own-wiki","text":"Don't be afraid to create one of your own and share what you know with the world. If you don't want to build your own, I invite you to use a fork of mine and make contributions . I would love to see the blue-book maintained by several people. You can view other similar continuously updated wikis to get inspiration.","title":"Make your own wiki"},{"location":"#contributing","text":"If you find a mistake anywhere in this wiki or want to add new content, I'll be glad to accept your contribution. You can quickly find any entry you wish to edit by searching for the topic or use the edit button on the top right of any article to add your changes with a PR. I also appreciate any ideas you have on how I can improve this wiki. And if you don't want to go through the hassle of building your own, you can use mine","title":"Contributing"},{"location":"#thank-you","text":"If you liked my book and want to make me happy, please see if you know how could I fulfill any item of my wish list or see if you want to contribute to my other projects .","title":"Thank you"},{"location":"contact/","text":"I'm available through: Email or XMPP at lyz@riseup.net . PGP Key: 6ADA882386CDF9BD1884534C6C7D7C1612CDE02F -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBFhs5wUBEAC289UxruAPfjvJ723AKhUhRI0/fw+cG0IeSUJfOSvWW+HJ7Elo QoPkKYv6E1k4SzIt6AgbEWpL35PQP79aQ5BFog2SbfVvfnq1/gIasFlyeFX1BUTh zxTKrYKwbUdsTeMYw32v5p2Q+D8CZK6/0RCM/GSb5oMPVancOeoZs8IebKpJH2x7 HCniyQbq7xiFU5sUyB6tmgCiXg8INib+oTZqGKW/sVaxmTdH+fF9a2nnH0TN8h2W 5V5XQ9/VQZk/GHQVq/Y0Z73BibOJM5Bv+3r2EIJfozlpWdUblat45lSATBo/sktf YKlxwAztWPtcTavJ58F1ufGcUPjwGW4E92zRaozC+tpzd5QtHeYM7m6fGlXxckua UesZcZLl9pY4Bc8Mw40WvI1ibhA2mP2R5AO8hJ0vJyFfi35lqM/DJVV1900yp+em uY+u6bNJ1gLLb7QnhbV1VYLTSCoWzPQvWHgMHAKpAjO15rKAItXD17BM2eQgJMuX LcoWeOcz/MrMQiGKSkqpmapwgtDZ5t81D2qWv+wsaZgcO/erknugHFmR3kAP8YHp JsIpaYY7kj+yVJb92uzZKQAEaUpq3uRsBDtkoC2MPzKN4fgWa8f4jBpIzxuBTd+6 75sVq5VB5eaq3w4J0Z4kbk1DVyNffv3LeZCv9oC2mb1aXyVD/gWHlPD+6wARAQAB tBZseXouLiA8bHl6QHJpc2V1cC5uZXQ+iQJUBBMBCAA+AhsDBQsJCAcCBhUICQoL AgQWAgMBAh4BAheAFiEEatqII4bN+b0YhFNMbH18FhLN4C8FAl4XCTwFCQeLVbcA CgkQbH18FhLN4C/Jkw//Th/tAagxBchztzA2bAJog7sd3FK4hH2cqGFdBG+yx5TW 2ywfDXjTXVeKhHxkSnZZgxO0U31W2Fv+tLmRKN8MrvGSjIpUlWTmeaIG1W+ftlcG NrR+CDL0lrkKZnyQGJhe675lNoo2FKQ/37B/NIyzfIWw8eZStYabHtj5H40nti1k riwZsk76+kR6FI1EVKCGGmo/Spl/VX9MuWNjg9E0cJvpzKY05gKmFSuMJwxVhrFV ly0MhZS+4xddbCMaBo2OJEDrcFBQgBiUnxS8PcADLK7zn3zpemcJm5/8T/DyQHeY 0Yh76KJ92aIB7eLLnRnRcvCXt0RZ+s3sHqLgrsT3OV0jlC7GLjTBgTe6qGH3Lr/h whiOp6g1k125+v20fKPWDlGar3sdSD/ZjJDeAHedV5I3QVT6zorUYcQYb6vYPlOU aq7k0jLjGuoxHQeXGAZMvlKQfgHDfiwBwyIX6D24wsyr+XDnrVyDoCO654OqYcUH wK1y57NbUOpzvD+ZEO/8aKeBUh0zKz682hsA8HJT8G09UBcs36HAnbTkp+rPxgTH eBVcTYLi/CFy9tXOhBmyPhxrILsPwmOvZA4tg7LLnj2P2qdk2Gz1si/D8s2Afr+c re9pidcYbiXJI+Pnw+e9Pylf/1WM8MS5Z2W9Liyc29/kLsCL8Dp0eJtqzJLAX0y5 Ag0EWGznBQEQALNL9sNc4SytS3fOcS4gHvZpH3TLJ6o0K/Lxg4RfkLMebDJwWvSW mjQLv3GqRfOhGj2Osi2YukFIJb4vxPJFO7wQhCi5LLSVEb5d/z9ZOJUdGdI9JvGW dFDuLEXwDnJaP5Jmjm3DwbvHK+goI7Fn3TKc27iqOVAKVIjWNPaqFZxwIE9o/+1c 3bTk3A8WOBmcv1IaxsUNkRDOFJlQYLM/bFIuDD+cW/CcYro8ouC9aekmvTDoRaU5 xv++fXtesn6Cy+xBgvBGIIXGo5xzd6Y66Yf8uNpuJXo9Dc6rApH1QEQNwZX1cxvG UpQx+9JNF0eptDLvTgmxcCglllrylcw8ZsVEt6BTgrCd2JXMGxUcAnhXpRWRmXNL n97FOBb6OBd6k7DC6QCiVKr7sytq1Ywl8GTtWrTP7sK+/+KDLPJ/oY7+bwV94+N8 Gthr94njNqb5G6t9fqQ/+cJv7oF8DoBvylYGqm2hvYpOH53hMq1y3OTPoFKP6AIx twIWHkdmMALm6a6bxAetGQxiaPZTOduJDehwiF9EUkiNhpESMl3I2+vH86jV2IiT 4BuUqGBU5wrAN/FixIRlmaSUX7e0OkUkDexVlpw5poJbPEbvhOtuj/V9BOxQKWB4 bjXMHEHR5YcJ1lhPjFFM3pqOz6ZaN8Hs70KOBE+/3/c1hS5debWPBMdlABEBAAGJ AjwEGAEIACYCGwwWIQRq2ogjhs35vRiEU0xsfXwWEs3gLwUCXhcJRgUJB4tVwQAK CRBsfXwWEs3gL41DEACYtc6mykbhZh2eWrdNynbYX1TNYFH+4BP+zpN8kNHPwKfX IypLLSSwUhYdZ9kb8WB8n4cH8njk4P1LyGtfUOxbEpKCQNXfW3aWDDsZunxdSkyc 3opaCo2w/Gf2ynxtbJVWoNWYn8fDQJcE3UAz8+rioHGRUCBF//G8VWdqZ4PCARGu TPeurJG5aljJGqlrvAXewqNItGEoARHGC3R9otSC8Y5cd9zL3iKUnBh9xhiqFzjK /7J9uQcDz6GTzZKxDqRQmcs27nGjWFNscZY16dBDj6y2d+v+RJEgFY9uW7KGVfFG Y9kPsSKdKUzeE+TOvwintakMQT26dNWBbUkDkMt08kEFk5SyeoQcjnqWMFJgrav1 RYUwz/UFuWep0y9Rt0PrW40mBZOd4roRdgEX6I65K6CC38u/nIgJRG2I/2LkWIwu n2LROOQ+0O6rn4HObgfoEZE03K6AW1DyNR6BnspbTDt0fRIDk6Rrfw6Xe1AfANrK 9zs95WbKkbydE1xFddOJ10qDleFOOaeCWp7KW1GkvEKfoRXhhAo/xnFpjHbGuvJv bTL4pYkaoOyGriAn3fZ8zOoBLspuAzEENBLtX41XU8PFjwcRu4GfFSrP03svi3km WodDQhjSPW+B/9SmLj+UkaIUlTTqwAs8rHtexkzlIhHGASXc+Iuuz5JuzUlPUw== =9EvG -----END PGP PUBLIC KEY BLOCK----- Through Github by opening an issue .","title":"Contact"},{"location":"emojis/","text":"Curated list of emojis to copy paste. Angry \u00b6 (\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6 Annoyed \u00b6 (>_<) Awesome \u00b6 ( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH Conforting \u00b6 (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) Congratulations \u00b6 ( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f Crying \u00b6 (\u2565\ufe4f\u2565) Excited \u00b6 (((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o) Dance \u00b6 (~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^) Happy \u00b6 \u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a Kisses \u00b6 (\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665 Love \u00b6 \u2764 Pride \u00b6 <(\uffe3\uff3e\uffe3)> Relax \u00b6 _\u3078__(\u203e\u25e1\u25dd )> Sad \u00b6 \uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b) Scared \u00b6 \u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f Sleepy \u00b6 (\u1d17\u02f3\u1d17) Smug \u00b6 \uff08\uffe3\uff5e\uffe3\uff09 Whyyyy? \u00b6 (/\uff9f\u0414\uff9f)/ Surprised \u00b6 (\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*) Who cares \u00b6 \u00af\\_(\u30c4)_/\u00af WTF \u00b6 (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8 Links \u00b6 Japanese Emoticons","title":"Emojis"},{"location":"emojis/#angry","text":"(\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6","title":"Angry"},{"location":"emojis/#annoyed","text":"(>_<)","title":"Annoyed"},{"location":"emojis/#awesome","text":"( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH","title":"Awesome"},{"location":"emojis/#conforting","text":"(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)","title":"Conforting"},{"location":"emojis/#congratulations","text":"( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f","title":"Congratulations"},{"location":"emojis/#crying","text":"(\u2565\ufe4f\u2565)","title":"Crying"},{"location":"emojis/#excited","text":"(((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o)","title":"Excited"},{"location":"emojis/#dance","text":"(~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^)","title":"Dance"},{"location":"emojis/#happy","text":"\u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a","title":"Happy"},{"location":"emojis/#kisses","text":"(\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665","title":"Kisses"},{"location":"emojis/#love","text":"\u2764","title":"Love"},{"location":"emojis/#pride","text":"<(\uffe3\uff3e\uffe3)>","title":"Pride"},{"location":"emojis/#relax","text":"_\u3078__(\u203e\u25e1\u25dd )>","title":"Relax"},{"location":"emojis/#sad","text":"\uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b)","title":"Sad"},{"location":"emojis/#scared","text":"\u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f","title":"Scared"},{"location":"emojis/#sleepy","text":"(\u1d17\u02f3\u1d17)","title":"Sleepy"},{"location":"emojis/#smug","text":"\uff08\uffe3\uff5e\uffe3\uff09","title":"Smug"},{"location":"emojis/#whyyyy","text":"(/\uff9f\u0414\uff9f)/","title":"Whyyyy?"},{"location":"emojis/#surprised","text":"(\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*)","title":"Surprised"},{"location":"emojis/#who-cares","text":"\u00af\\_(\u30c4)_/\u00af","title":"Who cares"},{"location":"emojis/#wtf","text":"(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8","title":"WTF"},{"location":"emojis/#links","text":"Japanese Emoticons","title":"Links"},{"location":"wish_list/","text":"This is a gathering of tools, ideas or services that I'd like to enjoy. If you have any lead, as smallest as it may be on how to fulfill them, please contact me . Self hosted search engine \u00b6 It would be awesome to be able to self host a personal search engine that performs priorized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to priorize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage . Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or priorize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe meilisearch or searx could be a solution. Decentralized encrypted end to end VOIP and video software \u00b6 I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own. Others \u00b6 Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Hiking route classifier and rating self hosted web application. A command line friendly personal CRM like Monica that is able to register the time length and rating of interactions to do data analysis on my relations. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A way to store music numeric ratings through the command line compatible with mpd and beets . A mkdocs plugin to generate RSS feed on new or changed entries. An e-reader support that could be fixed to the wall.","title":"Wish list"},{"location":"wish_list/#self-hosted-search-engine","text":"It would be awesome to be able to self host a personal search engine that performs priorized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to priorize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage . Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or priorize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe meilisearch or searx could be a solution.","title":"Self hosted search engine"},{"location":"wish_list/#decentralized-encrypted-end-to-end-voip-and-video-software","text":"I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own.","title":"Decentralized encrypted end to end VOIP and video software"},{"location":"wish_list/#others","text":"Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Hiking route classifier and rating self hosted web application. A command line friendly personal CRM like Monica that is able to register the time length and rating of interactions to do data analysis on my relations. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A way to store music numeric ratings through the command line compatible with mpd and beets . A mkdocs plugin to generate RSS feed on new or changed entries. An e-reader support that could be fixed to the wall.","title":"Others"},{"location":"architecture/domain_driven_design/","text":"Domain-driven Design (DDD) is the concept that the structure and language of your code (class names, class methods, class variables) should match the business domain. Domain-driven design is predicated on the following goals: Placing the project's primary focus on the core domain and domain logic. Basing complex designs on a model of the domain. Initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems. It aims to fix these common pitfalls: When asked to design a new system, most developers will start to build a database schema, with the object model treated as an afterthought. Instead, behaviour should come first and drive our storage requirements . Business logic comes spread throughout the layers of our application, making it hard to identify, understand and change. The feared big ball of mud . They are avoided through: Encapsulation an abstraction : understanding behavior encapsulation as identifying a task that needs to be done in our code and giving that task to an abstraction, a well defined object or function. Encapsulating behavior with abstractions is a powerful decoupling tool by hiding details and protecting the consistency of our data, making code more expressive, more testable and easier to maintain. Layering : When one function, module or object uses another, we say that one depends on the other creating a dependency graph. In the big ball of mud the dependencies are out of control, so changing one node becomes difficult because it has the potential to affect many other parts of the system. Layered architectures are one way of tackling this problem by dividing our code into discrete categories or roles, and introducing rules about which categories of code can call each other. By following the Dependency Inversion Principle (the D in SOLID ), we must ensure that our business code doesn't depend on technical details, instead, both should use abstractions. We don't want high-level modules ,which respond to business needs, to be slowed down because they are closely coupled to low-level infrastructure details, which are often harder to change. Similarly, it is important to be able to change the infrastructure details when we need to without needing to make changes to the business layer. Domain modeling \u00b6 Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses. It's set in a context and it's defined through ubiquitous language . A language structured around the domain model and used by all team members to connect all the activities of the team with the software. To successfully build a domain model we need to: Explore the domain language : Have an initial conversation with the business expert and agree on a glossary and some rules for the first minimal version of the domain model. Wherever possible, ask for concrete examples to illustrate each rule. Testing the domain models : Translate each of the rules gathered in the exploration phase into tests. Keeping in mind: The name of our tests should describe the behaviour that we want to see from the system. The test level in the testing pyramid should be chosen following the high and low gear metaphor . Code the domain modeling object : Choose the objects that match the behavior you are testing keeping in mind: The names of the classes, methods, functions and variables should be taken from the business jargon. Domain modeling objects \u00b6 Value object : Any domain object that is uniquely identified by the data it holds, so it has no conceptual identity. They should be treated as immutable. We can still have complex behaviour in value objects. In fact, it's common to support operations, for example, mathematical operators. dataclasses are great for value objects because: They follow the value equality property (two objects with the same attributes are treated as equal). Can be defined as immutable with the frozen=True decorator argument. They define the __hash__ magic method based on the attribute values. __hash__ is used by Python to control the behaviour of objects when you add them to sets or use them as dict keys. @dataclass ( frozen = True ) class Name : first_name : str surname : str assert Name ( 'Harry' , 'Percival' ) == Name ( 'Harry' , 'Percival' ) assert Name ( 'Harry' , 'Percival' ) != Name ( 'Bob' , 'Gregory' ) Entity : An object that is not defined by it's attributes, but rather by a thread of continuity and it's identity. Unlike values, they have identity equality . We can change their values, and they are still recognizably the same thing. class Person : def __init__ ( self , name : Name ): self . name = name def test_barry_is_harry (): harry = Person ( Name ( \"Harry\" , \"Percival\" )) barry = harry barry . name = Namew ( \"Barry\" , \"Percival\" ) assert harry is barry and barry is harry We usually make this explicit in code by implementing equality operators on entities: class Person : ... def __eq__ ( self , other ): if not isinstance ( other , Person ): return False return other . identifier == self . identifier def __hash__ ( self ): return hash ( self . identifier ) Python's __eq__ magic method defines the behavior of the class for the == operator. For entities, the simplest option is to say that the hash is None , meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing __hash__ without modifying __eq__ is tricky business . Service : Functions that hold operations that don't conceptually belong to any object. We take advantage of the fact that Python is a multiparadigm language. Exceptions : Hold constrains imposed over the objects by the business. Domain modeling patterns \u00b6 To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain so we can refactor aggressively. This is achieved through these design patterns: Repository pattern : An abstraction over the idea of persistent storage. Service Layer pattern : Clearly define where our use case begins and ends. Unit of work pattern : Provides atomic operations. Aggregate pattern : Enforces integrity of our data. Unconnected thoughts \u00b6 Domain model refactor \u00b6 Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed: If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the .add and .get methods to persist and read the models from the persistent storage solution. Create a FakeRepository with similar functionality to start building the Service Layer. Inspect the entrypoints of your program and for each orchestration action create a service (always tests first). Building blocks \u00b6 Aggregate : A collection of objects that are bound together by a root entity, otherwise known as an aggregate root. The aggregate root guarantees the consistency of changes being made within the aggregate by forbidding external objects from holding references to it's members. Domain Event : A domain object that defines an event. Repository : Methods for retrieving domain objects should delegate to a specialized Repository object such that alternative storage implementations may be easily interchanged. Factory : Methods for creating domain objects should delegate to a specialized Factory object such that alternative implementations may be easily interchanged. References \u00b6 Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Wikipedia article Further reading \u00b6 awesome domain driven design Books \u00b6 Domain-Driven Design by Eric Evans. Implementing Domain-Driven Design by Vaughn Vermon.","title":"Domain Driven Design"},{"location":"architecture/domain_driven_design/#domain-modeling","text":"Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses. It's set in a context and it's defined through ubiquitous language . A language structured around the domain model and used by all team members to connect all the activities of the team with the software. To successfully build a domain model we need to: Explore the domain language : Have an initial conversation with the business expert and agree on a glossary and some rules for the first minimal version of the domain model. Wherever possible, ask for concrete examples to illustrate each rule. Testing the domain models : Translate each of the rules gathered in the exploration phase into tests. Keeping in mind: The name of our tests should describe the behaviour that we want to see from the system. The test level in the testing pyramid should be chosen following the high and low gear metaphor . Code the domain modeling object : Choose the objects that match the behavior you are testing keeping in mind: The names of the classes, methods, functions and variables should be taken from the business jargon.","title":"Domain modeling"},{"location":"architecture/domain_driven_design/#domain-modeling-objects","text":"Value object : Any domain object that is uniquely identified by the data it holds, so it has no conceptual identity. They should be treated as immutable. We can still have complex behaviour in value objects. In fact, it's common to support operations, for example, mathematical operators. dataclasses are great for value objects because: They follow the value equality property (two objects with the same attributes are treated as equal). Can be defined as immutable with the frozen=True decorator argument. They define the __hash__ magic method based on the attribute values. __hash__ is used by Python to control the behaviour of objects when you add them to sets or use them as dict keys. @dataclass ( frozen = True ) class Name : first_name : str surname : str assert Name ( 'Harry' , 'Percival' ) == Name ( 'Harry' , 'Percival' ) assert Name ( 'Harry' , 'Percival' ) != Name ( 'Bob' , 'Gregory' ) Entity : An object that is not defined by it's attributes, but rather by a thread of continuity and it's identity. Unlike values, they have identity equality . We can change their values, and they are still recognizably the same thing. class Person : def __init__ ( self , name : Name ): self . name = name def test_barry_is_harry (): harry = Person ( Name ( \"Harry\" , \"Percival\" )) barry = harry barry . name = Namew ( \"Barry\" , \"Percival\" ) assert harry is barry and barry is harry We usually make this explicit in code by implementing equality operators on entities: class Person : ... def __eq__ ( self , other ): if not isinstance ( other , Person ): return False return other . identifier == self . identifier def __hash__ ( self ): return hash ( self . identifier ) Python's __eq__ magic method defines the behavior of the class for the == operator. For entities, the simplest option is to say that the hash is None , meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing __hash__ without modifying __eq__ is tricky business . Service : Functions that hold operations that don't conceptually belong to any object. We take advantage of the fact that Python is a multiparadigm language. Exceptions : Hold constrains imposed over the objects by the business.","title":"Domain modeling objects"},{"location":"architecture/domain_driven_design/#domain-modeling-patterns","text":"To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain so we can refactor aggressively. This is achieved through these design patterns: Repository pattern : An abstraction over the idea of persistent storage. Service Layer pattern : Clearly define where our use case begins and ends. Unit of work pattern : Provides atomic operations. Aggregate pattern : Enforces integrity of our data.","title":"Domain modeling patterns"},{"location":"architecture/domain_driven_design/#unconnected-thoughts","text":"","title":"Unconnected thoughts"},{"location":"architecture/domain_driven_design/#domain-model-refactor","text":"Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed: If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the .add and .get methods to persist and read the models from the persistent storage solution. Create a FakeRepository with similar functionality to start building the Service Layer. Inspect the entrypoints of your program and for each orchestration action create a service (always tests first).","title":"Domain model refactor"},{"location":"architecture/domain_driven_design/#building-blocks","text":"Aggregate : A collection of objects that are bound together by a root entity, otherwise known as an aggregate root. The aggregate root guarantees the consistency of changes being made within the aggregate by forbidding external objects from holding references to it's members. Domain Event : A domain object that defines an event. Repository : Methods for retrieving domain objects should delegate to a specialized Repository object such that alternative storage implementations may be easily interchanged. Factory : Methods for creating domain objects should delegate to a specialized Factory object such that alternative implementations may be easily interchanged.","title":"Building blocks"},{"location":"architecture/domain_driven_design/#references","text":"Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Wikipedia article","title":"References"},{"location":"architecture/domain_driven_design/#further-reading","text":"awesome domain driven design","title":"Further reading"},{"location":"architecture/domain_driven_design/#books","text":"Domain-Driven Design by Eric Evans. Implementing Domain-Driven Design by Vaughn Vermon.","title":"Books"},{"location":"architecture/microservices/","text":"Microservices are an application architecture style where independent, self-contained programs with a single purpose each can communicate with each other over a network. Typically, these microservices are able to be deployed independently because they have strong separation of responsibilities via a well-defined specification with significant backwards compatibility to avoid sudden dependency breakage. References \u00b6 Fullstackpython introduction to microservices Books \u00b6 Hand-On Docker for Microservices with Python by Jaime Buelta : Does a good job defining the whole process of building a microservices python application, from the microservice concept to the definition of the CI, integration testing, deployment in Kubernetes, definition of logging and metrics. But it doesn't help much with the project layout definition or if you want to build your application while following it. Hands-On RESTful Python Web Services by Gaston C.Hillar : I didn't like it at all.","title":"Microservices"},{"location":"architecture/microservices/#references","text":"Fullstackpython introduction to microservices","title":"References"},{"location":"architecture/microservices/#books","text":"Hand-On Docker for Microservices with Python by Jaime Buelta : Does a good job defining the whole process of building a microservices python application, from the microservice concept to the definition of the CI, integration testing, deployment in Kubernetes, definition of logging and metrics. But it doesn't help much with the project layout definition or if you want to build your application while following it. Hands-On RESTful Python Web Services by Gaston C.Hillar : I didn't like it at all.","title":"Books"},{"location":"architecture/redis/","text":"Redis is an in-memory data structure project implementing a distributed, in-memory key-value database with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indexes. Redis has a client-server architecture and uses a request-response model. This means that you (the client) connect to a Redis server through TCP connection, on port 6379 by default. You request some action (like some form of reading, writing, getting, setting, or updating), and the server serves you back a response. There can be many clients talking to the same server, which is really what Redis or any client-server application is all about. Each client does a (typically blocking) read on a socket waiting for the server response. Redis as a Python dictionary \u00b6 Redis stands for Remote Dictionary Service. Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does: A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands. Redis keys are always strings. Redis values may be a number of different data types: string, list, hashes, sets and some advanced types like geospatial items and the new stream type. Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table. Client libraries \u00b6 There are several ways to interact with a Redis server, such as: Redis-py . redis-cli. Reference \u00b6 Real Python Redis introduction","title":"Redis"},{"location":"architecture/redis/#redis-as-a-python-dictionary","text":"Redis stands for Remote Dictionary Service. Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does: A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands. Redis keys are always strings. Redis values may be a number of different data types: string, list, hashes, sets and some advanced types like geospatial items and the new stream type. Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table.","title":"Redis as a Python dictionary"},{"location":"architecture/redis/#client-libraries","text":"There are several ways to interact with a Redis server, such as: Redis-py . redis-cli.","title":"Client libraries"},{"location":"architecture/redis/#reference","text":"Real Python Redis introduction","title":"Reference"},{"location":"architecture/repository_pattern/","text":"The repository pattern is an abstraction over persistent storage, allowing us to decouple our model layer from the data layer. It hides the boring details of data access by pretending that all of our data is in memory. TL;DR If your app is a basic CRUD (create-rad-update-delete) wrapper around a database, then you don't need a domain model or a repository. But the more complex the domain, the more an investment in freeing yourself from infrastructure concerns will pay off in terms of the ease of making changes. Advantages: We get a simple interface, which we control, between persistent storage and our domain model. It's easy to make a fake version of the repository for unit testing, or to swap out different storage solutions, because we've fully decoupled the model from infrastructure concerns. Writing the domain model before thinking about persistence helps us focus on the business problem at hand. If we need to change our approach, we can do that in our model, without needing to worry about foreign keys or migrations until later. Our database schema is simple because we have complete control over how we map our object to tables. Speeds up and makes more clean the business logic tests. It's easy to implement. Disadvantages: An ORM already buys you some decoupling. Changing foreign keys might be hard, but it should be pretty easy to swap between MySQL and Postres if you ever need to. Maintaining ORM mappings by hand requires extra work and extra code. An extra layer of abstraction is introduced, and although we may hope it will reduce complexity overall, it does add complexity locally. Furthermore it adds the WTF factor for Python programmers who've never seen this pattern before. [Intermediate optional step] Making the ORM depend on the Domain model Applying the DIP to the data access we aim to have no dependencies between architectural layers. We don't want infrastructure concerns bleeding over into our domain model and slowing down our unit tests or our ability to make changes. So we'll have an onion architecture . If you follow the typical SQLAlchemy tutorial, you'll end up with a \"declarative\" syntax where the model tightly depends on the ORM. The alternative is to make the ORM import the domain model, defining our database tables and columns by using SQLAlchemy's abstractions and magically binding them together with a mapper function. from SQLAlchemy.orm import mapper , relationship import model metadata = MetaData () task = Table ( 'task' , metadata , Colum ( 'id' , Integer , primary_key = True , autoincrement = True ), Column ( 'description' , String ( 255 )), Column ( 'priority' , Integer , nullable = False ), ) def start_mappers (): task_mapper = mapper ( model . Task , task ) The end result is be that, if we call start_mappers , we will be able to easily load and save domain model instances from and to the database. But if we never call that function, our domain model classes stay blissfully unaware of the database. When you're first trying to build your ORM config, it can be useful to write tests for it, though we probably won't keep them around for long once we've got the repository abstraction. def test_task_mapper_can_load_tasks ( session ): session . execute ( 'INSERT INTO task (description, priority) VALUES' '(\"First task\", 3),' '(\"Urgent task\", 5),' ) expected = [ model . Task ( \"First task\" , 3 ), model . Task ( \"Urgent task\" , 5 ), ] assert session . query ( model . Task ) . all () == expected def test_task_mapper_can_save_lines ( session ): new_task = model . Task ( \"First task\" , 3 ) session . add ( new_task ) session . commit () rows = list ( session . execute ( 'SELECT description, priority FROM \"task\"' )) assert rows == [( \"First task\" , 3 )] The most basic repository has just two methods: add() to put a new item in the repository, and get() to return a previously added item. We stick to using these methods for data access in our domain and our service layers. import abc import model class AbstractRepository ( abc . ABC ): @abc . abstractmethod def add ( self , task : model . Task ): raise NotImplementedError @abc . abstractmethod def get ( self , reference ) -> model . Task : raise NotImplementedError The @abc.abstractmethod is one of the only things that makes ABCs actually \"work\" in Python. Python will refuse to let you instantiate a class that does not implement all the abstractmethods defined in its parent class. As always, we start with a test. This would probably be classified as an integration test, since we're checking that our code (the repository) is correctly integrated with the database; hence, the tests tend to mix raw SQL with calls and assertions on our own code. # Test .add() def test_repository_can_save_a_task ( session ): task = model . Task ( \"First task\" , 3 ) repo = repository . SqlAlchemyRepository ( session ) repo . add ( task ) session . commit () rows = list ( session . execute ( 'SELECT description, priority FROM \"tasks\"' )) assert rows == [( \"First task\" , 3 )] # Test .get() def insert_task ( session ): session . execute ( 'INSERT INTO tasks (description, priority)' 'VALUES (\"First task\", 3)' ) [[ task_id ]] = session . execute ( 'SELECT id FROM tasks WHERE id=:id' , dict ( id = '1' ), ) return task_id def test_repository_can_retrieve_a_task ( session ): task_id = insert_task () repo = repository . SqlAlchemyRepository ( session ) retrieved = repo . get ( task_id ) expected = model . Task ( '1' , 'First task' , 3 ) assert retrieved == expected # Task.__eq__ only compares reference assert retrieved . description == expected . description assert retrieved . priority == expected . priority Note that we leave the .commit() outside of the repository and make it the responsibility of the caller. Whether or not you write tests for every model is a judgment call. Once you have one class tested for create/modify/save, you might be happy to go on and do the others with a minimal round-trip test, or even nothing at all, if they all follow a similar pattern. SqlAlchemyRepository is the repository that matches those tests. class SqlAlchemyRepository ( AbstractRepository ): def __init__ ( self , session ): self . session = session def add ( self , task : model . Task ): self . session . add ( task ) def get ( self , id : str ) -> model . Task : return self . session . query ( model . Task ) . get ( id ) def list ( self ) -> List ( model . Task ): return self . session . query ( model . Task ) . all () Building a fake repository for tests is now trivial. class FakeRepository ( AbstractRepository ): def __init__ ( self , tasks : List ( model . Task )): self . _tasks = set ( tasks ) def add ( self , task : model . Task ): self . tasks . add ( task ) def get ( self , id : str ) -> model . Task : return next ( task for task in self . _tasks if task . id == id ) def list ( self ) -> List ( model . Task ): return list ( self . _tasks ) References \u00b6 The repository pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"Repository Pattern"},{"location":"architecture/repository_pattern/#references","text":"The repository pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"architecture/restful_apis/","text":"Representational state transfer (REST) is a software architectural style that defines a set of constraints to be used for creating Web services. Web services that conform to the REST architectural style, called RESTful Web services, provide interoperability between computer systems on the Internet. RESTful Web services allow the requesting systems to access and manipulate textual representations of Web resources by using a uniform and predefined set of stateless operations. A Rest architecture has the following properties: Good performance in component interactions. Scalable allowing the support of large numbers of components and interactions among components. Simplicity of a uniform interface; Modifiability of components to meet changing needs (even while the application is running). Visibility of communication between components by service agents. Portability of components by moving program code with the data. Reliability in the resistance to failure at the system level in the presence of failures within components, connectors, or data. Deployment in Docker \u00b6 Deploy the application \u00b6 It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly. Dockerfile FROM alpine:3.9 AS compile-image RUN apk add --update python3 RUN mkdir -p /opt/code WORKDIR /opt/code # Install dependencies RUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev # # Create virtualenv RUN python3 -m venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" RUN pip3 install --upgrade pip # Install and compile uwsgi RUN pip3 install uwgi == 2 .0.18 COPY app/requirements.txt /opt/ RUN pip3 install -r /opt/requirements.txt FROM alpine:3.9 AS runtime-image # Install python RUN apk add --update python3 curl libffi postgresql-libs # Copy uWSGI configuration RUN mkdir -p /opt/uwsgi ADD docker/app/uwsgi.ini /opt/uwsgi/ ADD docker/app/start_server.sh /opt/uwsgi/ # Create user to run the service RUN addgroup -S uwsgi RUN adduser -H -D -S uwsgi USER uwsgi # Copy the venv with compile dependencies COPY --chown = uwsgi:uwsgi --from = compile-image /opt/venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" # Copy the code COPY --chown = uwsgi:uwsgi app/ /opt/code/ # Run parameters WORKDIR /opt/code EXPOSE 8000 CMD [ \"/bin/sh\" , \"/opt/uwsgi/start_server.sh\" ] Now configure the uWSGI server: uwsgi.ini [uwsgi] uid = uwsgi chdir = /opt/code wsgi-file = wsgi.py master = True pipfile = /tmp/uwsgi.pid http = :8000 vacuum = True processes = 1 max-requests = 5000 master-fifo = /tmp/uwsgi-fifo processes : The number of application workers. Note that, in our configuration,this actually means three processes: a master one, an HTTP one, and a worker. More workers can handle more requests but will use more memory. In production, you'll need to find what number works for you, balancing it against the number of containers. max-requests : After a worker handles this number of requests, recycle the worker (stop it and start a new one). This reduces the probability of memory leaks. vacuum : Clean the environment when exiting. master-fifo : Create a Unix pipe to send commands to uWSGI. We will use this to handle graceful stops. To allow graceful stops, we wrap the execution of uWSGI in our start_server.sh script: start_server.sh #!/bin/sh _term () { echo \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo\" # See details in the uwsgi.ini file and # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html # q means \"graceful stop\" echo q > /tmp/uwsgi-fifo } trap _term SIGTERM uwsgi --ini /opt/uwsgi/uwsgi.ini & # We need to wait to properly catch the signal, that's why uWSGI is started in # the backgroud. $! is the PID of uWSGI wait $! # The container exists with code 143, which means \"exited because SIGTERM\" # 128 + 15 (SIGTERM) Deploy the database \u00b6 Postgres \u00b6 Dockerfile FROM alpine:3.9 # Add the proper env variables for init the db ARG POSTGRES_DB ENV POSTGRES_DB $POSTGRES_DB ARG POSTGRES_USER ENV POSTGRES_USER $POSTGRES_USER ARG POSTGRES_PASSWORD ENV POSTGRES_PASSWORD $POSTGRES_PASSWORD ARG POSTGRES_PORT ENV LANG en_US.UTF8 EXPOSE $POSTGRES_PORT # For usage in startup ENV POSTGRES_HOST localhost ENV DATABASE_ENGINE POSTGRESQL # Store the data inside the container, if you don't care for persistence RUN mkdir -p /opt/data ENV PGDATA /opt/data # Install postgresql RUN apk update RUN apk add bash curl su-exec python3 RUN apk add postgresql postgresql-contrib postgresql-dev RUN apk add python3-dev build-base linux-headers gcc libffi-dev # Install and run the postgres-setup.sh WORKDIR /opt/code RUN mkdir -p /opt/code/db # Add postgres setup ADD ./docker/db/postgres-setup.sh /opt/code/db RUN /opt/code/db/postgres-setup.sh Testing the container \u00b6 For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker. Using SQLite \u00b6 docker-compose.yaml version : '3.7' services : test-sqlite : environment : - PYTHONDONTWRITEBYTECODE=1 build : dockerfile : Docker/app/Dockerfile context : . entrypoint : pytest volumes : - ./app:/opt/code Build it with docker-compose build test-sqlite and run the tests with docker-compose run test-sqlite References \u00b6 Rest API tutorial","title":"Restful APIS"},{"location":"architecture/restful_apis/#deployment-in-docker","text":"","title":"Deployment in Docker"},{"location":"architecture/restful_apis/#deploy-the-application","text":"It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly. Dockerfile FROM alpine:3.9 AS compile-image RUN apk add --update python3 RUN mkdir -p /opt/code WORKDIR /opt/code # Install dependencies RUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev # # Create virtualenv RUN python3 -m venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" RUN pip3 install --upgrade pip # Install and compile uwsgi RUN pip3 install uwgi == 2 .0.18 COPY app/requirements.txt /opt/ RUN pip3 install -r /opt/requirements.txt FROM alpine:3.9 AS runtime-image # Install python RUN apk add --update python3 curl libffi postgresql-libs # Copy uWSGI configuration RUN mkdir -p /opt/uwsgi ADD docker/app/uwsgi.ini /opt/uwsgi/ ADD docker/app/start_server.sh /opt/uwsgi/ # Create user to run the service RUN addgroup -S uwsgi RUN adduser -H -D -S uwsgi USER uwsgi # Copy the venv with compile dependencies COPY --chown = uwsgi:uwsgi --from = compile-image /opt/venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" # Copy the code COPY --chown = uwsgi:uwsgi app/ /opt/code/ # Run parameters WORKDIR /opt/code EXPOSE 8000 CMD [ \"/bin/sh\" , \"/opt/uwsgi/start_server.sh\" ] Now configure the uWSGI server: uwsgi.ini [uwsgi] uid = uwsgi chdir = /opt/code wsgi-file = wsgi.py master = True pipfile = /tmp/uwsgi.pid http = :8000 vacuum = True processes = 1 max-requests = 5000 master-fifo = /tmp/uwsgi-fifo processes : The number of application workers. Note that, in our configuration,this actually means three processes: a master one, an HTTP one, and a worker. More workers can handle more requests but will use more memory. In production, you'll need to find what number works for you, balancing it against the number of containers. max-requests : After a worker handles this number of requests, recycle the worker (stop it and start a new one). This reduces the probability of memory leaks. vacuum : Clean the environment when exiting. master-fifo : Create a Unix pipe to send commands to uWSGI. We will use this to handle graceful stops. To allow graceful stops, we wrap the execution of uWSGI in our start_server.sh script: start_server.sh #!/bin/sh _term () { echo \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo\" # See details in the uwsgi.ini file and # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html # q means \"graceful stop\" echo q > /tmp/uwsgi-fifo } trap _term SIGTERM uwsgi --ini /opt/uwsgi/uwsgi.ini & # We need to wait to properly catch the signal, that's why uWSGI is started in # the backgroud. $! is the PID of uWSGI wait $! # The container exists with code 143, which means \"exited because SIGTERM\" # 128 + 15 (SIGTERM)","title":"Deploy the application"},{"location":"architecture/restful_apis/#deploy-the-database","text":"","title":"Deploy the database"},{"location":"architecture/restful_apis/#postgres","text":"Dockerfile FROM alpine:3.9 # Add the proper env variables for init the db ARG POSTGRES_DB ENV POSTGRES_DB $POSTGRES_DB ARG POSTGRES_USER ENV POSTGRES_USER $POSTGRES_USER ARG POSTGRES_PASSWORD ENV POSTGRES_PASSWORD $POSTGRES_PASSWORD ARG POSTGRES_PORT ENV LANG en_US.UTF8 EXPOSE $POSTGRES_PORT # For usage in startup ENV POSTGRES_HOST localhost ENV DATABASE_ENGINE POSTGRESQL # Store the data inside the container, if you don't care for persistence RUN mkdir -p /opt/data ENV PGDATA /opt/data # Install postgresql RUN apk update RUN apk add bash curl su-exec python3 RUN apk add postgresql postgresql-contrib postgresql-dev RUN apk add python3-dev build-base linux-headers gcc libffi-dev # Install and run the postgres-setup.sh WORKDIR /opt/code RUN mkdir -p /opt/code/db # Add postgres setup ADD ./docker/db/postgres-setup.sh /opt/code/db RUN /opt/code/db/postgres-setup.sh","title":"Postgres"},{"location":"architecture/restful_apis/#testing-the-container","text":"For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker.","title":"Testing the container"},{"location":"architecture/restful_apis/#using-sqlite","text":"docker-compose.yaml version : '3.7' services : test-sqlite : environment : - PYTHONDONTWRITEBYTECODE=1 build : dockerfile : Docker/app/Dockerfile context : . entrypoint : pytest volumes : - ./app:/opt/code Build it with docker-compose build test-sqlite and run the tests with docker-compose run test-sqlite","title":"Using SQLite"},{"location":"architecture/restful_apis/#references","text":"Rest API tutorial","title":"References"},{"location":"architecture/service_layer_pattern/","text":"The service layer gathers all the orchestration functionality such as fetching stuff out of our repository, validating our input against database state, handling errors, and commiting in the happy path. Most of these things don't have anything to do with the view layer (an API or a command line tool), so they're not really things that need to be tested by end-to-end tests. Unconnected thoughts \u00b6 By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case. References \u00b6 The service layer pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"Service Layer Pattern"},{"location":"architecture/service_layer_pattern/#unconnected-thoughts","text":"By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case.","title":"Unconnected thoughts"},{"location":"architecture/service_layer_pattern/#references","text":"The service layer pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"architecture/solid/","text":"SOLID is a mnemonic acronym for five design principles intended to make software designs more understandable, flexible and maintainable. Single-responsibility (SRP) \u00b6 Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility. As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times. Open-closed \u00b6 Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface. Liskov substitution (LSP) \u00b6 If S is a subtype of T , then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. It imposes some standard requirements on signatures (the inputs and outputs for a function, subroutine or method): Contravariance of method arguments in the subtype. Covariance of return types in the subtype. No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exception thrown by the methods of the supertype. Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance: Preconditions cannot be strengthened in a subtype. Postconditions cannot be weakened in a subtype. Invariants of the supertype must be preserved in a subtype. History constraint. Objects are regarded as being modifiable only through their methods. Because subtypes may introduce methods that are not present in the supertype, the introduction of these methods may allow state changes in the subtype that are not permissible in the supertype. This is not allowed. Fields added to the subtype may however be safely modified because they are not observable through the supertype methods. Interface segregation (ISP) \u00b6 No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class. Dependency inversion \u00b6 Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces). Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module. Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema. References \u00b6 SOLID Wikipedia article Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"SOLID"},{"location":"architecture/solid/#single-responsibilitysrp","text":"Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility. As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times.","title":"Single-responsibility(SRP)"},{"location":"architecture/solid/#open-closed","text":"Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface.","title":"Open-closed"},{"location":"architecture/solid/#liskov-substitutionlsp","text":"If S is a subtype of T , then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. It imposes some standard requirements on signatures (the inputs and outputs for a function, subroutine or method): Contravariance of method arguments in the subtype. Covariance of return types in the subtype. No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exception thrown by the methods of the supertype. Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance: Preconditions cannot be strengthened in a subtype. Postconditions cannot be weakened in a subtype. Invariants of the supertype must be preserved in a subtype. History constraint. Objects are regarded as being modifiable only through their methods. Because subtypes may introduce methods that are not present in the supertype, the introduction of these methods may allow state changes in the subtype that are not permissible in the supertype. This is not allowed. Fields added to the subtype may however be safely modified because they are not observable through the supertype methods.","title":"Liskov substitution(LSP)"},{"location":"architecture/solid/#interface-segregation-isp","text":"No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class.","title":"Interface segregation (ISP)"},{"location":"architecture/solid/#dependency-inversion","text":"Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces). Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module. Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema.","title":"Dependency inversion"},{"location":"architecture/solid/#references","text":"SOLID Wikipedia article Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"coding/tdd/","text":"Test-driven development (TDD) is a software development process that relies on the repetition of a very short development cycle: requirements are turned into very specific test cases, then the code is improved so that the tests pass. This is opposed to software development that allows code to be added that is not proven to meet requirements. Abstractions in testing \u00b6 Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy. To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code. Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around. Fakes vs Mocks \u00b6 Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way. Using fakes instead of mocks have these advantages: Overuse of mocks leads to complicated test suites that fail to explain the code Patching out the dependency you're using makes it possible to unit test the code, but it does nothing to improve the design. Faking makes you identify the responsibilities of your codebase, and to separate those responsibilities into small, focused objects that are easy to replace. Tests that use mocks tend to be more coupled to the implementation details of the codebase. That's because mock tests verify the interactions between things. This coupling between code and test tends to make tests more brittle. Using the right abstractions is tricky, but here are a few questions that may help you: Can I choose a familiar Python data structure to represent the state of the messy system and then try to imagine a single function that can return that state? Where can I draw a line between my systems, where can I carve out a seam to stick that abstraction in? What is a sensible way of dividing things into components with different responsibilities? What implicit concepts can I make explicit? What are the dependencies, and what is the core business logic? TDD in High Gear and Low Gear \u00b6 Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests. Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things. Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage. A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code. At the other end of the spectrum, tests in the domain model help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve. We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation. Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage. When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent. Note When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again. References \u00b6 Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Further reading \u00b6 Martin Fowler o Mocks aren't stubs","title":"TDD"},{"location":"coding/tdd/#abstractions-in-testing","text":"Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy. To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code. Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around.","title":"Abstractions in testing"},{"location":"coding/tdd/#fakes-vs-mocks","text":"Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way. Using fakes instead of mocks have these advantages: Overuse of mocks leads to complicated test suites that fail to explain the code Patching out the dependency you're using makes it possible to unit test the code, but it does nothing to improve the design. Faking makes you identify the responsibilities of your codebase, and to separate those responsibilities into small, focused objects that are easy to replace. Tests that use mocks tend to be more coupled to the implementation details of the codebase. That's because mock tests verify the interactions between things. This coupling between code and test tends to make tests more brittle. Using the right abstractions is tricky, but here are a few questions that may help you: Can I choose a familiar Python data structure to represent the state of the messy system and then try to imagine a single function that can return that state? Where can I draw a line between my systems, where can I carve out a seam to stick that abstraction in? What is a sensible way of dividing things into components with different responsibilities? What implicit concepts can I make explicit? What are the dependencies, and what is the core business logic?","title":"Fakes vs Mocks"},{"location":"coding/tdd/#tdd-in-high-gear-and-low-gear","text":"Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests. Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things. Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage. A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code. At the other end of the spectrum, tests in the domain model help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve. We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation. Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage. When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent. Note When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again.","title":"TDD in High Gear and Low Gear"},{"location":"coding/tdd/#references","text":"Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"coding/tdd/#further-reading","text":"Martin Fowler o Mocks aren't stubs","title":"Further reading"},{"location":"coding/javascript/javascript/","text":"JavaScript is a multi-paradigm, dynamic language with types and operators, standard built-in objects, and methods. Its syntax is based on the Java and C languages \u2014 many structures from those languages apply to JavaScript as well. JavaScript supports object-oriented programming with object prototypes, instead of classes. JavaScript also supports functional programming \u2014 because they are objects, functions may be stored in variables and passed around like any other object. The basics \u00b6 Javascript types \u00b6 JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined Numbers \u00b6 Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable. Convert a string to an integer \u00b6 Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10 Convert a string into a float \u00b6 Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10. Strings \u00b6 Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods . Find the length of a string \u00b6 'hello' . length ; // 5 Booleans \u00b6 JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: ! Variables \u00b6 New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here Operators \u00b6 Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== . Control structures \u00b6 If conditionals \u00b6 var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ; Switch cases \u00b6 switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional While loops \u00b6 while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input )); For loops \u00b6 It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } * for ... of . for ( let value of array ) { // do something with value } * for ... in . for ( let property in object ) { // do something with object property } Objects \u00b6 Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24. Arrays \u00b6 Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3 Iterate over the values of an array \u00b6 for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] } Append an item to an array \u00b6 Although push() could be used, is better to use concat() as it doesn't mutate the original array. a . concat ( item ); Apply a function to the elements of an array \u00b6 const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6] Functions \u00b6 function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop. Define variable number of arguments \u00b6 function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5 Custom objects \u00b6 JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one. Constructor functions \u00b6 We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\" Split code for readability \u00b6 To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); } Links \u00b6 Re-introduction to JavaScript","title":"Javascript"},{"location":"coding/javascript/javascript/#the-basics","text":"","title":"The basics"},{"location":"coding/javascript/javascript/#javascript-types","text":"JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined","title":"Javascript types"},{"location":"coding/javascript/javascript/#numbers","text":"Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable.","title":"Numbers"},{"location":"coding/javascript/javascript/#convert-a-string-to-an-integer","text":"Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10","title":"Convert a string to an integer"},{"location":"coding/javascript/javascript/#convert-a-string-into-a-float","text":"Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10.","title":"Convert a string into a float"},{"location":"coding/javascript/javascript/#strings","text":"Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods .","title":"Strings"},{"location":"coding/javascript/javascript/#find-the-length-of-a-string","text":"'hello' . length ; // 5","title":"Find the length of a string"},{"location":"coding/javascript/javascript/#booleans","text":"JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: !","title":"Booleans"},{"location":"coding/javascript/javascript/#variables","text":"New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here","title":"Variables"},{"location":"coding/javascript/javascript/#operators","text":"Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== .","title":"Operators"},{"location":"coding/javascript/javascript/#control-structures","text":"","title":"Control structures"},{"location":"coding/javascript/javascript/#if-conditionals","text":"var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ;","title":"If conditionals"},{"location":"coding/javascript/javascript/#switch-cases","text":"switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional","title":"Switch cases"},{"location":"coding/javascript/javascript/#while-loops","text":"while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input ));","title":"While loops"},{"location":"coding/javascript/javascript/#for-loops","text":"It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } * for ... of . for ( let value of array ) { // do something with value } * for ... in . for ( let property in object ) { // do something with object property }","title":"For loops"},{"location":"coding/javascript/javascript/#objects","text":"Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24.","title":"Objects"},{"location":"coding/javascript/javascript/#arrays","text":"Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3","title":"Arrays"},{"location":"coding/javascript/javascript/#iterate-over-the-values-of-an-array","text":"for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] }","title":"Iterate over the values of an array"},{"location":"coding/javascript/javascript/#append-an-item-to-an-array","text":"Although push() could be used, is better to use concat() as it doesn't mutate the original array. a . concat ( item );","title":"Append an item to an array"},{"location":"coding/javascript/javascript/#apply-a-function-to-the-elements-of-an-array","text":"const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6]","title":"Apply a function to the elements of an array"},{"location":"coding/javascript/javascript/#functions","text":"function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop.","title":"Functions"},{"location":"coding/javascript/javascript/#define-variable-number-of-arguments","text":"function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5","title":"Define variable number of arguments"},{"location":"coding/javascript/javascript/#custom-objects","text":"JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one.","title":"Custom objects"},{"location":"coding/javascript/javascript/#constructor-functions","text":"We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\"","title":"Constructor functions"},{"location":"coding/javascript/javascript/#split-code-for-readability","text":"To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); }","title":"Split code for readability"},{"location":"coding/javascript/javascript/#links","text":"Re-introduction to JavaScript","title":"Links"},{"location":"coding/json/json/","text":"JavaScript Object Notation (JSON) , is an open standard file format, and data interchange format, that uses human-readable text to store and send data objects consisting of attribute\u2013value pairs and array data types (or any other serializable value). Linters and fixers \u00b6 jsonlint \u00b6 jsonlint is a pure JavaScript version of the service provided at jsonlint.com. Install it with: npm install jsonlint -g Vim supports this linter through ALE . jq \u00b6 jq is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that sed , awk , grep and friends let you play with text. Install it with: apt-get install jq Vim supports this linter through ALE .","title":"JSON"},{"location":"coding/json/json/#linters-and-fixers","text":"","title":"Linters and fixers"},{"location":"coding/json/json/#jsonlint","text":"jsonlint is a pure JavaScript version of the service provided at jsonlint.com. Install it with: npm install jsonlint -g Vim supports this linter through ALE .","title":"jsonlint"},{"location":"coding/json/json/#jq","text":"jq is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that sed , awk , grep and friends let you play with text. Install it with: apt-get install jq Vim supports this linter through ALE .","title":"jq"},{"location":"coding/python/alembic/","text":"Alembic is a lightweight database migration tool for SQLAlchemy. It is created by the author of SQLAlchemy and it has become the de-facto standard tool to perform migrations on SQLAlchemy backed databases. Database Migration in SQLAlchemy \u00b6 A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema. Alembic repository initialization \u00b6 It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand. Database Migration \u00b6 When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current Seed database with data \u00b6 Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] ) Database downgrade or rollback \u00b6 If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file. References \u00b6 Git Docs Articles \u00b6 Migrate SQLAlchemy databases with Alembic","title":"Alembic"},{"location":"coding/python/alembic/#database-migration-in-sqlalchemy","text":"A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema.","title":"Database Migration in SQLAlchemy"},{"location":"coding/python/alembic/#alembic-repository-initialization","text":"It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand.","title":"Alembic repository initialization"},{"location":"coding/python/alembic/#database-migration","text":"When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current","title":"Database Migration"},{"location":"coding/python/alembic/#seed-database-with-data","text":"Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] )","title":"Seed database with data"},{"location":"coding/python/alembic/#database-downgrade-or-rollback","text":"If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file.","title":"Database downgrade or rollback"},{"location":"coding/python/alembic/#references","text":"Git Docs","title":"References"},{"location":"coding/python/alembic/#articles","text":"Migrate SQLAlchemy databases with Alembic","title":"Articles"},{"location":"coding/python/data_classes/","text":"A data class is a regular Python class that has basic data model methods like __init__() , __repr__() , and __eq__() implemented for you. Introduced in Python 3.7 , they typically containing mainly data, although there aren\u2019t really any restrictions. from dataclasses import dataclass @dataclass class DataClassCard : rank : str suit : str They behave similar to named tuples but come with many more features. At the same time, named tuples have some other features that are not necessarily desirable, such as: By design it's a regular tuple, which can lead to subtle and hard to find bugs. It's hard to add default values to some fields. It's by nature immutable. That being said, if you need your data structure to behave like a tuple, then a named tuple is a great alternative. Advantages over regular classes \u00b6 Simplify the class definition @dataclass class DataClassCard : rank : str suit : str # Versus class RegularCard def __init__ ( self , rank , suit ): self . rank = rank self . suit = suit More descriptive object representation through a better default __repr__() method. >>> queen_of_hearts = DataClassCard ( 'Q' , 'Hearts' ) >>> queen_of_hearts DataClassCard ( rank = 'Q' , suit = 'Hearts' ) # Versus >>> queen_of_spades = RegularCard ( 'Q' , 'Spades' ) >>> queen_of_spades < __main__ . RegularCard object at 0x7fb6eee35d30 > * Instance comparison out of the box through a better default __eq__() method. >>> queen_of_hearts == DataClassCard ( 'Q' , 'Hearts' ) True # Versus >>> queen_of_spades == RegularCard ( 'Q' , 'Spades' ) False Usage \u00b6 Definition \u00b6 from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float What makes this a data class is the @dataclass decorator. Beneath the class Position: , simply list the fields you want in your data class. The data class decorator support the following parameters : init : Add .__init__() method? (Default is True). repr : Add .__repr__() method? (Default is True). eq : Add .__eq__() method? (Default is True). order : Add ordering methods? (Default is False). unsafe_hash : Force the addition of a .__hash__() method? (Default is False). frozen : If True , assigning to fields raise an exception. (Default is False). Default values \u00b6 It's easy to add default values to the fields of your data class: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck: from dataclasses import dataclass , field from typing import List RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () def make_french_deck (): return [ PlayingCard ( r , s ) for s in SUITS for r in RANKS ] @dataclass class PlayingCard : rank : str suit : str @dataclass class Deck : cards : List [ PlayingCard ] = field ( default_factory = make_french_deck ) Using cards: List[PlayingCard] = make_french_deck() introduces the using mutable default arguments anti-pattern. Instead, data classes use the default_factory to handle mutable default values. To use it, you need to use the field() specifier which is used to customize each field of a data class individually. It supports the following parameters: default : Default value of the field. default_factory : Function that returns the initial value of the field. init : Use field in .__init__() method? (Default is True ). repr : Use field in repr of the object? (Default is True ). For example to hide a parameter from the repr , use lat: float = field(default=0.0, repr=False) . compare : Include the field in comparisons? (Default is True ). hash : Include the field when calculating hash() ? (Default is to use the same as compare ). metadata : A mapping with information about the field. It's not used by the data classes themselves but is available for you to attach information to fields. For example: from dataclasses import dataclass , field @dataclass class Position : name : str lon : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) lat : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) To retrieve the information use the fields() function. >>> from dataclasses import fields >>> fields ( Position ) ( Field ( name = 'name' , type =< class ' str '>,...,metadata= {} ), Field ( name = 'lon' , type =< class ' float '>,...,metadata={' unit ': ' degrees '}), Field ( name = 'lat' , type =< class ' float '>,...,metadata={' unit ': ' degrees '})) >>> lat_unit = fields ( Position )[ 2 ] . metadata [ 'unit' ] >>> lat_unit 'degrees' Type hints \u00b6 They support typing out of the box. Without a type hint, the field will not be a part of the data class. While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language . Adding methods \u00b6 Same as with a normal class. Adding complex order comparison logic \u00b6 from dataclasses import dataclass @dataclass ( order = True ) class PlayingCard : rank : str suit : str def __str__ ( self ): return f ' { self . suit }{ self . rank } ' After setting order=True in the decorator definition the instances of PlayingCard can be compared. >>> queen_of_hearts = PlayingCard ( 'Q' , '\u2661' ) >>> ace_of_spades = PlayingCard ( 'A' , '\u2660' ) >>> ace_of_spades > queen_of_hearts False Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because Q comes after A in the alphabet. >>> ( 'A' , '\u2660' ) > ( 'Q' , '\u2661' ) False To use more complex comparisons, we need to add the field .sort_index to the class. However, this field should be calculated from the other fields automatically. That's what the special method .__post_init__() is for. It allows for special processing after the regular .__init__() method is called. from dataclasses import dataclass , field RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () @dataclass ( order = True ) class PlayingCard : sort_index : int = field ( init = False , repr = False ) rank : str suit : str def __post_init__ ( self ): self . sort_index = ( RANKS . index ( self . rank ) * len ( SUITS ) + SUITS . index ( self . suit )) def __str__ ( self ): return f ' { self . suit }{ self . rank } ' Note that .sort_index is added as the first field of the class. That way, the comparison is first done using .sort_index and only if there are ties are the other fields used. Using field() , you must also specify that .sort_index should not be included as a parameter in the .__init__() method (because it is calculated from the .rank and .suit fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the repr of the class. Immutable data classes \u00b6 To make a data class immutable, set frozen=True when you create it. from dataclasses import dataclass @dataclass ( frozen = True ) class Position : name : str lon : float = 0.0 lat : float = 0.0 In a frozen data class, you can not assign values to the fields after creation: >>> pos = Position ( 'Oslo' , 10.8 , 59.9 ) >>> pos . name 'Oslo' >>> pos . name = 'Stockholm' dataclasses . FrozenInstanceError : cannot assign to field 'name' Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python: from dataclasses import dataclass from typing import List @dataclass ( frozen = True ) class ImmutableCard : rank : str suit : str @dataclass ( frozen = True ) class ImmutableDeck : cards : List [ PlayingCard ] Even though both ImmutableCard and ImmutableDeck are immutable, the list holding cards is not. You can therefore still change the cards in the deck: >>> queen_of_hearts = ImmutableCard ( 'Q' , '\u2661' ) >>> ace_of_spades = ImmutableCard ( 'A' , '\u2660' ) >>> deck = ImmutableDeck ([ queen_of_hearts , ace_of_spades ]) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = 'Q' , suit = '\u2661' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) >>> deck . cards [ 0 ] = ImmutableCard ( '7' , '\u2662' ) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = '7' , suit = '\u2662' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The ImmutableDeck should be implemented using a tuple instead of a list. Inheritance \u00b6 You can subclass data classes quite freely. from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float @dataclass class Capital ( Position ): country : str >>> Capital ( 'Oslo' , 10.8 , 59.9 , 'Norway' ) Capital ( name = 'Oslo' , lon = 10.8 , lat = 59.9 , country = 'Norway' ) Warning This won't work if the base class have default values unless all the subclass parameters also have default values. Warning If you redefine a base class field, you need to keep the fields order after the subclass new fields: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 @dataclass class Capital ( Position ): country : str = 'Unknown' lat : float = 40.0 Optimizing Data Classes \u00b6 Slots can be used to make classes faster and use less memory. from dataclasses import dataclass @dataclass class SimplePosition : name : str lon : float lat : float @dataclass class SlotPosition : __slots__ = [ 'name' , 'lon' , 'lat' ] name : str lon : float lat : float Essentially, slots are defined using .__slots__ to list the variables on a class. Variables or attributes not present in .__slots__ may not be defined. Furthermore, a slots class may not have default values . References \u00b6 Real Python Data classes article","title":"Data Classes"},{"location":"coding/python/data_classes/#advantages-over-regular-classes","text":"Simplify the class definition @dataclass class DataClassCard : rank : str suit : str # Versus class RegularCard def __init__ ( self , rank , suit ): self . rank = rank self . suit = suit More descriptive object representation through a better default __repr__() method. >>> queen_of_hearts = DataClassCard ( 'Q' , 'Hearts' ) >>> queen_of_hearts DataClassCard ( rank = 'Q' , suit = 'Hearts' ) # Versus >>> queen_of_spades = RegularCard ( 'Q' , 'Spades' ) >>> queen_of_spades < __main__ . RegularCard object at 0x7fb6eee35d30 > * Instance comparison out of the box through a better default __eq__() method. >>> queen_of_hearts == DataClassCard ( 'Q' , 'Hearts' ) True # Versus >>> queen_of_spades == RegularCard ( 'Q' , 'Spades' ) False","title":"Advantages over regular classes"},{"location":"coding/python/data_classes/#usage","text":"","title":"Usage"},{"location":"coding/python/data_classes/#definition","text":"from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float What makes this a data class is the @dataclass decorator. Beneath the class Position: , simply list the fields you want in your data class. The data class decorator support the following parameters : init : Add .__init__() method? (Default is True). repr : Add .__repr__() method? (Default is True). eq : Add .__eq__() method? (Default is True). order : Add ordering methods? (Default is False). unsafe_hash : Force the addition of a .__hash__() method? (Default is False). frozen : If True , assigning to fields raise an exception. (Default is False).","title":"Definition"},{"location":"coding/python/data_classes/#default-values","text":"It's easy to add default values to the fields of your data class: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck: from dataclasses import dataclass , field from typing import List RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () def make_french_deck (): return [ PlayingCard ( r , s ) for s in SUITS for r in RANKS ] @dataclass class PlayingCard : rank : str suit : str @dataclass class Deck : cards : List [ PlayingCard ] = field ( default_factory = make_french_deck ) Using cards: List[PlayingCard] = make_french_deck() introduces the using mutable default arguments anti-pattern. Instead, data classes use the default_factory to handle mutable default values. To use it, you need to use the field() specifier which is used to customize each field of a data class individually. It supports the following parameters: default : Default value of the field. default_factory : Function that returns the initial value of the field. init : Use field in .__init__() method? (Default is True ). repr : Use field in repr of the object? (Default is True ). For example to hide a parameter from the repr , use lat: float = field(default=0.0, repr=False) . compare : Include the field in comparisons? (Default is True ). hash : Include the field when calculating hash() ? (Default is to use the same as compare ). metadata : A mapping with information about the field. It's not used by the data classes themselves but is available for you to attach information to fields. For example: from dataclasses import dataclass , field @dataclass class Position : name : str lon : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) lat : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) To retrieve the information use the fields() function. >>> from dataclasses import fields >>> fields ( Position ) ( Field ( name = 'name' , type =< class ' str '>,...,metadata= {} ), Field ( name = 'lon' , type =< class ' float '>,...,metadata={' unit ': ' degrees '}), Field ( name = 'lat' , type =< class ' float '>,...,metadata={' unit ': ' degrees '})) >>> lat_unit = fields ( Position )[ 2 ] . metadata [ 'unit' ] >>> lat_unit 'degrees'","title":"Default values"},{"location":"coding/python/data_classes/#type-hints","text":"They support typing out of the box. Without a type hint, the field will not be a part of the data class. While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language .","title":"Type hints"},{"location":"coding/python/data_classes/#adding-methods","text":"Same as with a normal class.","title":"Adding methods"},{"location":"coding/python/data_classes/#adding-complex-order-comparison-logic","text":"from dataclasses import dataclass @dataclass ( order = True ) class PlayingCard : rank : str suit : str def __str__ ( self ): return f ' { self . suit }{ self . rank } ' After setting order=True in the decorator definition the instances of PlayingCard can be compared. >>> queen_of_hearts = PlayingCard ( 'Q' , '\u2661' ) >>> ace_of_spades = PlayingCard ( 'A' , '\u2660' ) >>> ace_of_spades > queen_of_hearts False Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because Q comes after A in the alphabet. >>> ( 'A' , '\u2660' ) > ( 'Q' , '\u2661' ) False To use more complex comparisons, we need to add the field .sort_index to the class. However, this field should be calculated from the other fields automatically. That's what the special method .__post_init__() is for. It allows for special processing after the regular .__init__() method is called. from dataclasses import dataclass , field RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () @dataclass ( order = True ) class PlayingCard : sort_index : int = field ( init = False , repr = False ) rank : str suit : str def __post_init__ ( self ): self . sort_index = ( RANKS . index ( self . rank ) * len ( SUITS ) + SUITS . index ( self . suit )) def __str__ ( self ): return f ' { self . suit }{ self . rank } ' Note that .sort_index is added as the first field of the class. That way, the comparison is first done using .sort_index and only if there are ties are the other fields used. Using field() , you must also specify that .sort_index should not be included as a parameter in the .__init__() method (because it is calculated from the .rank and .suit fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the repr of the class.","title":"Adding complex order comparison logic"},{"location":"coding/python/data_classes/#immutable-data-classes","text":"To make a data class immutable, set frozen=True when you create it. from dataclasses import dataclass @dataclass ( frozen = True ) class Position : name : str lon : float = 0.0 lat : float = 0.0 In a frozen data class, you can not assign values to the fields after creation: >>> pos = Position ( 'Oslo' , 10.8 , 59.9 ) >>> pos . name 'Oslo' >>> pos . name = 'Stockholm' dataclasses . FrozenInstanceError : cannot assign to field 'name' Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python: from dataclasses import dataclass from typing import List @dataclass ( frozen = True ) class ImmutableCard : rank : str suit : str @dataclass ( frozen = True ) class ImmutableDeck : cards : List [ PlayingCard ] Even though both ImmutableCard and ImmutableDeck are immutable, the list holding cards is not. You can therefore still change the cards in the deck: >>> queen_of_hearts = ImmutableCard ( 'Q' , '\u2661' ) >>> ace_of_spades = ImmutableCard ( 'A' , '\u2660' ) >>> deck = ImmutableDeck ([ queen_of_hearts , ace_of_spades ]) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = 'Q' , suit = '\u2661' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) >>> deck . cards [ 0 ] = ImmutableCard ( '7' , '\u2662' ) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = '7' , suit = '\u2662' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The ImmutableDeck should be implemented using a tuple instead of a list.","title":"Immutable data classes"},{"location":"coding/python/data_classes/#inheritance","text":"You can subclass data classes quite freely. from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float @dataclass class Capital ( Position ): country : str >>> Capital ( 'Oslo' , 10.8 , 59.9 , 'Norway' ) Capital ( name = 'Oslo' , lon = 10.8 , lat = 59.9 , country = 'Norway' ) Warning This won't work if the base class have default values unless all the subclass parameters also have default values. Warning If you redefine a base class field, you need to keep the fields order after the subclass new fields: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 @dataclass class Capital ( Position ): country : str = 'Unknown' lat : float = 40.0","title":"Inheritance"},{"location":"coding/python/data_classes/#optimizing-data-classes","text":"Slots can be used to make classes faster and use less memory. from dataclasses import dataclass @dataclass class SimplePosition : name : str lon : float lat : float @dataclass class SlotPosition : __slots__ = [ 'name' , 'lon' , 'lat' ] name : str lon : float lat : float Essentially, slots are defined using .__slots__ to list the variables on a class. Variables or attributes not present in .__slots__ may not be defined. Furthermore, a slots class may not have default values .","title":"Optimizing Data Classes"},{"location":"coding/python/data_classes/#references","text":"Real Python Data classes article","title":"References"},{"location":"coding/python/factoryboy/","text":"Factoryboy is a fixtures replacement library to generate fake data for your program. As it's designed to work well with different ORMs (Django, SQLAlchemy , Mongo) it serves the purpose of building real objects for your tests. Install \u00b6 pip install factory_boy Or add it to the project requirements.txt . Define a factory class \u00b6 Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py . Use the factory class \u00b6 Create an instance. UserFactory . create () Create an instance with a defined attribute. UserFactory . create ( name = 'John' ) Create 100 instances of objects with an attribute defined. UserFactory . create_batch ( 100 , name = 'John' ) Define attributes \u00b6 I like to use the faker integration of factory boy to generate most of the attributes. Generate numbers \u00b6 Sequential numbers \u00b6 Ideal for IDs id = factory . Sequence ( lambda n : n ) Random number or integer \u00b6 author_id = factory . Faker ( 'random_number' ) Random float \u00b6 score = factory . Faker ( 'pyfloat' ) Generate strings \u00b6 Word \u00b6 default = factory . Faker ( 'word' ) Word from a list \u00b6 user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ]) Sentences \u00b6 description = factory . Faker ( 'sentence' ) Names \u00b6 name = factory . Faker ( 'name' ) Urls \u00b6 url = factory . Faker ( 'url' ) Files \u00b6 file_path = factory . Faker ( 'file_path' ) Generate Datetime \u00b6 factory . Faker ( 'date_time' ) Generate bool \u00b6 factory . Faker ( 'pybool' ) Generate your own attribute \u00b6 Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({}) Define relationships \u00b6 Factory Inheritance \u00b6 class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit' Dependent objects direct ForeignKey \u00b6 When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' References \u00b6 Docs Git Common recipes","title":"FactoryBoy"},{"location":"coding/python/factoryboy/#install","text":"pip install factory_boy Or add it to the project requirements.txt .","title":"Install"},{"location":"coding/python/factoryboy/#define-a-factory-class","text":"Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py .","title":"Define a factory class"},{"location":"coding/python/factoryboy/#use-the-factory-class","text":"Create an instance. UserFactory . create () Create an instance with a defined attribute. UserFactory . create ( name = 'John' ) Create 100 instances of objects with an attribute defined. UserFactory . create_batch ( 100 , name = 'John' )","title":"Use the factory class"},{"location":"coding/python/factoryboy/#define-attributes","text":"I like to use the faker integration of factory boy to generate most of the attributes.","title":"Define attributes"},{"location":"coding/python/factoryboy/#generate-numbers","text":"","title":"Generate numbers"},{"location":"coding/python/factoryboy/#sequential-numbers","text":"Ideal for IDs id = factory . Sequence ( lambda n : n )","title":"Sequential numbers"},{"location":"coding/python/factoryboy/#random-number-or-integer","text":"author_id = factory . Faker ( 'random_number' )","title":"Random number or integer"},{"location":"coding/python/factoryboy/#random-float","text":"score = factory . Faker ( 'pyfloat' )","title":"Random float"},{"location":"coding/python/factoryboy/#generate-strings","text":"","title":"Generate strings"},{"location":"coding/python/factoryboy/#word","text":"default = factory . Faker ( 'word' )","title":"Word"},{"location":"coding/python/factoryboy/#word-from-a-list","text":"user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ])","title":"Word from a list"},{"location":"coding/python/factoryboy/#sentences","text":"description = factory . Faker ( 'sentence' )","title":"Sentences"},{"location":"coding/python/factoryboy/#names","text":"name = factory . Faker ( 'name' )","title":"Names"},{"location":"coding/python/factoryboy/#urls","text":"url = factory . Faker ( 'url' )","title":"Urls"},{"location":"coding/python/factoryboy/#files","text":"file_path = factory . Faker ( 'file_path' )","title":"Files"},{"location":"coding/python/factoryboy/#generate-datetime","text":"factory . Faker ( 'date_time' )","title":"Generate Datetime"},{"location":"coding/python/factoryboy/#generate-bool","text":"factory . Faker ( 'pybool' )","title":"Generate bool"},{"location":"coding/python/factoryboy/#generate-your-own-attribute","text":"Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({})","title":"Generate your own attribute"},{"location":"coding/python/factoryboy/#define-relationships","text":"","title":"Define relationships"},{"location":"coding/python/factoryboy/#factory-inheritance","text":"class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit'","title":"Factory Inheritance"},{"location":"coding/python/factoryboy/#dependent-objects-direct-foreignkey","text":"When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit'","title":"Dependent objects direct ForeignKey"},{"location":"coding/python/factoryboy/#references","text":"Docs Git Common recipes","title":"References"},{"location":"coding/python/faker/","text":"Faker is a Python package that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you. Install \u00b6 If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt . Use \u00b6 Faker includes a faker fixture for pytest. def test_faker ( faker ): assert isinstance ( faker . name (), str ) Generate fake number \u00b6 fake . random_number () If you want to specify max and min values use: faker . pyint ( min_value = 0 , max_value = 99 ) Generate a fake dictionary \u00b6 fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list' Generate a fake date \u00b6 fake . date_time () Generate a random string \u00b6 faker . pystr () References \u00b6 Git Docs Faker python providers","title":"Faker"},{"location":"coding/python/faker/#install","text":"If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt .","title":"Install"},{"location":"coding/python/faker/#use","text":"Faker includes a faker fixture for pytest. def test_faker ( faker ): assert isinstance ( faker . name (), str )","title":"Use"},{"location":"coding/python/faker/#generate-fake-number","text":"fake . random_number () If you want to specify max and min values use: faker . pyint ( min_value = 0 , max_value = 99 )","title":"Generate fake number"},{"location":"coding/python/faker/#generate-a-fake-dictionary","text":"fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list'","title":"Generate a fake dictionary"},{"location":"coding/python/faker/#generate-a-fake-date","text":"fake . date_time ()","title":"Generate a fake date"},{"location":"coding/python/faker/#generate-a-random-string","text":"faker . pystr ()","title":"Generate a random string"},{"location":"coding/python/faker/#references","text":"Git Docs Faker python providers","title":"References"},{"location":"coding/python/feedparser/","text":"Parse Atom and RSS feeds in Python. Install \u00b6 pip install feedparser Basic Usage \u00b6 Parse a feed from a remote URL \u00b6 >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed' Access common elements \u00b6 The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID. Channel elements \u00b6 >>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ])) Item elements \u00b6 >>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60 Advanced usage \u00b6 It is possible to interact with feeds that are protected with credentials . Links \u00b6 Git Docs","title":"Feedparser"},{"location":"coding/python/feedparser/#install","text":"pip install feedparser","title":"Install"},{"location":"coding/python/feedparser/#basic-usage","text":"","title":"Basic Usage"},{"location":"coding/python/feedparser/#parse-a-feed-from-a-remote-url","text":">>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed'","title":"Parse a feed from a remote URL"},{"location":"coding/python/feedparser/#access-common-elements","text":"The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID.","title":"Access common elements"},{"location":"coding/python/feedparser/#channel-elements","text":">>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ]))","title":"Channel elements"},{"location":"coding/python/feedparser/#item-elements","text":">>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60","title":"Item elements"},{"location":"coding/python/feedparser/#advanced-usage","text":"It is possible to interact with feeds that are protected with credentials .","title":"Advanced usage"},{"location":"coding/python/feedparser/#links","text":"Git Docs","title":"Links"},{"location":"coding/python/flask/","text":"Flask is a micro web framework written in Python. It has no database abstraction layer, form validation, or any other components where pre-existing third-party libraries provide common functions. However, Flask supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies and several common framework related tools. Extensions are updated far more frequently than the core Flask program. Install \u00b6 pip install flask How flask blueprints work \u00b6 A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint. Making a Flask Blueprint \u00b6 References \u00b6 Docs Tutorials \u00b6 Miguel's Flask Mega-Tutorial Patrick's Software Flask Tutorial Blueprints \u00b6 Flask docs on blueprints Explore flask article on Blueprints","title":"Flask"},{"location":"coding/python/flask/#install","text":"pip install flask","title":"Install"},{"location":"coding/python/flask/#how-flask-blueprints-work","text":"A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint.","title":"How flask blueprints work"},{"location":"coding/python/flask/#making-a-flask-blueprint","text":"","title":"Making a Flask Blueprint"},{"location":"coding/python/flask/#references","text":"Docs","title":"References"},{"location":"coding/python/flask/#tutorials","text":"Miguel's Flask Mega-Tutorial Patrick's Software Flask Tutorial","title":"Tutorials"},{"location":"coding/python/flask/#blueprints","text":"Flask docs on blueprints Explore flask article on Blueprints","title":"Blueprints"},{"location":"coding/python/folium/","text":"folium makes it easy to visualize data that\u2019s been manipulated in Python on an interactive leaflet map. It enables both the binding of data to a map for choropleth visualizations as well as passing rich vector/raster/HTML visualizations as markers on the map. The library has a number of built-in tilesets from OpenStreetMap, Mapbox, and Stamen, and supports custom tilesets with Mapbox or Cloudmade API keys. folium supports both Image, Video, GeoJSON and TopoJSON overlays. References \u00b6 Git Docs Quickstart Examples , more examples Use examples \u00b6 Flask example Build Interactive GPS activity maps from GPX files Use Open Data to build interactive maps Search examples \u00b6 Official docs Leafleft search control Leafleft search examples ( source code ) Searching in OSM data","title":"Folium"},{"location":"coding/python/folium/#references","text":"Git Docs Quickstart Examples , more examples","title":"References"},{"location":"coding/python/folium/#use-examples","text":"Flask example Build Interactive GPS activity maps from GPX files Use Open Data to build interactive maps","title":"Use examples"},{"location":"coding/python/folium/#search-examples","text":"Official docs Leafleft search control Leafleft search examples ( source code ) Searching in OSM data","title":"Search examples"},{"location":"coding/python/pandas/","text":"Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Install \u00b6 pip3 install pandas Import import pandas as pd Snippets \u00b6 Load csv \u00b6 data = pd . read_csv ( \"filename.csv\" ) If you want to parse the dates of the start column give read_csv the argument parse_dates=['start'] . Do operation on column data and save it in other column \u00b6 # make a simple dataframe df = pd . DataFrame ({ 'a' :[ 1 , 2 ], 'b' :[ 3 , 4 ]}) df # a b # 0 1 3 # 1 2 4 # create an unattached column with an index df . apply ( lambda row : row . a + row . b , axis = 1 ) # 0 4 # 1 6 # do same but attach it to the dataframe df [ 'c' ] = df . apply ( lambda row : row . a + row . b , axis = 1 ) df # a b c # 0 1 3 4 # 1 2 4 6 Get unique values of column \u00b6 If we want to get the unique values of the name column: df . name . unique () Extract columns of dataframe \u00b6 df1 = df [[ 'a' , 'b' ]] Remove dumplicate rows \u00b6 df = df . drop_duplicates () Remove column from dataframe \u00b6 del df [ 'name' ] Count unique combinations of values in selected columns \u00b6 df1 . groupby ([ 'A' , 'B' ]) . size () . reset_index () . rename ( columns = { 0 : 'count' }) A B count 0 no no 1 1 no yes 2 2 yes no 4 3 yes yes 3 Get row that contains the maximum value of a column \u00b6 df . loc [ df [ 'Value' ] . idxmax ()] References \u00b6 Homepage","title":"Pandas"},{"location":"coding/python/pandas/#install","text":"pip3 install pandas Import import pandas as pd","title":"Install"},{"location":"coding/python/pandas/#snippets","text":"","title":"Snippets"},{"location":"coding/python/pandas/#load-csv","text":"data = pd . read_csv ( \"filename.csv\" ) If you want to parse the dates of the start column give read_csv the argument parse_dates=['start'] .","title":"Load csv"},{"location":"coding/python/pandas/#do-operation-on-column-data-and-save-it-in-other-column","text":"# make a simple dataframe df = pd . DataFrame ({ 'a' :[ 1 , 2 ], 'b' :[ 3 , 4 ]}) df # a b # 0 1 3 # 1 2 4 # create an unattached column with an index df . apply ( lambda row : row . a + row . b , axis = 1 ) # 0 4 # 1 6 # do same but attach it to the dataframe df [ 'c' ] = df . apply ( lambda row : row . a + row . b , axis = 1 ) df # a b c # 0 1 3 4 # 1 2 4 6","title":"Do operation on column data and save it in other column"},{"location":"coding/python/pandas/#get-unique-values-of-column","text":"If we want to get the unique values of the name column: df . name . unique ()","title":"Get unique values of column"},{"location":"coding/python/pandas/#extract-columns-of-dataframe","text":"df1 = df [[ 'a' , 'b' ]]","title":"Extract columns of dataframe"},{"location":"coding/python/pandas/#remove-dumplicate-rows","text":"df = df . drop_duplicates ()","title":"Remove dumplicate rows"},{"location":"coding/python/pandas/#remove-column-from-dataframe","text":"del df [ 'name' ]","title":"Remove column from dataframe"},{"location":"coding/python/pandas/#count-unique-combinations-of-values-in-selected-columns","text":"df1 . groupby ([ 'A' , 'B' ]) . size () . reset_index () . rename ( columns = { 0 : 'count' }) A B count 0 no no 1 1 no yes 2 2 yes no 4 3 yes yes 3","title":"Count unique combinations of values in selected columns"},{"location":"coding/python/pandas/#get-row-that-contains-the-maximum-value-of-a-column","text":"df . loc [ df [ 'Value' ] . idxmax ()]","title":"Get row that contains the maximum value of a column"},{"location":"coding/python/pandas/#references","text":"Homepage","title":"References"},{"location":"coding/python/plotly/","text":"Plotly is a Python graphing library that makes interactive, publication-quality graphs. Install \u00b6 pip3 install plotly Import import plotly.graph_objects as go Snippets \u00b6 Select graph source using dropdown \u00b6 # imports import plotly.graph_objects as go import numpy as np # data x = list ( np . linspace ( - np . pi , np . pi , 100 )) values_1 = list ( np . sin ( x )) values_1b = [ elem *- 1 for elem in values_1 ] values_2 = list ( np . tan ( x )) values_2b = [ elem *- 1 for elem in values_2 ] # plotly setup] fig = go . Figure () # Add one ore more traces fig . add_traces ( go . Scatter ( x = x , y = values_1 )) fig . add_traces ( go . Scatter ( x = x , y = values_1b )) # construct menus updatemenus = [{ 'buttons' : [{ 'method' : 'update' , 'label' : 'Val 1' , 'args' : [{ 'y' : [ values_1 , values_1b ]},] }, { 'method' : 'update' , 'label' : 'Val 2' , 'args' : [{ 'y' : [ values_2 , values_2b ]},]}], 'direction' : 'down' , 'showactive' : True ,}] # update layout with buttons, and show the figure fig . update_layout ( updatemenus = updatemenus ) fig . show () References \u00b6 Homepage Git","title":"Plotly"},{"location":"coding/python/plotly/#install","text":"pip3 install plotly Import import plotly.graph_objects as go","title":"Install"},{"location":"coding/python/plotly/#snippets","text":"","title":"Snippets"},{"location":"coding/python/plotly/#select-graph-source-using-dropdown","text":"# imports import plotly.graph_objects as go import numpy as np # data x = list ( np . linspace ( - np . pi , np . pi , 100 )) values_1 = list ( np . sin ( x )) values_1b = [ elem *- 1 for elem in values_1 ] values_2 = list ( np . tan ( x )) values_2b = [ elem *- 1 for elem in values_2 ] # plotly setup] fig = go . Figure () # Add one ore more traces fig . add_traces ( go . Scatter ( x = x , y = values_1 )) fig . add_traces ( go . Scatter ( x = x , y = values_1b )) # construct menus updatemenus = [{ 'buttons' : [{ 'method' : 'update' , 'label' : 'Val 1' , 'args' : [{ 'y' : [ values_1 , values_1b ]},] }, { 'method' : 'update' , 'label' : 'Val 2' , 'args' : [{ 'y' : [ values_2 , values_2b ]},]}], 'direction' : 'down' , 'showactive' : True ,}] # update layout with buttons, and show the figure fig . update_layout ( updatemenus = updatemenus ) fig . show ()","title":"Select graph source using dropdown"},{"location":"coding/python/plotly/#references","text":"Homepage Git","title":"References"},{"location":"coding/python/pytest/","text":"pytest is a Python framework to makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. Pytest stands out over other test frameworks in: Simple tests are simple to write in pytest. Complex tests are still simple to write. Tests are easy to read. You can get started in seconds. You use assert to fail a test, not things like self.assertEqual() or self.assertLessThan() . Just assert . You can use pytest to run tests written for unittest or nose. Install \u00b6 pip install pytest Usage \u00b6 Run in the project directory. pytest If you need more information run it with -v . Pytest automatically finds which tests to run in a phase called test discovery . It will get the tests that match one of the following conditions: Test files that are named test_{{ something }}.py or {{ something }}_test.py . Test methods and functions named test_{{ something }} . Test classes named Test{{ Something }} . There are several possible outcomes of a test function: PASSED (.) : The test ran successfully. FAILED (F) : The test did not run usccessfully (or XPASS + strict). SKIPPED (s) : The test was skipped. You can tell pytest to skip a test by using enter the @pytest.mark.skip() or pytest.mark.skipif() decorators. xfail (x) : The test was not supposed to pass, ran, and failed. You can tell pytest that a test is expected to fail by using the @pytest.mark.xfail() decorator. XPASS (X) : The tests was not supposed to pass, ran, and passed. ERROR (E) : An exception happened outside of the test function, in either a fixture or a hook function. Pytest supports several cool flags like: -k EXPRESSION : Used to select a subset of tests to run. For example pytest -k \"asdict or defaults\" will run both test_asdict() and test_defaults() . --lf or --last-failed : Just run the tests that have failed in the previous run. -x , or --exitfirst : Exit on first failed test. -l or --showlocals : Print out the local variables in a test if the test fails. -s Allows any output that normally would be printed to stdout to actually be printed to stdout . It's an alias of --capture=no , so the output is not captured when the tests are run, which is the default behavior. This is useful to debug with print() statements. --durations=N : It reports the slowest N number of tests/setups/teardowns after the test run. If you pass in --durations=0 , it reports everything in order of slowest to fastest. Parametrized testing \u00b6 Parametrized testing is a way to send multiple sets of data through the same test and have pytest report if any of the sets failed. File tests/unit/test_func.py tasks_to_try = ( Task ( 'sleep' , done = True ), Task ( 'wake' , 'brian' ), Task ( 'wake' , 'brian' ), Task ( 'breathe' , 'BRIAN' , True ), Task ( 'exercise' , 'BrIaN' , False ), ) task_ids = [ f 'Task( { task . summary } , { task . owner } , { task . done } )' for task in tasks_to_try ] @pytest . mark . parametrize ( 'task' , tasks_to_try , ids = task_ids ) def test_add_4 ( task ): task_id = tasks . add ( task ) t_from_db = tasks . get ( task_id ) assert equivalent ( t_from_db , task ) $ pytest -v test_func.py::test_add_4 ===================== test session starts ====================== collected 5 items test_add_variety.py::test_add_4 [ Task ( sleep,None,True )] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 0 ] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 1 ] PASSED test_add_variety.py::test_add_4 [ Task ( breathe,BRIAN,True )] PASSED test_add_variety.py::test_add_4 [ Task ( exercise,BrIaN,False )] PASSED =================== 5 passed in 0 .04 seconds =================== Those identifiers can be used to run that specific test. For example pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\" . parametrize() can be applied to classes as well. If the test id can't be derived from the parameter value, use the id argument for the pytest.param : @pytest . mark . parametrize ( 'task' , [ pytest . param ( Task ( 'create' ), id = 'just summary' ), pytest . param ( Task ( 'inspire' , 'Michelle' ), id = 'summary/owner' ), ]) def test_add_6 ( task ): ... Will yield: $ pytest-v test_add_variety.py::test_add_6 =================== test session starts ==================== collected 2 items test_add_variety.py::test_add_6 [ justsummary ] PASSED test_add_variety.py::test_add_6 [ summary/owner ] PASSED ================= 2 passed in 0 .05 seconds ================= Parametrize fixture arguments \u00b6 Sometimes is interesting to parametrize the arguments of a fixture. For example, imagine that we've got two fixtures to generate test data, then when the test is run, it must call the correct fixture for each case. @pytest . fixture def insert_task (): task = factories . TaskFactory . create () session . execute ( 'INSERT INTO task (id, description, state, type)' 'VALUES (' f '\" { task . id } \", \" { task . description } \", \" { task . state } \", \"task\"' ')' ) return task @pytest . fixture def insert_project ( session ): project = factories . ProjectFactory . create () session . execute ( 'INSERT INTO project (id, description)' f 'VALUES (\" { project . id } \", \" { project . description } \")' ) return project We can create a fixture that uses both and returns the correct one based on an argument that is given. @pytest . fixture def insert_object ( request , insert_task , insert_project ): if request . param == 'insert_task' : return insert_task elif request . param == 'insert_project' : return insert_project @pytest . mark . parametrize ( 'table,insert_object' , [ ( 'task' , 'insert_task' ), ( 'project' , 'insert_project' ), ] models_to_try , indirect = [ 'insert_object' ], ) def test_repository_can_retrieve_an_object ( self , session , table , insert_object , ): expected_obj = insert_object repo = repository . SqlAlchemyRepository ( session ) retrieved_obj = repo . get ( table , expected_obj . id ) assert retrieved_obj == expected_obj # Task.__eq__ only compares reference assert retrieved_obj . description == expected_obj . description Snippets \u00b6 Mocking sys.exit \u00b6 with pytest . raises ( SystemExit ): # Code to test pytest integration with Vim \u00b6 Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD. I use Janko-m's Vim-test plugin (which can be installed through Vundle ) with the following configuration. nmap < silent > t :TestNearest < CR > nmap < silent > < leader > t :TestSuite tests/unit < CR > nmap < silent > < leader > i :TestSuite tests/integration < CR > nmap < silent > < leader > T :TestFile < CR > let test#python#runner = 'pytest' let test#strategy = \"neovim\" I often open Vim with a vertical split ( :vs ), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press t when the cursor is inside that test. If you need to make changes in the code, you can press t again while the cursor is at the code you are testing and it will run the last test. Once the unit test has passed, I run the whole unit tests with ;t (as ; is my <leader> ). And finally I use ;i to run the integration tests. Finally, if the test suite is huge, I use ;T to run only the tests of a single file. Reference \u00b6 Book Python Testing with pytest by Brian Okken . Docs Vim-test plugin","title":"Pytest"},{"location":"coding/python/pytest/#install","text":"pip install pytest","title":"Install"},{"location":"coding/python/pytest/#usage","text":"Run in the project directory. pytest If you need more information run it with -v . Pytest automatically finds which tests to run in a phase called test discovery . It will get the tests that match one of the following conditions: Test files that are named test_{{ something }}.py or {{ something }}_test.py . Test methods and functions named test_{{ something }} . Test classes named Test{{ Something }} . There are several possible outcomes of a test function: PASSED (.) : The test ran successfully. FAILED (F) : The test did not run usccessfully (or XPASS + strict). SKIPPED (s) : The test was skipped. You can tell pytest to skip a test by using enter the @pytest.mark.skip() or pytest.mark.skipif() decorators. xfail (x) : The test was not supposed to pass, ran, and failed. You can tell pytest that a test is expected to fail by using the @pytest.mark.xfail() decorator. XPASS (X) : The tests was not supposed to pass, ran, and passed. ERROR (E) : An exception happened outside of the test function, in either a fixture or a hook function. Pytest supports several cool flags like: -k EXPRESSION : Used to select a subset of tests to run. For example pytest -k \"asdict or defaults\" will run both test_asdict() and test_defaults() . --lf or --last-failed : Just run the tests that have failed in the previous run. -x , or --exitfirst : Exit on first failed test. -l or --showlocals : Print out the local variables in a test if the test fails. -s Allows any output that normally would be printed to stdout to actually be printed to stdout . It's an alias of --capture=no , so the output is not captured when the tests are run, which is the default behavior. This is useful to debug with print() statements. --durations=N : It reports the slowest N number of tests/setups/teardowns after the test run. If you pass in --durations=0 , it reports everything in order of slowest to fastest.","title":"Usage"},{"location":"coding/python/pytest/#parametrized-testing","text":"Parametrized testing is a way to send multiple sets of data through the same test and have pytest report if any of the sets failed. File tests/unit/test_func.py tasks_to_try = ( Task ( 'sleep' , done = True ), Task ( 'wake' , 'brian' ), Task ( 'wake' , 'brian' ), Task ( 'breathe' , 'BRIAN' , True ), Task ( 'exercise' , 'BrIaN' , False ), ) task_ids = [ f 'Task( { task . summary } , { task . owner } , { task . done } )' for task in tasks_to_try ] @pytest . mark . parametrize ( 'task' , tasks_to_try , ids = task_ids ) def test_add_4 ( task ): task_id = tasks . add ( task ) t_from_db = tasks . get ( task_id ) assert equivalent ( t_from_db , task ) $ pytest -v test_func.py::test_add_4 ===================== test session starts ====================== collected 5 items test_add_variety.py::test_add_4 [ Task ( sleep,None,True )] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 0 ] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 1 ] PASSED test_add_variety.py::test_add_4 [ Task ( breathe,BRIAN,True )] PASSED test_add_variety.py::test_add_4 [ Task ( exercise,BrIaN,False )] PASSED =================== 5 passed in 0 .04 seconds =================== Those identifiers can be used to run that specific test. For example pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\" . parametrize() can be applied to classes as well. If the test id can't be derived from the parameter value, use the id argument for the pytest.param : @pytest . mark . parametrize ( 'task' , [ pytest . param ( Task ( 'create' ), id = 'just summary' ), pytest . param ( Task ( 'inspire' , 'Michelle' ), id = 'summary/owner' ), ]) def test_add_6 ( task ): ... Will yield: $ pytest-v test_add_variety.py::test_add_6 =================== test session starts ==================== collected 2 items test_add_variety.py::test_add_6 [ justsummary ] PASSED test_add_variety.py::test_add_6 [ summary/owner ] PASSED ================= 2 passed in 0 .05 seconds =================","title":"Parametrized testing"},{"location":"coding/python/pytest/#parametrize-fixture-arguments","text":"Sometimes is interesting to parametrize the arguments of a fixture. For example, imagine that we've got two fixtures to generate test data, then when the test is run, it must call the correct fixture for each case. @pytest . fixture def insert_task (): task = factories . TaskFactory . create () session . execute ( 'INSERT INTO task (id, description, state, type)' 'VALUES (' f '\" { task . id } \", \" { task . description } \", \" { task . state } \", \"task\"' ')' ) return task @pytest . fixture def insert_project ( session ): project = factories . ProjectFactory . create () session . execute ( 'INSERT INTO project (id, description)' f 'VALUES (\" { project . id } \", \" { project . description } \")' ) return project We can create a fixture that uses both and returns the correct one based on an argument that is given. @pytest . fixture def insert_object ( request , insert_task , insert_project ): if request . param == 'insert_task' : return insert_task elif request . param == 'insert_project' : return insert_project @pytest . mark . parametrize ( 'table,insert_object' , [ ( 'task' , 'insert_task' ), ( 'project' , 'insert_project' ), ] models_to_try , indirect = [ 'insert_object' ], ) def test_repository_can_retrieve_an_object ( self , session , table , insert_object , ): expected_obj = insert_object repo = repository . SqlAlchemyRepository ( session ) retrieved_obj = repo . get ( table , expected_obj . id ) assert retrieved_obj == expected_obj # Task.__eq__ only compares reference assert retrieved_obj . description == expected_obj . description","title":"Parametrize fixture arguments"},{"location":"coding/python/pytest/#snippets","text":"","title":"Snippets"},{"location":"coding/python/pytest/#mocking-sysexit","text":"with pytest . raises ( SystemExit ): # Code to test","title":"Mocking sys.exit"},{"location":"coding/python/pytest/#pytest-integration-with-vim","text":"Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD. I use Janko-m's Vim-test plugin (which can be installed through Vundle ) with the following configuration. nmap < silent > t :TestNearest < CR > nmap < silent > < leader > t :TestSuite tests/unit < CR > nmap < silent > < leader > i :TestSuite tests/integration < CR > nmap < silent > < leader > T :TestFile < CR > let test#python#runner = 'pytest' let test#strategy = \"neovim\" I often open Vim with a vertical split ( :vs ), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press t when the cursor is inside that test. If you need to make changes in the code, you can press t again while the cursor is at the code you are testing and it will run the last test. Once the unit test has passed, I run the whole unit tests with ;t (as ; is my <leader> ). And finally I use ;i to run the integration tests. Finally, if the test suite is huge, I use ;T to run only the tests of a single file.","title":"pytest integration with Vim"},{"location":"coding/python/pytest/#reference","text":"Book Python Testing with pytest by Brian Okken . Docs Vim-test plugin","title":"Reference"},{"location":"coding/python/python_anti_patterns/","text":"Mutable default arguments \u00b6 What You Wrote \u00b6 def append_to ( element , to = []): to . append ( element ) return to What You Might Have Expected to Happen \u00b6 my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is: [ 12 ] [ 42 ] What Does Happen [ 12 ] [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well. What You Should Do Instead \u00b6 Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice). def append_to ( element , to = None ): if to is None : to = [] to . append ( element ) return to Do not forget, you are passing a list object as the second argument. When the Gotcha Isn\u2019t a Gotcha \u00b6 Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.","title":"Anti-Patterns"},{"location":"coding/python/python_anti_patterns/#mutable-default-arguments","text":"","title":"Mutable default arguments"},{"location":"coding/python/python_anti_patterns/#what-you-wrote","text":"def append_to ( element , to = []): to . append ( element ) return to","title":"What You Wrote"},{"location":"coding/python/python_anti_patterns/#what-you-might-have-expected-to-happen","text":"my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is: [ 12 ] [ 42 ] What Does Happen [ 12 ] [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.","title":"What You Might Have Expected to Happen"},{"location":"coding/python/python_anti_patterns/#what-you-should-do-instead","text":"Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice). def append_to ( element , to = None ): if to is None : to = [] to . append ( element ) return to Do not forget, you are passing a list object as the second argument.","title":"What You Should Do Instead"},{"location":"coding/python/python_anti_patterns/#when-the-gotcha-isnt-a-gotcha","text":"Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.","title":"When the Gotcha Isn\u2019t a Gotcha"},{"location":"coding/python/python_ci/","text":"Continuous Integration (CI) allows to automatically run processes on the code each time a commit is pushed. For example it can be used to run the tests, build the documentation or build a package. There are three non exclusive ways to run the tests: Integrate them in your editor, so it's executed each time you save the file. Through a pre-commit hook to make it easy for the collaborator to submit correctly formatted code. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. Through a ensure that all the collaborator submit correctly formatted code. pre-commit is a framework for managing and maintaining multi-language Through a CI server (like Drone or Github Actions) to ensure that the commited code meets the quality standards. Developers can bypass the pre-commit filter, so we need to set up the quality gate in an agnostic environment. Depending on the time the test takes to run and their different implementations, we'll choose from one to three of the choices above. Configuring pre-commit \u00b6 To adopt pre-commit to our system we have to: Install pre-commit: pip3 install pre-commit and add it to the development requirements.txt . Define .pre-commit-config.yaml with the hooks you want to include. Execute pre-commit install to install git hooks in your .git/ directory. Execute pre-commit run --all-files to tests all the files. Usually pre-commit will only run on the changed files during git hooks. Linting tests \u00b6 Black \u00b6 Black is a style guide enforcement tool. Its configuration is stored in pyproject.toml . File: pyproject.toml # Example configuration for Black. # NOTE: you have to use single-quoted strings in TOML for regular expressions. # It's the equivalent of r-strings in Python. Multiline strings are treated as # verbose regular expressions by Black. Use [ ] to denote a significant space # character. [tool.black] line-length = 88 target-version = ['py36', 'py37', 'py38'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist # The following are specific to Black, you probably don't want those. | blib2to3 | tests/data | profiling )/ ''' Trigger hooks: Vim plugin . Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/ambv/black rev : stable hooks : - id : black language_version : python3.7 Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Black : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Black uses : psf/black@stable Flake8 \u00b6 Flake8 is another style guide enforcement tool. Its configuration is stored in setup.cfg , tox.ini or .flake8 . File: .flake8 [flake8] # ignore = E203, E266, E501, W503, F403, F401 max-line-length = 88 # max-complexity = 18 # select = B,C,E,F,W,T4,B9 Trigger hooks: Pre-commit File: .pre-commit-config.yaml repos : - repo : https://gitlab.com/pycqa/flake8 rev : master hooks : - id : flake8 Github Actions File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Flake8 : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Flake8 uses : cclauss/GitHub-Action-for-Flake8@v0.5.0 Mypy \u00b6 Mypy is an optional static type checker for Python that aims to combine the benefits of dynamic (or \"duck\") typing and static typing. Mypy combines the expressive power and convenience of Python with a powerful type system and compile-time type checking. Mypy configuration is saved in the mypy.ini file. File: mypy.ini [mypy] ignore_missing_imports = False [mypy-alembic.*] ignore_missing_imports = True [mypy-argcomplete.*] ignore_missing_imports = True [mypy-factory.*] ignore_missing_imports = True [mypy-faker.*] ignore_missing_imports = True [mypy-pytest.*] ignore_missing_imports = True [mypy-sqlalchemy.*] ignore_missing_imports = True Trigger hooks: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/mirrors-mypy rev : v0.782 hooks : - name : Run mypy static analysis tool id : mypy Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Mypy : runs-on : ubuntu-latest name : Mypy steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install Dependencies run : pip install mypy - name : mypy run : mypy Other pre-commit tests \u00b6 Pre-commit comes with several tests by default. These are the ones I've chosen. File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.1.0 hooks : - id : trailing-whitespace - id : check-added-large-files - id : check-docstring-first - id : check-merge-conflict - id : end-of-file-fixer - id : detect-private-key Test yaml syntax File: .pre-commit-config.yaml - repo : https://github.com/adrienverge/yamllint rev : v1.21.0 hooks : - id : yamllint Unit, integration, end-to-end, edge-to-edge tests \u00b6 These tests define the behaviour of the application. Trigger hooks: Github Actions: To run the tests each time a push or pull request is created in Github, create the .github/workflows/pythonpackage.yml file with the following Jinja template. Make sure to check: The correct Python versions are configured. The steps make sense to your case scenario. Variables to substitute: program_name : your program name name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov {{ program_name }} tests If you want to add a badge stating the last build status in your readme, use the following template. Variables to substitute: repository_url : Github repository url, like https://github.com/lyz-code/pydo . [![Actions Status]({{ repository_url }}/workflows/Python%20package/badge.svg)]({{ repository_url }}/actions) Security tests \u00b6 Safety \u00b6 Safety checks your installed dependencies for known security vulnerabilities. Trigger hooks: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-safety rev : v1.1.3 hooks : - id : python-safety-dependencies-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : safety : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install safety - name : Execute safety run : safety check Bandit \u00b6 Safety finds common security issues in Python code. To do this, Bandit processes each file, builds an AST from it, and runs appropriate plugins against the AST nodes. Once Bandit has finished scanning all the files, it generates a report. Trigger hooks: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-bandit rev : v1.0.4 hooks : - id : python-bandit-vulnerability-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : bandit : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install bandit - name : Execute bandit run : bandit -r project Update package dependencies \u00b6 For stability reasons it's a good idea to hardcode the dependencies versions. Furthermore, safety needs them to work properly. We've got three places where the dependencies are defined: setup.py should declare the loosest possible dependency versions that are still workable. Its job is to say what a particular package can work with. requirements.txt is a deployment manifest that defines an entire installation job, and shouldn't be thought of as tied to any one package. Its job is to declare an exhaustive list of all the necessary packages to make a deployment work. dev-requirements.txt Adds the dependencies required for development of the program. With pip-tools , the dependency management is trivial. Install the tool: pip install pip-tools . Set the general dependencies in the setup.py install_requires . Generate the requirements.txt file: pip-compile . Add the additional testing dependencies in the dev-requirements.in file. File: dev-requirements.in ini -c requirements.txt pip-tools factory_boy pytest pytest-cov Compile the development requirements dev-requirements.txt with pip-compile dev-requirements.in . If you have another requirements.txt for the mkdocs documentation, run pip-compile docs/requirements.txt . Trigger hooks: Pre-commit: File: .pre-commit-config.yaml - repo : https://github.com/jazzband/pip-tools rev : 5.0.0 hooks : - name : Build requirements.txt id : pip-compile - name : Build dev-requirements.txt id : pip-compile args : [ 'dev-requirements.in' ] - name : Build mkdocs requirements.txt id : pip-compile args : [ 'docs/requirements.txt' ] Coverage tests \u00b6 Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test. Save the secret in the repository configuration and add this step to your tests job. - name : Coveralls uses : coverallsapp/github-action@master with : github-token : ${{ secrets.COVERALLS_TOKEN }} Add the following badge to your README.md. Variables to substitute: repository_path : Github repository path, like lyz-code/pydo . [![Coverage Status](https://coveralls.io/repos/github/{{ repository_path }}/badge.svg?branch=master)](https://coveralls.io/github/{{ repository_path }}?branch=master)","title":"Continuous integration pipelines"},{"location":"coding/python/python_ci/#configuring-pre-commit","text":"To adopt pre-commit to our system we have to: Install pre-commit: pip3 install pre-commit and add it to the development requirements.txt . Define .pre-commit-config.yaml with the hooks you want to include. Execute pre-commit install to install git hooks in your .git/ directory. Execute pre-commit run --all-files to tests all the files. Usually pre-commit will only run on the changed files during git hooks.","title":"Configuring pre-commit"},{"location":"coding/python/python_ci/#linting-tests","text":"","title":"Linting tests"},{"location":"coding/python/python_ci/#black","text":"Black is a style guide enforcement tool. Its configuration is stored in pyproject.toml . File: pyproject.toml # Example configuration for Black. # NOTE: you have to use single-quoted strings in TOML for regular expressions. # It's the equivalent of r-strings in Python. Multiline strings are treated as # verbose regular expressions by Black. Use [ ] to denote a significant space # character. [tool.black] line-length = 88 target-version = ['py36', 'py37', 'py38'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist # The following are specific to Black, you probably don't want those. | blib2to3 | tests/data | profiling )/ ''' Trigger hooks: Vim plugin . Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/ambv/black rev : stable hooks : - id : black language_version : python3.7 Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Black : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Black uses : psf/black@stable","title":"Black"},{"location":"coding/python/python_ci/#flake8","text":"Flake8 is another style guide enforcement tool. Its configuration is stored in setup.cfg , tox.ini or .flake8 . File: .flake8 [flake8] # ignore = E203, E266, E501, W503, F403, F401 max-line-length = 88 # max-complexity = 18 # select = B,C,E,F,W,T4,B9 Trigger hooks: Pre-commit File: .pre-commit-config.yaml repos : - repo : https://gitlab.com/pycqa/flake8 rev : master hooks : - id : flake8 Github Actions File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Flake8 : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Flake8 uses : cclauss/GitHub-Action-for-Flake8@v0.5.0","title":"Flake8"},{"location":"coding/python/python_ci/#mypy","text":"Mypy is an optional static type checker for Python that aims to combine the benefits of dynamic (or \"duck\") typing and static typing. Mypy combines the expressive power and convenience of Python with a powerful type system and compile-time type checking. Mypy configuration is saved in the mypy.ini file. File: mypy.ini [mypy] ignore_missing_imports = False [mypy-alembic.*] ignore_missing_imports = True [mypy-argcomplete.*] ignore_missing_imports = True [mypy-factory.*] ignore_missing_imports = True [mypy-faker.*] ignore_missing_imports = True [mypy-pytest.*] ignore_missing_imports = True [mypy-sqlalchemy.*] ignore_missing_imports = True Trigger hooks: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/mirrors-mypy rev : v0.782 hooks : - name : Run mypy static analysis tool id : mypy Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Mypy : runs-on : ubuntu-latest name : Mypy steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install Dependencies run : pip install mypy - name : mypy run : mypy","title":"Mypy"},{"location":"coding/python/python_ci/#other-pre-commit-tests","text":"Pre-commit comes with several tests by default. These are the ones I've chosen. File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.1.0 hooks : - id : trailing-whitespace - id : check-added-large-files - id : check-docstring-first - id : check-merge-conflict - id : end-of-file-fixer - id : detect-private-key Test yaml syntax File: .pre-commit-config.yaml - repo : https://github.com/adrienverge/yamllint rev : v1.21.0 hooks : - id : yamllint","title":"Other pre-commit tests"},{"location":"coding/python/python_ci/#unit-integration-end-to-end-edge-to-edge-tests","text":"These tests define the behaviour of the application. Trigger hooks: Github Actions: To run the tests each time a push or pull request is created in Github, create the .github/workflows/pythonpackage.yml file with the following Jinja template. Make sure to check: The correct Python versions are configured. The steps make sense to your case scenario. Variables to substitute: program_name : your program name name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov {{ program_name }} tests If you want to add a badge stating the last build status in your readme, use the following template. Variables to substitute: repository_url : Github repository url, like https://github.com/lyz-code/pydo . [![Actions Status]({{ repository_url }}/workflows/Python%20package/badge.svg)]({{ repository_url }}/actions)","title":"Unit, integration, end-to-end, edge-to-edge tests"},{"location":"coding/python/python_ci/#security-tests","text":"","title":"Security tests"},{"location":"coding/python/python_ci/#safety","text":"Safety checks your installed dependencies for known security vulnerabilities. Trigger hooks: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-safety rev : v1.1.3 hooks : - id : python-safety-dependencies-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : safety : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install safety - name : Execute safety run : safety check","title":"Safety"},{"location":"coding/python/python_ci/#bandit","text":"Safety finds common security issues in Python code. To do this, Bandit processes each file, builds an AST from it, and runs appropriate plugins against the AST nodes. Once Bandit has finished scanning all the files, it generates a report. Trigger hooks: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-bandit rev : v1.0.4 hooks : - id : python-bandit-vulnerability-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : bandit : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install bandit - name : Execute bandit run : bandit -r project","title":"Bandit"},{"location":"coding/python/python_ci/#update-package-dependencies","text":"For stability reasons it's a good idea to hardcode the dependencies versions. Furthermore, safety needs them to work properly. We've got three places where the dependencies are defined: setup.py should declare the loosest possible dependency versions that are still workable. Its job is to say what a particular package can work with. requirements.txt is a deployment manifest that defines an entire installation job, and shouldn't be thought of as tied to any one package. Its job is to declare an exhaustive list of all the necessary packages to make a deployment work. dev-requirements.txt Adds the dependencies required for development of the program. With pip-tools , the dependency management is trivial. Install the tool: pip install pip-tools . Set the general dependencies in the setup.py install_requires . Generate the requirements.txt file: pip-compile . Add the additional testing dependencies in the dev-requirements.in file. File: dev-requirements.in ini -c requirements.txt pip-tools factory_boy pytest pytest-cov Compile the development requirements dev-requirements.txt with pip-compile dev-requirements.in . If you have another requirements.txt for the mkdocs documentation, run pip-compile docs/requirements.txt . Trigger hooks: Pre-commit: File: .pre-commit-config.yaml - repo : https://github.com/jazzband/pip-tools rev : 5.0.0 hooks : - name : Build requirements.txt id : pip-compile - name : Build dev-requirements.txt id : pip-compile args : [ 'dev-requirements.in' ] - name : Build mkdocs requirements.txt id : pip-compile args : [ 'docs/requirements.txt' ]","title":"Update package dependencies"},{"location":"coding/python/python_ci/#coverage-tests","text":"Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test. Save the secret in the repository configuration and add this step to your tests job. - name : Coveralls uses : coverallsapp/github-action@master with : github-token : ${{ secrets.COVERALLS_TOKEN }} Add the following badge to your README.md. Variables to substitute: repository_path : Github repository path, like lyz-code/pydo . [![Coverage Status](https://coveralls.io/repos/github/{{ repository_path }}/badge.svg?branch=master)](https://coveralls.io/github/{{ repository_path }}?branch=master)","title":"Coverage tests"},{"location":"coding/python/python_code_styling/","text":"Commit message guidelines \u00b6 I'm following the Angular commit convention that is backed up by python-semantic-release , with the idea of implementing automatic semantic versioning sometime in the future. Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject: <type>(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <footer> The header is mandatory and the scope of the header is optional. Any line of the commit message cannot be longer 100 characters. The footer should contain a closing reference to an issue if any. Samples: (even more samples) docs(changelog): update changelog to beta.5 fix(release): need to depend on latest rxjs and zone.js The version in our package.json gets copied to the one we publish, and users need the latest of these. docs(router): fix typo 'containa' to 'contains' (#36764) Closes #36763 PR Close #36764 Revert \u00b6 If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit. In the body it should say: This reverts commit <hash>. , where the hash is the SHA of the commit to revert. Type \u00b6 Must be one of the following: build : Changes that affect the build system or external dependencies. ci : Changes to our CI configuration files and scripts. docs : Documentation changes. feat : A new feature. fix : A bug fix. perf : A code change that improves performance. refactor : A code change that neither fixes a bug nor adds a feature. style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc). test : Adding missing tests or correcting existing tests. Subject \u00b6 The subject contains a succinct description of the change: Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". Don't capitalize the first letter. No dot (.) at the end. Body \u00b6 Same as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior. Footer \u00b6 The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this. Pre-commit \u00b6 To ensure that your project follows these guidelines, add the following to your pre-commit configuration : File: .pre-commit-config.yaml - repo : https://github.com/commitizen-tools/commitizen rev : master hooks : - id : commitizen stages : [ commit-msg ] To make your life easier, change your workflow to use commitizen . In Vim, if you're using Vim fugitive change the configuration to: nnoremap <leader>gc :terminal cz c<CR> nnoremap <leader>gr :terminal cz c --retry<CR> \" Open terminal mode in insert mode if has('nvim') autocmd TermOpen term://* startinsert endif autocmd BufLeave term://* stopinsert If some pre-commit hook fails, make the changes and then use <leader>gr to repeat the same commit message. Black code style \u00b6 Black is a style guide enforcement tool. Flake8 \u00b6 Flake8 is another style guide enforcement tool. Type hints \u00b6 Traditionally, the Python interpreter handles types in a flexible but implicit way. Recent versions of Python allow you to specify explicit type hints that different tools can use to help you develop your code more efficiently. TL;DR Use Type hints whenever unit tests are worth writing def headline ( text : str , align : bool = True ) -> str : if align : return f \" { text . title () } \\n { '-' * len ( text ) } \" else : return f \" { text . title () } \" . center ( 50 , \"o\" ) Type hints are not enforced on their own by python. So you won't catch an error if you try to run headline(\"use mypy\", align=\"center\") unless you use a static type checker like Mypy . Advantages and disadvantages \u00b6 Advantages: Help catch certain errors if used with a static type checker. Help check your code . It's not trivial to use docstrings to do automatic checks. Help to reason about code: Knowing the parameters type makes it a lot easier to understand and maintain a code base. It can speed up the time required to catch up with a code snippet. Always remember that you read code a lot more often than you write it, so you should optimize for ease of reading. Help you build and maintain a cleaner architecture . The act of writing type hints force you to think about the types in your program. Helps refactoring: Type hints make it trivial to find where a given class is used when you're trying to refactor your code base. Improve IDEs and linters. Cons: Type hints take developer time and effort to add . Even though it probably pays off in spending less time debugging, you will spend more time entering code. Introduce a slight penalty in start-up time . If you need to use the typing module, the import time may be significant, even more in short scripts. Work best in modern Pythons. Follow these guidelines when deciding if you want to add types to your project: In libraries that will be used by others, they add a lot of value. In complex projects, type hints help you understand how types flow through your code and are highly recommended. If you are beginning to learn Python, don't use them yet. If you are writing throw-away scripts, don't use them. So, Use Type hints whenever unit tests are worth writing . Usage \u00b6 Function annotations \u00b6 def func ( arg : arg_type , optarg : arg_type = default ) -> return_type : ... For arguments the syntax is argument: annotation , while the return type is annotated using -> annotation . Note that the annotation must be a valid Python expression. When running the code, the special .__annotations__ attribute on the function stores the typing information. Variable annotations \u00b6 Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar: pi : float = 3.142 def circumference ( radius : float ) -> float : return 2 * pi * radius Composite types \u00b6 If you need to hint other types than str , float and bool , you'll need to import the typing module. For example to define the hint types of list, dictionaries and tuples: >>> from typing import Dict , List , Tuple >>> names : List [ str ] = [ \"Guido\" , \"Jukka\" , \"Ivan\" ] >>> version : Tuple [ int , int , int ] = ( 3 , 7 , 1 ) >>> options : Dict [ str , bool ] = { \"centered\" : False , \"capitalize\" : True } If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the typing.Sequence object. Functions without return values \u00b6 Some functions aren't meant to return anything. Use the -> None hint in these cases. def play ( player_name : str ) -> None : print ( f \" { player_name } plays\" ) ret_val = play ( \"Filip\" ) The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value. If your function doesn't return any object, use the NoReturn type. Note This is the first iteration of the synoptical reading of the full Real python article on type checking . Optional arguments \u00b6 A common pattern is to use None as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior. This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be None . We use the Optional type to address this case. from typing import Optional def player ( name : str , start : Optional [ str ] = None ) -> str : ... A similar way would be to use Union[None, str] . Allow any subclass \u00b6 It's not yet supported, so the expected format class A : pass class B ( A ): pass def process_any_subclass_type_of_A ( cls : A ): pass process_any_subclass_type_of_A ( B ) Will fail with error: Argument 1 to \"process_any_subclass_type_of_A\" has incompatible type \"Type[B]\"; expected \"A\" . The solution is to use the Union operator: class A : pass class B ( A ): pass class C ( A ): pass def process_any_subclass_type_of_A ( cls : Union [ B , C ]): pass Type aliases \u00b6 Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead. from typing import List , Tuple Card = Tuple [ str , str ] Deck = List [ Card ] def deal_hands ( deck : Deck ) -> Tuple [ Deck , Deck , Deck , Deck ]: \"\"\"Deal the cards in the deck into four hands\"\"\" return ( deck [ 0 :: 4 ], deck [ 1 :: 4 ], deck [ 2 :: 4 ], deck [ 3 :: 4 ]) This can be useful when you need lists of subclasses or optional list of subclasses. The expected behavior doesn't work. Entity = Union [ model . Project , model . Tag , model . Task ] Entities = List [ Entity ] Instead, you need to use: Entities = Union [ List [ model . Project ], List [ model . Tag ], List [ model . Task ]] OptionalEntities = Union [ Optional [ List [ model . Project ]], Optional [ List [ model . Tag ]], Optional [ List [ model . Task ]] ] Still Ugly, but it mitigates the problem. Using mypy with an existing codebase \u00b6 These steps will get you started with mypy on an existing codebase: Start small : Pick a subset of your codebase to run mypy on, without any annotations. You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding # type: ignore comments to silence errors you don\u2019t want to fix now. Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors : Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this: python3 - m pip install mypy == 0.600 # Pinned version avoids surprises scripts / mypy # Runs with the correct options * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions: Developers should add annotations for any new code. It\u2019s also encouraged to write annotations when you change existing code. f-strings \u00b6 f-strings , also known as formatted string literals , are strings that have an f at the beginning and curly braces containing expressions that will be replaced with their values. Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster. >>> name = \"Eric\" >>> age = 74 >>> f \"Hello, { name } . You are { age } .\" 'Hello, Eric. You are 74.' Arbitrary expressions \u00b6 Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within. >>> f \" { name . lower () } is funny.\" 'eric idle is funny.' Multiline f-strings \u00b6 >>> name = \"Eric\" >>> profession = \"comedian\" >>> affiliation = \"Monty Python\" >>> message = ( ... f \"Hi { name } . \" ... f \"You are a { profession } . \" ... f \"You were in { affiliation } .\" ... ) >>> message 'Hi Eric. You are a comedian. You were in Monty Python.' Reference \u00b6 Type hints \u00b6 Bernat gabor article on the state of type hints in python Real python article on type checking","title":"Code Styling"},{"location":"coding/python/python_code_styling/#commit-message-guidelines","text":"I'm following the Angular commit convention that is backed up by python-semantic-release , with the idea of implementing automatic semantic versioning sometime in the future. Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject: <type>(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <footer> The header is mandatory and the scope of the header is optional. Any line of the commit message cannot be longer 100 characters. The footer should contain a closing reference to an issue if any. Samples: (even more samples) docs(changelog): update changelog to beta.5 fix(release): need to depend on latest rxjs and zone.js The version in our package.json gets copied to the one we publish, and users need the latest of these. docs(router): fix typo 'containa' to 'contains' (#36764) Closes #36763 PR Close #36764","title":"Commit message guidelines"},{"location":"coding/python/python_code_styling/#revert","text":"If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit. In the body it should say: This reverts commit <hash>. , where the hash is the SHA of the commit to revert.","title":"Revert"},{"location":"coding/python/python_code_styling/#type","text":"Must be one of the following: build : Changes that affect the build system or external dependencies. ci : Changes to our CI configuration files and scripts. docs : Documentation changes. feat : A new feature. fix : A bug fix. perf : A code change that improves performance. refactor : A code change that neither fixes a bug nor adds a feature. style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc). test : Adding missing tests or correcting existing tests.","title":"Type"},{"location":"coding/python/python_code_styling/#subject","text":"The subject contains a succinct description of the change: Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". Don't capitalize the first letter. No dot (.) at the end.","title":"Subject"},{"location":"coding/python/python_code_styling/#body","text":"Same as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior.","title":"Body"},{"location":"coding/python/python_code_styling/#footer","text":"The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this.","title":"Footer"},{"location":"coding/python/python_code_styling/#pre-commit","text":"To ensure that your project follows these guidelines, add the following to your pre-commit configuration : File: .pre-commit-config.yaml - repo : https://github.com/commitizen-tools/commitizen rev : master hooks : - id : commitizen stages : [ commit-msg ] To make your life easier, change your workflow to use commitizen . In Vim, if you're using Vim fugitive change the configuration to: nnoremap <leader>gc :terminal cz c<CR> nnoremap <leader>gr :terminal cz c --retry<CR> \" Open terminal mode in insert mode if has('nvim') autocmd TermOpen term://* startinsert endif autocmd BufLeave term://* stopinsert If some pre-commit hook fails, make the changes and then use <leader>gr to repeat the same commit message.","title":"Pre-commit"},{"location":"coding/python/python_code_styling/#black-code-style","text":"Black is a style guide enforcement tool.","title":"Black code style"},{"location":"coding/python/python_code_styling/#flake8","text":"Flake8 is another style guide enforcement tool.","title":"Flake8"},{"location":"coding/python/python_code_styling/#type-hints","text":"Traditionally, the Python interpreter handles types in a flexible but implicit way. Recent versions of Python allow you to specify explicit type hints that different tools can use to help you develop your code more efficiently. TL;DR Use Type hints whenever unit tests are worth writing def headline ( text : str , align : bool = True ) -> str : if align : return f \" { text . title () } \\n { '-' * len ( text ) } \" else : return f \" { text . title () } \" . center ( 50 , \"o\" ) Type hints are not enforced on their own by python. So you won't catch an error if you try to run headline(\"use mypy\", align=\"center\") unless you use a static type checker like Mypy .","title":"Type hints"},{"location":"coding/python/python_code_styling/#advantages-and-disadvantages","text":"Advantages: Help catch certain errors if used with a static type checker. Help check your code . It's not trivial to use docstrings to do automatic checks. Help to reason about code: Knowing the parameters type makes it a lot easier to understand and maintain a code base. It can speed up the time required to catch up with a code snippet. Always remember that you read code a lot more often than you write it, so you should optimize for ease of reading. Help you build and maintain a cleaner architecture . The act of writing type hints force you to think about the types in your program. Helps refactoring: Type hints make it trivial to find where a given class is used when you're trying to refactor your code base. Improve IDEs and linters. Cons: Type hints take developer time and effort to add . Even though it probably pays off in spending less time debugging, you will spend more time entering code. Introduce a slight penalty in start-up time . If you need to use the typing module, the import time may be significant, even more in short scripts. Work best in modern Pythons. Follow these guidelines when deciding if you want to add types to your project: In libraries that will be used by others, they add a lot of value. In complex projects, type hints help you understand how types flow through your code and are highly recommended. If you are beginning to learn Python, don't use them yet. If you are writing throw-away scripts, don't use them. So, Use Type hints whenever unit tests are worth writing .","title":"Advantages and disadvantages"},{"location":"coding/python/python_code_styling/#usage","text":"","title":"Usage"},{"location":"coding/python/python_code_styling/#function-annotations","text":"def func ( arg : arg_type , optarg : arg_type = default ) -> return_type : ... For arguments the syntax is argument: annotation , while the return type is annotated using -> annotation . Note that the annotation must be a valid Python expression. When running the code, the special .__annotations__ attribute on the function stores the typing information.","title":"Function annotations"},{"location":"coding/python/python_code_styling/#variable-annotations","text":"Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar: pi : float = 3.142 def circumference ( radius : float ) -> float : return 2 * pi * radius","title":"Variable annotations"},{"location":"coding/python/python_code_styling/#composite-types","text":"If you need to hint other types than str , float and bool , you'll need to import the typing module. For example to define the hint types of list, dictionaries and tuples: >>> from typing import Dict , List , Tuple >>> names : List [ str ] = [ \"Guido\" , \"Jukka\" , \"Ivan\" ] >>> version : Tuple [ int , int , int ] = ( 3 , 7 , 1 ) >>> options : Dict [ str , bool ] = { \"centered\" : False , \"capitalize\" : True } If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the typing.Sequence object.","title":"Composite types"},{"location":"coding/python/python_code_styling/#functions-without-return-values","text":"Some functions aren't meant to return anything. Use the -> None hint in these cases. def play ( player_name : str ) -> None : print ( f \" { player_name } plays\" ) ret_val = play ( \"Filip\" ) The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value. If your function doesn't return any object, use the NoReturn type. Note This is the first iteration of the synoptical reading of the full Real python article on type checking .","title":"Functions without return values"},{"location":"coding/python/python_code_styling/#optional-arguments","text":"A common pattern is to use None as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior. This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be None . We use the Optional type to address this case. from typing import Optional def player ( name : str , start : Optional [ str ] = None ) -> str : ... A similar way would be to use Union[None, str] .","title":"Optional arguments"},{"location":"coding/python/python_code_styling/#allow-any-subclass","text":"It's not yet supported, so the expected format class A : pass class B ( A ): pass def process_any_subclass_type_of_A ( cls : A ): pass process_any_subclass_type_of_A ( B ) Will fail with error: Argument 1 to \"process_any_subclass_type_of_A\" has incompatible type \"Type[B]\"; expected \"A\" . The solution is to use the Union operator: class A : pass class B ( A ): pass class C ( A ): pass def process_any_subclass_type_of_A ( cls : Union [ B , C ]): pass","title":"Allow any subclass"},{"location":"coding/python/python_code_styling/#type-aliases","text":"Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead. from typing import List , Tuple Card = Tuple [ str , str ] Deck = List [ Card ] def deal_hands ( deck : Deck ) -> Tuple [ Deck , Deck , Deck , Deck ]: \"\"\"Deal the cards in the deck into four hands\"\"\" return ( deck [ 0 :: 4 ], deck [ 1 :: 4 ], deck [ 2 :: 4 ], deck [ 3 :: 4 ]) This can be useful when you need lists of subclasses or optional list of subclasses. The expected behavior doesn't work. Entity = Union [ model . Project , model . Tag , model . Task ] Entities = List [ Entity ] Instead, you need to use: Entities = Union [ List [ model . Project ], List [ model . Tag ], List [ model . Task ]] OptionalEntities = Union [ Optional [ List [ model . Project ]], Optional [ List [ model . Tag ]], Optional [ List [ model . Task ]] ] Still Ugly, but it mitigates the problem.","title":"Type aliases"},{"location":"coding/python/python_code_styling/#using-mypy-with-an-existing-codebase","text":"These steps will get you started with mypy on an existing codebase: Start small : Pick a subset of your codebase to run mypy on, without any annotations. You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding # type: ignore comments to silence errors you don\u2019t want to fix now. Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors : Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this: python3 - m pip install mypy == 0.600 # Pinned version avoids surprises scripts / mypy # Runs with the correct options * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions: Developers should add annotations for any new code. It\u2019s also encouraged to write annotations when you change existing code.","title":"Using mypy with an existing codebase"},{"location":"coding/python/python_code_styling/#f-strings","text":"f-strings , also known as formatted string literals , are strings that have an f at the beginning and curly braces containing expressions that will be replaced with their values. Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster. >>> name = \"Eric\" >>> age = 74 >>> f \"Hello, { name } . You are { age } .\" 'Hello, Eric. You are 74.'","title":"f-strings"},{"location":"coding/python/python_code_styling/#arbitrary-expressions","text":"Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within. >>> f \" { name . lower () } is funny.\" 'eric idle is funny.'","title":"Arbitrary expressions"},{"location":"coding/python/python_code_styling/#multiline-f-strings","text":">>> name = \"Eric\" >>> profession = \"comedian\" >>> affiliation = \"Monty Python\" >>> message = ( ... f \"Hi { name } . \" ... f \"You are a { profession } . \" ... f \"You were in { affiliation } .\" ... ) >>> message 'Hi Eric. You are a comedian. You were in Monty Python.'","title":"Multiline f-strings"},{"location":"coding/python/python_code_styling/#reference","text":"","title":"Reference"},{"location":"coding/python/python_code_styling/#type-hints_1","text":"Bernat gabor article on the state of type hints in python Real python article on type checking","title":"Type hints"},{"location":"coding/python/python_config_yaml/","text":"Several programs load the configuration from file. After trying ini, json and yaml I've seen that the last one is the most comfortable. So here are the templates for the tests and class that loads the data from a yaml file and exposes it as a dictionary. In the following sections Jinja templating is used, so substitute everything between {{ }} to their correct values. It's assumed that: The root directory of the project has the same name as the program. A file with a valid config exists in assets/config.yaml . We'll use this file in the documentation, so comment it through. Code \u00b6 The class code below is expected to introduced in the file configuration.py . Variables to substitute: program_name File {{ program_name }}/configuration.py \"\"\" Module to define the configuration of the main program. Classes: Config: Class to manipulate the configuration of the program. \"\"\" from collections import UserDict from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError import logging import os import sys log = logging . getLogger ( __name__ ) class Config ( UserDict ): \"\"\" Class to manipulate the configuration of the program. Arguments: config_path (str): Path to the configuration file. Default: ~/.local/share/{{ program_name }}/config.yaml Public methods: get: Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. load: Loads configuration from configuration YAML file. save: Saves configuration in the configuration YAML file. Attributes and properties: config_path (str): Path to the configuration file. data(dict): Program configuration. \"\"\" def __init__ ( self , config_path = '~/.local/share/{{ program_name }}/config.yaml' ): self . config_path = os . path . expanduser ( config_path ) self . load () def get ( self , key ): \"\"\" Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. So if the configuration contents are: self.data = { 'first': { 'second': 'value' }, } self.data.get('first.second') == 'value' Arguments: key(str): Configuration key to fetch \"\"\" keys = key . split ( '.' ) value = self . data . copy () for key in keys : value = value [ key ] return value def load ( self ): \"\"\" Loads configuration from configuration YAML file. \"\"\" try : with open ( os . path . expanduser ( self . config_path ), 'r' ) as f : try : self . data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( self . config_path ) ) sys . exit ( 1 ) def save ( self ): \"\"\" Saves configuration in the configuration YAML file. \"\"\" with open ( os . path . expanduser ( self . config_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( self . data , f ) We use ruamel PyYAML implementation to preserve the file comments. That class is meant to be loaded in the main __init__.py file, below the logging configuration (if there is any). Variables to substitute: program_name config_environmental_variable : The optional environmental variable where the path to the configuration is set. For example PYDO_CONFIG . This will be used in the tests to override the default path. We don't load this configuration from the program argument parser because it's definition often depends on the config file. File {{ program_name}}/ init .py # ... # (optional logging definition) import os from {{ program_name }} . configuration import Config config = Config ( os . getenv ( '{{ config_environmental_variable }}' , '~/.local/share/{{ program_name }}/config.yaml' )) # (Rest of the program) # ... If you want to use the config object in a module of your program, import them from the above file like this: File {{ program_name }}/cli.py from {{ program_name }} import config Tests \u00b6 I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the conftest.py file that gets executed by pytest in the tests setup. Variables to substitute: config_environmental_variable : Same as the one defined in the last section. File tests/conftest.py import os os . environ [{{ config_environmental_variable }}] = 'assets/config.yaml' Variables to substitute: program_name As it's really dependent in the config structure, you can improve the test_config_load test to make it more meaningful. File tests/unit/test_configuration.py from {{ program_name }} . configuration import Config from unittest.mock import patch from ruamel.yaml.scanner import ScannerError import os import pytest import shutil import tempfile class TestConfig : \"\"\" Class to test the Config object. Public attributes: config (Config object): Config object to test \"\"\" @pytest . fixture ( autouse = True ) def setup ( self ): self . config_path = 'assets/config.yaml' self . log_patch = patch ( '{{ program_name }}.configuration.log' , autospect = True ) self . log = self . log_patch . start () self . sys_patch = patch ( '{{ program_name }}.configuration.sys' , autospect = True ) self . sys = self . sys_patch . start () self . config = Config ( self . config_path ) yield 'setup' self . log_patch . stop () self . sys_patch . stop () def test_get_can_fetch_nested_items_with_dots ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config . get ( 'first.second' ) == 'value' def test_config_can_fetch_nested_items_with_dictionary_notation ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config [ 'first' ][ 'second' ] == 'value' def test_config_load ( self ): self . config . load () assert len ( self . config . data ) > 0 @patch ( '{{ program_name }}.configuration.YAML' ) def test_load_handles_wrong_file_format ( self , yamlMock ): yamlMock . return_value . load . side_effect = ScannerError ( 'error' , '' , 'problem' , 'mark' , ) self . config . load () self . log . error . assert_called_once_with ( 'Error parsing yaml of configuration file mark: problem' ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.open' ) def test_load_handles_file_not_found ( self , openMock ): openMock . side_effect = FileNotFoundError () self . config . load () self . log . error . assert_called_once_with ( 'Error opening configuration file {} ' . format ( self . config_path ) ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.Config.load' ) def test_init_calls_config_load ( self , loadMock ): Config () loadMock . assert_called_once_with () def test_save_config ( self ): tmp = tempfile . mkdtemp () save_file = os . path . join ( tmp , 'yaml_save_test.yaml' ) self . config = Config ( save_file ) self . config . data = { 'a' : 'b' } self . config . save () with open ( save_file , 'r' ) as f : assert \"a:\" in f . read () shutil . rmtree ( tmp ) Installation \u00b6 It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file. Variables to substitute in both files: program_name File setup.py import shutil ... Class PostInstallCommand ( install ): ... def run ( self ): install . run ( self ) try : data_directory = os . path . expanduser ( \"~/.local/share/{{ program_name }}\" ) os . makedirs ( data_directory ) log . info ( \"Data directory created\" ) except FileExistsError : log . info ( \"Data directory already exits\" ) config_path = os . path . join ( data_directory , 'config.yaml' ) if os . path . isfile ( config_path ) and os . access ( config_path , os . R_OK ): log . info ( \"Configuration file already exists, check the documentation \" \"for the new version changes.\" ) else : shutil . copyfile ( 'assets/config.yaml' , config_path ) log . info ( \"Copied default configuration template\" ) README.md ... {{ program_name }} configuration is done through the yaml file located at ~/.local/share/{{ program_name }}/config.yaml . The default template is provided at installation time. ... It's also necessary to add the ruamel.yaml pip package to your setup.py and requirements.txt files.","title":"Load config from YAML"},{"location":"coding/python/python_config_yaml/#code","text":"The class code below is expected to introduced in the file configuration.py . Variables to substitute: program_name File {{ program_name }}/configuration.py \"\"\" Module to define the configuration of the main program. Classes: Config: Class to manipulate the configuration of the program. \"\"\" from collections import UserDict from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError import logging import os import sys log = logging . getLogger ( __name__ ) class Config ( UserDict ): \"\"\" Class to manipulate the configuration of the program. Arguments: config_path (str): Path to the configuration file. Default: ~/.local/share/{{ program_name }}/config.yaml Public methods: get: Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. load: Loads configuration from configuration YAML file. save: Saves configuration in the configuration YAML file. Attributes and properties: config_path (str): Path to the configuration file. data(dict): Program configuration. \"\"\" def __init__ ( self , config_path = '~/.local/share/{{ program_name }}/config.yaml' ): self . config_path = os . path . expanduser ( config_path ) self . load () def get ( self , key ): \"\"\" Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. So if the configuration contents are: self.data = { 'first': { 'second': 'value' }, } self.data.get('first.second') == 'value' Arguments: key(str): Configuration key to fetch \"\"\" keys = key . split ( '.' ) value = self . data . copy () for key in keys : value = value [ key ] return value def load ( self ): \"\"\" Loads configuration from configuration YAML file. \"\"\" try : with open ( os . path . expanduser ( self . config_path ), 'r' ) as f : try : self . data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( self . config_path ) ) sys . exit ( 1 ) def save ( self ): \"\"\" Saves configuration in the configuration YAML file. \"\"\" with open ( os . path . expanduser ( self . config_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( self . data , f ) We use ruamel PyYAML implementation to preserve the file comments. That class is meant to be loaded in the main __init__.py file, below the logging configuration (if there is any). Variables to substitute: program_name config_environmental_variable : The optional environmental variable where the path to the configuration is set. For example PYDO_CONFIG . This will be used in the tests to override the default path. We don't load this configuration from the program argument parser because it's definition often depends on the config file. File {{ program_name}}/ init .py # ... # (optional logging definition) import os from {{ program_name }} . configuration import Config config = Config ( os . getenv ( '{{ config_environmental_variable }}' , '~/.local/share/{{ program_name }}/config.yaml' )) # (Rest of the program) # ... If you want to use the config object in a module of your program, import them from the above file like this: File {{ program_name }}/cli.py from {{ program_name }} import config","title":"Code"},{"location":"coding/python/python_config_yaml/#tests","text":"I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the conftest.py file that gets executed by pytest in the tests setup. Variables to substitute: config_environmental_variable : Same as the one defined in the last section. File tests/conftest.py import os os . environ [{{ config_environmental_variable }}] = 'assets/config.yaml' Variables to substitute: program_name As it's really dependent in the config structure, you can improve the test_config_load test to make it more meaningful. File tests/unit/test_configuration.py from {{ program_name }} . configuration import Config from unittest.mock import patch from ruamel.yaml.scanner import ScannerError import os import pytest import shutil import tempfile class TestConfig : \"\"\" Class to test the Config object. Public attributes: config (Config object): Config object to test \"\"\" @pytest . fixture ( autouse = True ) def setup ( self ): self . config_path = 'assets/config.yaml' self . log_patch = patch ( '{{ program_name }}.configuration.log' , autospect = True ) self . log = self . log_patch . start () self . sys_patch = patch ( '{{ program_name }}.configuration.sys' , autospect = True ) self . sys = self . sys_patch . start () self . config = Config ( self . config_path ) yield 'setup' self . log_patch . stop () self . sys_patch . stop () def test_get_can_fetch_nested_items_with_dots ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config . get ( 'first.second' ) == 'value' def test_config_can_fetch_nested_items_with_dictionary_notation ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config [ 'first' ][ 'second' ] == 'value' def test_config_load ( self ): self . config . load () assert len ( self . config . data ) > 0 @patch ( '{{ program_name }}.configuration.YAML' ) def test_load_handles_wrong_file_format ( self , yamlMock ): yamlMock . return_value . load . side_effect = ScannerError ( 'error' , '' , 'problem' , 'mark' , ) self . config . load () self . log . error . assert_called_once_with ( 'Error parsing yaml of configuration file mark: problem' ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.open' ) def test_load_handles_file_not_found ( self , openMock ): openMock . side_effect = FileNotFoundError () self . config . load () self . log . error . assert_called_once_with ( 'Error opening configuration file {} ' . format ( self . config_path ) ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.Config.load' ) def test_init_calls_config_load ( self , loadMock ): Config () loadMock . assert_called_once_with () def test_save_config ( self ): tmp = tempfile . mkdtemp () save_file = os . path . join ( tmp , 'yaml_save_test.yaml' ) self . config = Config ( save_file ) self . config . data = { 'a' : 'b' } self . config . save () with open ( save_file , 'r' ) as f : assert \"a:\" in f . read () shutil . rmtree ( tmp )","title":"Tests"},{"location":"coding/python/python_config_yaml/#installation","text":"It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file. Variables to substitute in both files: program_name File setup.py import shutil ... Class PostInstallCommand ( install ): ... def run ( self ): install . run ( self ) try : data_directory = os . path . expanduser ( \"~/.local/share/{{ program_name }}\" ) os . makedirs ( data_directory ) log . info ( \"Data directory created\" ) except FileExistsError : log . info ( \"Data directory already exits\" ) config_path = os . path . join ( data_directory , 'config.yaml' ) if os . path . isfile ( config_path ) and os . access ( config_path , os . R_OK ): log . info ( \"Configuration file already exists, check the documentation \" \"for the new version changes.\" ) else : shutil . copyfile ( 'assets/config.yaml' , config_path ) log . info ( \"Copied default configuration template\" ) README.md ... {{ program_name }} configuration is done through the yaml file located at ~/.local/share/{{ program_name }}/config.yaml . The default template is provided at installation time. ... It's also necessary to add the ruamel.yaml pip package to your setup.py and requirements.txt files.","title":"Installation"},{"location":"coding/python/python_project_template/","text":"It's hard to correctly define the directory structure to make python programs work as expected. Even more if testing, documentation or databases are involved. Basic Python project \u00b6 Create virtualenv mkdir {{ project_directory }} mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository workon {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration' Project types \u00b6 Depending on the type of project you want to build there are different layouts: Command-line program . A single Flask web application . Multiple interconnected Flask microservices . Additional configurations \u00b6 Once the basic project structure is defined, there are several common enhancements to be applied: Continuous integration pipelines Create the documentation repository Configure SQLAlchemy to use the MariaDB/Mysql backend Configure Docker and Docker compose to host the application Load config from YAML Configure a Flask project References \u00b6 ionel packaging a python library post","title":"Project Template"},{"location":"coding/python/python_project_template/#basic-python-project","text":"Create virtualenv mkdir {{ project_directory }} mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository workon {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration'","title":"Basic Python project"},{"location":"coding/python/python_project_template/#project-types","text":"Depending on the type of project you want to build there are different layouts: Command-line program . A single Flask web application . Multiple interconnected Flask microservices .","title":"Project types"},{"location":"coding/python/python_project_template/#additional-configurations","text":"Once the basic project structure is defined, there are several common enhancements to be applied: Continuous integration pipelines Create the documentation repository Configure SQLAlchemy to use the MariaDB/Mysql backend Configure Docker and Docker compose to host the application Load config from YAML Configure a Flask project","title":"Additional configurations"},{"location":"coding/python/python_project_template/#references","text":"ionel packaging a python library post","title":"References"},{"location":"coding/python/redis-py/","text":"Redis-py is The Python interface to the Redis key-value store. The library encapsulates an actual TCP connection to a Redis server and sends raw commands, as bytes serialized using the REdis Serialization Protocol (RESP) , to the server. It then takes the raw reply and parses it back into a Python object such as bytes, int, or even datetime.datetime. Installation \u00b6 pip install redis Usage \u00b6 import redis r = redis . Redis ( host = 'localhost' , port = 6379 , db = 0 , password = None , socket_timeout = None , ) The arguments specified above are the default ones, so it's the same as calling r = redis.Redis() . The db parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default. Common pitfalls \u00b6 Redis returned objects are bytes type, so you may need to convert it to string with r.get(\"Bahamas\").decode(\"utf-8\") . References \u00b6 Real Python introduction to Redis-py Git Docs : Very technical and small.","title":"Redis-py"},{"location":"coding/python/redis-py/#installation","text":"pip install redis","title":"Installation"},{"location":"coding/python/redis-py/#usage","text":"import redis r = redis . Redis ( host = 'localhost' , port = 6379 , db = 0 , password = None , socket_timeout = None , ) The arguments specified above are the default ones, so it's the same as calling r = redis.Redis() . The db parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default.","title":"Usage"},{"location":"coding/python/redis-py/#common-pitfalls","text":"Redis returned objects are bytes type, so you may need to convert it to string with r.get(\"Bahamas\").decode(\"utf-8\") .","title":"Common pitfalls"},{"location":"coding/python/redis-py/#references","text":"Real Python introduction to Redis-py Git Docs : Very technical and small.","title":"References"},{"location":"coding/python/requests_mock/","text":"The requests-mock library is a requests transport adapter that can be preloaded with responses that are returned if certain URIs are requested. This is particularly useful in unit tests where you want to return known responses from HTTP requests without making actual calls. Installation \u00b6 pip install requests-mock Usage \u00b6 Object initialization \u00b6 Select one of the following ways to initialize the mock. As a pytest fixture \u00b6 The ease of use with pytest it is awesome. requests-mock provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition. >>> import pytest >>> import requests >>> def test_url ( requests_mock ): ... requests_mock . get ( 'http://test.com' , text = 'data' ) ... assert 'data' == requests . get ( 'http://test.com' ) . text ... As a function decorator \u00b6 >>> @requests_mock . Mocker () ... def test_function ( m ): ... m . get ( 'http://test.com' , text = 'resp' ) ... return requests . get ( 'http://test.com' ) . text ... >>> test_function () 'resp' As a context manager \u00b6 >>> import requests >>> import requests_mock >>> with requests_mock . Mocker () as m : ... m . get ( 'http://test.com' , text = 'resp' ) ... requests . get ( 'http://test.com' ) . text ... 'resp' Mocking responses \u00b6 Return a json \u00b6 requests_mock . get ( ' {} /api/repos/owner/repository/builds' . format ( self . url ), json = { \"id\" : 882 , \"number\" : 209 , \"finished\" : 1591197904 , }, ) Multiple responses \u00b6 Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list. requests_mock . get ( 'mock://test.com/4' , [ { 'text' : 'resp1' , 'status_code' : 300 }, { 'text' : 'resp2' , 'status_code' : 200 } ] ) Get requests history \u00b6 Called \u00b6 The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property. >>> import requests >>> import requests_mock >>> with requests_mock . mock () as m : ... m . get ( 'http://test.com, text=' resp ') ... resp = requests . get ( 'http://test.com' ) ... >>> m . called True >>> m . call_count 1 Requests history \u00b6 The history of objects that passed through the mocker/adapter can also be retrieved. >>> history = m . request_history >>> len ( history ) 1 >>> history [ 0 ] . method 'GET' >>> history [ 0 ] . url 'http://test.com/' References \u00b6 Docs Git","title":"Requests-mock"},{"location":"coding/python/requests_mock/#installation","text":"pip install requests-mock","title":"Installation"},{"location":"coding/python/requests_mock/#usage","text":"","title":"Usage"},{"location":"coding/python/requests_mock/#object-initialization","text":"Select one of the following ways to initialize the mock.","title":"Object initialization"},{"location":"coding/python/requests_mock/#as-a-pytest-fixture","text":"The ease of use with pytest it is awesome. requests-mock provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition. >>> import pytest >>> import requests >>> def test_url ( requests_mock ): ... requests_mock . get ( 'http://test.com' , text = 'data' ) ... assert 'data' == requests . get ( 'http://test.com' ) . text ...","title":"As a pytest fixture"},{"location":"coding/python/requests_mock/#as-a-function-decorator","text":">>> @requests_mock . Mocker () ... def test_function ( m ): ... m . get ( 'http://test.com' , text = 'resp' ) ... return requests . get ( 'http://test.com' ) . text ... >>> test_function () 'resp'","title":"As a function decorator"},{"location":"coding/python/requests_mock/#as-a-context-manager","text":">>> import requests >>> import requests_mock >>> with requests_mock . Mocker () as m : ... m . get ( 'http://test.com' , text = 'resp' ) ... requests . get ( 'http://test.com' ) . text ... 'resp'","title":"As a context manager"},{"location":"coding/python/requests_mock/#mocking-responses","text":"","title":"Mocking responses"},{"location":"coding/python/requests_mock/#return-a-json","text":"requests_mock . get ( ' {} /api/repos/owner/repository/builds' . format ( self . url ), json = { \"id\" : 882 , \"number\" : 209 , \"finished\" : 1591197904 , }, )","title":"Return a json"},{"location":"coding/python/requests_mock/#multiple-responses","text":"Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list. requests_mock . get ( 'mock://test.com/4' , [ { 'text' : 'resp1' , 'status_code' : 300 }, { 'text' : 'resp2' , 'status_code' : 200 } ] )","title":"Multiple responses"},{"location":"coding/python/requests_mock/#get-requests-history","text":"","title":"Get requests history"},{"location":"coding/python/requests_mock/#called","text":"The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property. >>> import requests >>> import requests_mock >>> with requests_mock . mock () as m : ... m . get ( 'http://test.com, text=' resp ') ... resp = requests . get ( 'http://test.com' ) ... >>> m . called True >>> m . call_count 1","title":"Called"},{"location":"coding/python/requests_mock/#requests-history","text":"The history of objects that passed through the mocker/adapter can also be retrieved. >>> history = m . request_history >>> len ( history ) 1 >>> history [ 0 ] . method 'GET' >>> history [ 0 ] . url 'http://test.com/'","title":"Requests history"},{"location":"coding/python/requests_mock/#references","text":"Docs Git","title":"References"},{"location":"coding/python/rq/","text":"RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry. Getting started \u00b6 Assuming that a Redis server is running, define the function you want to run: import requests def count_words_at_url ( url ): resp = requests . get ( url ) return len ( resp . text . split ()) The, create a RQ queue: from redis import Redis from rq import Queue q = Queue ( connection = Redis ()) And enqueue the function call: from my_module import count_words_at_url result = q . enqueue ( count_words_at_url , 'http://nvie.com' ) To start executing enqueued function calls in the background, start a worker from your project\u2019s directory: $ rq worker *** Listening for work on default Got count_words_at_url ( 'http://nvie.com' ) from default Job result = 818 *** Listening for work on default Install \u00b6 pip install rq Reference \u00b6 Homepage Git Docs","title":"Rq"},{"location":"coding/python/rq/#getting-started","text":"Assuming that a Redis server is running, define the function you want to run: import requests def count_words_at_url ( url ): resp = requests . get ( url ) return len ( resp . text . split ()) The, create a RQ queue: from redis import Redis from rq import Queue q = Queue ( connection = Redis ()) And enqueue the function call: from my_module import count_words_at_url result = q . enqueue ( count_words_at_url , 'http://nvie.com' ) To start executing enqueued function calls in the background, start a worker from your project\u2019s directory: $ rq worker *** Listening for work on default Got count_words_at_url ( 'http://nvie.com' ) from default Job result = 818 *** Listening for work on default","title":"Getting started"},{"location":"coding/python/rq/#install","text":"pip install rq","title":"Install"},{"location":"coding/python/rq/#reference","text":"Homepage Git Docs","title":"Reference"},{"location":"coding/python/ruamel_yaml/","text":"ruamel.yaml is a YAML 1.2 loader/dumper package for Python. It is a derivative of Kirill Simonov\u2019s PyYAML 3.11. It has the following enhancements: Comments. Block style and key ordering are kept, so you can diff the round-tripped source. Flow style sequences ( \u2018a: b, c, d\u2019). Anchor names that are hand-crafted (i.e. not of the form idNNN ). Merges in dictionaries are preserved. Installation \u00b6 pip install ruamel.yaml Usage \u00b6 Very similar to PyYAML. If invoked with YAML(typ='safe') either the load or the write of the data, the comments of the yaml will be lost. Load from file \u00b6 from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError try : with open ( os . path . expanduser ( file_path ), 'r' ) as f : try : data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( file_path ) ) sys . exit ( 1 ) Save to file \u00b6 with open ( os . path . expanduser ( file_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( data , f ) References \u00b6 Docs Code","title":"Ruamel YAML"},{"location":"coding/python/ruamel_yaml/#installation","text":"pip install ruamel.yaml","title":"Installation"},{"location":"coding/python/ruamel_yaml/#usage","text":"Very similar to PyYAML. If invoked with YAML(typ='safe') either the load or the write of the data, the comments of the yaml will be lost.","title":"Usage"},{"location":"coding/python/ruamel_yaml/#load-from-file","text":"from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError try : with open ( os . path . expanduser ( file_path ), 'r' ) as f : try : data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( file_path ) ) sys . exit ( 1 )","title":"Load from file"},{"location":"coding/python/ruamel_yaml/#save-to-file","text":"with open ( os . path . expanduser ( file_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( data , f )","title":"Save to file"},{"location":"coding/python/ruamel_yaml/#references","text":"Docs Code","title":"References"},{"location":"coding/python/sh/","text":"sh is a full-fledged subprocess replacement so beautiful that makes you want to cry. It allows you to call any program as if it were a function: from sh import ifconfig print ( ifconfig ( \"wlan0\" )) Output: wlan0 Link encap : Ethernet HWaddr 00 : 00 : 00 : 00 : 00 : 00 inet addr : 192.168 . 1.100 Bcast : 192.168 . 1.255 Mask : 255.255 . 255.0 inet6 addr : ffff :: ffff : ffff : ffff : fff / 64 Scope : Link UP BROADCAST RUNNING MULTICAST MTU : 1500 Metric : 1 RX packets : 0 errors : 0 dropped : 0 overruns : 0 frame : 0 TX packets : 0 errors : 0 dropped : 0 overruns : 0 carrier : 0 collisions : 0 txqueuelen : 1000 RX bytes : 0 ( 0 GB ) TX bytes : 0 ( 0 GB ) Note that these aren't Python functions, these are running the binary commands on your system by dynamically resolving your $PATH, much like Bash does, and then wrapping the binary in a function. In this way, all the programs on your system are available to you from within Python. Usage \u00b6 Passing arguments \u00b6 sh . ls ( \"-l\" , \"/tmp\" , color = \"never\" ) If the command gives you a syntax error (like pass ), you can use bash. sh . bash ( \"-c\" , \"pass\" ) Handling exceptions \u00b6 Normal processes exit with exit code 0. Access the program return code with RunningCommand.exit_code : output = ls ( \"/\" ) print ( output . exit_code ) # should be 0 If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class ErrorReturnCode : try : print ( ls ( \"/some/non-existant/folder\" )) except sh . ErrorReturnCode_2 : print ( \"folder doesn't exist!\" ) create_the_folder () except sh . ErrorReturnCode : print ( \"unknown error\" ) The exception object has the stderr and stdout bytes attributes with the errors, to show them use: except sh . ErrorReturnCode as e : print ( str ( e . stderr , 'utf8' )) Redirecting output \u00b6 sh . ifconfig ( _out = \"/tmp/interfaces\" ) References \u00b6 Docs Git","title":"sh"},{"location":"coding/python/sh/#usage","text":"","title":"Usage"},{"location":"coding/python/sh/#passing-arguments","text":"sh . ls ( \"-l\" , \"/tmp\" , color = \"never\" ) If the command gives you a syntax error (like pass ), you can use bash. sh . bash ( \"-c\" , \"pass\" )","title":"Passing arguments"},{"location":"coding/python/sh/#handling-exceptions","text":"Normal processes exit with exit code 0. Access the program return code with RunningCommand.exit_code : output = ls ( \"/\" ) print ( output . exit_code ) # should be 0 If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class ErrorReturnCode : try : print ( ls ( \"/some/non-existant/folder\" )) except sh . ErrorReturnCode_2 : print ( \"folder doesn't exist!\" ) create_the_folder () except sh . ErrorReturnCode : print ( \"unknown error\" ) The exception object has the stderr and stdout bytes attributes with the errors, to show them use: except sh . ErrorReturnCode as e : print ( str ( e . stderr , 'utf8' ))","title":"Handling exceptions"},{"location":"coding/python/sh/#redirecting-output","text":"sh . ifconfig ( _out = \"/tmp/interfaces\" )","title":"Redirecting output"},{"location":"coding/python/sh/#references","text":"Docs Git","title":"References"},{"location":"coding/python/sqlalchemy/","text":"SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. Creating an SQL Schema \u00b6 First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward. Creating Tables \u00b6 If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object. class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . Text: long_text = Column(Text) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example String(16) . For reference this are the common lengths: url: 2083 name: 64 (it occupies the same 2 and 255). email: 64 (it occupies the same 2 and 255). username: 64 (it occupies the same 2 and 255). Creating relationships \u00b6 Joined table inheritance \u00b6 In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class. class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , } One to many \u00b6 from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'user' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey . Self referenced one to many \u00b6 class Task ( Base ): __tablename__ = 'task' id = Column ( String , primary_key = True , doc = 'fulid of creation' ) parent_id = Column ( String , ForeignKey ( 'task.id' )) parent = relationship ( 'Task' , remote_side = [ id ], backref = 'children' ) Many to many \u00b6 # Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , ) Self referenced many to many \u00b6 Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side. Testing SQLAlchemy Code \u00b6 The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest . Exporting database to json \u00b6 import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result ) Cloning an SQLAlchemy object \u00b6 The following function: Copies all the non-primary-key columns from the input model to a new model instance. Allows definition of specific arguments. Leaves the original model object unmodified. def clone_model ( model , ** kwargs ): \"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\" table = model . __table__ non_primary_key_columns = [ column_name for column_name in table . __mapper__ . attrs .. keys () if column_name not in table . primary_key ] data = { column_name : getattr ( model , column_name ) for column_name in non_pk_columns } data . update ( kwargs ) return model . __class__ ( ** data ) References \u00b6 Home Docs","title":"SQLAlchemy"},{"location":"coding/python/sqlalchemy/#creating-an-sql-schema","text":"First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward.","title":"Creating an SQL Schema"},{"location":"coding/python/sqlalchemy/#creating-tables","text":"If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object. class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . Text: long_text = Column(Text) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example String(16) . For reference this are the common lengths: url: 2083 name: 64 (it occupies the same 2 and 255). email: 64 (it occupies the same 2 and 255). username: 64 (it occupies the same 2 and 255).","title":"Creating Tables"},{"location":"coding/python/sqlalchemy/#creating-relationships","text":"","title":"Creating relationships"},{"location":"coding/python/sqlalchemy/#joined-table-inheritance","text":"In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class. class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , }","title":"Joined table inheritance"},{"location":"coding/python/sqlalchemy/#one-to-many","text":"from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'user' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey .","title":"One to many"},{"location":"coding/python/sqlalchemy/#self-referenced-one-to-many","text":"class Task ( Base ): __tablename__ = 'task' id = Column ( String , primary_key = True , doc = 'fulid of creation' ) parent_id = Column ( String , ForeignKey ( 'task.id' )) parent = relationship ( 'Task' , remote_side = [ id ], backref = 'children' )","title":"Self referenced one to many"},{"location":"coding/python/sqlalchemy/#many-to-many","text":"# Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , )","title":"Many to many"},{"location":"coding/python/sqlalchemy/#self-referenced-many-to-many","text":"Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side.","title":"Self referenced many to many"},{"location":"coding/python/sqlalchemy/#testing-sqlalchemy-code","text":"The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest .","title":"Testing SQLAlchemy Code"},{"location":"coding/python/sqlalchemy/#exporting-database-to-json","text":"import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result )","title":"Exporting database to json"},{"location":"coding/python/sqlalchemy/#cloning-an-sqlalchemy-object","text":"The following function: Copies all the non-primary-key columns from the input model to a new model instance. Allows definition of specific arguments. Leaves the original model object unmodified. def clone_model ( model , ** kwargs ): \"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\" table = model . __table__ non_primary_key_columns = [ column_name for column_name in table . __mapper__ . attrs .. keys () if column_name not in table . primary_key ] data = { column_name : getattr ( model , column_name ) for column_name in non_pk_columns } data . update ( kwargs ) return model . __class__ ( ** data )","title":"Cloning an SQLAlchemy object"},{"location":"coding/python/sqlalchemy/#references","text":"Home Docs","title":"References"},{"location":"coding/python/tinydb/","text":"Tinydb is a document oriented database that stores data in a json file. It's the closest solution to a NoSQL SQLite solution that I've found. The advantages are that you can use a NoSQL database without installing a server. Tinydb is small, simple to use, well tested, optimized and extensible . On the other hand, if you are searching for advanced database features like more than one connection or high performance, you should consider using databases like SQLite or MongoDB. I think it's the perfect solution for initial versions of a program, when the database schema is variable and there is no need of high performance. Once the program is stabilized and the performance drops, you can change the storage provider to a production ready one. To make this change doable, I recommend implementing the repository pattern to decouple the storage layer from your application logic. Install \u00b6 pip install tinydb Basic usage \u00b6 TL;DR: Operation Cheatsheet Inserting data : db.insert(...) . Getting data : db.all() : Get all documents. iter(db) : Iterate over all the documents. db.search(query) : Get a list of documents matching the query. Updating : db.update(fields, query) : Update all documents matching the query to contain fields. Removing : db.remove(query) : Remove all documents matching the query. db.truncate() : Remove all documents. Querying : Query() : Create a new query object. Query().field == 2 : Match any document that has a key field with value == 2 (also possible: != , > , >= , < , <= ). First you need to setup the database: from tinydb import TinyDB , Query db = TinyDB ( 'db.json' ) TinyDB expects the data to be Python dictionaries: db . insert ({ 'type' : 'apple' , 'count' : 7 }) db . insert ({ 'type' : 'peach' , 'count' : 3 }) You can also iterate over stored documents: >>> for item in db : >>> print ( item ) { 'count' : 7 , 'type' : 'apple' } { 'count' : 3 , 'type' : 'peach' } You can search for specific documents: >>> Fruit = Query () >>> db . search ( Fruit . type == 'peach' ) [{ 'count' : 3 , 'type' : 'peach' }] >>> db . search ( Fruit . count > 5 ) [{ 'count' : 7 , 'type' : 'apple' }] You can update fields: >>> db . update ({ 'count' : 10 }, Fruit . type == 'apple' ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }, { 'count' : 3 , 'type' : 'peach' }] And remove documents: >>> db . remove ( Fruit . count < 5 ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }] Query construction \u00b6 Match any document where a field called field exists: Query().field.exists() . Match any document with the whole field matching the regular expression: Query().field.matches(regex) . Match any document with a substring of the field matching the regular expression: Query().field.search(regex) . Match any document for which the function returns True : Query().field.test(func, *args) . If given a query, match all documents where all documents in the list field match the query. If given a list, matches all documents where all documents in the list field are a member of the given list: Query().field.all(query | list) . If given a query, match all documents where at least one document in the list field match the query. If given a list, matches all documents where at least one documents in the list field are a member of the given list: Query().field.any(query | list) . Match if the field is contained in the list: Query().field.one_of(list) . Logical operations on queries Match documents that don't match the query: ~ (query) . Match documents that match both queries: (query1) & (query2) . Match documents that match at least one of the queries: (query1) | (query2) . To retrieve the data from the database, you need to use Query objects in a similar way as you do with ORMs. from tinydb import Query User = Query () db . search ( User . name == 'John' ) db . search ( User . birthday . year == 1990 ) If the field is not a valid Python identifier use the following syntax: db . search ( User [ 'country-code' ] == 'foo' ) Advanced queries \u00b6 TinyDB supports other ways to search in your data: Testing the existence of a field: db . search ( User . name . exists ()) Testing values against regular expressions: # Full item has to match the regex: db . search ( User . name . matches ( '[aZ]*' )) # Any part of the item has to match the regex: db . search ( User . name . search ( 'b+' )) Testing using custom tests: # Custom test: test_func = lambda s : s == 'John' db . search ( User . name . test ( test_func )) # Custom test with parameters: def test_func ( val , m , n ): return m <= val <= n db . search ( User . age . test ( test_func , 0 , 21 )) db . search ( User . age . test ( test_func , 21 , 99 )) Testing fields that contain lists with the any and all methods: Assuming we have a user object with a groups list like this: db . insert ({ 'name' : 'user1' , 'groups' : [ 'user' ]}) db . insert ({ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}) db . insert ({ 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}) You can use the following queries: # User's groups include at least one value from ['admin', 'sudo'] >>> db . search ( User . groups . any ([ 'admin' , 'sudo' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}, { 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}] # User's groups include all values from ['admin', 'user'] >>> db . search ( User . groups . all ([ 'admin' , 'user' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}] Testing nested queries: Assuming we have the following table: Group = Query () Permission = Query () groups = db . table ( 'groups' ) groups . insert ({ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}) groups . insert ({ 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}) groups . insert ({ 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}) You can search this table using nested any / all queries: # Group has a permission with type 'read' >>> groups . search ( Group . permissions . any ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}, { 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}, { 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}] # Group has ONLY permission 'read' >>> groups . search ( Group . permissions . all ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}] any tests if there is at least one document matching the query while all ensures all documents match the query. The opposite operation, checking if a list contains a single item, is also possible using one_of : >>> db . search ( User . name . one_of ([ 'jane' , 'john' ])) Query modifiers \u00b6 TinyDB allows you to use logical operations to change and combine queries Negate a query: db.search(~ (User.name == 'John')) . Logical AND : db.search((User.name == 'John') & (User.age <= 30)) . Logical OR : db.search((User.name == 'John') | (User.name == 'Bob')) . Inserting more than one document \u00b6 In case you want to insert more than one document, you can use db.insert_multiple(...) : >>> db . insert_multiple ([ { 'name' : 'John' , 'age' : 22 }, { 'name' : 'John' , 'age' : 37 }]) >>> db . insert_multiple ({ 'int' : 1 , 'value' : i } for i in range ( 2 )) Updating data \u00b6 To update all the documents of the database, leave out the query argument: db . update ({ 'foo' : 'bar' }) When you pass a dictionary to db.update(fields, query) , you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data: delete(key) : Delete a key from the document. increment(key) : Increment the value of a key. decrement(key) : Decrement the value of a key. add(key, value) : Add value to the value of a key (also works for strings). subtract(key, value) : Subtract value from the value of a key. set(key, value) : Set key to value. >>> from tinydb.operations import delete >>> db . update ( delete ( 'key1' ), User . name == 'John' ) This will remove the key key1 from all matching documents. You also can write your own operations: >>> def your_operation ( your_arguments ): ... def transform ( doc ): ... # do something with the document ... # ... ... return transform ... >>> db . update ( your_operation ( arguments ), query ) Retrieving data \u00b6 If you want to get one element use db.get(...) . Be warned, if more than one document match the query, a random will be returned. >>> db . get ( User . name == 'John' ) { 'name' : 'John' , 'age' : 22 } If you want to know if the database stores a document, use db.contains(...) . >>> db . contains ( User . name == 'John' ) True If you want to know the number of documents that match a query use db.count(...) . >>> db . count ( User . name == 'John' ) 2 Tables \u00b6 TinyDB supports working with more than one table. To create and use a table, use db.table(name) . They behave as the TinyDB class. >>> table = db . table ( 'table_name' ) >>> table . insert ({ 'value' : True }) >>> table . all () [{ 'value' : True }] >>> for row in table : >>> print ( row ) { 'value' : True } To remove a table from a database, use: db . drop_table ( 'table_name' ) To remove all tables, use: db . drop_tables () To get a list with the names of all tables in your database: >>> db . tables () { '_default' , 'table_name' } Query caching \u00b6 TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the table(...) function: table = db . table ( 'table_name' , cache_size = 30 ) You can set cache_size to None to make the cache unlimited in size. Also, you can set cache_size to 0 to disable it. Storage types \u00b6 TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it: from tinydb import TinyDB , where db = TinyDB ( 'path/to/db.json' ) To use the in-memory storage, use: from tinydb.storages import MemoryStorage db = TinyDB ( storage = MemoryStorage ) All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s json.dump(\u2026) method. For example, you can set it to create prettified JSON files like this: >>> db = TinyDB ( 'db.json' , sort_keys = True , indent = 4 , separators = ( ',' , ': ' )) References \u00b6 Docs Git Issues","title":"TinyDB"},{"location":"coding/python/tinydb/#install","text":"pip install tinydb","title":"Install"},{"location":"coding/python/tinydb/#basic-usage","text":"TL;DR: Operation Cheatsheet Inserting data : db.insert(...) . Getting data : db.all() : Get all documents. iter(db) : Iterate over all the documents. db.search(query) : Get a list of documents matching the query. Updating : db.update(fields, query) : Update all documents matching the query to contain fields. Removing : db.remove(query) : Remove all documents matching the query. db.truncate() : Remove all documents. Querying : Query() : Create a new query object. Query().field == 2 : Match any document that has a key field with value == 2 (also possible: != , > , >= , < , <= ). First you need to setup the database: from tinydb import TinyDB , Query db = TinyDB ( 'db.json' ) TinyDB expects the data to be Python dictionaries: db . insert ({ 'type' : 'apple' , 'count' : 7 }) db . insert ({ 'type' : 'peach' , 'count' : 3 }) You can also iterate over stored documents: >>> for item in db : >>> print ( item ) { 'count' : 7 , 'type' : 'apple' } { 'count' : 3 , 'type' : 'peach' } You can search for specific documents: >>> Fruit = Query () >>> db . search ( Fruit . type == 'peach' ) [{ 'count' : 3 , 'type' : 'peach' }] >>> db . search ( Fruit . count > 5 ) [{ 'count' : 7 , 'type' : 'apple' }] You can update fields: >>> db . update ({ 'count' : 10 }, Fruit . type == 'apple' ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }, { 'count' : 3 , 'type' : 'peach' }] And remove documents: >>> db . remove ( Fruit . count < 5 ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }]","title":"Basic usage"},{"location":"coding/python/tinydb/#query-construction","text":"Match any document where a field called field exists: Query().field.exists() . Match any document with the whole field matching the regular expression: Query().field.matches(regex) . Match any document with a substring of the field matching the regular expression: Query().field.search(regex) . Match any document for which the function returns True : Query().field.test(func, *args) . If given a query, match all documents where all documents in the list field match the query. If given a list, matches all documents where all documents in the list field are a member of the given list: Query().field.all(query | list) . If given a query, match all documents where at least one document in the list field match the query. If given a list, matches all documents where at least one documents in the list field are a member of the given list: Query().field.any(query | list) . Match if the field is contained in the list: Query().field.one_of(list) . Logical operations on queries Match documents that don't match the query: ~ (query) . Match documents that match both queries: (query1) & (query2) . Match documents that match at least one of the queries: (query1) | (query2) . To retrieve the data from the database, you need to use Query objects in a similar way as you do with ORMs. from tinydb import Query User = Query () db . search ( User . name == 'John' ) db . search ( User . birthday . year == 1990 ) If the field is not a valid Python identifier use the following syntax: db . search ( User [ 'country-code' ] == 'foo' )","title":"Query construction"},{"location":"coding/python/tinydb/#advanced-queries","text":"TinyDB supports other ways to search in your data: Testing the existence of a field: db . search ( User . name . exists ()) Testing values against regular expressions: # Full item has to match the regex: db . search ( User . name . matches ( '[aZ]*' )) # Any part of the item has to match the regex: db . search ( User . name . search ( 'b+' )) Testing using custom tests: # Custom test: test_func = lambda s : s == 'John' db . search ( User . name . test ( test_func )) # Custom test with parameters: def test_func ( val , m , n ): return m <= val <= n db . search ( User . age . test ( test_func , 0 , 21 )) db . search ( User . age . test ( test_func , 21 , 99 )) Testing fields that contain lists with the any and all methods: Assuming we have a user object with a groups list like this: db . insert ({ 'name' : 'user1' , 'groups' : [ 'user' ]}) db . insert ({ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}) db . insert ({ 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}) You can use the following queries: # User's groups include at least one value from ['admin', 'sudo'] >>> db . search ( User . groups . any ([ 'admin' , 'sudo' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}, { 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}] # User's groups include all values from ['admin', 'user'] >>> db . search ( User . groups . all ([ 'admin' , 'user' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}] Testing nested queries: Assuming we have the following table: Group = Query () Permission = Query () groups = db . table ( 'groups' ) groups . insert ({ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}) groups . insert ({ 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}) groups . insert ({ 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}) You can search this table using nested any / all queries: # Group has a permission with type 'read' >>> groups . search ( Group . permissions . any ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}, { 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}, { 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}] # Group has ONLY permission 'read' >>> groups . search ( Group . permissions . all ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}] any tests if there is at least one document matching the query while all ensures all documents match the query. The opposite operation, checking if a list contains a single item, is also possible using one_of : >>> db . search ( User . name . one_of ([ 'jane' , 'john' ]))","title":"Advanced queries"},{"location":"coding/python/tinydb/#query-modifiers","text":"TinyDB allows you to use logical operations to change and combine queries Negate a query: db.search(~ (User.name == 'John')) . Logical AND : db.search((User.name == 'John') & (User.age <= 30)) . Logical OR : db.search((User.name == 'John') | (User.name == 'Bob')) .","title":"Query modifiers"},{"location":"coding/python/tinydb/#inserting-more-than-one-document","text":"In case you want to insert more than one document, you can use db.insert_multiple(...) : >>> db . insert_multiple ([ { 'name' : 'John' , 'age' : 22 }, { 'name' : 'John' , 'age' : 37 }]) >>> db . insert_multiple ({ 'int' : 1 , 'value' : i } for i in range ( 2 ))","title":"Inserting more than one document"},{"location":"coding/python/tinydb/#updating-data","text":"To update all the documents of the database, leave out the query argument: db . update ({ 'foo' : 'bar' }) When you pass a dictionary to db.update(fields, query) , you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data: delete(key) : Delete a key from the document. increment(key) : Increment the value of a key. decrement(key) : Decrement the value of a key. add(key, value) : Add value to the value of a key (also works for strings). subtract(key, value) : Subtract value from the value of a key. set(key, value) : Set key to value. >>> from tinydb.operations import delete >>> db . update ( delete ( 'key1' ), User . name == 'John' ) This will remove the key key1 from all matching documents. You also can write your own operations: >>> def your_operation ( your_arguments ): ... def transform ( doc ): ... # do something with the document ... # ... ... return transform ... >>> db . update ( your_operation ( arguments ), query )","title":"Updating data"},{"location":"coding/python/tinydb/#retrieving-data","text":"If you want to get one element use db.get(...) . Be warned, if more than one document match the query, a random will be returned. >>> db . get ( User . name == 'John' ) { 'name' : 'John' , 'age' : 22 } If you want to know if the database stores a document, use db.contains(...) . >>> db . contains ( User . name == 'John' ) True If you want to know the number of documents that match a query use db.count(...) . >>> db . count ( User . name == 'John' ) 2","title":"Retrieving data"},{"location":"coding/python/tinydb/#tables","text":"TinyDB supports working with more than one table. To create and use a table, use db.table(name) . They behave as the TinyDB class. >>> table = db . table ( 'table_name' ) >>> table . insert ({ 'value' : True }) >>> table . all () [{ 'value' : True }] >>> for row in table : >>> print ( row ) { 'value' : True } To remove a table from a database, use: db . drop_table ( 'table_name' ) To remove all tables, use: db . drop_tables () To get a list with the names of all tables in your database: >>> db . tables () { '_default' , 'table_name' }","title":"Tables"},{"location":"coding/python/tinydb/#query-caching","text":"TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the table(...) function: table = db . table ( 'table_name' , cache_size = 30 ) You can set cache_size to None to make the cache unlimited in size. Also, you can set cache_size to 0 to disable it.","title":"Query caching"},{"location":"coding/python/tinydb/#storage-types","text":"TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it: from tinydb import TinyDB , where db = TinyDB ( 'path/to/db.json' ) To use the in-memory storage, use: from tinydb.storages import MemoryStorage db = TinyDB ( storage = MemoryStorage ) All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s json.dump(\u2026) method. For example, you can set it to create prettified JSON files like this: >>> db = TinyDB ( 'db.json' , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ))","title":"Storage types"},{"location":"coding/python/tinydb/#references","text":"Docs Git Issues","title":"References"},{"location":"coding/python/python_project_template/python_cli_template/","text":"Create the tests directories mkdir -p tests/unit touch tests/__init__.py touch tests/unit/__init__.py Create the program module structure mkdir {{ program_name }} Create the program setup.py file from setuptools import find_packages , setup # Get the version from drode/version.py without importing the package exec ( compile ( open ( '{{ program_name }}/version.py' ) . read (), '{{ program_name }}/version.py' , 'exec' )) setup ( name = '{{ program_name }}' , version = __version__ , # noqa: F821 description = '{{ program_description }}' , author = '{{ author }}' , author_email = '{{ author_email }}' , license = 'GPLv3' , long_description = open ( 'README.md' ) . read (), packages = find_packages ( exclude = ( 'tests' ,)), package_data = { '{{ program_name }}' : [ 'migrations/*' , 'migrations/versions/*' , ]}, entry_points = { 'console_scripts' : [ '{{ program_name }} = {{ program_name }}:main' ]}, install_requires = [ ] ) Remember to fill up the install_requirements with the dependencies that need to be installed at installation time. Create the {{ program_name }}/version.py file with the following contents: __version__ = '1.0.0' This ugly way of loading the __version__ was stolen from youtube-dl, it loads and executes the version.py without loading the whole module. Solutions like from {{ program_name }}.version import __version__ will fail as it tries to load the whole module. Defining it in the setup.py file doesn't work either if you need to load the version in your program code. from setup.py import __version__ will also fail. The only problem with this approach is that as the __version__ is not defined in the code it will raise a Flake8 error, therefore the # noqa: F821 in the setup.py code. Create the requirements.txt file. It should contain the install_requirements in addition to the testing requirements such as: pytest pytest-cov Configure SQLAlchemy for projects without flask","title":"Command-line Project Template"},{"location":"coding/python/python_project_template/python_docker/","text":"Docker is a popular way to distribute applications. Assuming that you've set all required dependencies in the setup.py , we're going to create an image with these properties: Run by an unprivileged user : Create an unprivileged user with permissions to run our program. Robust to vulnerabilities: Don't use Alpine as it's known to react slow to new vulnerabilities. Use a base of Debian instead. Smallest possible: Use Docker multi build step. Create a builder Docker that will run pip install and copies the required executables to the final image. FROM python:3.8-slim-buster as base FROM base as builder RUN python -m venv /opt/venv # Make sure we use the virtualenv: ENV PATH = \"/opt/venv/bin: $PATH \" COPY . /app WORKDIR /app RUN pip install . FROM base COPY --from = builder /opt/venv /opt/venv RUN useradd -m myapp WORKDIR /home/myapp # Copy the required directories for your program to work. COPY --from = builder /root/.local/share/myapp /home/myapp/.local/share/myapp COPY --from = builder /app/myapp /home/myapp/myapp RUN chown -R myapp:myapp /home/myapp/.local USER myapp ENV PATH = \"/opt/venv/bin: $PATH \" ENTRYPOINT [ \"/opt/venv/bin/myapp\" ] If we need to use it with MariaDB or with Redis, the easiest way is to use docker-compose . version : '3.8' services : myapp : image : myapp:latest restart : always links : - db depends_on : - db environment : - AIRSS_DATABASE_URL=mysql+pymysql://myapp:supersecurepassword@db/myapp db : image : mariadb:latest restart : always environment : - MYSQL_USER=myapp - MYSQL_PASSWORD=supersecurepassword - MYSQL_DATABASE=myapp - MYSQL_ALLOW_EMPTY_PASSWORD=yes ports : - 3306:3306 command : - '--character-set-server=utf8mb4' - '--collation-server=utf8mb4_unicode_ci' volumes : - /data/myapp/mariadb:/var/lib/mysql The depends_on flag is not enough to ensure that the database is up when our application tries to connect. So we need to use external programs like wait-for-it . To use it, change the earlier Dockerfile to match these lines: ... FROM base RUN apt-get update && apt-get install -y \\ wait-for-it \\ && rm -rf /var/lib/apt/lists/* ... ENTRYPOINT [ \"/home/myapp/entrypoint.sh\" ] Where entrypoint.sh is something like: #!/bin/bash # Wait for the database to be up if [[ -n $DATABASE_URL ]] ; then wait-for-it db:3306 fi # Execute database migrations /opt/venv/bin/myapp install # Enter in daemon mode /opt/venv/bin/myapp daemon Remember to add the permissions to run the script: chmod +x entrypoint.sh","title":"Configure Docker to host the application"},{"location":"coding/python/python_project_template/python_docs/","text":"It's important to create the documentation at the same time as you code your project, otherwise you won't ever do it or you'll die trying. Right now I use mkdocs with Github Pages for the documentation. Follow the steps under Installation to configure it.","title":"Create the documentation repository"},{"location":"coding/python/python_project_template/python_flask_template/","text":"Flask is very flexible when it comes to define the project layout, as a result, there are several different approaches, which can be confusing if you're building your first application. Follow this template if you want an application that meets these requirements: Use SQLAlchemy as ORM. Use pytest as testing framework (instead of unittest). Sets a robust foundation for application growth. Set a clear defined project structure that can be used for frontend applications as well as backend APIs. Microservice friendly. I've crafted this template after studying the following projects: Miguel's Flask mega tutorial ( code ). Greb Obinna Flask-RESTPlus tutorial ( code ). Abhinav Suri Flask tutorial ( code ). Patrick's software blog project layout and pytest definition ( code ). Jaime Buelta Hands On Docker for Microservices with Python ( code ). Each has it's strengths and weaknesses: Project Alembic Pytest Complex app Friendly layout Strong points Miguel True False True False Has a book explaining the code Greb False False False False flask-restplus Abhinav True False True True flask-base Patrick False True False True pytest Jaime False True True False Microservices, CI, Kubernetes, logging, metrics I'm going to start with Abhinav base layout as it's the most clear and complete. Furthermore, it's based in flask-base , a simple Flask boilerplate app with SQLAlchemy, Redis, User Authentication, and more. Which can be used directly to start a frontend flask project. I won't use it for a backend API though. With that base layout, I'm going to take Patrick's pytest layout to configure the tests using pytest-flask , Greb flask-restplus code to create the API and Miguel's book to glue everything together. Finally, I'll follow Jaime's book to merge the different microservices into an integrated project. As well as defining the deployment process, CI definition, logging, metrics and integration with Kubernetes.","title":"Flask Project Template"},{"location":"coding/python/python_project_template/python_microservices_template/","text":"Follow this template if you want to build a project that meets these requirements: Based on Python Flask microservices. Easily expandable. Tested by unit, functional and integration tests through continuous integration pipelines. Deployed through uWSGI and Nginx dockers. Orchestrated through docker-compose or Kubernetes. Defining the project layout of a flask application is not easy , even less one with several","title":"Microservices Project Template"},{"location":"coding/python/python_project_template/python_sqlalchemy_mariadb/","text":"To use Mysql you'll need to first install (or add to your requirements) pymysql : pip install pymysql The url to connect to the database will be: 'mysql+pymysql:// {} : {} @ {} : {} / {} ' . format ( DB_USER , DB_PASS , DB_HOST , DB_PORT , DATABASE ) It's probable that you'll need to use UTF8 with multi byte , otherwise the addition of some strings into the database will fail. I've tried adding it to the database url without success. So I've modified the MariaDB Docker-compose section to use that character and collation set: services : db : image : mariadb:latest restart : always environment : - MYSQL_USER=xxxx - MYSQL_PASSWORD=xxxx - MYSQL_DATABASE=xxxx - MYSQL_ALLOW_EMPTY_PASSWORD=yes ports : - 3306:3306 command : - '--character-set-server=utf8mb4' - '--collation-server=utf8mb4_unicode_ci'","title":"Configure SQLAlchemy to use the MariaDB/Mysql backend"},{"location":"coding/python/python_project_template/python_sqlalchemy_without_flask/","text":"Install Alembic : pip install alembic It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations Create the basic models.py file under the project code. \"\"\" Module to store the models. Classes: Class_name: Class description. ... \"\"\" import os from sqlalchemy import \\ create_engine , \\ Column , \\ Integer from sqlalchemy.ext.declarative import declarative_base db_path = os . path . expanduser ( '{{ path_to_sqlite_file }}' ) engine = create_engine ( os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or 'sqlite:///' + db_path ) Base = declarative_base ( bind = engine ) class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) Create the migrations/env.py file as specified in the alembic article . Create the first alembic revision. alembic \\ -c {{ program_name }} /migrations/alembic.ini \\ revision \\ --autogenerate \\ -m \"Initial schema\" Set up the testing environment for SQLAlchemy","title":"Configure SQLAlchemy for projects without flask"},{"location":"coding/react/react/","text":"React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called \u201ccomponents\u201d. Set up a new project \u00b6 Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start . Start a React + Flask project \u00b6 Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run The basics \u00b6 Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components. Pass data between components \u00b6 Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... } Use of the state \u00b6 React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); } Share the state between parent and children \u00b6 To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components. Handling data change \u00b6 There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering. Function components \u00b6 Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); } Miscellaneous \u00b6 List rendering \u00b6 When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key . Links \u00b6 React tutorial Awesome React Awesome React components Responsive React \u00b6 Responsive react Responsive websites without css react-responsive library With Flask \u00b6 How to create a react + flask project How to deploy a react + flask project","title":"React"},{"location":"coding/react/react/#set-up-a-new-project","text":"Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start .","title":"Set up a new project"},{"location":"coding/react/react/#start-a-react-flask-project","text":"Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run","title":"Start a React + Flask project"},{"location":"coding/react/react/#the-basics","text":"Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components.","title":"The basics"},{"location":"coding/react/react/#pass-data-between-components","text":"Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... }","title":"Pass data between components"},{"location":"coding/react/react/#use-of-the-state","text":"React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); }","title":"Use of the state"},{"location":"coding/react/react/#share-the-state-between-parent-and-children","text":"To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components.","title":"Share the state between parent and children"},{"location":"coding/react/react/#handling-data-change","text":"There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering.","title":"Handling data change"},{"location":"coding/react/react/#function-components","text":"Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); }","title":"Function components"},{"location":"coding/react/react/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"coding/react/react/#list-rendering","text":"When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key .","title":"List rendering"},{"location":"coding/react/react/#links","text":"React tutorial Awesome React Awesome React components","title":"Links"},{"location":"coding/react/react/#responsive-react","text":"Responsive react Responsive websites without css react-responsive library","title":"Responsive React"},{"location":"coding/react/react/#with-flask","text":"How to create a react + flask project How to deploy a react + flask project","title":"With Flask"},{"location":"coding/yaml/yaml/","text":"YAML (a recursive acronym for \"YAML Ain't Markup Language\") is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted. YAML targets many of the same communications applications as Extensible Markup Language (XML) but has a minimal syntax which intentionally differs from SGML. It uses both Python-style indentation to indicate nesting, and a more compact format that uses [...] for lists and {...} for maps making YAML 1.2 a superset of JSON. Break long lines \u00b6 Use > most of the time: interior line breaks are stripped out, although you get one at the end: key : > Your long string here. Use | if you want those line breaks to be preserved as \\n (for instance, embedded markdown with paragraphs): key : | ### Heading * Bullet * Points Use >- or |- instead if you don't want a line break appended at the end. Use \"...\" if you need to split lines in the middle of words or want to literally type line breaks as \\n : key : \"Antidisestab\\ lishmentarianism.\\n\\nGet on it.\" YAML is crazy.","title":"YAML"},{"location":"coding/yaml/yaml/#break-long-lines","text":"Use > most of the time: interior line breaks are stripped out, although you get one at the end: key : > Your long string here. Use | if you want those line breaks to be preserved as \\n (for instance, embedded markdown with paragraphs): key : | ### Heading * Bullet * Points Use >- or |- instead if you don't want a line break appended at the end. Use \"...\" if you need to split lines in the middle of words or want to literally type line breaks as \\n : key : \"Antidisestab\\ lishmentarianism.\\n\\nGet on it.\" YAML is crazy.","title":"Break long lines"},{"location":"data_analysis/recommender_systems/recommender_systems/","text":"A recommender system, or a recommendation system , is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. The entity to which the recommendation is provided is referred to as the user, and the product being recommended is also referred to as an item. Therefore, recommendation analysis is often based on the previous interaction between users and items, because past interests and proclivities are often good indicators of future choices. These relations can be learned in a data-driven manner from the ratings matrix, and the resulting model is used to make predictions for target users.The larger the number of rated items that are available for a user, the easier it is to make robust predictions about the future behavior of the user. The problem may be formulated in two ways: Prediction version of problem: This approach aims to predict the rating value for a user-item combination. It is assumed that training data is available, indicating user preferences for items. For m users and n items, this corresponds to an incomplete m x n matrix, hwere the specified (or observed) values are used for training. The missing (or unobserved) values are predicted using this training model. This problem is also referred to as the matrix completion problem . Ranking version of problem: This approach aims to recommend the top- k items for a particular user, or determine the top- k users to target for a particular item. Being the first one more common. The problem is also referred to as the top-k recommendation problem . Goals of recommender systems \u00b6 The common operational and technical goals of recommender systems are: Relevance : Recommend items that are relevant to the user at hand. Although it's the primary operational goal, it is not sufficient in isolation. Novelty : Recommend items that the user has not seen in the past. Serendipity : Recommend items that are outside the user's bubble, rather than simply something they did not know about before. For example, if a new Indian restaurant opens in a neighborhood, then the recommendation of that restaurant to a user who normally eats Indian food is novel but not necessarily serendipitous. On the other hand, when the same user is recommended Ethiopian food, and it was unknown to the user that such food might appeal to her, then the recommendation is serendipitous. Serendipity has the beneficial side effect of beginning new areas of interest. Increase recommendation diversity: Recommend items that aren't similar to increase the chances that the user likes at least one of these items. Basic Models of recommender systems \u00b6 There are four types of basic models: Collaborative filtering : Use collaborative power of the ratings provided by multiple users to make recommendations. Content-based : Use the descriptive attributes of the user rated items to create a user-specific model that predicts the rating of unobserved items. Knowledge-based : Use the similarities between customer requirements and item descriptions. Hybrid systems : Combine the above to benefit from the mix of their strengths to perform more robustly. Collaborative Filtering Models \u00b6 These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items. There are two common types of methods. Memory-based methods : Also referred to as neighborhood-based collaborative filtering algorithms , predict ratings on the basis of their neighborhoods. These can be defined through two ways: User-based collaborative filtering : Ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the goal is to determine users who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing the averages of the ratings of this peer group. Item-based collaborative filtering : In order to make the rating predictions for target item B by user A, the first step is to determine a set S of items that are most similar to target item B. The ratings in the item set S, which are defined by A, are used to predict whether the user A will like item B. These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices. Model-based methods : Machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterized, the parameters of this model are learned within the context of an optimization framework. Some examples of such methods include decision trees, rule-based models, Bayesian methods and latent factor models. Many of these methods have a high level of coverage even for sparse ratings matrices. Types of ratings \u00b6 The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings: interval-based : A discrete set of ordered numbers are used to quantify like or dislike. ordinal : A discrete set of ordered categorical values, such as agree or strongly agree, are used to achieve the same goal. binary : Only the like or dislike for the item can be specified. unary : Only liking of an item can be specified. Another categorization of rating systems is based in the way the feedback is retrieved: explicit ratings : Users actively give information on their preferences. implicit ratings : Users preferences are derived from their behavior. Such as visiting a link. Therefore, implicit ratings are usually unary. Content-Based Recommender Systems \u00b6 In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items. These systems have the following advantages: Works well for new items, when sufficient data is not available. If the user has rated items with similar attributes. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. It requires large number of rating user data to produce robust predictions without overfitting. Knowledge-based Recommender Systems \u00b6 The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process. Knowledge-based systems can be classified by the type of user interface: Constraint-based recommender systems : Users specify requirements on the item attributes or give information about their attributes. Then domain specific rules are used to select the items to recommend. Case-based recommender systems : Specific cases are selected by the user as targets or anchor points. Similarity metrics are defined in a domain specific way on the item attributes to retrieve similar items to these cases. These user interfaces can interact with the users through several ways: Conversational systems : User preferences are determined iteratively in the context of a feedback loop. It's useful if the item domain is complex. Search-based systems : User preferences are elicited by using a preset of questions. Navigation-based recommendation : Users specify a number of attribute changes to the item being recommended. Also known as critiquing recommender systems . The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests. These systems have the following advantages: Works well for items with varied properties and/or few ratings. Such as in cold start scenarios, if it's difficult to capture the user interests with historical data or if the item is not often consumed. Allows the users to explicitly specify what they want, thus giving them a greater control over the recommendation process. Allows the user to iteratively change their specified requirements to reach the desired items. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. Domain-Specific recommender systems \u00b6 Demographic recommender systems \u00b6 In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings. Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems. Pitfalls to avoid \u00b6 Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior. Interesting resources \u00b6 Content indexers \u00b6 Open Library : Open, editable library catalog, building towards a web page for every book ever published. Data can be retrieved through their API or bulk downloaded . Rating Datasets \u00b6 Books \u00b6 Book-Crossing : 278,858 users providing 1,149,780 ratings (explicit / implicit) about 271,379 books. Movies \u00b6 MovieLens : 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. HetRec 2011 Movielens + IMDB/rotten Tomatoes : 86,000 ratings from 2113 users. Netflix prize dataset : 480,000 users doing 100 million ratings on 17,000 movies. Music \u00b6 HetRec 2011 Last.FM : 92,800 artist listening records from 1892 users. Web \u00b6 HetRec 2011 Delicious : 105,000 bookmarks from 1867 users. Miscelaneous \u00b6 Wikilens : generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items. Past Projects \u00b6 GroupLens: Pioneer recommender system to recommend Usenet news. BookLens : Books implementation of Grouplens. MovieLens : Movies implementation of Grouplens. References \u00b6 Books \u00b6 Recommender Systems by Chary C.Aggarwal . Recommender systems, an introduction by Dietmar Jannach, Markus Zanker, Alexander Felfernig and Gerhard Friedrich. Practical Recommender Systems by Kim Falk. Hands On recommendation systems in Python by Rounak Banik. Awesome recommender systems \u00b6 Grahamjenson","title":"Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#goals-of-recommender-systems","text":"The common operational and technical goals of recommender systems are: Relevance : Recommend items that are relevant to the user at hand. Although it's the primary operational goal, it is not sufficient in isolation. Novelty : Recommend items that the user has not seen in the past. Serendipity : Recommend items that are outside the user's bubble, rather than simply something they did not know about before. For example, if a new Indian restaurant opens in a neighborhood, then the recommendation of that restaurant to a user who normally eats Indian food is novel but not necessarily serendipitous. On the other hand, when the same user is recommended Ethiopian food, and it was unknown to the user that such food might appeal to her, then the recommendation is serendipitous. Serendipity has the beneficial side effect of beginning new areas of interest. Increase recommendation diversity: Recommend items that aren't similar to increase the chances that the user likes at least one of these items.","title":"Goals of recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#basic-models-of-recommender-systems","text":"There are four types of basic models: Collaborative filtering : Use collaborative power of the ratings provided by multiple users to make recommendations. Content-based : Use the descriptive attributes of the user rated items to create a user-specific model that predicts the rating of unobserved items. Knowledge-based : Use the similarities between customer requirements and item descriptions. Hybrid systems : Combine the above to benefit from the mix of their strengths to perform more robustly.","title":"Basic Models of recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#collaborative-filtering-models","text":"These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items. There are two common types of methods. Memory-based methods : Also referred to as neighborhood-based collaborative filtering algorithms , predict ratings on the basis of their neighborhoods. These can be defined through two ways: User-based collaborative filtering : Ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the goal is to determine users who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing the averages of the ratings of this peer group. Item-based collaborative filtering : In order to make the rating predictions for target item B by user A, the first step is to determine a set S of items that are most similar to target item B. The ratings in the item set S, which are defined by A, are used to predict whether the user A will like item B. These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices. Model-based methods : Machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterized, the parameters of this model are learned within the context of an optimization framework. Some examples of such methods include decision trees, rule-based models, Bayesian methods and latent factor models. Many of these methods have a high level of coverage even for sparse ratings matrices.","title":"Collaborative Filtering Models"},{"location":"data_analysis/recommender_systems/recommender_systems/#types-of-ratings","text":"The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings: interval-based : A discrete set of ordered numbers are used to quantify like or dislike. ordinal : A discrete set of ordered categorical values, such as agree or strongly agree, are used to achieve the same goal. binary : Only the like or dislike for the item can be specified. unary : Only liking of an item can be specified. Another categorization of rating systems is based in the way the feedback is retrieved: explicit ratings : Users actively give information on their preferences. implicit ratings : Users preferences are derived from their behavior. Such as visiting a link. Therefore, implicit ratings are usually unary.","title":"Types of ratings"},{"location":"data_analysis/recommender_systems/recommender_systems/#content-based-recommender-systems","text":"In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items. These systems have the following advantages: Works well for new items, when sufficient data is not available. If the user has rated items with similar attributes. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. It requires large number of rating user data to produce robust predictions without overfitting.","title":"Content-Based Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#knowledge-based-recommender-systems","text":"The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process. Knowledge-based systems can be classified by the type of user interface: Constraint-based recommender systems : Users specify requirements on the item attributes or give information about their attributes. Then domain specific rules are used to select the items to recommend. Case-based recommender systems : Specific cases are selected by the user as targets or anchor points. Similarity metrics are defined in a domain specific way on the item attributes to retrieve similar items to these cases. These user interfaces can interact with the users through several ways: Conversational systems : User preferences are determined iteratively in the context of a feedback loop. It's useful if the item domain is complex. Search-based systems : User preferences are elicited by using a preset of questions. Navigation-based recommendation : Users specify a number of attribute changes to the item being recommended. Also known as critiquing recommender systems . The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests. These systems have the following advantages: Works well for items with varied properties and/or few ratings. Such as in cold start scenarios, if it's difficult to capture the user interests with historical data or if the item is not often consumed. Allows the users to explicitly specify what they want, thus giving them a greater control over the recommendation process. Allows the user to iteratively change their specified requirements to reach the desired items. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items.","title":"Knowledge-based Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#domain-specific-recommender-systems","text":"","title":"Domain-Specific recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#demographic-recommender-systems","text":"In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings. Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems.","title":"Demographic recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#pitfalls-to-avoid","text":"Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior.","title":"Pitfalls to avoid"},{"location":"data_analysis/recommender_systems/recommender_systems/#interesting-resources","text":"","title":"Interesting resources"},{"location":"data_analysis/recommender_systems/recommender_systems/#content-indexers","text":"Open Library : Open, editable library catalog, building towards a web page for every book ever published. Data can be retrieved through their API or bulk downloaded .","title":"Content indexers"},{"location":"data_analysis/recommender_systems/recommender_systems/#rating-datasets","text":"","title":"Rating Datasets"},{"location":"data_analysis/recommender_systems/recommender_systems/#books","text":"Book-Crossing : 278,858 users providing 1,149,780 ratings (explicit / implicit) about 271,379 books.","title":"Books"},{"location":"data_analysis/recommender_systems/recommender_systems/#movies","text":"MovieLens : 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. HetRec 2011 Movielens + IMDB/rotten Tomatoes : 86,000 ratings from 2113 users. Netflix prize dataset : 480,000 users doing 100 million ratings on 17,000 movies.","title":"Movies"},{"location":"data_analysis/recommender_systems/recommender_systems/#music","text":"HetRec 2011 Last.FM : 92,800 artist listening records from 1892 users.","title":"Music"},{"location":"data_analysis/recommender_systems/recommender_systems/#web","text":"HetRec 2011 Delicious : 105,000 bookmarks from 1867 users.","title":"Web"},{"location":"data_analysis/recommender_systems/recommender_systems/#miscelaneous","text":"Wikilens : generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items.","title":"Miscelaneous"},{"location":"data_analysis/recommender_systems/recommender_systems/#past-projects","text":"GroupLens: Pioneer recommender system to recommend Usenet news. BookLens : Books implementation of Grouplens. MovieLens : Movies implementation of Grouplens.","title":"Past Projects"},{"location":"data_analysis/recommender_systems/recommender_systems/#references","text":"","title":"References"},{"location":"data_analysis/recommender_systems/recommender_systems/#books_1","text":"Recommender Systems by Chary C.Aggarwal . Recommender systems, an introduction by Dietmar Jannach, Markus Zanker, Alexander Felfernig and Gerhard Friedrich. Practical Recommender Systems by Kim Falk. Hands On recommendation systems in Python by Rounak Banik.","title":"Books"},{"location":"data_analysis/recommender_systems/recommender_systems/#awesome-recommender-systems","text":"Grahamjenson","title":"Awesome recommender systems"},{"location":"devops/api_management/","text":"API management is the process of creating and publishing web application programming interfaces (APIs) under a service that: Enforces the usage of policies. Controls access. Collects and analyzes usage statistics. Reports on performance. Components \u00b6 While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","title":"API Management"},{"location":"devops/api_management/#components","text":"While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","title":"Components"},{"location":"devops/devops/","text":"DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops) which aims to shorten the systems development life cycle and provide continuous delivery with high software quality. Learn path \u00b6 DevOps is has become a juicy work, if you want to introduce yourself into this world I suggest you to follow these steps: Learn basic Linux administration, otherwise you'll be lost. Learn how to use Git. If you can host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions .","title":"DevOps"},{"location":"devops/devops/#learn-path","text":"DevOps is has become a juicy work, if you want to introduce yourself into this world I suggest you to follow these steps: Learn basic Linux administration, otherwise you'll be lost. Learn how to use Git. If you can host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions .","title":"Learn path"},{"location":"devops/helmfile/","text":"Helmfile is a declarative spec for deploying Helm charts. It lets you: Keep a directory of chart value files and maintain changes in version control. Apply CI/CD to configuration changes. Environmental chart promotion. Periodically sync to avoid skew in environments. To avoid upgrades for each iteration of helm, the helmfile executable delegates to helm - as a result, helm must be installed. All information is saved in the helmfile.yaml file. In case we need custom yamls, we'll use kustomize . Installation \u00b6 Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile Usage \u00b6 How to deploy a new chart \u00b6 When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are updated, if they are not, run helmfile apply . Configure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Be careful becase some charts specify the docker image version in the name. Comment out that line because upgrading the chart version without upgrading the image tag can break the service. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes. Keep charts updated \u00b6 To have your charts updated, this would be my suggested workflow, although the developers haven't confirmed it yet : A periodic CI job would run helmfile deps , once a change is detected in the lock file, the job will run helmfile --environment=staging apply . Developers are notified that the new version is deployed and are prompted to test it. Once it's validated, the developers will manually introduce the new version in the lockfile and run helmfile --environment=production apply . Delegate to the developers the manual introduction of the version in the lockfile isn't the ideal solution, but it's the one I can come up to avoid race conditions on chart releases. Uninstall charts \u00b6 Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands. Force the reinstallation of everything \u00b6 If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases. Debugging helmfile \u00b6 Error: \"release-name\" has no deployed releases \u00b6 This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again. Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help) \u00b6 I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false. Links \u00b6 Git","title":"Helmfile"},{"location":"devops/helmfile/#installation","text":"Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile","title":"Installation"},{"location":"devops/helmfile/#usage","text":"","title":"Usage"},{"location":"devops/helmfile/#how-to-deploy-a-new-chart","text":"When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are updated, if they are not, run helmfile apply . Configure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Be careful becase some charts specify the docker image version in the name. Comment out that line because upgrading the chart version without upgrading the image tag can break the service. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes.","title":"How to deploy a new chart"},{"location":"devops/helmfile/#keep-charts-updated","text":"To have your charts updated, this would be my suggested workflow, although the developers haven't confirmed it yet : A periodic CI job would run helmfile deps , once a change is detected in the lock file, the job will run helmfile --environment=staging apply . Developers are notified that the new version is deployed and are prompted to test it. Once it's validated, the developers will manually introduce the new version in the lockfile and run helmfile --environment=production apply . Delegate to the developers the manual introduction of the version in the lockfile isn't the ideal solution, but it's the one I can come up to avoid race conditions on chart releases.","title":"Keep charts updated"},{"location":"devops/helmfile/#uninstall-charts","text":"Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands.","title":"Uninstall charts"},{"location":"devops/helmfile/#force-the-reinstallation-of-everything","text":"If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases.","title":"Force the reinstallation of everything"},{"location":"devops/helmfile/#debugging-helmfile","text":"","title":"Debugging helmfile"},{"location":"devops/helmfile/#error-release-name-has-no-deployed-releases","text":"This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again.","title":"Error: \"release-name\" has no deployed releases"},{"location":"devops/helmfile/#error-failed-to-download-stablemetrics-server-hint-running-helm-repo-update-may-help","text":"I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false.","title":"Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help)"},{"location":"devops/helmfile/#links","text":"Git","title":"Links"},{"location":"devops/aws/aws/","text":"Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. In aggregate, these cloud computing web services provide a set of primitive abstract technical infrastructure and distributed computing building blocks and tools. Learn path \u00b6 TBD","title":"AWS"},{"location":"devops/aws/aws/#learn-path","text":"TBD","title":"Learn path"},{"location":"devops/aws/eks/","text":"Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane. Upgrade an EKS cluster \u00b6 New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes. To upgrade a cluster follow these steps: Upgrade all your charts to the latest version with helmfile . helmfile deps helmfile apply Check your current version and compare it with the one you want to upgrade. kubectl version --short kubectl get nodes Check the docs to see if the version you want to upgrade requires some special steps. If your worker nodes aren't at the same version as the cluster control plane upgrade them to the control plane version (never higher). Edit the cluster_version attribute of the eks terraform module and apply the changes (reviewing them first). terraform apply This is a long step (approximately 40 minutes) * Upgrade your charts again. References \u00b6 Docs","title":"EKS"},{"location":"devops/aws/eks/#upgrade-an-eks-cluster","text":"New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes. To upgrade a cluster follow these steps: Upgrade all your charts to the latest version with helmfile . helmfile deps helmfile apply Check your current version and compare it with the one you want to upgrade. kubectl version --short kubectl get nodes Check the docs to see if the version you want to upgrade requires some special steps. If your worker nodes aren't at the same version as the cluster control plane upgrade them to the control plane version (never higher). Edit the cluster_version attribute of the eks terraform module and apply the changes (reviewing them first). terraform apply This is a long step (approximately 40 minutes) * Upgrade your charts again.","title":"Upgrade an EKS cluster"},{"location":"devops/aws/eks/#references","text":"Docs","title":"References"},{"location":"devops/aws/s3/","text":"S3 is the secure, durable, and scalable object storage infrastructure of AWS. Often used for serving static website content or holding backups or data. Commands \u00b6 Bucket management \u00b6 List buckets \u00b6 aws s3 ls Create bucket \u00b6 aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1 Enable versioning \u00b6 aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled Enable encryption \u00b6 aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' Download bucket \u00b6 aws s3 cp --recursive s3:// {{ bucket_name }} . Audit the S3 bucket policy \u00b6 IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done Add cache header to all items in a bucket \u00b6 Log in to AWS Management Console. Go into S3 bucket. Select all files by route. Choose \"More\" from the menu. Select \"Change metadata\". In the \"Key\" field, select \"Cache-Control\" from the drop down menu max-age=604800Enter (7 days in seconds) for Value. Press \"Save\" button. Object management \u00b6 Remove an object \u00b6 aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }} Upload \u00b6 Upload a local file with the cli \u00b6 aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }} Upload a file unauthenticated \u00b6 curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/ Restore an object \u00b6 First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }} Copy objects between buckets \u00b6 aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME Troubleshooting \u00b6 get_environ_proxies() missing 1 required positional argument: 'no_proxy' \u00b6 sudo pip3 install --upgrade boto3 Links \u00b6 User guide","title":"S3"},{"location":"devops/aws/s3/#commands","text":"","title":"Commands"},{"location":"devops/aws/s3/#bucket-management","text":"","title":"Bucket management"},{"location":"devops/aws/s3/#list-buckets","text":"aws s3 ls","title":"List buckets"},{"location":"devops/aws/s3/#create-bucket","text":"aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1","title":"Create bucket"},{"location":"devops/aws/s3/#enable-versioning","text":"aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled","title":"Enable versioning"},{"location":"devops/aws/s3/#enable-encryption","text":"aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }'","title":"Enable encryption"},{"location":"devops/aws/s3/#download-bucket","text":"aws s3 cp --recursive s3:// {{ bucket_name }} .","title":"Download bucket"},{"location":"devops/aws/s3/#audit-the-s3-bucket-policy","text":"IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done","title":"Audit the S3 bucket policy"},{"location":"devops/aws/s3/#add-cache-header-to-all-items-in-a-bucket","text":"Log in to AWS Management Console. Go into S3 bucket. Select all files by route. Choose \"More\" from the menu. Select \"Change metadata\". In the \"Key\" field, select \"Cache-Control\" from the drop down menu max-age=604800Enter (7 days in seconds) for Value. Press \"Save\" button.","title":"Add cache header to all items in a bucket"},{"location":"devops/aws/s3/#object-management","text":"","title":"Object management"},{"location":"devops/aws/s3/#remove-an-object","text":"aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }}","title":"Remove an object"},{"location":"devops/aws/s3/#upload","text":"","title":"Upload"},{"location":"devops/aws/s3/#upload-a-local-file-with-the-cli","text":"aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }}","title":"Upload a local file with the cli"},{"location":"devops/aws/s3/#upload-a-file-unauthenticated","text":"curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/","title":"Upload a file unauthenticated"},{"location":"devops/aws/s3/#restore-an-object","text":"First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }}","title":"Restore an object"},{"location":"devops/aws/s3/#copy-objects-between-buckets","text":"aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME","title":"Copy objects between buckets"},{"location":"devops/aws/s3/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"devops/aws/s3/#get_environ_proxies-missing-1-required-positional-argument-no_proxy","text":"sudo pip3 install --upgrade boto3","title":"get_environ_proxies() missing 1 required positional argument: 'no_proxy'"},{"location":"devops/aws/s3/#links","text":"User guide","title":"Links"},{"location":"devops/aws/security_groups/","text":"Security groups are the AWS way of defining firewall rules between the resources. If not handled properly they can soon become hard to read, which can lead to an insecure infrastructure. It has helped me to use four types of security groups: Default security groups: Security groups created by AWS per VPC and region, they can't be deleted. Naming security groups: Used to identify an aws resource. They are usually referenced in other security groups. Ingress security groups: Used to define the rules of ingress traffic to the resource. Egress security groups: Used to define the rules of egress traffic to the resource. But what helped most has been using clinv while refactoring all the security groups. With clinv unused I got rid of all the security groups that weren't used by any AWS resource (beware of #16 , 17 , #18 and #19 ), then used the clinv unassigned security_groups to methodically decide if they were correct and add them to my inventory or if I needed to refactor them. Best practices \u00b6 Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them. Naming convention \u00b6 A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources Default security groups \u00b6 There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws . {{ region_id }} } Where the provider must be configured in the `terraform_config . tf` file , for example: ``` terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" } Naming security groups \u00b6 For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$ { 3:vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2 . id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file. Ingress security groups \u00b6 For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Egress security groups \u00b6 For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Instance security group definition \u00b6 When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data . terraform_remote_state . security_groups . outputs . gitea_instance_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_monitoring_at_privatedmz_id , data . terraform_remote_state . security_groups . outputs . ingress_administration_from_bastion_at_connectiondmz_id , data . terraform_remote_state . security_groups . outputs . egress_ldap_to_ldap_instance_from_publicdmz_id , data . terraform_remote_state . security_groups . outputs . egress_https_to_debian_repositories_from_publicdmz_id , ]","title":"Security groups workflow"},{"location":"devops/aws/security_groups/#best-practices","text":"Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them.","title":"Best practices"},{"location":"devops/aws/security_groups/#naming-convention","text":"A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources","title":"Naming convention"},{"location":"devops/aws/security_groups/#default-security-groups","text":"There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws . {{ region_id }} } Where the provider must be configured in the `terraform_config . tf` file , for example: ``` terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" }","title":"Default security groups"},{"location":"devops/aws/security_groups/#naming-security-groups","text":"For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$ { 3:vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2 . id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Naming security groups"},{"location":"devops/aws/security_groups/#ingress-security-groups","text":"For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Ingress security groups"},{"location":"devops/aws/security_groups/#egress-security-groups","text":"For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Egress security groups"},{"location":"devops/aws/security_groups/#instance-security-group-definition","text":"When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data . terraform_remote_state . security_groups . outputs . gitea_instance_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_monitoring_at_privatedmz_id , data . terraform_remote_state . security_groups . outputs . ingress_administration_from_bastion_at_connectiondmz_id , data . terraform_remote_state . security_groups . outputs . egress_ldap_to_ldap_instance_from_publicdmz_id , data . terraform_remote_state . security_groups . outputs . egress_https_to_debian_repositories_from_publicdmz_id , ]","title":"Instance security group definition"},{"location":"devops/aws/iam/iam/","text":"AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization). Configurable AWS access controls: Grant access to AWS Management console, APIs Create individual users Manage permissions with groups Configure a strong password policy Enable Multi-Factor Authentication for privileged users Use IAM roles for EC2 instances Use IAM roles to share access Rotate security credentials regularly Restrict privileged access further with conditions Use your corporate directory system or a third party authentication Links \u00b6 Docs","title":"IAM"},{"location":"devops/aws/iam/iam/#links","text":"Docs","title":"Links"},{"location":"devops/aws/iam/iam_commands/","text":"Information gathering \u00b6 List roles \u00b6 aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table List policies \u00b6 aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table List attached policies \u00b6 aws iam list-attached-role-policies --role-name {{ role_name }} Get role configuration \u00b6 aws iam get-role --role-name {{ role_name }} Get role policies \u00b6 aws iam list-role-policies --role-name {{ role_name }}","title":"IAM Commands"},{"location":"devops/aws/iam/iam_commands/#information-gathering","text":"","title":"Information gathering"},{"location":"devops/aws/iam/iam_commands/#list-roles","text":"aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table","title":"List roles"},{"location":"devops/aws/iam/iam_commands/#list-policies","text":"aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table","title":"List policies"},{"location":"devops/aws/iam/iam_commands/#list-attached-policies","text":"aws iam list-attached-role-policies --role-name {{ role_name }}","title":"List attached policies"},{"location":"devops/aws/iam/iam_commands/#get-role-configuration","text":"aws iam get-role --role-name {{ role_name }}","title":"Get role configuration"},{"location":"devops/aws/iam/iam_commands/#get-role-policies","text":"aws iam list-role-policies --role-name {{ role_name }}","title":"Get role policies"},{"location":"devops/aws/iam/iam_debug/","text":"MFADevice entity at the same path and name already exists \u00b6 It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"IAM Debugging"},{"location":"devops/aws/iam/iam_debug/#mfadevice-entity-at-the-same-path-and-name-already-exists","text":"It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"MFADevice entity at the same path and name already exists"},{"location":"devops/helm/helm/","text":"Helm is the package manager for Kubernetes. Through charts it helps you define, install and upgrade even the most complex Kubernetes applications. The advantages of using helm over kubectl apply are the easiness of: Repeatable application installation. CI integration. Versioning and sharing. Charts are a group of Go templates of kubernetes yaml resource manifests, they are easy to create, version, share, and publish. Helm alone lacks some features, that are satisfied through some external programs: Helmfile is used to declaratively configure your charts, so they can be versioned through git. Helm-secrets is used to remove hardcoded credentials from values.yaml files. Helm has an open issue to integrate it into it's codebase. Links \u00b6 Homepage Docs Git Chart hub Git charts repositories","title":"Helm"},{"location":"devops/helm/helm/#links","text":"Homepage Docs Git Chart hub Git charts repositories","title":"Links"},{"location":"devops/helm/helm_commands/","text":"Small cheatsheet on how to use the helm command. List charts \u00b6 helm ls Get information of chart \u00b6 helm inspect {{ package_name }} List all the available versions of a chart \u00b6 helm search -l {{ package_name }} Download a chart \u00b6 helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }} Search charts \u00b6 helm search {{ package_name }} Operations you should do with helmfile \u00b6 The following operations can be done with helm, but consider using helmfile instead. Install chart \u00b6 Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }} Give it a name \u00b6 helm install --name {{ release_name }} {{ package_name }} Give it a namespace \u00b6 helm install --namespace {{ namespace }} {{ package_name }} Customize the chart before installing \u00b6 helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }} Upgrade a release \u00b6 If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }} Rollback an upgrade \u00b6 First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }} Delete a release \u00b6 helm delete --purge {{ release_name }} Working with repositories \u00b6 List repositories \u00b6 helm repo list Add repository \u00b6 helm repo add {{ repo_name }} {{ repo_url }} Update repositories \u00b6 helm repo update","title":"Helm Commands"},{"location":"devops/helm/helm_commands/#list-charts","text":"helm ls","title":"List charts"},{"location":"devops/helm/helm_commands/#get-information-of-chart","text":"helm inspect {{ package_name }}","title":"Get information of chart"},{"location":"devops/helm/helm_commands/#list-all-the-available-versions-of-a-chart","text":"helm search -l {{ package_name }}","title":"List all the available versions of a chart"},{"location":"devops/helm/helm_commands/#download-a-chart","text":"helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }}","title":"Download a chart"},{"location":"devops/helm/helm_commands/#search-charts","text":"helm search {{ package_name }}","title":"Search charts"},{"location":"devops/helm/helm_commands/#operations-you-should-do-with-helmfile","text":"The following operations can be done with helm, but consider using helmfile instead.","title":"Operations you should do with helmfile"},{"location":"devops/helm/helm_commands/#install-chart","text":"Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }}","title":"Install chart"},{"location":"devops/helm/helm_commands/#give-it-a-name","text":"helm install --name {{ release_name }} {{ package_name }}","title":"Give it a name"},{"location":"devops/helm/helm_commands/#give-it-a-namespace","text":"helm install --namespace {{ namespace }} {{ package_name }}","title":"Give it a namespace"},{"location":"devops/helm/helm_commands/#customize-the-chart-before-installing","text":"helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }}","title":"Customize the chart before installing"},{"location":"devops/helm/helm_commands/#upgrade-a-release","text":"If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }}","title":"Upgrade a release"},{"location":"devops/helm/helm_commands/#rollback-an-upgrade","text":"First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }}","title":"Rollback an upgrade"},{"location":"devops/helm/helm_commands/#delete-a-release","text":"helm delete --purge {{ release_name }}","title":"Delete a release"},{"location":"devops/helm/helm_commands/#working-with-repositories","text":"","title":"Working with repositories"},{"location":"devops/helm/helm_commands/#list-repositories","text":"helm repo list","title":"List repositories"},{"location":"devops/helm/helm_commands/#add-repository","text":"helm repo add {{ repo_name }} {{ repo_url }}","title":"Add repository"},{"location":"devops/helm/helm_commands/#update-repositories","text":"helm repo update","title":"Update repositories"},{"location":"devops/helm/helm_installation/","text":"There are two usable versions of Helm, v2 and v3, the latter is quite new so some of the things we need to install as of 2020-01-27 are not yet supported (Prometheus operator), so we are going to stick to the version 2. Helm has a client-server architecture, the server is installed in the Kubernetes cluster and the client is a Go executable installed in the user computer. Helm client \u00b6 You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm Installation"},{"location":"devops/helm/helm_installation/#helm-client","text":"You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm client"},{"location":"devops/helm/helm_secrets/","text":"Helm-secrets is a helm plugin that manages secrets with Git workflow and stores them anywhere. It delegates the cryptographic operations to Mozilla's Sops tool, which supports PGP, AWS KMS and GCP KMS. The configuration is stored in .sops.yaml files. You can find in Mozilla's documentation a detailed configuration guide. For my use case, I'm only going to use a list of PGP keys, so the following contents should be in the .sops.yaml file at the project root directory. creation_rules : - pgp : >- {{ gpg_key_1 }}, {{ gpg_key_2}} Prevent committing decrypted files to git \u00b6 From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit Usage \u00b6 Encrypt secret files \u00b6 Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials. Edit secret files \u00b6 helm secrets edit secrets.yaml Decrypt secret files \u00b6 helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git . Clean all the decrypted files \u00b6 helm secrets clean . Add or remove keys \u00b6 Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options. Links \u00b6 Git","title":"Helm Secrets"},{"location":"devops/helm/helm_secrets/#prevent-committing-decrypted-files-to-git","text":"From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit","title":"Prevent committing decrypted files to git"},{"location":"devops/helm/helm_secrets/#usage","text":"","title":"Usage"},{"location":"devops/helm/helm_secrets/#encrypt-secret-files","text":"Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials.","title":"Encrypt secret files"},{"location":"devops/helm/helm_secrets/#edit-secret-files","text":"helm secrets edit secrets.yaml","title":"Edit secret files"},{"location":"devops/helm/helm_secrets/#decrypt-secret-files","text":"helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git .","title":"Decrypt secret files"},{"location":"devops/helm/helm_secrets/#clean-all-the-decrypted-files","text":"helm secrets clean .","title":"Clean all the decrypted files"},{"location":"devops/helm/helm_secrets/#add-or-remove-keys","text":"Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options.","title":"Add or remove keys"},{"location":"devops/helm/helm_secrets/#links","text":"Git","title":"Links"},{"location":"devops/kong/kong/","text":"Kong is a lua application API platform running in Nginx. Installation \u00b6 Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use. Links \u00b6 Homepage Docs","title":"Kong"},{"location":"devops/kong/kong/#installation","text":"Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use.","title":"Installation"},{"location":"devops/kong/kong/#links","text":"Homepage Docs","title":"Links"},{"location":"devops/kubectl/kubectl/","text":"Kubectl Definition Kubectl is a command line tool for controlling Kubernetes clusters. kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag. Resource types and it's aliases \u00b6 Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources Links \u00b6 Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.","title":"Kubectl"},{"location":"devops/kubectl/kubectl/#resource-types-and-its-aliases","text":"Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources","title":"Resource types and it's aliases"},{"location":"devops/kubectl/kubectl/#links","text":"Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.","title":"Links"},{"location":"devops/kubectl/kubectl_installation/","text":"Kubectl is not yet in the distribution package managers, so we'll need to install it manually. curl -LO \"https://storage.googleapis.com/kubernetes-release/release/ $( \\ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt \\ ) /bin/linux/amd64/kubectl\" chmod +x kubectl mv kubectl ~/.local/bin/kubectl Configure kubectl \u00b6 Set editor \u00b6 # File ~/.bashrc KUBE_EDITOR = \"vim\" Set auto completion \u00b6 # File ~/.bashrc source < ( kubectl completion bash ) Configure EKS cluster \u00b6 To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}","title":"Kubectl Installation"},{"location":"devops/kubectl/kubectl_installation/#configure-kubectl","text":"","title":"Configure kubectl"},{"location":"devops/kubectl/kubectl_installation/#set-editor","text":"# File ~/.bashrc KUBE_EDITOR = \"vim\"","title":"Set editor"},{"location":"devops/kubectl/kubectl_installation/#set-auto-completion","text":"# File ~/.bashrc source < ( kubectl completion bash )","title":"Set auto completion"},{"location":"devops/kubectl/kubectl_installation/#configure-eks-cluster","text":"To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}","title":"Configure EKS cluster"},{"location":"devops/kubernetes/kubernetes/","text":"Kubernetes (commonly stylized as k8s) is an open-source container-orchestration system for automating application deployment, scaling, and management. Developed by Google in Go under the Apache 2.0 license, it was first released on June 7, 2014 reaching 1.0 by July 21, 2015. It works with a range of container tools, including Docker. Many cloud services offer a Kubernetes-based platform or infrastructure as a service ( PaaS or IaaS ) on which Kubernetes can be deployed as a platform-providing service. Many vendors also provide their own branded Kubernetes distributions. It has become the standard infrastructure to manage containers in production environments. Docker Swarm would be an alternative but it falls short in features compared with Kubernetes. These are some of the advantages of using Kubernetes: Widely used in production and actively developed. Ensure high availability of your services with autohealing and autoscaling. Easy, quickly and predictable deployment and promotion of applications. Seamless roll out of features. Optimize hardware use while guaranteeing resource isolation. Easiest way to build multi-cloud and baremetal environments. Several companies have used Kubernetes to release their own PaaS : OpenShift by Red Hat. Tectonic by CoreOS. Rancher labs by Rancher. Learn roadmap \u00b6 K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af . Links \u00b6 Docs Awesome K8s Katacoda playground Comic Diving deeper \u00b6 Architecture Resources Kubectl Additional Components Networking Helm Tools Reference \u00b6 References API conventions","title":"Kubernetes"},{"location":"devops/kubernetes/kubernetes/#learn-roadmap","text":"K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af .","title":"Learn roadmap"},{"location":"devops/kubernetes/kubernetes/#links","text":"Docs Awesome K8s Katacoda playground Comic","title":"Links"},{"location":"devops/kubernetes/kubernetes/#diving-deeper","text":"Architecture Resources Kubectl Additional Components Networking Helm Tools","title":"Diving deeper"},{"location":"devops/kubernetes/kubernetes/#reference","text":"References API conventions","title":"Reference"},{"location":"devops/kubernetes/kubernetes_annotations/","text":"Annotations are non-identifying metadata key/value pairs attached to objects, such as pods. Annotations are intended to give meaningful and relevant information to libraries and tools. Annotations, like labels, are key/value maps: \"annotations\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" } Here are some examples of information that could be recorded in annotations: Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto generated fields and fields set by auto sizing or auto scaling systems. Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address. Pointers to logging, monitoring, analytics, or audit repositories. Client library or tool information that can be used for debugging purposes, for example, name, version, and build information. User or tool/system provenance information, such as URLs of related objects from other ecosystem components. Lightweight rollout tool metadata: for example, config or checkpoints.","title":"Annotations"},{"location":"devops/kubernetes/kubernetes_architecture/","text":"Kubernetes is a combination of components distributed between two kind of nodes, Masters and Workers . Master Nodes \u00b6 Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes. Worker Nodes \u00b6 Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself. kube-proxy operation modes \u00b6 kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode. Kubectl \u00b6 The kubectl is the command line client used to communicate with the Masters. Links \u00b6 Kubernetes components overview","title":"Architecture"},{"location":"devops/kubernetes/kubernetes_architecture/#master-nodes","text":"Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.","title":"Master Nodes"},{"location":"devops/kubernetes/kubernetes_architecture/#worker-nodes","text":"Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself.","title":"Worker Nodes"},{"location":"devops/kubernetes/kubernetes_architecture/#kube-proxy-operation-modes","text":"kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode.","title":"kube-proxy operation modes"},{"location":"devops/kubernetes/kubernetes_architecture/#kubectl","text":"The kubectl is the command line client used to communicate with the Masters.","title":"Kubectl"},{"location":"devops/kubernetes/kubernetes_architecture/#links","text":"Kubernetes components overview","title":"Links"},{"location":"devops/kubernetes/kubernetes_cluster_autoscaler/","text":"While Horizontal pod autoscaling allows a deployment to scale given the resources needed, they are limited to the kubernetes existing working nodes. To autoscale the number of working nodes we need the cluster autoscaler . For AWS, there are the Amazon guidelines to enable it . But I'd use the cluster-autoscaler helm chart.","title":"Cluster Autoscaler"},{"location":"devops/kubernetes/kubernetes_dashboard/","text":"Dashboard definition Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. Deployment \u00b6 The best way to install it is with the stable/kubernetes-dashboard chart with helmfile . Links \u00b6 Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide","title":"Dashboard"},{"location":"devops/kubernetes/kubernetes_dashboard/#deployment","text":"The best way to install it is with the stable/kubernetes-dashboard chart with helmfile .","title":"Deployment"},{"location":"devops/kubernetes/kubernetes_dashboard/#links","text":"Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide","title":"Links"},{"location":"devops/kubernetes/kubernetes_deployments/","text":"The different types of deployments configure a ReplicaSet and a PodSchema for your application. Depending on the type of application we'll use one of the following types. Deployments \u00b6 Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80 StatefulSets \u00b6 StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator . DaemonSet \u00b6 DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd Job \u00b6 Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.","title":"Deployments"},{"location":"devops/kubernetes/kubernetes_deployments/#deployments","text":"Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80","title":"Deployments"},{"location":"devops/kubernetes/kubernetes_deployments/#statefulsets","text":"StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator .","title":"StatefulSets"},{"location":"devops/kubernetes/kubernetes_deployments/#daemonset","text":"DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd","title":"DaemonSet"},{"location":"devops/kubernetes/kubernetes_deployments/#job","text":"Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.","title":"Job"},{"location":"devops/kubernetes/kubernetes_external_dns/","text":"The external-dns resource allows the creation of DNS records from within kubernetes inside the definition of service and ingress resources. It currently supports the following providers: Provider Status Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta AzureDNS Beta CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha Akamai FastDNS Alpha There are two reasons to enable it: If there is any change in the ingress or service load balancer endpoint, due to a deployment, the dns records are automatically changed. It's easier for developers to connect their applications. Deployment in AWS \u00b6 To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .","title":"External DNS"},{"location":"devops/kubernetes/kubernetes_external_dns/#deployment-in-aws","text":"To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .","title":"Deployment in AWS"},{"location":"devops/kubernetes/kubernetes_hpa/","text":"With Horizontal pod autoscaling , Kubernetes automatically scales the number of pods in a deployment or replication controller based on observed CPU utilization or on some other application provided metrics. The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user. To make it work, the definition of pod resource consumption needs to be specified.","title":"Horizontal Pod Autoscaling"},{"location":"devops/kubernetes/kubernetes_ingress/","text":"An Ingress is An API object that manages external access to the services in a cluster, typically HTTP. Ingress provide a centralized way to: Load balancing. SSL termination. Dynamic service discovery. Traffic routing. Authentication. Traffic distribution: canary deployments, A/B testing, mirroring/shadowing. Graphical user interface. JWT validation. WAF and DDOS protection. Requests tracing. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer .","title":"Ingress"},{"location":"devops/kubernetes/kubernetes_ingress_controller/","text":"Ingress controllers monitor the cluster events for the creation or modification of Ingress resources, modifying accordingly the underlying load balancers. They are not part of the master kube-controller-manager , so you'll need to install them manually. There are different Ingress controllers, such as AWS ALB, Nginx, HAProxy or Traefik, using one or other depends on your needs. Almost all controllers are open sourced and support dynamic service discovery, SSL termination or WebSockets. But they differ in: Supported protocols : HTTP, HTTPS, gRPC, HTTP/2.0, TCP (with SNI) or UDP. Underlying software : NGINX, Traefik, HAProxy or Envoy. Traffic routing : host and path, regular expression support. Namespace limitations : supported or not. Upstream probes : active checks, passive checks, retries, circuit breakers, custom health checks... Load balancing algorithms : round-robin, sticky sessions, rdp-cookie... Authentication : Basic, digest, Oauth, external auth, SSL certificate... Traffic distribution : canary deployments, A/B testing, mirroring/shadowing. Paid subscription : extended functionality or technical support. Graphical user interface : JWT validation : Customization of configuration : Basic DDOS protection mechanisms : rate limit, traffic filtering. WAF : Requests tracing : monitor, trace and debug requests via OpenTracing or other options. Both ITNext and Flant provide good ingress controller comparisons, a synoptical resume of both articles follows. Kubernetes Ingress controller \u00b6 The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s. Traefik \u00b6 Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik. HAProxy \u00b6 HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1. Istio Ingress \u00b6 Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio. ALB Ingress controller \u00b6 The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service. ALB ingress deployment \u00b6 This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so. Links \u00b6 ITNext ingress controller comparison Flant ingress controller comparison","title":"Ingress Controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#kubernetes-ingress-controller","text":"The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s.","title":"Kubernetes Ingress controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#traefik","text":"Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik.","title":"Traefik"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#haproxy","text":"HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1.","title":"HAProxy"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#istio-ingress","text":"Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio.","title":"Istio Ingress"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-controller","text":"The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service.","title":"ALB Ingress controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-deployment","text":"This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so.","title":"ALB ingress deployment"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#links","text":"ITNext ingress controller comparison Flant ingress controller comparison","title":"Links"},{"location":"devops/kubernetes/kubernetes_labels/","text":"Labels are identifying metadata key/value pairs attached to objects, such as pods. Labels are intended to give meaningful and relevant information to users, but which do not directly imply semantics to the core system. \"labels\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" }","title":"Labels"},{"location":"devops/kubernetes/kubernetes_metric_server/","text":"The metrics server monitors the resource consumption inside the cluster. It populates the information in kubectl top nodes to get the node status and gives the information to automatically autoscale deployments with Horizontal pod autoscaling . To install it, you can use the metrics-server helm chart. To test that the horizontal pod autoscaling is working, follow the AWS EKS guide .","title":"Metrics Server"},{"location":"devops/kubernetes/kubernetes_namespaces/","text":"Namespaces are virtual clusters backed by the same physical cluster. It's the first level of isolation between applications. When to Use Multiple Namespaces \u00b6 Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","title":"Namespaces"},{"location":"devops/kubernetes/kubernetes_namespaces/#when-to-use-multiple-namespaces","text":"Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","title":"When to Use Multiple Namespaces"},{"location":"devops/kubernetes/kubernetes_networking/","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. If you want to get a quickly grasp on how k8s networking works, I suggest you to read StackRox's Kubernetes networking demystified article . CNI comparison \u00b6 Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis . TL;DR \u00b6 When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast. Flannel \u00b6 Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work. Calico \u00b6 Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide . Canal \u00b6 Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition. Weave Net \u00b6 Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting. AWS CNI \u00b6 AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS. Advantages of the AWS CNI \u00b6 Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive. Disadvantages of the AWS CNI \u00b6 On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet. VPC CNI Pod Density Limitations \u00b6 First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached. Cloud Portability \u00b6 Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea. Links \u00b6 StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Networking"},{"location":"devops/kubernetes/kubernetes_networking/#cni-comparison","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis .","title":"CNI comparison"},{"location":"devops/kubernetes/kubernetes_networking/#tldr","text":"When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast.","title":"TL;DR"},{"location":"devops/kubernetes/kubernetes_networking/#flannel","text":"Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work.","title":"Flannel"},{"location":"devops/kubernetes/kubernetes_networking/#calico","text":"Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide .","title":"Calico"},{"location":"devops/kubernetes/kubernetes_networking/#canal","text":"Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition.","title":"Canal"},{"location":"devops/kubernetes/kubernetes_networking/#weave-net","text":"Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting.","title":"Weave Net"},{"location":"devops/kubernetes/kubernetes_networking/#aws-cni","text":"AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS.","title":"AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#advantages-of-the-aws-cni","text":"Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive.","title":"Advantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#disadvantages-of-the-aws-cni","text":"On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet.","title":"Disadvantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#vpc-cni-pod-density-limitations","text":"First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached.","title":"VPC CNI Pod Density Limitations"},{"location":"devops/kubernetes/kubernetes_networking/#cloud-portability","text":"Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea.","title":"Cloud Portability"},{"location":"devops/kubernetes/kubernetes_networking/#links","text":"StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Links"},{"location":"devops/kubernetes/kubernetes_operators/","text":"Operators are Kubernetes specific applications (pods) that configure, manage and optimize other Kubernetes deployments automatically. A Kubernetes Operator might be able to: Install and provide sane initial configuration and sizing for your deployment, according to the specs of your Kubernetes cluster. Perform live reloading of deployments and pods to accommodate for any user requested parameter modification (hot config reloading). Safe coordination of application upgrades. Automatically scale up or down according to performance metrics. Service discovery via native Kubernetes APIs Application TLS certificate configuration Disaster recovery. Perform backups to offsite storage, integrity checks or any other maintenance task. How do they work? \u00b6 An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1. Links \u00b6 CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Operators"},{"location":"devops/kubernetes/kubernetes_operators/#how-do-they-work","text":"An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1.","title":"How do they work?"},{"location":"devops/kubernetes/kubernetes_operators/#links","text":"CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Links"},{"location":"devops/kubernetes/kubernetes_pods/","text":"Pods are the basic building block of Kubernetes, the smallest and simplest unit in the object model that you create or deploy. A Pod represents a running process on your cluster. A Pod represents a unit of deployment. It encapsulates: An application container (or, in some cases, multiple tightly coupled containers). Storage resources. A unique network IP. Options that govern how the container(s) should run.","title":"Pods"},{"location":"devops/kubernetes/kubernetes_replicasets/","text":"ReplicaSet maintains a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. You'll probably never manually use these resources, as they are defined inside the deployments . The older version of this resource are the Replication controllers .","title":"ReplicaSets"},{"location":"devops/kubernetes/kubernetes_services/","text":"A Service defines a policy to access a logical set of Pods using a reliable endpoint. Users and other programs can access pods running on your cluster seamlessly. Therefore allowing a loose coupling between dependent Pods. When a request arrives the endpoint, the kube-proxy pod of the node forwards the request to the Pods that match the service LabelSelector. Services can be exposed in different ways by specifying a type in the ServiceSpec: ClusterIP (default): Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster. NodePort : Exposes the Service on the same port of each selected Node in the cluster using NAT to the outside. LoadBalancer : Creates an external load balancer in the current cloud and assigns a fixed, external IP to the Service. To create an internal ELB of AWs add to the annotations: annotations : service.beta.kubernetes.io/aws-load-balancer-internal : 0.0.0.0/0 ExternalName : Exposes the Service using an arbitrary name by returning a CNAME record with the name. No proxy is used. If no RBAC or NetworkPolicies are applied, you can call a service of another namespace with the following nomenclature. curl {{ service_name }} . {{ service_namespace }} .svc.cluster.local","title":"Services"},{"location":"devops/kubernetes/kubernetes_storage_driver/","text":"Storage drivers are pods that through the Container Storage Interface or CSI provide an interface to use external storage services from within Kubernetes. Amazon EBS CSI storage driver \u00b6 Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .","title":"Storage Driver"},{"location":"devops/kubernetes/kubernetes_storage_driver/#amazon-ebs-csi-storage-driver","text":"Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .","title":"Amazon EBS CSI storage driver"},{"location":"devops/kubernetes/kubernetes_tools/","text":"There are several tools built to enhance the operation, installation and use of Kubernetes. Tried \u00b6 K3s : Recommended small kubernetes, like hyperkube. To try \u00b6 crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.","title":"Tools"},{"location":"devops/kubernetes/kubernetes_tools/#tried","text":"K3s : Recommended small kubernetes, like hyperkube.","title":"Tried"},{"location":"devops/kubernetes/kubernetes_tools/#to-try","text":"crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.","title":"To try"},{"location":"devops/kubernetes/kubernetes_vertical_pod_autoscaler/","text":"Kubernetes knows the amount of resources a pod needs to operate through some metadata specified in the deployment. Generally this values change and manually maintaining all the resources requested and limits is a nightmare. The Vertical pod autoscaler does data analysis on the pod metrics to automatically adjust these values. Nevertheless it's still not suggested to use it in conjunction with the horizontal pod autoscaler , so we'll need to watch out for future improvements.","title":"Vertical Pod Autoscaler"},{"location":"devops/kubernetes/kubernetes_volumes/","text":"On disk files in a Container are ephemeral by default, which presents the following issues: When a Container crashes, kubelet will restart it, but the files will be lost. When running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems with several types . configMap \u00b6 The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod. emptyDir \u00b6 An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever. hostPath \u00b6 A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys . secret \u00b6 A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage. awsElasticBlockStore \u00b6 An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume. nfs \u00b6 An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously. local \u00b6 A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk. Others \u00b6 glusterfs cephfs","title":"Volumes"},{"location":"devops/kubernetes/kubernetes_volumes/#configmap","text":"The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod.","title":"configMap"},{"location":"devops/kubernetes/kubernetes_volumes/#emptydir","text":"An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever.","title":"emptyDir"},{"location":"devops/kubernetes/kubernetes_volumes/#hostpath","text":"A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys .","title":"hostPath"},{"location":"devops/kubernetes/kubernetes_volumes/#secret","text":"A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.","title":"secret"},{"location":"devops/kubernetes/kubernetes_volumes/#awselasticblockstore","text":"An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume.","title":"awsElasticBlockStore"},{"location":"devops/kubernetes/kubernetes_volumes/#nfs","text":"An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously.","title":"nfs"},{"location":"devops/kubernetes/kubernetes_volumes/#local","text":"A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.","title":"local"},{"location":"devops/kubernetes/kubernetes_volumes/#others","text":"glusterfs cephfs","title":"Others"},{"location":"devops/prometheus/alertmanager/","text":"The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. It is configured through the alertmanager.config key of the values.yaml of the helm chart. As stated in the configuration file , it has four main keys (as templates is handled in alertmanager.config.templateFiles ): global : SMTP and API main configuration, it will be inherited by the other elements. route : Route tree definition. receivers : Notification integrations configuration. inhibit_rules : Alert inhibition configuration. Route \u00b6 A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node. Receivers \u00b6 Notification receivers are the named configurations of one or more notification integrations. Email notifications \u00b6 To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts. Rocketchat Notifications \u00b6 Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }} Inhibit rules \u00b6 Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch Alert rules \u00b6 Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules Links \u00b6 Awesome prometheus alert rules","title":"AlertManager"},{"location":"devops/prometheus/alertmanager/#route","text":"A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node.","title":"Route"},{"location":"devops/prometheus/alertmanager/#receivers","text":"Notification receivers are the named configurations of one or more notification integrations.","title":"Receivers"},{"location":"devops/prometheus/alertmanager/#email-notifications","text":"To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts.","title":"Email notifications"},{"location":"devops/prometheus/alertmanager/#rocketchat-notifications","text":"Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }}","title":"Rocketchat Notifications"},{"location":"devops/prometheus/alertmanager/#inhibit-rules","text":"Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch","title":"Inhibit rules"},{"location":"devops/prometheus/alertmanager/#alert-rules","text":"Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules","title":"Alert rules"},{"location":"devops/prometheus/alertmanager/#links","text":"Awesome prometheus alert rules","title":"Links"},{"location":"devops/prometheus/blackbox_exporter/","text":"The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP. It can be used to test: Website accessibility . Both for availability and security purposes. Website loading time . DNS response times to diagnose network latency issues. SSL certificates expiration . ICMP requests to gather network health information . Security protections such as if and endpoint stops being protected by VPN, WAF or SSL client certificate. Unauthorized read or write S3 buckets . When running, the Blackbox exporter is going to expose a HTTP endpoint that can be used in order to monitor targets over the network. By default, the Blackbox exporter exposes the /probe endpoint that is used to retrieve those metrics. The blackbox exporter is configured with a YAML configuration file made of modules . Installation \u00b6 To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply Blackbox exporter probes \u00b6 Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx HTTP endpoint working correctly \u00b6 http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint working correctly \u00b6 https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint behind client SSL certificate \u00b6 https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation. HTTPS endpoint with an specific error \u00b6 If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*' HTTP endpoint returning an error \u00b6 http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] no_follow_redirects : false HTTPS endpoint through an HTTP proxy \u00b6 https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\" HTTPS endpoint with basic auth \u00b6 https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }} HTTPs endpoint with API key \u00b6 https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }} HTTPS Put file \u00b6 Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" Check open port \u00b6 tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect Ping to the resource \u00b6 Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\" Blackbox exporter alerts \u00b6 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests Availability alerts \u00b6 The most basic probes, test if the service is up and returning. Blackbox probe failed \u00b6 Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" message : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0 Blackbox probe HTTP failure \u00b6 HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" message : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Performance alerts \u00b6 Blackbox slow probe \u00b6 Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" message : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow HTTP \u00b6 HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" message : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow ping \u00b6 Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" message : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" SSL certificate alerts \u00b6 Blackbox SSL certificate will expire in a month \u00b6 SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Blackbox SSL certificate will expire in a few days \u00b6 SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\" Blackbox SSL certificate expired \u00b6 SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" message : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Security alerts \u00b6 To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx Test endpoints protected with network policies \u00b6 Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with SSL client certificate \u00b6 Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with credentials. \u00b6 Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with WAF. \u00b6 Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized read of S3 buckets \u00b6 Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized write of S3 buckets \u00b6 Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Monitoring external access to internal services \u00b6 There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic. Troubleshooting \u00b6 To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true . Service monitors are not being created \u00b6 When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again. probe_success == 0 when using an http proxy \u00b6 Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. Links \u00b6 Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .","title":"Blackbox Exporter"},{"location":"devops/prometheus/blackbox_exporter/#installation","text":"To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply","title":"Installation"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-exporter-probes","text":"Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx","title":"Blackbox exporter probes"},{"location":"devops/prometheus/blackbox_exporter/#http-endpoint-working-correctly","text":"http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTP endpoint working correctly"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-working-correctly","text":"https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTPS endpoint working correctly"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-behind-client-ssl-certificate","text":"https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation.","title":"HTTPS endpoint behind client SSL certificate"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-an-specific-error","text":"If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*'","title":"HTTPS endpoint with an specific error"},{"location":"devops/prometheus/blackbox_exporter/#http-endpoint-returning-an-error","text":"http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] no_follow_redirects : false","title":"HTTP endpoint returning an error"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-through-an-http-proxy","text":"https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\"","title":"HTTPS endpoint through an HTTP proxy"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-basic-auth","text":"https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }}","title":"HTTPS endpoint with basic auth"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-api-key","text":"https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }}","title":"HTTPs endpoint with API key"},{"location":"devops/prometheus/blackbox_exporter/#https-put-file","text":"Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTPS Put file"},{"location":"devops/prometheus/blackbox_exporter/#check-open-port","text":"tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect","title":"Check open port"},{"location":"devops/prometheus/blackbox_exporter/#ping-to-the-resource","text":"Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\"","title":"Ping to the resource"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-exporter-alerts","text":"Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests","title":"Blackbox exporter alerts"},{"location":"devops/prometheus/blackbox_exporter/#availability-alerts","text":"The most basic probes, test if the service is up and returning.","title":"Availability alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-failed","text":"Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" message : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0","title":"Blackbox probe failed"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-http-failure","text":"HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" message : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"","title":"Blackbox probe HTTP failure"},{"location":"devops/prometheus/blackbox_exporter/#performance-alerts","text":"","title":"Performance alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-slow-probe","text":"Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" message : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1","title":"Blackbox slow probe"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-slow-http","text":"HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" message : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1","title":"Blackbox probe slow HTTP"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-slow-ping","text":"Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" message : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\"","title":"Blackbox probe slow ping"},{"location":"devops/prometheus/blackbox_exporter/#ssl-certificate-alerts","text":"","title":"SSL certificate alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-month","text":"SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"","title":"Blackbox SSL certificate will expire in a month"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-few-days","text":"SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\"","title":"Blackbox SSL certificate will expire in a few days"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-expired","text":"SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" message : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Blackbox SSL certificate expired"},{"location":"devops/prometheus/blackbox_exporter/#security-alerts","text":"To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx","title":"Security alerts"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-network-policies","text":"Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with network policies"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-ssl-client-certificate","text":"Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with SSL client certificate"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-credentials","text":"Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with credentials."},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-waf","text":"Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with WAF."},{"location":"devops/prometheus/blackbox_exporter/#unauthorized-read-of-s3-buckets","text":"Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Unauthorized read of S3 buckets"},{"location":"devops/prometheus/blackbox_exporter/#unauthorized-write-of-s3-buckets","text":"Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Unauthorized write of S3 buckets"},{"location":"devops/prometheus/blackbox_exporter/#monitoring-external-access-to-internal-services","text":"There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic.","title":"Monitoring external access to internal services"},{"location":"devops/prometheus/blackbox_exporter/#troubleshooting","text":"To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true .","title":"Troubleshooting"},{"location":"devops/prometheus/blackbox_exporter/#service-monitors-are-not-being-created","text":"When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again.","title":"Service monitors are not being created"},{"location":"devops/prometheus/blackbox_exporter/#probe_success-0-when-using-an-http-proxy","text":"Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist.","title":"probe_success == 0 when using an http proxy"},{"location":"devops/prometheus/blackbox_exporter/#links","text":"Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .","title":"Links"},{"location":"devops/prometheus/node_exporter/","text":"Node Exporter is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors. Install \u00b6 To install in kubernetes nodes, use this chart . Elsewhere use this ansible role . If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them. To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key prometheus.prometheusSpec.additionalScrapeConfigs - job_name : node_exporter ec2_sd_configs : - region : us-east-1 port : 9100 refresh_interval : 1m relabel_configs : - source_labels : [ '__meta_ec2_tag_Name' , '__meta_ec2_private_ip' ] separator : ':' target_label : instance The relabel_configs part will substitute the instance label of each target from {{ instance_ip }}:9100 to {{ instance_name }}:{{ instance_ip }} . If the worker nodes already have an IAM role with the ec2:DescribeInstances permission there is no need to specify the role_arn or access_keys and secret_key . I'm using the 11074 grafana dashboards for the blackbox exporter, which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : node_exporter : # Ref: https://grafana.com/dashboards/11074 gnetId : 11074 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply Node exporter alerts \u00b6 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. Host out of memory \u00b6 Node memory is filling up ( < 10% left). - alert : HostOutOfMemory expr : node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of memory (instance {{ $labels.instance }})\" message : \"Node memory is filling up (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host memory under memory pressure \u00b6 The node is under heavy memory pressure. High rate of major page faults. - alert : HostMemoryUnderMemoryPressure expr : rate(node_vmstat_pgmajfault[1m]) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host memory under memory pressure (instance {{ $labels.instance }})\" message : \"The node is under heavy memory pressure. High rate of major page faults.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual network throughput in \u00b6 Host network interfaces are probably receiving too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputIn expr : sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput in (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably receiving too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual network throughput out \u00b6 Host network interfaces are probably sending too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputOut expr : sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput out (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably sending too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk read rate \u00b6 Disk is probably reading too much data (> 50 MB/s) - alert : HostUnusualDiskReadRate expr : sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read rate (instance {{ $labels.instance }})\" message : \"Disk is probably reading too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk write rate \u00b6 Disk is probably writing too much data (> 50 MB/s) - alert : HostUnusualDiskWriteRate expr : sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write rate (instance {{ $labels.instance }})\" message : \"Disk is probably writing too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host out of disk space \u00b6 Disk is worryingly almost full ( < 10% left ). - alert : HostOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 10 for : 5m labels : severity : critical annotations : summary : \"Host out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Disk is almost full ( < 20% left ) - alert : HostReachingOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 20 for : 5m labels : severity : warning annotations : summary : \"Host reaching out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 20% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host disk will fill in 4 hours \u00b6 Disk will fill in 4 hours at current write rate - alert : HostDiskWillFillIn4Hours expr : predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) < 0 for : 5m labels : severity : critical annotations : summary : \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\" message : \"Disk will fill in 4 hours at current write rate\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host out of inodes \u00b6 Disk is almost running out of available inodes ( < 10% left ). - alert : HostOutOfInodes expr : node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of inodes (instance {{ $labels.instance }})\" message : \"Disk is almost running out of available inodes (< 10% left)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk read latency \u00b6 Disk latency is growing (read operations > 100ms). - alert : HostUnusualDiskReadLatency expr : rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (read operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk write latency \u00b6 Disk latency is growing (write operations > 100ms) - alert : HostUnusualDiskWriteLatency expr : rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (write operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host high CPU load \u00b6 CPU load is > 80% - alert : HostHighCpuLoad expr : 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80 for : 5m labels : severity : warning annotations : summary : \"Host high CPU load (instance {{ $labels.instance }})\" message : \"CPU load is > 80%\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host context switching \u00b6 Context switching is growing on node (> 1000 / s) # 1000 context switches is an arbitrary number. # Alert threshold depends on nature of application. # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58 - alert : HostContextSwitching expr : (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host context switching (instance {{ $labels.instance }})\" message : \"Context switching is growing on node (> 1000 / s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host swap is filling up \u00b6 Swap is filling up (>80%) - alert : HostSwapIsFillingUp expr : (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80 for : 5m labels : severity : warning annotations : summary : \"Host swap is filling up (instance {{ $labels.instance }})\" message : \"Swap is filling up (>80%)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host SystemD service crashed \u00b6 SystemD service crashed - alert : HostSystemdServiceCrashed expr : node_systemd_unit_state{state=\"failed\"} == 1 for : 5m labels : severity : warning annotations : summary : \"Host SystemD service crashed (instance {{ $labels.instance }})\" message : \"SystemD service crashed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host physical component too hot \u00b6 Physical hardware component too hot - alert : HostPhysicalComponentTooHot expr : node_hwmon_temp_celsius > 75 for : 5m labels : severity : warning annotations : summary : \"Host physical component too hot (instance {{ $labels.instance }})\" message : \"Physical hardware component too hot\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host node overtemperature alarm \u00b6 Physical node temperature alarm triggered - alert : HostNodeOvertemperatureAlarm expr : node_hwmon_temp_alarm == 1 for : 5m labels : severity : critical annotations : summary : \"Host node overtemperature alarm (instance {{ $labels.instance }})\" message : \"Physical node temperature alarm triggered\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host RAID array got inactive \u00b6 RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. - alert : HostRaidArrayGotInactive expr : node_md_state{state=\"inactive\"} > 0 for : 5m labels : severity : critical annotations : summary : \"Host RAID array got inactive (instance {{ $labels.instance }})\" message : \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host RAID disk failure \u00b6 At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap. - alert : HostRaidDiskFailure expr : node_md_disks{state=\"fail\"} > 0 for : 5m labels : severity : warning annotations : summary : \"Host RAID disk failure (instance {{ $labels.instance }})\" message : \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host kernel version deviations \u00b6 Different kernel versions are running. - alert : HostKernelVersionDeviations expr : count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) > 1 for : 5m labels : severity : warning annotations : summary : \"Host kernel version deviations (instance {{ $labels.instance }})\" message : \"Different kernel versions are running\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host OOM kill detected \u00b6 OOM kill detected - alert : HostOomKillDetected expr : increase(node_vmstat_oom_kill[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host OOM kill detected (instance {{ $labels.instance }})\" message : \"OOM kill detected\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host Network Receive Errors \u00b6 {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes. - alert : HostNetworkReceiveErrors expr : increase(node_network_receive_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Receive Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host Network Transmit Errors \u00b6 {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes. - alert : HostNetworkTransmitErrors expr : increase(node_network_transmit_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Transmit Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" References \u00b6 Git Prometheus node exporter guide Node exporter alerts","title":"Node Exporter"},{"location":"devops/prometheus/node_exporter/#install","text":"To install in kubernetes nodes, use this chart . Elsewhere use this ansible role . If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them. To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key prometheus.prometheusSpec.additionalScrapeConfigs - job_name : node_exporter ec2_sd_configs : - region : us-east-1 port : 9100 refresh_interval : 1m relabel_configs : - source_labels : [ '__meta_ec2_tag_Name' , '__meta_ec2_private_ip' ] separator : ':' target_label : instance The relabel_configs part will substitute the instance label of each target from {{ instance_ip }}:9100 to {{ instance_name }}:{{ instance_ip }} . If the worker nodes already have an IAM role with the ec2:DescribeInstances permission there is no need to specify the role_arn or access_keys and secret_key . I'm using the 11074 grafana dashboards for the blackbox exporter, which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : node_exporter : # Ref: https://grafana.com/dashboards/11074 gnetId : 11074 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply","title":"Install"},{"location":"devops/prometheus/node_exporter/#node-exporter-alerts","text":"Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection.","title":"Node exporter alerts"},{"location":"devops/prometheus/node_exporter/#host-out-of-memory","text":"Node memory is filling up ( < 10% left). - alert : HostOutOfMemory expr : node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of memory (instance {{ $labels.instance }})\" message : \"Node memory is filling up (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of memory"},{"location":"devops/prometheus/node_exporter/#host-memory-under-memory-pressure","text":"The node is under heavy memory pressure. High rate of major page faults. - alert : HostMemoryUnderMemoryPressure expr : rate(node_vmstat_pgmajfault[1m]) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host memory under memory pressure (instance {{ $labels.instance }})\" message : \"The node is under heavy memory pressure. High rate of major page faults.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host memory under memory pressure"},{"location":"devops/prometheus/node_exporter/#host-unusual-network-throughput-in","text":"Host network interfaces are probably receiving too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputIn expr : sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput in (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably receiving too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual network throughput in"},{"location":"devops/prometheus/node_exporter/#host-unusual-network-throughput-out","text":"Host network interfaces are probably sending too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputOut expr : sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput out (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably sending too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual network throughput out"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-read-rate","text":"Disk is probably reading too much data (> 50 MB/s) - alert : HostUnusualDiskReadRate expr : sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read rate (instance {{ $labels.instance }})\" message : \"Disk is probably reading too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk read rate"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-write-rate","text":"Disk is probably writing too much data (> 50 MB/s) - alert : HostUnusualDiskWriteRate expr : sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write rate (instance {{ $labels.instance }})\" message : \"Disk is probably writing too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk write rate"},{"location":"devops/prometheus/node_exporter/#host-out-of-disk-space","text":"Disk is worryingly almost full ( < 10% left ). - alert : HostOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 10 for : 5m labels : severity : critical annotations : summary : \"Host out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Disk is almost full ( < 20% left ) - alert : HostReachingOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 20 for : 5m labels : severity : warning annotations : summary : \"Host reaching out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 20% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of disk space"},{"location":"devops/prometheus/node_exporter/#host-disk-will-fill-in-4-hours","text":"Disk will fill in 4 hours at current write rate - alert : HostDiskWillFillIn4Hours expr : predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) < 0 for : 5m labels : severity : critical annotations : summary : \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\" message : \"Disk will fill in 4 hours at current write rate\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host disk will fill in 4 hours"},{"location":"devops/prometheus/node_exporter/#host-out-of-inodes","text":"Disk is almost running out of available inodes ( < 10% left ). - alert : HostOutOfInodes expr : node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of inodes (instance {{ $labels.instance }})\" message : \"Disk is almost running out of available inodes (< 10% left)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of inodes"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-read-latency","text":"Disk latency is growing (read operations > 100ms). - alert : HostUnusualDiskReadLatency expr : rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (read operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk read latency"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-write-latency","text":"Disk latency is growing (write operations > 100ms) - alert : HostUnusualDiskWriteLatency expr : rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (write operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk write latency"},{"location":"devops/prometheus/node_exporter/#host-high-cpu-load","text":"CPU load is > 80% - alert : HostHighCpuLoad expr : 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80 for : 5m labels : severity : warning annotations : summary : \"Host high CPU load (instance {{ $labels.instance }})\" message : \"CPU load is > 80%\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host high CPU load"},{"location":"devops/prometheus/node_exporter/#host-context-switching","text":"Context switching is growing on node (> 1000 / s) # 1000 context switches is an arbitrary number. # Alert threshold depends on nature of application. # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58 - alert : HostContextSwitching expr : (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host context switching (instance {{ $labels.instance }})\" message : \"Context switching is growing on node (> 1000 / s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host context switching"},{"location":"devops/prometheus/node_exporter/#host-swap-is-filling-up","text":"Swap is filling up (>80%) - alert : HostSwapIsFillingUp expr : (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80 for : 5m labels : severity : warning annotations : summary : \"Host swap is filling up (instance {{ $labels.instance }})\" message : \"Swap is filling up (>80%)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host swap is filling up"},{"location":"devops/prometheus/node_exporter/#host-systemd-service-crashed","text":"SystemD service crashed - alert : HostSystemdServiceCrashed expr : node_systemd_unit_state{state=\"failed\"} == 1 for : 5m labels : severity : warning annotations : summary : \"Host SystemD service crashed (instance {{ $labels.instance }})\" message : \"SystemD service crashed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host SystemD service crashed"},{"location":"devops/prometheus/node_exporter/#host-physical-component-too-hot","text":"Physical hardware component too hot - alert : HostPhysicalComponentTooHot expr : node_hwmon_temp_celsius > 75 for : 5m labels : severity : warning annotations : summary : \"Host physical component too hot (instance {{ $labels.instance }})\" message : \"Physical hardware component too hot\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host physical component too hot"},{"location":"devops/prometheus/node_exporter/#host-node-overtemperature-alarm","text":"Physical node temperature alarm triggered - alert : HostNodeOvertemperatureAlarm expr : node_hwmon_temp_alarm == 1 for : 5m labels : severity : critical annotations : summary : \"Host node overtemperature alarm (instance {{ $labels.instance }})\" message : \"Physical node temperature alarm triggered\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host node overtemperature alarm"},{"location":"devops/prometheus/node_exporter/#host-raid-array-got-inactive","text":"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. - alert : HostRaidArrayGotInactive expr : node_md_state{state=\"inactive\"} > 0 for : 5m labels : severity : critical annotations : summary : \"Host RAID array got inactive (instance {{ $labels.instance }})\" message : \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host RAID array got inactive"},{"location":"devops/prometheus/node_exporter/#host-raid-disk-failure","text":"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap. - alert : HostRaidDiskFailure expr : node_md_disks{state=\"fail\"} > 0 for : 5m labels : severity : warning annotations : summary : \"Host RAID disk failure (instance {{ $labels.instance }})\" message : \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host RAID disk failure"},{"location":"devops/prometheus/node_exporter/#host-kernel-version-deviations","text":"Different kernel versions are running. - alert : HostKernelVersionDeviations expr : count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) > 1 for : 5m labels : severity : warning annotations : summary : \"Host kernel version deviations (instance {{ $labels.instance }})\" message : \"Different kernel versions are running\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host kernel version deviations"},{"location":"devops/prometheus/node_exporter/#host-oom-kill-detected","text":"OOM kill detected - alert : HostOomKillDetected expr : increase(node_vmstat_oom_kill[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host OOM kill detected (instance {{ $labels.instance }})\" message : \"OOM kill detected\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host OOM kill detected"},{"location":"devops/prometheus/node_exporter/#host-network-receive-errors","text":"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes. - alert : HostNetworkReceiveErrors expr : increase(node_network_receive_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Receive Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host Network Receive Errors"},{"location":"devops/prometheus/node_exporter/#host-network-transmit-errors","text":"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes. - alert : HostNetworkTransmitErrors expr : increase(node_network_transmit_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Transmit Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host Network Transmit Errors"},{"location":"devops/prometheus/node_exporter/#references","text":"Git Prometheus node exporter guide Node exporter alerts","title":"References"},{"location":"devops/prometheus/prometheus/","text":"Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. A quick overview of Prometheus would be, as stated in the coreos article : At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented. Go to the Prometheus architecture post for more details. We are living a shift to the DevOps culture, containers and Kubernetes. So nowadays: Developers need to integrate app and business related metrics as an organic part of the infrastructure. So monitoring needs to be democratized, made more accessible and cover additional layers of the stack. Container based infrastructures are changing how we monitor the resources. Now we have a huge number of volatile software entities, services, virtual network addresses, exposed metrics that suddenly appear or vanish. Traditional monitoring tools are not designed to handle this. These reasons pushed Soundcloud to build a new monitoring system that had the following features Multi-dimensional data model : The model is based on key-value pairs, similar to how Kubernetes itself organizes infrastructure metadata using labels. It allows for flexible and accurate time series data, powering its Prometheus query language. Accessible format and protocols : Exposing prometheus metrics is a pretty straightforward task. Metrics are human readable, are in a self-explanatory format, and are published using a standard HTTP transport. You can check that the metrics are correctly exposed just using your web browser. Service discovery : The Prometheus server is in charge of periodically scraping the targets, so that applications and services don\u2019t need to worry about emitting data (metrics are pulled, not pushed). These Prometheus servers have several methods to auto-discover scrape targets, some of them can be configured to filter and match container metadata, making it an excellent fit for ephemeral Kubernetes workloads. Modular and highly available components : Metric collection, alerting, graphical visualization, etc, are performed by different composable services. All these services are designed to support redundancy and sharding. Pull based metrics : Most monitoring systems are pushing metrics to a centralized collection platform. Prometheus flips this model on it's head with the following advantages: No need to install custom software in the physical servers or containers. Doesn't require applications to use CPU cycles pushing metrics. Handles service failure/unavailability gracefully. If a target goes down, Prometheus can record it was unable to retrieve data. You can use the Pushgateway if pulling metrics is not feasible. Installation \u00b6 There are several ways to install prometheus , but I'd recommend using the Kubernetes Prometheus operator . Exposing your metrics \u00b6 Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics . Metric types \u00b6 There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately. Using labels \u00b6 Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics. Links \u00b6 Homepage . Docs . Awesome Prometheus . Diving deeper \u00b6 Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Node Exporter Prometheus Troubleshooting Introduction posts \u00b6 Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview Books \u00b6 Prometheus Up & Running . Monitoring With Prometheus .","title":"Prometheus"},{"location":"devops/prometheus/prometheus/#installation","text":"There are several ways to install prometheus , but I'd recommend using the Kubernetes Prometheus operator .","title":"Installation"},{"location":"devops/prometheus/prometheus/#exposing-your-metrics","text":"Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics .","title":"Exposing your metrics"},{"location":"devops/prometheus/prometheus/#metric-types","text":"There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately.","title":"Metric types"},{"location":"devops/prometheus/prometheus/#using-labels","text":"Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics.","title":"Using labels"},{"location":"devops/prometheus/prometheus/#links","text":"Homepage . Docs . Awesome Prometheus .","title":"Links"},{"location":"devops/prometheus/prometheus/#diving-deeper","text":"Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Node Exporter Prometheus Troubleshooting","title":"Diving deeper"},{"location":"devops/prometheus/prometheus/#introduction-posts","text":"Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview","title":"Introduction posts"},{"location":"devops/prometheus/prometheus/#books","text":"Prometheus Up & Running . Monitoring With Prometheus .","title":"Books"},{"location":"devops/prometheus/prometheus_architecture/","text":"Prometheus Server \u00b6 Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries. Prometheus Targets \u00b6 Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX . Pushgateway \u00b6 In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server. Service discovery \u00b6 Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring. Alertmanager \u00b6 The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. Data visualization and export \u00b6 There are several ways to visualize or export data from Prometheus. Prometheus web UI \u00b6 Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery. Grafana \u00b6 Grafana is the best way to visually analyze the evolution of the metrics throughout time. API clients \u00b6 Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too. Links \u00b6 Prometheus Overview Open Source for U architecture overview","title":"Architecture"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-server","text":"Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries.","title":"Prometheus Server"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-targets","text":"Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX .","title":"Prometheus Targets"},{"location":"devops/prometheus/prometheus_architecture/#pushgateway","text":"In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server.","title":"Pushgateway"},{"location":"devops/prometheus/prometheus_architecture/#service-discovery","text":"Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring.","title":"Service discovery"},{"location":"devops/prometheus/prometheus_architecture/#alertmanager","text":"The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.","title":"Alertmanager"},{"location":"devops/prometheus/prometheus_architecture/#data-visualization-and-export","text":"There are several ways to visualize or export data from Prometheus.","title":"Data visualization and export"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-web-ui","text":"Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery.","title":"Prometheus web UI"},{"location":"devops/prometheus/prometheus_architecture/#grafana","text":"Grafana is the best way to visually analyze the evolution of the metrics throughout time.","title":"Grafana"},{"location":"devops/prometheus/prometheus_architecture/#api-clients","text":"Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too.","title":"API clients"},{"location":"devops/prometheus/prometheus_architecture/#links","text":"Prometheus Overview Open Source for U architecture overview","title":"Links"},{"location":"devops/prometheus/prometheus_installation/","text":"To install the operator we'll use helmfile to install the stable/prometheus-operator chart . Add the following lines to your helmfile.yaml . - name : prometheus-operator namespace : monitoring chart : stable/prometheus-operator values : - prometheus-operator/values.yaml Edit the chart values. mkdir prometheus-operator helm inspect values stable/prometheus-operator > prometheus-operator/values.yaml vi prometheus-operator/values.yaml I've implemented the following changes: If you are using a managed solution like EKS, the provider will hide kube-scheduler and kube-controller-manager so those metrics will fail. Therefore you need to disable: defaultRules.rules.kubeScheduler: false . kubeScheduler.enabled: false . kubeControllerManager.enabled: false . Configure the alertmanager . Enabled the ingress of alertmanager , grafana and prometheus . Set up the storage of alertmanager and prometheus with storageClassName: gp2 (for AWS). Configure the grafana dashboards: Blackbox grafana dashboard Change additionalPrometheusRules to additionalPrometheusRulesMap as the former is going to be deprecated in future releases. For private clusters, disable the admission webhook . prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false And install. helmfile diff helmfile apply Once it's installed you can check everything is working by accessing the grafana dashboard. First of all get the pod name (we'll asume you've used the monitoring namespace). kubectl get pods -n monitoring | grep grafana Then set up the proxies kubectl port-forward {{ grafana_pod }} -n monitoring 3000 :3000 kubectl port-forward -n monitoring \\ prometheus-prometheus-operator-prometheus-0 9090 :9090 To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090 . If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved. Edit the 127.0.0.1 value to 0.0.0.0 for the key metricsBindAddress in kubectl -n kube-system edit cm kube-proxy-config And restart the DaemonSet: kubectl rollout restart -n kube-system daemonset.apps/kube-proxy Creating alerts \u00b6 Alerts are configured in Installing other exporters \u00b6 Blackbox Exporter","title":"Prometheus Install"},{"location":"devops/prometheus/prometheus_installation/#creating-alerts","text":"Alerts are configured in","title":"Creating alerts"},{"location":"devops/prometheus/prometheus_installation/#installing-other-exporters","text":"Blackbox Exporter","title":"Installing other exporters"},{"location":"devops/prometheus/prometheus_operator/","text":"Prometheus has it's own kubernetes operator , which makes it simple to install with helm, and enables users to configure and manage instances of Prometheus using simple declarative configuration that will, in response, create, configure, and manage Prometheus monitoring instances. Once installed the Prometheus Operator provides the following features: Create/Destroy : Easily launch a Prometheus instance for your Kubernetes namespace, a specific application or team easily using the Operator. Simple Configuration : Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource. Target Services via Labels : Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. How it works \u00b6 The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running. Links \u00b6 Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3","title":"Prometheus Operator"},{"location":"devops/prometheus/prometheus_operator/#how-it-works","text":"The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running.","title":"How it works"},{"location":"devops/prometheus/prometheus_operator/#links","text":"Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3","title":"Links"},{"location":"devops/prometheus/prometheus_troubleshooting/","text":"Solutions for problems with Prometheus. Service monitor not being recognized \u00b6 Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work. Failed calling webhook prometheusrulemutate.monitoring.coreos.com \u00b6 Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.","title":"Prometheus Troubleshooting"},{"location":"devops/prometheus/prometheus_troubleshooting/#service-monitor-not-being-recognized","text":"Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work.","title":"Service monitor not being recognized"},{"location":"devops/prometheus/prometheus_troubleshooting/#failed-calling-webhook-prometheusrulemutatemonitoringcoreoscom","text":"Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.","title":"Failed calling webhook prometheusrulemutate.monitoring.coreos.com"},{"location":"drawing/drawing/","text":"It's really difficult to choose where to start if you want to learn how to draw from scratch as there are too many resources. I'm starting with Drawabox , a free course based on series of practical exercises to teach the basics. I'd probably go to Ctrl+Paint once I'm done. The basics \u00b6 Changing the mindset \u00b6 Start your drawing path with the following guidelines to make your progression smoother: Focus on the experience of drawing instead of the result. Understand that doing something badly does not define who you are. So tackle the I can't draw that feeling with I can't draw that well . If you're afraid the thing you want to draw is out of your reach, draw it anyway. At least half of the time spent drawing must be devoted to drawing purely for its own sake. If you don't have much time, alternate the purpose of your sessions. Don't over control your hand with your brain to try to be absolutely precise and accurate. Doing so will result in numerous course corrections making your strokes wobbly, stiff and erratic. Furthermore, spending all the focus resources in precision, will result in a lack to solve the other problems involved. Once muscle memory is gained, the strokes will be cleaner. Draw exactly what you see, while you see it . Don't trust you memory, as it will simplify things without you noticing it. Draw from your shoulder . We are used to pivot on the wrist as it makes stiff and accurate linework, suitable for writing. But falls apart when making smooth and consistent strokes. So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get yo u in the habit of taking the path of least resistance . So try to use the shoulder. This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow. If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder. Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should: Flow continuously : When making a line between two points, do it with a single continuous stroke even if you miss the end. Flow smoothly : Draw with a confident and persistent pace (enough to keep your brain from interfering and attempting to course correct as you go). Again we favor flow over accuracy , so expect to make your lines less accurate. Maintain a consistent trayectory : Split lines into derivable strokes. Otherwise, you'll make mindless zigzags. Drawing skills \u00b6 The course focuses on these psychological skills and concepts : Confidence : The willingness to push forwards without hesitation once your preparations are complete. Control : The ability to decide ahead of time what kind of mark you wish to puto down on the page, and to execute as intended. Patience : The path is hard. Spatial Reasoning : To be able to understand the things we draw as being three dimensional forms that exist in and relate to one another within the three dimensional world. Construction : The ability to look at a complex object and break it down into simple components that can be drawn individually and combined to reconstruct our complex object on a page. Visual Communication : The ability to take a concept, idea, or amount of information, and to convey it clearly and directly to an audience using visual means. Ghosting \u00b6 Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down. Planning : Lay out the terms of the line you want to draw, paint a dot for the start and another for the end. Rotating the page : Find the most comfortable angle of approach for the line you've planned. Usually it's roughly a 45 degree angle fom left to right (if you're right handed). Ghosting : Go through the motion of drawing your line, over and over, in one direction, without actually touching the page, so as to build muscle memory. Execution : Once you feel comfortable with the motion, without missing a beat or breaking the rhythm of repetition, lower your pen to the page and go through the motion one more time. Drawabox course guidelines \u00b6 When doing the course exercises, try to: Read only the instruction pages that are assigned to each exercise. Don't redo the exercises until you achieve perfection, even when you don't feel satisfied with your results. Accept this now, and it will save you a lot of grief and wasted time in the future. Your only focus should be on following the instructions to the best of your current ability. Read and follow the instructions carefully to ensure you understand them. Each time you finish an exercise incorporate into a pool from which take two or three at the beginning of each session to do for 10 or 15 minutes. Tools to use \u00b6 For the Drawabox course, you need a fineliner , also called felt tips or technical pens . The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5). When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink. For the paper, use the regular printer one. Links \u00b6 Drawabox Ctrl+Paint Dive deeper \u00b6 Pool of drawing exercises","title":"Drawing"},{"location":"drawing/drawing/#the-basics","text":"","title":"The basics"},{"location":"drawing/drawing/#changing-the-mindset","text":"Start your drawing path with the following guidelines to make your progression smoother: Focus on the experience of drawing instead of the result. Understand that doing something badly does not define who you are. So tackle the I can't draw that feeling with I can't draw that well . If you're afraid the thing you want to draw is out of your reach, draw it anyway. At least half of the time spent drawing must be devoted to drawing purely for its own sake. If you don't have much time, alternate the purpose of your sessions. Don't over control your hand with your brain to try to be absolutely precise and accurate. Doing so will result in numerous course corrections making your strokes wobbly, stiff and erratic. Furthermore, spending all the focus resources in precision, will result in a lack to solve the other problems involved. Once muscle memory is gained, the strokes will be cleaner. Draw exactly what you see, while you see it . Don't trust you memory, as it will simplify things without you noticing it. Draw from your shoulder . We are used to pivot on the wrist as it makes stiff and accurate linework, suitable for writing. But falls apart when making smooth and consistent strokes. So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get yo u in the habit of taking the path of least resistance . So try to use the shoulder. This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow. If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder. Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should: Flow continuously : When making a line between two points, do it with a single continuous stroke even if you miss the end. Flow smoothly : Draw with a confident and persistent pace (enough to keep your brain from interfering and attempting to course correct as you go). Again we favor flow over accuracy , so expect to make your lines less accurate. Maintain a consistent trayectory : Split lines into derivable strokes. Otherwise, you'll make mindless zigzags.","title":"Changing the mindset"},{"location":"drawing/drawing/#drawing-skills","text":"The course focuses on these psychological skills and concepts : Confidence : The willingness to push forwards without hesitation once your preparations are complete. Control : The ability to decide ahead of time what kind of mark you wish to puto down on the page, and to execute as intended. Patience : The path is hard. Spatial Reasoning : To be able to understand the things we draw as being three dimensional forms that exist in and relate to one another within the three dimensional world. Construction : The ability to look at a complex object and break it down into simple components that can be drawn individually and combined to reconstruct our complex object on a page. Visual Communication : The ability to take a concept, idea, or amount of information, and to convey it clearly and directly to an audience using visual means.","title":"Drawing skills"},{"location":"drawing/drawing/#ghosting","text":"Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down. Planning : Lay out the terms of the line you want to draw, paint a dot for the start and another for the end. Rotating the page : Find the most comfortable angle of approach for the line you've planned. Usually it's roughly a 45 degree angle fom left to right (if you're right handed). Ghosting : Go through the motion of drawing your line, over and over, in one direction, without actually touching the page, so as to build muscle memory. Execution : Once you feel comfortable with the motion, without missing a beat or breaking the rhythm of repetition, lower your pen to the page and go through the motion one more time.","title":"Ghosting"},{"location":"drawing/drawing/#drawabox-course-guidelines","text":"When doing the course exercises, try to: Read only the instruction pages that are assigned to each exercise. Don't redo the exercises until you achieve perfection, even when you don't feel satisfied with your results. Accept this now, and it will save you a lot of grief and wasted time in the future. Your only focus should be on following the instructions to the best of your current ability. Read and follow the instructions carefully to ensure you understand them. Each time you finish an exercise incorporate into a pool from which take two or three at the beginning of each session to do for 10 or 15 minutes.","title":"Drawabox course guidelines"},{"location":"drawing/drawing/#tools-to-use","text":"For the Drawabox course, you need a fineliner , also called felt tips or technical pens . The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5). When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink. For the paper, use the regular printer one.","title":"Tools to use"},{"location":"drawing/drawing/#links","text":"Drawabox Ctrl+Paint","title":"Links"},{"location":"drawing/drawing/#dive-deeper","text":"Pool of drawing exercises","title":"Dive deeper"},{"location":"drawing/exercise_pool/","text":"Set of exercises to maintain the fundamental skills required for drawing. Before doing a drawing session, spend 10-20 minutes doing one or several of these exercises. Superimposed lines : for different increasing lengths (4cm, 8cm, half the width and full width), draw a line with a ruler and repeat the stroke freehand eight times. Also try some arcing lines, and even some waves. Make sure you fray only at the end. Example: Ghosted lines : Fill up a page with straight lines following the ghosting method . Special things to avoid: Wobbly lines Arcing lines There are several levels of success with this exercise: Level 1 : Line is smooth and consistent without any visible wobbling, but doesn't quite pass through A or B, due to not following the right trajectory. It's a straight shot, but misses the mark a bit. Level 2 : Like level 1 and maintains the correct trajectory. It does however either fall short or overshot one or both points. Level 3 : Like level 2 and also starts at right at one point and ends exactly at the other. Example: Ghosted planes : Fill up a page with planes using the ghosting method . Start with 4 points, join them, then fill in the two diagonals, and then make a cross through the center of the X. Special things to avoid: Wobbly lines Arcing lines Example: As you repeat the exercise, you can start to envision these planes as being three dimensional rectilinear surfaces. The third and fourth steps, where we construct the diagonals and the cross can be treated as being a subdivision of the plane. The cross will require some estimation to find the center of each edge in space.","title":"Exercise Pool"},{"location":"life_automation/life_automation/","text":"Life Automation is the act of analyzing your interactions with the world to find ways to reduce the time or willpower spent on unwanted processes. Once you've covered some minimum life requirements (health, money or happiness), time is your most valued asset. It's sad to waste it doing stuff that we need but don't increase our happiness. So the idea is to identify which are those processes and find optimizations that allows us to do them in less time or using less willpower. I've also faced the problem of having so much stuff in my mind. Having background processes increase your brain load and are a constant sink of willpower. As a result, when you really need that CPU time, your brain is tired and doesn't work to it's full performance. Automating processes, like life logging and task management, allows you to delegate those worries. Life automation can lead to habit building, which reduces even more the willpower consumption of processes, at the same time it reduces the error rate. Automating life management \u00b6 Week automation , or how to review and plan the week. Automating home chores \u00b6 Using grocy to maintain the house stock, shopping lists and meal plans.","title":"Life Automation"},{"location":"life_automation/life_automation/#automating-life-management","text":"Week automation , or how to review and plan the week.","title":"Automating life management"},{"location":"life_automation/life_automation/#automating-home-chores","text":"Using grocy to maintain the house stock, shopping lists and meal plans.","title":"Automating home chores"},{"location":"life_automation/vim_automation/","text":"Abbreviations \u00b6 In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, ~/.vim/abbreviations.vim for abbreviations that can be used in every type of format and ~/.vim/markdown-abbreviations.vim for the ones that can interfere with programming typing. Those files are sourced in my .vimrc \" Abbreviations source ~ /.vim/ abbreviations. vim autocmd BufNewFile , BufReadPost *.md source ~ /.vim/ markdown - abbreviations. vim To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as teh . The process has it's inconveniences: You need different abbreviations for the capitalized versions, so you'd need two abbreviations for iab cant can't and iab Cant Can't It's not user friendly to add new words, as you need to open a file. The Vim Abolish plugin solves that. For example: \" Typing the following: Abolish seperate separate \" Is equivalent to: iabbrev seperate separate iabbrev Seperate Separate iabbrev SEPERATE SEPARATE Or create more complex rules, were each {} gets captured and expanded with different caps : Abolish {despa , sepe}rat{ e , es , ed , ing , ely , ion , ions , or} {despe , sepa}rat{} With a bang ( :Abolish! ) the abbreviation is also appended to the file in g:abolish_save_file . By default after/plugin/abolish.vim which is loaded by default. Typing :Abolish! im I'm will append the following to the end of this file: Abolish im I' m To make it quicker I've added a mapping for <leader>s . nnoremap < leader > s :Abolish !< Space > Check the README for more details. Troubleshooting \u00b6 Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue . Abolish knobas knowledge - based Abolish w what Will yield KnowledgeBased if invoked with Knobas , and WHAT if invoked with W . Therefore the following definitions are preferred: Abolish Knobas Knowledge - based Abolish W What","title":"Vim Automation"},{"location":"life_automation/vim_automation/#abbreviations","text":"In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, ~/.vim/abbreviations.vim for abbreviations that can be used in every type of format and ~/.vim/markdown-abbreviations.vim for the ones that can interfere with programming typing. Those files are sourced in my .vimrc \" Abbreviations source ~ /.vim/ abbreviations. vim autocmd BufNewFile , BufReadPost *.md source ~ /.vim/ markdown - abbreviations. vim To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as teh . The process has it's inconveniences: You need different abbreviations for the capitalized versions, so you'd need two abbreviations for iab cant can't and iab Cant Can't It's not user friendly to add new words, as you need to open a file. The Vim Abolish plugin solves that. For example: \" Typing the following: Abolish seperate separate \" Is equivalent to: iabbrev seperate separate iabbrev Seperate Separate iabbrev SEPERATE SEPARATE Or create more complex rules, were each {} gets captured and expanded with different caps : Abolish {despa , sepe}rat{ e , es , ed , ing , ely , ion , ions , or} {despe , sepa}rat{} With a bang ( :Abolish! ) the abbreviation is also appended to the file in g:abolish_save_file . By default after/plugin/abolish.vim which is loaded by default. Typing :Abolish! im I'm will append the following to the end of this file: Abolish im I' m To make it quicker I've added a mapping for <leader>s . nnoremap < leader > s :Abolish !< Space > Check the README for more details.","title":"Abbreviations"},{"location":"life_automation/vim_automation/#troubleshooting","text":"Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue . Abolish knobas knowledge - based Abolish w what Will yield KnowledgeBased if invoked with Knobas , and WHAT if invoked with W . Therefore the following definitions are preferred: Abolish Knobas Knowledge - based Abolish W What","title":"Troubleshooting"},{"location":"life_automation/week_automation/","text":"I've been polishing a week reviewing and planning method that suits my needs. I usually follow it on Wednesdays, as I'm too busy on Mondays and Tuesdays and it gives enough time to plan the weekend. Until I've got pydo ready to natively incorporate all this processes, I heavily use taskwarrior to manage my tasks and logs. To make the process faster and reproducible, I've written small python scripts using tasklib. Week review \u00b6 Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date. Week planning \u00b6 The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week Automation"},{"location":"life_automation/week_automation/#week-review","text":"Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date.","title":"Week review"},{"location":"life_automation/week_automation/#week-planning","text":"The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week planning"},{"location":"linux/brew/","text":"Complementary package manager to manage the programs that aren't in the Debian repositories. Usage \u00b6 TBC References \u00b6 Homebrew formula for a Go app","title":"brew"},{"location":"linux/brew/#usage","text":"TBC","title":"Usage"},{"location":"linux/brew/#references","text":"Homebrew formula for a Go app","title":"References"},{"location":"linux/cookiecutter/","text":"Cookiecutter is a command-line utility that creates projects from cookiecutters (project templates). Install \u00b6 pip install cookiecutter Use \u00b6 cookiecutter {{ path_or_url_to_cookiecutter_template }} Write your own cookietemplates \u00b6 Create files or directories with conditions \u00b6 For files use a filename like '{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}' . For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information. Add some text to a file if a condition is met \u00b6 Use jinja2 conditionals. Note the - at the end of the conditional opening, play with {%- ... -%} and {% ... %} for different results on line appending. { % if cookiecutter.install_docker == 'yes' -% } - src : git+ssh://mywebpage.org/ansible-roles/docker.git version : 1.0.3 { % endif % } Initialize git repository on the created cookiecutter \u00b6 Added the following to the post generation hooks. File: hooks/post_gen_project.py import subprocess subprocess . call ([ 'git' , 'init' ]) subprocess . call ([ 'git' , 'add' , '*' ]) subprocess . call ([ 'git' , 'commit' , '-m' , 'Initial commit' ]) Testing your own cookiecutter templates \u00b6 The pytest-cookies plugin comes with a cookies fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests. Install \u00b6 pip install pytest-cookies Usage \u00b6 @pytest.fixture def context(): return { \"playbook_name\": \"My Test Playbook\", } The cookies.bake() method generates a new project from your template based on the default values specified in cookiecutter.json: def test_bake_project ( cookies ): result = cookies . bake ( extra_context = { 'repo_name' : 'hello world' }) assert result . exit_code == 0 assert result . exception is None assert result . project . basename == 'hello world' assert result . project . isdir () It accepts the extra_context keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data. The cookiecutter-django has a nice test file using this fixture. Mocking the contents of the cookiecutter hooks \u00b6 Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline. If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the cookies.bake() code. Instead it delegates in cookiecutter to run them, which opens a subprocess to run them , so the mocks don't work. The alternative is setting an environmental variable in your tests to skip those steps: File: tests/conftest.py import os os . environ [ \"COOKIECUTTER_TESTING\" ] = \"true\" File: hooks/pre_gen_project.py def main (): # ... pre_hook content ... if __name__ == \"__main__\" : if os . environ . get ( \"COOKIECUTTER_TESTING\" ) != \"true\" : main () If you want to test the content of main , you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors. References \u00b6 Git Docs","title":"cookiecutter"},{"location":"linux/cookiecutter/#install","text":"pip install cookiecutter","title":"Install"},{"location":"linux/cookiecutter/#use","text":"cookiecutter {{ path_or_url_to_cookiecutter_template }}","title":"Use"},{"location":"linux/cookiecutter/#write-your-own-cookietemplates","text":"","title":"Write your own cookietemplates"},{"location":"linux/cookiecutter/#create-files-or-directories-with-conditions","text":"For files use a filename like '{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}' . For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information.","title":"Create files or directories with conditions"},{"location":"linux/cookiecutter/#add-some-text-to-a-file-if-a-condition-is-met","text":"Use jinja2 conditionals. Note the - at the end of the conditional opening, play with {%- ... -%} and {% ... %} for different results on line appending. { % if cookiecutter.install_docker == 'yes' -% } - src : git+ssh://mywebpage.org/ansible-roles/docker.git version : 1.0.3 { % endif % }","title":"Add some text to a file if a condition is met"},{"location":"linux/cookiecutter/#initialize-git-repository-on-the-created-cookiecutter","text":"Added the following to the post generation hooks. File: hooks/post_gen_project.py import subprocess subprocess . call ([ 'git' , 'init' ]) subprocess . call ([ 'git' , 'add' , '*' ]) subprocess . call ([ 'git' , 'commit' , '-m' , 'Initial commit' ])","title":"Initialize git repository on the created cookiecutter"},{"location":"linux/cookiecutter/#testing-your-own-cookiecutter-templates","text":"The pytest-cookies plugin comes with a cookies fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests.","title":"Testing your own cookiecutter templates"},{"location":"linux/cookiecutter/#install_1","text":"pip install pytest-cookies","title":"Install"},{"location":"linux/cookiecutter/#usage","text":"@pytest.fixture def context(): return { \"playbook_name\": \"My Test Playbook\", } The cookies.bake() method generates a new project from your template based on the default values specified in cookiecutter.json: def test_bake_project ( cookies ): result = cookies . bake ( extra_context = { 'repo_name' : 'hello world' }) assert result . exit_code == 0 assert result . exception is None assert result . project . basename == 'hello world' assert result . project . isdir () It accepts the extra_context keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data. The cookiecutter-django has a nice test file using this fixture.","title":"Usage"},{"location":"linux/cookiecutter/#mocking-the-contents-of-the-cookiecutter-hooks","text":"Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline. If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the cookies.bake() code. Instead it delegates in cookiecutter to run them, which opens a subprocess to run them , so the mocks don't work. The alternative is setting an environmental variable in your tests to skip those steps: File: tests/conftest.py import os os . environ [ \"COOKIECUTTER_TESTING\" ] = \"true\" File: hooks/pre_gen_project.py def main (): # ... pre_hook content ... if __name__ == \"__main__\" : if os . environ . get ( \"COOKIECUTTER_TESTING\" ) != \"true\" : main () If you want to test the content of main , you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors.","title":"Mocking the contents of the cookiecutter hooks"},{"location":"linux/cookiecutter/#references","text":"Git Docs","title":"References"},{"location":"linux/fail2ban/","text":"Usage \u00b6 Unban IP \u00b6 fail2ban-client set {{ jail }} unbanip {{ ip }} Where jail can be ssh .","title":"fail2ban"},{"location":"linux/fail2ban/#usage","text":"","title":"Usage"},{"location":"linux/fail2ban/#unban-ip","text":"fail2ban-client set {{ jail }} unbanip {{ ip }} Where jail can be ssh .","title":"Unban IP"},{"location":"linux/google_chrome/","text":"Although I hate it, there are web pages that don't work on Firefox or Chromium. In those cases I install google-chrome and uninstall as soon as I don't need to use that service. Installation \u00b6 Debian \u00b6 wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb","title":"google chrome"},{"location":"linux/google_chrome/#installation","text":"","title":"Installation"},{"location":"linux/google_chrome/#debian","text":"wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb","title":"Debian"},{"location":"linux/hypothesis/","text":"Hypothesis is an open-source software project that aims to collect comments about statements made in any web-accessible content, and filter and rank those comments to assess each statement's credibility. It offers an online web application where registered users share highlights and annotations over any webpage. As of 2020-06-11, although the service can be self-hosted, it's not yet easy to do so. Install \u00b6 Client \u00b6 If you're using Chrome or any derivative there is an official extension. Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine . Alternatively you can use the Hypothesis bookmarklet . The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings . Though there is yet no documentation on this topic. I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me . Server \u00b6 The infrastructure can be deployed with Docker-compose. version : '3' services : postgres : image : postgres:11.5-alpine ports : - 5432 # - '5432:5432' elasticsearch : image : hypothesis/elasticsearch:latest ports : - 9200 #- '9200:9200' environment : - discovery.type=single-node rabbit : image : rabbitmq:3.6-management-alpine ports : - 5672 - 15672 #- '5672:5672' #- '15672:15672' web : image : hypothesis/hypothesis:latest environment : - APP_URL=http://localhost:5000 - AUTHORITY=localhost - BROKER_URL=amqp://guest:guest@rabbit:5672// - CLIENT_OAUTH_ID - CLIENT_URL=http://localhost:3001/hypothesis - DATABASE_URL=postgresql://postgres@postgres/postgres - ELASTICSEARCH_URL=http://elasticsearch:9200 - NEW_RELIC_APP_NAME=h (dev) - NEW_RELIC_LICENSE_KEY - SECRET_KEY=notasecret ports : - '5000:5000' depends_on : - postgres - elasticsearch - rabbit docker-compose up Initialize the database and create the admin user. docker-compose exec web /bin/sh hypothesis init hypothesis user add hypothesis user admin <username> The service is available at http://localhost:5000 . To check the latest developments of the Docker compose deployment follow the issue #4899 . They also provide the tools they use to deploy the production service into AWS. References \u00b6 Homepage FAQ Bug tracker Feature request tracker Server deployment open issues \u00b6 Self-hosting Docker compose Create admin user when using Docker compose Steps required to run both h and serve the client from internal server How to deploy h on VM","title":"hypothesis"},{"location":"linux/hypothesis/#install","text":"","title":"Install"},{"location":"linux/hypothesis/#client","text":"If you're using Chrome or any derivative there is an official extension. Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine . Alternatively you can use the Hypothesis bookmarklet . The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings . Though there is yet no documentation on this topic. I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me .","title":"Client"},{"location":"linux/hypothesis/#server","text":"The infrastructure can be deployed with Docker-compose. version : '3' services : postgres : image : postgres:11.5-alpine ports : - 5432 # - '5432:5432' elasticsearch : image : hypothesis/elasticsearch:latest ports : - 9200 #- '9200:9200' environment : - discovery.type=single-node rabbit : image : rabbitmq:3.6-management-alpine ports : - 5672 - 15672 #- '5672:5672' #- '15672:15672' web : image : hypothesis/hypothesis:latest environment : - APP_URL=http://localhost:5000 - AUTHORITY=localhost - BROKER_URL=amqp://guest:guest@rabbit:5672// - CLIENT_OAUTH_ID - CLIENT_URL=http://localhost:3001/hypothesis - DATABASE_URL=postgresql://postgres@postgres/postgres - ELASTICSEARCH_URL=http://elasticsearch:9200 - NEW_RELIC_APP_NAME=h (dev) - NEW_RELIC_LICENSE_KEY - SECRET_KEY=notasecret ports : - '5000:5000' depends_on : - postgres - elasticsearch - rabbit docker-compose up Initialize the database and create the admin user. docker-compose exec web /bin/sh hypothesis init hypothesis user add hypothesis user admin <username> The service is available at http://localhost:5000 . To check the latest developments of the Docker compose deployment follow the issue #4899 . They also provide the tools they use to deploy the production service into AWS.","title":"Server"},{"location":"linux/hypothesis/#references","text":"Homepage FAQ Bug tracker Feature request tracker","title":"References"},{"location":"linux/hypothesis/#server-deployment-open-issues","text":"Self-hosting Docker compose Create admin user when using Docker compose Steps required to run both h and serve the client from internal server How to deploy h on VM","title":"Server deployment open issues"},{"location":"linux/mkdocs/","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Installation \u00b6 Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin * Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name }} site_author : {{ your_name }} site_url : {{ site_url }} nav : - Introduction : 'index.md' - Basic Usage : 'basic_usage.md' - Configuration : 'configuration.md' - Update : 'update.md' - Advanced Usage : - Projects : \"projects.md\" - Tags : \"tags.md\" plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : \"theme\" logo : \"images/logo.png\" palette : primary : 'blue grey' accent : 'light blue' extra_css : - 'stylesheets/extra.css' - 'stylesheets/links.css' repo_name : {{ repository_name }} # for example: 'lyz-code/pydo' repo_url : {{ repository_url }} # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve Add a github pages hook. \u00b6 Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : 'x64' - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working. Links \u00b6 Homepage . Material theme configuration guide","title":"mkdocs"},{"location":"linux/mkdocs/#installation","text":"Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin * Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name }} site_author : {{ your_name }} site_url : {{ site_url }} nav : - Introduction : 'index.md' - Basic Usage : 'basic_usage.md' - Configuration : 'configuration.md' - Update : 'update.md' - Advanced Usage : - Projects : \"projects.md\" - Tags : \"tags.md\" plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : \"theme\" logo : \"images/logo.png\" palette : primary : 'blue grey' accent : 'light blue' extra_css : - 'stylesheets/extra.css' - 'stylesheets/links.css' repo_name : {{ repository_name }} # for example: 'lyz-code/pydo' repo_url : {{ repository_url }} # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve","title":"Installation"},{"location":"linux/mkdocs/#add-a-github-pages-hook","text":"Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : 'x64' - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working.","title":"Add a github pages hook."},{"location":"linux/mkdocs/#links","text":"Homepage . Material theme configuration guide","title":"Links"},{"location":"linux/nodejs/","text":"Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. Install \u00b6 The debian base repositories are really outdated, so add the NodeSource repository curl -sL https://deb.nodesource.com/setup_12.x | sudo bash - sudo apt-get update sudo apt-get install nodejs npm nodejs --version Links \u00b6 Home","title":"nodejs"},{"location":"linux/nodejs/#install","text":"The debian base repositories are really outdated, so add the NodeSource repository curl -sL https://deb.nodesource.com/setup_12.x | sudo bash - sudo apt-get update sudo apt-get install nodejs npm nodejs --version","title":"Install"},{"location":"linux/nodejs/#links","text":"Home","title":"Links"},{"location":"linux/rm/","text":"rm definition In computing, rm (short for remove) is a basic command on Unix and Unix-like operating systems used to remove objects such as computer files, directories and symbolic links from file systems and also special files such as device nodes, pipes and sockets Debugging \u00b6 Cannot remove file: \u201cStructure needs cleaning\u201d \u00b6 From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"rm"},{"location":"linux/rm/#debugging","text":"","title":"Debugging"},{"location":"linux/rm/#cannot-remove-file-structure-needs-cleaning","text":"From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"Cannot remove file: \u201cStructure needs cleaning\u201d"},{"location":"linux/syncthing/","text":"Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it's transmitted over the internet. Installation \u00b6 Debian or Ubuntu \u00b6 # Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing Docker \u00b6 Use Linuxserver Docker Links \u00b6 Home Getting Started","title":"Syncthing"},{"location":"linux/syncthing/#installation","text":"","title":"Installation"},{"location":"linux/syncthing/#debian-or-ubuntu","text":"# Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing","title":"Debian or Ubuntu"},{"location":"linux/syncthing/#docker","text":"Use Linuxserver Docker","title":"Docker"},{"location":"linux/syncthing/#links","text":"Home Getting Started","title":"Links"},{"location":"linux/zip/","text":"zip is an UNIX command line tool to package and compress files. Usage \u00b6 Create a zip file \u00b6 zip -r {{ zip_file }} {{ files_to_save }} Split files to a specific size \u00b6 zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m Compress with password \u00b6 zip -er {{ zip_file }} {{ files_to_save }} Read files to compress from a file \u00b6 cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@ Uncompress a zip file \u00b6 unzip {{ zip_file }}","title":"zip"},{"location":"linux/zip/#usage","text":"","title":"Usage"},{"location":"linux/zip/#create-a-zip-file","text":"zip -r {{ zip_file }} {{ files_to_save }}","title":"Create a zip file"},{"location":"linux/zip/#split-files-to-a-specific-size","text":"zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m","title":"Split files to a specific size"},{"location":"linux/zip/#compress-with-password","text":"zip -er {{ zip_file }} {{ files_to_save }}","title":"Compress with password"},{"location":"linux/zip/#read-files-to-compress-from-a-file","text":"cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@","title":"Read files to compress from a file"},{"location":"linux/zip/#uncompress-a-zip-file","text":"unzip {{ zip_file }}","title":"Uncompress a zip file"},{"location":"linux/luks/luks/","text":"LUKS definition The Linux Unified Key Setup (LUKS) is a disk encryption specification created by Clemens Fruhwirth in 2004 and was originally intended for Linux. While most disk encryption software implements different, incompatible, and undocumented formats, LUKS implements a platform-independent standard on-disk format for use in various tools. This not only facilitates compatibility and interoperability among different programs, but also assures that they all implement password management in a secure and documented manner. The reference implementation for LUKS operates on Linux and is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend. LUKS is designed to conform to the TKS1 secure key setup scheme. LUKS Commands \u00b6 We use the cryptsetup command to interact with LUKS partitions. Header management \u00b6 Get the disk header \u00b6 cryptsetup luksDump /dev/sda3 Backup header \u00b6 cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }} Key management \u00b6 Add a key \u00b6 cryptsetup luksAddKey --key-slot 1 {{ luks_device }} Test if you remember the key \u00b6 Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }} Delete some keys \u00b6 cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }} Delete all keys \u00b6 cryptsetup luksErase {{ device }} Encrypt hard drive \u00b6 Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt LUKS debugging \u00b6 Resource busy \u00b6 Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"LUKS"},{"location":"linux/luks/luks/#luks-commands","text":"We use the cryptsetup command to interact with LUKS partitions.","title":"LUKS Commands"},{"location":"linux/luks/luks/#header-management","text":"","title":"Header management"},{"location":"linux/luks/luks/#get-the-disk-header","text":"cryptsetup luksDump /dev/sda3","title":"Get the disk header"},{"location":"linux/luks/luks/#backup-header","text":"cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }}","title":"Backup header"},{"location":"linux/luks/luks/#key-management","text":"","title":"Key management"},{"location":"linux/luks/luks/#add-a-key","text":"cryptsetup luksAddKey --key-slot 1 {{ luks_device }}","title":"Add a key"},{"location":"linux/luks/luks/#test-if-you-remember-the-key","text":"Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }}","title":"Test if you remember the key"},{"location":"linux/luks/luks/#delete-some-keys","text":"cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }}","title":"Delete some keys"},{"location":"linux/luks/luks/#delete-all-keys","text":"cryptsetup luksErase {{ device }}","title":"Delete all keys"},{"location":"linux/luks/luks/#encrypt-hard-drive","text":"Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt","title":"Encrypt hard drive"},{"location":"linux/luks/luks/#luks-debugging","text":"","title":"LUKS debugging"},{"location":"linux/luks/luks/#resource-busy","text":"Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"Resource busy"},{"location":"linux/vim/vim_plugins/","text":"Black \u00b6 To install Black you first need python3-venv . sudo apt-get install python3-venv Add the plugin and configure it so vim runs it each time you save. File ~/.vimrc Plugin 'psf/black' \" Black autocmd BufWritePre *.py execute ':Black' A configuration issue exists for neovim. If you encounter the error AttributeError: module 'black' has no attribute 'find_pyproject_toml' , do the following: cd ~/.vim/bundle/black git checkout 19 .10b0 As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well \"\" python indent autocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab \" python-mode let g:pymode_options_max_line_length = 88 let g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length} ALE \u00b6 ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client. ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem. In other words, this plugin allows you to lint while you type. ALE offers support for fixing code with command line tools in a non-blocking manner with the :ALEFix feature, supporting tools in many languages , like prettier, eslint, autopep8, and more. Installation \u00b6 Install with Vundle: Plugin 'dense-analysis/ale' And configure with: let g :ale_sign_error = '\u2718' let g :ale_sign_warning = '\u26a0' highlight ALEErrorSign ctermbg = NONE ctermfg = red highlight ALEWarningSign ctermbg = NONE ctermfg = yellow let g :ale_linters_explicit = 1 let g :ale_lint_on_text_changed = 'normal' \" let g:ale_lint_on_text_changed = 'never' let g :ale_lint_on_enter = 0 let g :ale_lint_on_save = 1 let g :ale_fix_on_save = 1 let g :ale_linters = { \\ 'markdown' : [ 'markdownlint' , 'writegood' , 'alex' , 'proselint' ] , \\ 'json' : [ 'jsonlint' ] , \\ 'python' : [ 'flake8' , 'mypy' , 'pylint' , 'alex' ] , \\ 'yaml' : [ 'yamllint' , 'alex' ] , \\ '*' : [ 'alex' , 'writegood' ] , \\} let g :ale_fixers = { \\ '*' : [ 'remove_trailing_lines' , 'trim_whitespace' ] , \\ 'json' : [ 'jq' ] , \\ 'python' : [ 'isort' ] \\ 'terraform' : [ 'terraform' ] , \\} inoremap < leader > e < esc > :ALENext < cr > nnoremap < leader > e :ALENext < cr > inoremap < leader > p < esc > :ALEPrevious < cr > nnoremap < leader > p :ALEPrevious < cr > Where: let g:ale_linters_explicit : Prevent ALE load only the selected linters. use <leader>e and <leader>p to navigate through the warnings. If you feel that it's too heavy, use ale_lint_on_enter or increase the ale_lint_delay . Use :ALEInfo to see the ALE configuration for the specific buffer. References \u00b6 ALE supported tools","title":"Vim Plugins"},{"location":"linux/vim/vim_plugins/#black","text":"To install Black you first need python3-venv . sudo apt-get install python3-venv Add the plugin and configure it so vim runs it each time you save. File ~/.vimrc Plugin 'psf/black' \" Black autocmd BufWritePre *.py execute ':Black' A configuration issue exists for neovim. If you encounter the error AttributeError: module 'black' has no attribute 'find_pyproject_toml' , do the following: cd ~/.vim/bundle/black git checkout 19 .10b0 As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well \"\" python indent autocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab \" python-mode let g:pymode_options_max_line_length = 88 let g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length}","title":"Black"},{"location":"linux/vim/vim_plugins/#ale","text":"ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client. ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem. In other words, this plugin allows you to lint while you type. ALE offers support for fixing code with command line tools in a non-blocking manner with the :ALEFix feature, supporting tools in many languages , like prettier, eslint, autopep8, and more.","title":"ALE"},{"location":"linux/vim/vim_plugins/#installation","text":"Install with Vundle: Plugin 'dense-analysis/ale' And configure with: let g :ale_sign_error = '\u2718' let g :ale_sign_warning = '\u26a0' highlight ALEErrorSign ctermbg = NONE ctermfg = red highlight ALEWarningSign ctermbg = NONE ctermfg = yellow let g :ale_linters_explicit = 1 let g :ale_lint_on_text_changed = 'normal' \" let g:ale_lint_on_text_changed = 'never' let g :ale_lint_on_enter = 0 let g :ale_lint_on_save = 1 let g :ale_fix_on_save = 1 let g :ale_linters = { \\ 'markdown' : [ 'markdownlint' , 'writegood' , 'alex' , 'proselint' ] , \\ 'json' : [ 'jsonlint' ] , \\ 'python' : [ 'flake8' , 'mypy' , 'pylint' , 'alex' ] , \\ 'yaml' : [ 'yamllint' , 'alex' ] , \\ '*' : [ 'alex' , 'writegood' ] , \\} let g :ale_fixers = { \\ '*' : [ 'remove_trailing_lines' , 'trim_whitespace' ] , \\ 'json' : [ 'jq' ] , \\ 'python' : [ 'isort' ] \\ 'terraform' : [ 'terraform' ] , \\} inoremap < leader > e < esc > :ALENext < cr > nnoremap < leader > e :ALENext < cr > inoremap < leader > p < esc > :ALEPrevious < cr > nnoremap < leader > p :ALEPrevious < cr > Where: let g:ale_linters_explicit : Prevent ALE load only the selected linters. use <leader>e and <leader>p to navigate through the warnings. If you feel that it's too heavy, use ale_lint_on_enter or increase the ale_lint_delay . Use :ALEInfo to see the ALE configuration for the specific buffer.","title":"Installation"},{"location":"linux/vim/vim_plugins/#references","text":"ALE supported tools","title":"References"},{"location":"meta/meta/","text":"In this book you'll find, in a wiki format, all the notes I made on a huge variety of topics, such as, Linux, DevOps , DevSecOps, feminism, rationalism, life automation , productivity or programming. The main goal is to store all the knowledge gathered throughout my life in a way that everyone can benefit from reading it or referencing in an easy and quickly way. I will be updating this wiki quite often as I use it myself daily both to keep an account of things I know as well as things I want to know and everything in between. History \u00b6 I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. Furthermore, as I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that I won't have an interesting RSS feed. You could go through the git log but it doesn't make any sense. That's why I'm thinking of generating a monthly newsletter similar to Gwern's Newsletters or Changelog . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born. Book structure \u00b6 Each directory is a topic that can include other subtopics under it related to the parent topic. As sometimes the strict hierarchical structure of the categories doesn't work, I also use tags to link articles. If this is your first time visiting this wiki, you can just start reading from the top entry down and see what sparks your interest. Content Structure \u00b6 Each topic will have a title, some description of it, usually my own thoughts and knowledge on it as well as referencing some resources or links I have liked or used that helped me either understand the topic or gain appreciation of it. The structure of each of the posts will often look roughly like this: Title Description - My thoughts on the topic. Subtopics - Various subtopics related to the main topic. Notes - My own personal notes on the matter as well as things I found interesting on the internet regarding the topic. I often give a link of where I got things from. Links - Links related to the topic. Links \u00b6 My blue book is heavily inspired in this two other second brains: Gwern's website Nikita's wiki","title":"Meta"},{"location":"meta/meta/#history","text":"I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. Furthermore, as I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that I won't have an interesting RSS feed. You could go through the git log but it doesn't make any sense. That's why I'm thinking of generating a monthly newsletter similar to Gwern's Newsletters or Changelog . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born.","title":"History"},{"location":"meta/meta/#book-structure","text":"Each directory is a topic that can include other subtopics under it related to the parent topic. As sometimes the strict hierarchical structure of the categories doesn't work, I also use tags to link articles. If this is your first time visiting this wiki, you can just start reading from the top entry down and see what sparks your interest.","title":"Book structure"},{"location":"meta/meta/#content-structure","text":"Each topic will have a title, some description of it, usually my own thoughts and knowledge on it as well as referencing some resources or links I have liked or used that helped me either understand the topic or gain appreciation of it. The structure of each of the posts will often look roughly like this: Title Description - My thoughts on the topic. Subtopics - Various subtopics related to the main topic. Notes - My own personal notes on the matter as well as things I found interesting on the internet regarding the topic. I often give a link of where I got things from. Links - Links related to the topic.","title":"Content Structure"},{"location":"meta/meta/#links","text":"My blue book is heavily inspired in this two other second brains: Gwern's website Nikita's wiki","title":"Links"},{"location":"projects/projects/","text":"Also known as where I'm spending my spare time. Pydo \u00b6 I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo . Blue book \u00b6 I'm refactoring all the knowledge gathered in the past in my cheat sheet repository into the blue book. This means migrating 7422 articles, almost 50 million lines, to the new structure. It's going to be a slow and painful process \u1559(\u21c0\u2038\u21bc\u2036)\u1557 . Clinv \u00b6 As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. I did some research in the past and there was no tool that had: As you can see in how do you document your infrastructure? , there is still a void. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv , Media indexation \u00b6 I've got a music collection of 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to an mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system. Projects I maintain \u00b6 Home Stock inventory \u00b6 I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I'm integrating grocy in my life. Mediarss \u00b6 I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service . So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor. Playlist_generator \u00b6 When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^). mep \u00b6 I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"Projects"},{"location":"projects/projects/#pydo","text":"I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo .","title":"Pydo"},{"location":"projects/projects/#blue-book","text":"I'm refactoring all the knowledge gathered in the past in my cheat sheet repository into the blue book. This means migrating 7422 articles, almost 50 million lines, to the new structure. It's going to be a slow and painful process \u1559(\u21c0\u2038\u21bc\u2036)\u1557 .","title":"Blue book"},{"location":"projects/projects/#clinv","text":"As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. I did some research in the past and there was no tool that had: As you can see in how do you document your infrastructure? , there is still a void. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv ,","title":"Clinv"},{"location":"projects/projects/#media-indexation","text":"I've got a music collection of 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to an mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system.","title":"Media indexation"},{"location":"projects/projects/#projects-i-maintain","text":"","title":"Projects I maintain"},{"location":"projects/projects/#home-stock-inventory","text":"I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I'm integrating grocy in my life.","title":"Home Stock inventory"},{"location":"projects/projects/#mediarss","text":"I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service . So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor.","title":"Mediarss"},{"location":"projects/projects/#playlist_generator","text":"When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^).","title":"Playlist_generator"},{"location":"projects/projects/#mep","text":"I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"mep"},{"location":"psychology/the_xy_problem/","text":"The XY problem is a communication or thinking problem encountered in situations where the real issue, X , of the human asking for help is obscured, because instead of asking directly about issue X , they ask how to solve a secondary issue, Y , which they believe will allow them to resolve issue X . However, resolving issue Y often does not resolve issue X , or is a poor way to resolve it, and the obscuring of the real issue and the introduction of the potentially strange secondary issue can lead to the person trying to help having unnecessary difficulties in communication and offering poor solutions. How to avoid it \u00b6 Always include information about a broader picture along with any attempted solution. If someone asks for more information, do provide details. If there are other solutions you've already ruled out, share why you've ruled them out. This gives more information about your requirements.","title":"The XY Problem"},{"location":"psychology/the_xy_problem/#how-to-avoid-it","text":"Always include information about a broader picture along with any attempted solution. If someone asks for more information, do provide details. If there are other solutions you've already ruled out, share why you've ruled them out. This gives more information about your requirements.","title":"How to avoid it"},{"location":"writing/orthography/","text":"My english writing is not so good, this article is an effort to gather all my common pitfalls. When to write Apostrophes before an s \u00b6 For most singular nouns, add apostrophe + s : The writer's desk . For most plural nouns, add apostrophe : The writers' desk (multiple writers). For plural nouns that do not end in s, add apostrophe + s : The geese's migration route . For singular proper nouns both apostrophe and apostrophe + s is accepted, but as the plural proper nouns ending in s, the correct form is apostrophe I'd use that for both, so: Charles Dickens' novels and The Smiths' vacation . The personal pronouns, do not have apostrophes to form possessives, this means that your , yours , hers , its , ours , their , whose , and theirs . In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive. Who vs Whom \u00b6 If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition. A vs An \u00b6 We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a . Comma before and \u00b6 There are two cases: It's required to put a comma before and when it\u2019s connecting two independent clauses. It\u2019s almost always optional the use of comma before and in lists. This case is also known as serial commas or Oxford commas . Avoid there is at the start of the sentence \u00b6 Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point.","title":"English Grammar and Ortography"},{"location":"writing/orthography/#when-to-write-apostrophes-before-an-s","text":"For most singular nouns, add apostrophe + s : The writer's desk . For most plural nouns, add apostrophe : The writers' desk (multiple writers). For plural nouns that do not end in s, add apostrophe + s : The geese's migration route . For singular proper nouns both apostrophe and apostrophe + s is accepted, but as the plural proper nouns ending in s, the correct form is apostrophe I'd use that for both, so: Charles Dickens' novels and The Smiths' vacation . The personal pronouns, do not have apostrophes to form possessives, this means that your , yours , hers , its , ours , their , whose , and theirs . In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive.","title":"When to write Apostrophes before an s"},{"location":"writing/orthography/#who-vs-whom","text":"If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition.","title":"Who vs Whom"},{"location":"writing/orthography/#a-vs-an","text":"We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a .","title":"A vs An"},{"location":"writing/orthography/#comma-before-and","text":"There are two cases: It's required to put a comma before and when it\u2019s connecting two independent clauses. It\u2019s almost always optional the use of comma before and in lists. This case is also known as serial commas or Oxford commas .","title":"Comma before and"},{"location":"writing/orthography/#avoid-there-is-at-the-start-of-the-sentence","text":"Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point.","title":"Avoid there is at the start of the sentence"}]}