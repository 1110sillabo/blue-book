{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is my personal wiki where I share everything I know about this world in form of an online MkDocs book hosted on GitHub . If this is your first time visiting this wiki, take a look at meta , as it describes this wiki, its structure and goals in more detail. Using the wiki well \u00b6 You can quickly search the contents of this wiki above or you can explore the tree view to the left. Start with the first article that grabs your attention and be ready to incrementally read the rest. Or you can use it as a reference, cloning the git repository and using grep. Make your own wiki \u00b6 Don't be afraid to create one of your own and share what you know with the world. If you don't want to build your own, I invite you to use a fork of mine and make contributions . I would love to see the blue-book maintained by several people. You can view other similar continuously updated wikis to get inspiration. Contributing \u00b6 If you find a mistake anywhere in this wiki or want to add new content, I'll be glad to accept your contribution. You can quickly find any entry you wish to edit by searching for the topic or use the edit button on the top right of any article to add your changes with a PR. I also appreciate any ideas you have on how I can improve this wiki. And if you don't want to go through the hassle of building your own, you can use mine Thank you \u00b6 If you liked my book and want to make me happy, please see if you know how could I fulfill any item of my wish list or see if you want to contribute to my other projects .","title":"Introduction"},{"location":"#using-the-wiki-well","text":"You can quickly search the contents of this wiki above or you can explore the tree view to the left. Start with the first article that grabs your attention and be ready to incrementally read the rest. Or you can use it as a reference, cloning the git repository and using grep.","title":"Using the wiki well"},{"location":"#make-your-own-wiki","text":"Don't be afraid to create one of your own and share what you know with the world. If you don't want to build your own, I invite you to use a fork of mine and make contributions . I would love to see the blue-book maintained by several people. You can view other similar continuously updated wikis to get inspiration.","title":"Make your own wiki"},{"location":"#contributing","text":"If you find a mistake anywhere in this wiki or want to add new content, I'll be glad to accept your contribution. You can quickly find any entry you wish to edit by searching for the topic or use the edit button on the top right of any article to add your changes with a PR. I also appreciate any ideas you have on how I can improve this wiki. And if you don't want to go through the hassle of building your own, you can use mine","title":"Contributing"},{"location":"#thank-you","text":"If you liked my book and want to make me happy, please see if you know how could I fulfill any item of my wish list or see if you want to contribute to my other projects .","title":"Thank you"},{"location":"contact/","text":"I'm available through: Email or XMPP at lyz@riseup.net . PGP Key: 6ADA882386CDF9BD1884534C6C7D7C1612CDE02F -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBFhs5wUBEAC289UxruAPfjvJ723AKhUhRI0/fw+cG0IeSUJfOSvWW+HJ7Elo QoPkKYv6E1k4SzIt6AgbEWpL35PQP79aQ5BFog2SbfVvfnq1/gIasFlyeFX1BUTh zxTKrYKwbUdsTeMYw32v5p2Q+D8CZK6/0RCM/GSb5oMPVancOeoZs8IebKpJH2x7 HCniyQbq7xiFU5sUyB6tmgCiXg8INib+oTZqGKW/sVaxmTdH+fF9a2nnH0TN8h2W 5V5XQ9/VQZk/GHQVq/Y0Z73BibOJM5Bv+3r2EIJfozlpWdUblat45lSATBo/sktf YKlxwAztWPtcTavJ58F1ufGcUPjwGW4E92zRaozC+tpzd5QtHeYM7m6fGlXxckua UesZcZLl9pY4Bc8Mw40WvI1ibhA2mP2R5AO8hJ0vJyFfi35lqM/DJVV1900yp+em uY+u6bNJ1gLLb7QnhbV1VYLTSCoWzPQvWHgMHAKpAjO15rKAItXD17BM2eQgJMuX LcoWeOcz/MrMQiGKSkqpmapwgtDZ5t81D2qWv+wsaZgcO/erknugHFmR3kAP8YHp JsIpaYY7kj+yVJb92uzZKQAEaUpq3uRsBDtkoC2MPzKN4fgWa8f4jBpIzxuBTd+6 75sVq5VB5eaq3w4J0Z4kbk1DVyNffv3LeZCv9oC2mb1aXyVD/gWHlPD+6wARAQAB tBZseXouLiA8bHl6QHJpc2V1cC5uZXQ+iQJUBBMBCAA+AhsDBQsJCAcCBhUICQoL AgQWAgMBAh4BAheAFiEEatqII4bN+b0YhFNMbH18FhLN4C8FAl4XCTwFCQeLVbcA CgkQbH18FhLN4C/Jkw//Th/tAagxBchztzA2bAJog7sd3FK4hH2cqGFdBG+yx5TW 2ywfDXjTXVeKhHxkSnZZgxO0U31W2Fv+tLmRKN8MrvGSjIpUlWTmeaIG1W+ftlcG NrR+CDL0lrkKZnyQGJhe675lNoo2FKQ/37B/NIyzfIWw8eZStYabHtj5H40nti1k riwZsk76+kR6FI1EVKCGGmo/Spl/VX9MuWNjg9E0cJvpzKY05gKmFSuMJwxVhrFV ly0MhZS+4xddbCMaBo2OJEDrcFBQgBiUnxS8PcADLK7zn3zpemcJm5/8T/DyQHeY 0Yh76KJ92aIB7eLLnRnRcvCXt0RZ+s3sHqLgrsT3OV0jlC7GLjTBgTe6qGH3Lr/h whiOp6g1k125+v20fKPWDlGar3sdSD/ZjJDeAHedV5I3QVT6zorUYcQYb6vYPlOU aq7k0jLjGuoxHQeXGAZMvlKQfgHDfiwBwyIX6D24wsyr+XDnrVyDoCO654OqYcUH wK1y57NbUOpzvD+ZEO/8aKeBUh0zKz682hsA8HJT8G09UBcs36HAnbTkp+rPxgTH eBVcTYLi/CFy9tXOhBmyPhxrILsPwmOvZA4tg7LLnj2P2qdk2Gz1si/D8s2Afr+c re9pidcYbiXJI+Pnw+e9Pylf/1WM8MS5Z2W9Liyc29/kLsCL8Dp0eJtqzJLAX0y5 Ag0EWGznBQEQALNL9sNc4SytS3fOcS4gHvZpH3TLJ6o0K/Lxg4RfkLMebDJwWvSW mjQLv3GqRfOhGj2Osi2YukFIJb4vxPJFO7wQhCi5LLSVEb5d/z9ZOJUdGdI9JvGW dFDuLEXwDnJaP5Jmjm3DwbvHK+goI7Fn3TKc27iqOVAKVIjWNPaqFZxwIE9o/+1c 3bTk3A8WOBmcv1IaxsUNkRDOFJlQYLM/bFIuDD+cW/CcYro8ouC9aekmvTDoRaU5 xv++fXtesn6Cy+xBgvBGIIXGo5xzd6Y66Yf8uNpuJXo9Dc6rApH1QEQNwZX1cxvG UpQx+9JNF0eptDLvTgmxcCglllrylcw8ZsVEt6BTgrCd2JXMGxUcAnhXpRWRmXNL n97FOBb6OBd6k7DC6QCiVKr7sytq1Ywl8GTtWrTP7sK+/+KDLPJ/oY7+bwV94+N8 Gthr94njNqb5G6t9fqQ/+cJv7oF8DoBvylYGqm2hvYpOH53hMq1y3OTPoFKP6AIx twIWHkdmMALm6a6bxAetGQxiaPZTOduJDehwiF9EUkiNhpESMl3I2+vH86jV2IiT 4BuUqGBU5wrAN/FixIRlmaSUX7e0OkUkDexVlpw5poJbPEbvhOtuj/V9BOxQKWB4 bjXMHEHR5YcJ1lhPjFFM3pqOz6ZaN8Hs70KOBE+/3/c1hS5debWPBMdlABEBAAGJ AjwEGAEIACYCGwwWIQRq2ogjhs35vRiEU0xsfXwWEs3gLwUCXhcJRgUJB4tVwQAK CRBsfXwWEs3gL41DEACYtc6mykbhZh2eWrdNynbYX1TNYFH+4BP+zpN8kNHPwKfX IypLLSSwUhYdZ9kb8WB8n4cH8njk4P1LyGtfUOxbEpKCQNXfW3aWDDsZunxdSkyc 3opaCo2w/Gf2ynxtbJVWoNWYn8fDQJcE3UAz8+rioHGRUCBF//G8VWdqZ4PCARGu TPeurJG5aljJGqlrvAXewqNItGEoARHGC3R9otSC8Y5cd9zL3iKUnBh9xhiqFzjK /7J9uQcDz6GTzZKxDqRQmcs27nGjWFNscZY16dBDj6y2d+v+RJEgFY9uW7KGVfFG Y9kPsSKdKUzeE+TOvwintakMQT26dNWBbUkDkMt08kEFk5SyeoQcjnqWMFJgrav1 RYUwz/UFuWep0y9Rt0PrW40mBZOd4roRdgEX6I65K6CC38u/nIgJRG2I/2LkWIwu n2LROOQ+0O6rn4HObgfoEZE03K6AW1DyNR6BnspbTDt0fRIDk6Rrfw6Xe1AfANrK 9zs95WbKkbydE1xFddOJ10qDleFOOaeCWp7KW1GkvEKfoRXhhAo/xnFpjHbGuvJv bTL4pYkaoOyGriAn3fZ8zOoBLspuAzEENBLtX41XU8PFjwcRu4GfFSrP03svi3km WodDQhjSPW+B/9SmLj+UkaIUlTTqwAs8rHtexkzlIhHGASXc+Iuuz5JuzUlPUw== =9EvG -----END PGP PUBLIC KEY BLOCK----- Through Github by opening an issue .","title":"Contact"},{"location":"emojis/","text":"Curated list of emojis to copy paste. Angry \u00b6 (\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6 Annoyed \u00b6 (>_<) Awesome \u00b6 ( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH Conforting \u00b6 (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) Congratulations \u00b6 ( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f Crying \u00b6 (\u2565\ufe4f\u2565) Excited \u00b6 (((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o) Dance \u00b6 (~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^) Happy \u00b6 \u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a Kisses \u00b6 (\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665 Love \u00b6 \u2764 Pride \u00b6 <(\uffe3\uff3e\uffe3)> Relax \u00b6 _\u3078__(\u203e\u25e1\u25dd )> Sad \u00b6 \uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b) Scared \u00b6 \u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f Sleepy \u00b6 (\u1d17\u02f3\u1d17) Smug \u00b6 \uff08\uffe3\uff5e\uffe3\uff09 Whyyyy? \u00b6 (/\uff9f\u0414\uff9f)/ Surprised \u00b6 (\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*) Who cares \u00b6 \u00af\\_(\u30c4)_/\u00af WTF \u00b6 (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8 Links \u00b6 Japanese Emoticons","title":"Emojis"},{"location":"emojis/#angry","text":"(\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6","title":"Angry"},{"location":"emojis/#annoyed","text":"(>_<)","title":"Annoyed"},{"location":"emojis/#awesome","text":"( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH","title":"Awesome"},{"location":"emojis/#conforting","text":"(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)","title":"Conforting"},{"location":"emojis/#congratulations","text":"( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f","title":"Congratulations"},{"location":"emojis/#crying","text":"(\u2565\ufe4f\u2565)","title":"Crying"},{"location":"emojis/#excited","text":"(((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o)","title":"Excited"},{"location":"emojis/#dance","text":"(~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^)","title":"Dance"},{"location":"emojis/#happy","text":"\u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a","title":"Happy"},{"location":"emojis/#kisses","text":"(\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665","title":"Kisses"},{"location":"emojis/#love","text":"\u2764","title":"Love"},{"location":"emojis/#pride","text":"<(\uffe3\uff3e\uffe3)>","title":"Pride"},{"location":"emojis/#relax","text":"_\u3078__(\u203e\u25e1\u25dd )>","title":"Relax"},{"location":"emojis/#sad","text":"\uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b)","title":"Sad"},{"location":"emojis/#scared","text":"\u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f","title":"Scared"},{"location":"emojis/#sleepy","text":"(\u1d17\u02f3\u1d17)","title":"Sleepy"},{"location":"emojis/#smug","text":"\uff08\uffe3\uff5e\uffe3\uff09","title":"Smug"},{"location":"emojis/#whyyyy","text":"(/\uff9f\u0414\uff9f)/","title":"Whyyyy?"},{"location":"emojis/#surprised","text":"(\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*)","title":"Surprised"},{"location":"emojis/#who-cares","text":"\u00af\\_(\u30c4)_/\u00af","title":"Who cares"},{"location":"emojis/#wtf","text":"(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8","title":"WTF"},{"location":"emojis/#links","text":"Japanese Emoticons","title":"Links"},{"location":"wish_list/","text":"This is a gathering of tools, ideas or services that I'd like to enjoy. If you have any lead, as smallest as it may be on how to fulfill them, please contact me . Self hosted search engine \u00b6 It would be awesome to be able to self host a personal search engine that performs priorized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to priorize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or priorize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe searx can be a solution. Decentralized encrypted end to end VOIP and video software \u00b6 I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own. Others \u00b6 Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A way to store music numeric ratings through the command line compatible with mpd and beets . An e-reader support that could be fixed to the wall.","title":"Wish list"},{"location":"wish_list/#self-hosted-search-engine","text":"It would be awesome to be able to self host a personal search engine that performs priorized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to priorize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or priorize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe searx can be a solution.","title":"Self hosted search engine"},{"location":"wish_list/#decentralized-encrypted-end-to-end-voip-and-video-software","text":"I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own.","title":"Decentralized encrypted end to end VOIP and video software"},{"location":"wish_list/#others","text":"Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A way to store music numeric ratings through the command line compatible with mpd and beets . An e-reader support that could be fixed to the wall.","title":"Others"},{"location":"devops/devops/","text":"DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops) which aims to shorten the systems development life cycle and provide continuous delivery with high software quality. Learn path \u00b6 DevOps is has become a juicy work, if you want to introduce yourself into this world I suggest you to follow these steps: Learn basic Linux administration, otherwise you'll be lost. Learn how to use Git. If you can host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions .","title":"DevOps"},{"location":"devops/devops/#learn-path","text":"DevOps is has become a juicy work, if you want to introduce yourself into this world I suggest you to follow these steps: Learn basic Linux administration, otherwise you'll be lost. Learn how to use Git. If you can host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions .","title":"Learn path"},{"location":"devops/aws/aws/","text":"Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. In aggregate, these cloud computing web services provide a set of primitive abstract technical infrastructure and distributed computing building blocks and tools. Learn path \u00b6 TBD","title":"AWS"},{"location":"devops/aws/aws/#learn-path","text":"TBD","title":"Learn path"},{"location":"devops/aws/iam/iam/","text":"AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization). Configurable AWS access controls: Grant access to AWS Management console, APIs Create individual users Manage permissions with groups Configure a strong password policy Enable Multi-Factor Authentication for privileged users Use IAM roles for EC2 instances Use IAM roles to share access Rotate security credentials regularly Restrict privileged access further with conditions Use your corporate directory system or a third party authentication Links \u00b6 Docs","title":"IAM"},{"location":"devops/aws/iam/iam/#links","text":"Docs","title":"Links"},{"location":"devops/aws/iam/iam_commands/","text":"Information gathering \u00b6 List roles \u00b6 aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table List policies \u00b6 aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table List attached policies \u00b6 aws iam list-attached-role-policies --role-name {{ role_name }} Get role configuration \u00b6 aws iam get-role --role-name {{ role_name }} Get role policies \u00b6 aws iam list-role-policies --role-name {{ role_name }}","title":"IAM Commands"},{"location":"devops/aws/iam/iam_commands/#information-gathering","text":"","title":"Information gathering"},{"location":"devops/aws/iam/iam_commands/#list-roles","text":"aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table","title":"List roles"},{"location":"devops/aws/iam/iam_commands/#list-policies","text":"aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table","title":"List policies"},{"location":"devops/aws/iam/iam_commands/#list-attached-policies","text":"aws iam list-attached-role-policies --role-name {{ role_name }}","title":"List attached policies"},{"location":"devops/aws/iam/iam_commands/#get-role-configuration","text":"aws iam get-role --role-name {{ role_name }}","title":"Get role configuration"},{"location":"devops/aws/iam/iam_commands/#get-role-policies","text":"aws iam list-role-policies --role-name {{ role_name }}","title":"Get role policies"},{"location":"devops/aws/iam/iam_debug/","text":"MFADevice entity at the same path and name already exists \u00b6 It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"IAM Debugging"},{"location":"devops/aws/iam/iam_debug/#mfadevice-entity-at-the-same-path-and-name-already-exists","text":"It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"MFADevice entity at the same path and name already exists"},{"location":"devops/kubernetes/kubernetes/","text":"Kubernetes (commonly stylized as k8s) is an open-source container-orchestration system for automating application deployment, scaling, and management. It works with a range of container tools, including Docker. Many cloud services offer a Kubernetes-based platform or infrastructure as a service (PaaS or IaaS) on which Kubernetes can be deployed as a platform-providing service. Many vendors also provide their own branded Kubernetes distributions. Learn roadmap \u00b6 K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af . Tools \u00b6 Tried \u00b6 K3s : Recommended small kubernetes, like hyperkube. To try \u00b6 crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes Links \u00b6 Docs Awesome K8s Introduction \u00b6 Playground Comic Reference \u00b6 References API conventions","title":"Kubernetes"},{"location":"devops/kubernetes/kubernetes/#learn-roadmap","text":"K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af .","title":"Learn roadmap"},{"location":"devops/kubernetes/kubernetes/#tools","text":"","title":"Tools"},{"location":"devops/kubernetes/kubernetes/#tried","text":"K3s : Recommended small kubernetes, like hyperkube.","title":"Tried"},{"location":"devops/kubernetes/kubernetes/#to-try","text":"crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes","title":"To try"},{"location":"devops/kubernetes/kubernetes/#links","text":"Docs Awesome K8s","title":"Links"},{"location":"devops/kubernetes/kubernetes/#introduction","text":"Playground Comic","title":"Introduction"},{"location":"devops/kubernetes/kubernetes/#reference","text":"References API conventions","title":"Reference"},{"location":"devops/kubernetes/kubernetes_networking/","text":"If you want to get a quickly grasp on how k8s networking works, I suggest you to read StackRox's Kubernetes networking demystified article . CNI comparison \u00b6 Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis . TL;DR \u00b6 When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast. Flannel \u00b6 Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work. Calico \u00b6 Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide . Canal \u00b6 Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition. Weave Net \u00b6 Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting. AWS CNI \u00b6 AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS. Advantages of the AWS CNI \u00b6 Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive. Disadvantages of the AWS CNI \u00b6 On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet. VPC CNI Pod Density Limitations \u00b6 First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom daemonset that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam daemonset, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have daemonsets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 daemonsets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond Daemonsets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached. Cloud Portability \u00b6 Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea. Links \u00b6 StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Networking"},{"location":"devops/kubernetes/kubernetes_networking/#cni-comparison","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis .","title":"CNI comparison"},{"location":"devops/kubernetes/kubernetes_networking/#tldr","text":"When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast.","title":"TL;DR"},{"location":"devops/kubernetes/kubernetes_networking/#flannel","text":"Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work.","title":"Flannel"},{"location":"devops/kubernetes/kubernetes_networking/#calico","text":"Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide .","title":"Calico"},{"location":"devops/kubernetes/kubernetes_networking/#canal","text":"Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition.","title":"Canal"},{"location":"devops/kubernetes/kubernetes_networking/#weave-net","text":"Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting.","title":"Weave Net"},{"location":"devops/kubernetes/kubernetes_networking/#aws-cni","text":"AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS.","title":"AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#advantages-of-the-aws-cni","text":"Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive.","title":"Advantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#disadvantages-of-the-aws-cni","text":"On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet.","title":"Disadvantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#vpc-cni-pod-density-limitations","text":"First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom daemonset that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam daemonset, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have daemonsets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 daemonsets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond Daemonsets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached.","title":"VPC CNI Pod Density Limitations"},{"location":"devops/kubernetes/kubernetes_networking/#cloud-portability","text":"Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea.","title":"Cloud Portability"},{"location":"devops/kubernetes/kubernetes_networking/#links","text":"StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Links"},{"location":"devops/kubernetes/operators/","text":"Operators are Kubernetes specific applications (pods) that configure, manage and optimize other Kubernetes deployments automatically. A Kubernetes Operator might be able to: Install and provide sane initial configuration and sizing for your deployment, according to the specs of your Kubernetes cluster. Perform live reloading of deployments and pods to accommodate for any user requested parameter modification (hot config reloading). Safe coordination of application upgrades. Automatically scale up or down according to performance metrics. Service discovery via native Kubernetes APIs Application TLS certificate configuration Disaster recovery. Perform backups to offsite storage, integrity checks or any other maintenance task. How do they work? \u00b6 An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1. Links \u00b6 CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Operators"},{"location":"devops/kubernetes/operators/#how-do-they-work","text":"An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1.","title":"How do they work?"},{"location":"devops/kubernetes/operators/#links","text":"CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Links"},{"location":"devops/kubernetes/helm/helm/","text":"Helm is the package manager for Kubernetes. Through charts it helps you define, install and upgrade even the most complex Kubernetes applications. Charts are a group of Go templates of kubernetes yaml resource manifests, they are easy to create, version, share, and publish. Helm alone lacks some features, that are satisfied through some external programs: Helmfile is used to declaratively configure your charts, so they can be versioned through git. Helm-secrets is used to remove hardcoded credentials from values.yaml files. Helm has an open issue to integrate it into it's codebase. Links \u00b6 Homepage Docs Git Chart hub Git charts repositories","title":"Helm"},{"location":"devops/kubernetes/helm/helm/#links","text":"Homepage Docs Git Chart hub Git charts repositories","title":"Links"},{"location":"devops/kubernetes/helm/helm_commands/","text":"Small cheatsheet on how to use the helm command. List charts \u00b6 helm ls Get information of chart \u00b6 helm inspect {{ package_name }} Download a chart \u00b6 helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }} Search charts \u00b6 helm search {{ package_name }} Operations you should do with helmfile \u00b6 The following operations can be done with helm, but consider using helmfile instead. Install chart \u00b6 Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }} Give it a name \u00b6 helm install --name {{ release_name }} {{ package_name }} Give it a namespace \u00b6 helm install --namespace {{ namespace }} {{ package_name }} Customize the chart before installing \u00b6 helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }} Upgrade a release \u00b6 If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }} Rollback an upgrade \u00b6 First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }} Delete a release \u00b6 helm delete --purge {{ release_name }} Working with repositories \u00b6 List repositories \u00b6 helm repo list Add repository \u00b6 helm repo add {{ repo_name }} {{ repo_url }} Update repositories \u00b6 helm repo update","title":"Helm Commands"},{"location":"devops/kubernetes/helm/helm_commands/#list-charts","text":"helm ls","title":"List charts"},{"location":"devops/kubernetes/helm/helm_commands/#get-information-of-chart","text":"helm inspect {{ package_name }}","title":"Get information of chart"},{"location":"devops/kubernetes/helm/helm_commands/#download-a-chart","text":"helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }}","title":"Download a chart"},{"location":"devops/kubernetes/helm/helm_commands/#search-charts","text":"helm search {{ package_name }}","title":"Search charts"},{"location":"devops/kubernetes/helm/helm_commands/#operations-you-should-do-with-helmfile","text":"The following operations can be done with helm, but consider using helmfile instead.","title":"Operations you should do with helmfile"},{"location":"devops/kubernetes/helm/helm_commands/#install-chart","text":"Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }}","title":"Install chart"},{"location":"devops/kubernetes/helm/helm_commands/#give-it-a-name","text":"helm install --name {{ release_name }} {{ package_name }}","title":"Give it a name"},{"location":"devops/kubernetes/helm/helm_commands/#give-it-a-namespace","text":"helm install --namespace {{ namespace }} {{ package_name }}","title":"Give it a namespace"},{"location":"devops/kubernetes/helm/helm_commands/#customize-the-chart-before-installing","text":"helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }}","title":"Customize the chart before installing"},{"location":"devops/kubernetes/helm/helm_commands/#upgrade-a-release","text":"If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }}","title":"Upgrade a release"},{"location":"devops/kubernetes/helm/helm_commands/#rollback-an-upgrade","text":"First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }}","title":"Rollback an upgrade"},{"location":"devops/kubernetes/helm/helm_commands/#delete-a-release","text":"helm delete --purge {{ release_name }}","title":"Delete a release"},{"location":"devops/kubernetes/helm/helm_commands/#working-with-repositories","text":"","title":"Working with repositories"},{"location":"devops/kubernetes/helm/helm_commands/#list-repositories","text":"helm repo list","title":"List repositories"},{"location":"devops/kubernetes/helm/helm_commands/#add-repository","text":"helm repo add {{ repo_name }} {{ repo_url }}","title":"Add repository"},{"location":"devops/kubernetes/helm/helm_commands/#update-repositories","text":"helm repo update","title":"Update repositories"},{"location":"devops/kubernetes/helm/helm_installation/","text":"There are two usable versions of Helm, v2 and v3, the latter is quite new so some of the things we need to install as of 2020-01-27 are not yet supported (Prometheus operator), so we are going to stick to the version 2. Helm has a client-server architecture, the server is installed in the Kubernetes cluster and the client is a Go executable installed in the user computer. Helm client \u00b6 You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm Installation"},{"location":"devops/kubernetes/helm/helm_installation/#helm-client","text":"You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm client"},{"location":"devops/kubernetes/helm/helm_secrets/","text":"Helm-secrets is a helm plugin that manages secrets with Git workflow and stores them anywhere. It delegates the cryptographic operations to Mozilla's Sops tool, which supports PGP, AWS KMS and GCP KMS. The configuration is stored in .sops.yaml files. You can find in Mozilla's documentation a detailed configuration guide. For my use case, I'm only going to use a list of PGP keys, so the following contents should be in the .sops.yaml file at the project root directory. creation_rules : - pgp : >- {{ gpg_key_1 }}, {{ gpg_key_2}} Prevent committing decrypted files to git \u00b6 From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit Usage \u00b6 Encrypt secret files \u00b6 Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials. Edit secret files \u00b6 helm secrets edit secrets.yaml Decrypt secret files \u00b6 helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git . Clean all the decrypted files \u00b6 helm secrets clean . Add or remove keys \u00b6 Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options. Links \u00b6 Git","title":"Helm Secrets"},{"location":"devops/kubernetes/helm/helm_secrets/#prevent-committing-decrypted-files-to-git","text":"From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit","title":"Prevent committing decrypted files to git"},{"location":"devops/kubernetes/helm/helm_secrets/#usage","text":"","title":"Usage"},{"location":"devops/kubernetes/helm/helm_secrets/#encrypt-secret-files","text":"Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials.","title":"Encrypt secret files"},{"location":"devops/kubernetes/helm/helm_secrets/#edit-secret-files","text":"helm secrets edit secrets.yaml","title":"Edit secret files"},{"location":"devops/kubernetes/helm/helm_secrets/#decrypt-secret-files","text":"helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git .","title":"Decrypt secret files"},{"location":"devops/kubernetes/helm/helm_secrets/#clean-all-the-decrypted-files","text":"helm secrets clean .","title":"Clean all the decrypted files"},{"location":"devops/kubernetes/helm/helm_secrets/#add-or-remove-keys","text":"Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options.","title":"Add or remove keys"},{"location":"devops/kubernetes/helm/helm_secrets/#links","text":"Git","title":"Links"},{"location":"devops/kubernetes/helm/helmfile/","text":"Helmfile is a declarative spec for deploying Helm charts. It lets you: Keep a directory of chart value files and maintain changes in version control. Apply CI/CD to configuration changes. Periodically sync to avoid skew in environments. To avoid upgrades for each iteration of helm, the helmfile executable delegates to helm - as a result, helm must be installed. All information is saved in the helmfile.yaml file. In case we need custom yamls, we'll use kustomize . Installation \u00b6 Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile Usage \u00b6 How to deploy a new chart \u00b6 Create a directory with the {{ chart_name }} mkdir {{ chart_name }} Get a copy of the chart values inside that directory helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation Execute the changes helmfile apply Keep charts updated \u00b6 TBD Uninstall charts \u00b6 Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands. Force the reinstallation of everything \u00b6 If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases. Debugging helmfile \u00b6 Error: \"release-name\" has no deployed releases \u00b6 This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again. Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help) \u00b6 I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false. Links \u00b6 Git","title":"Helmfile"},{"location":"devops/kubernetes/helm/helmfile/#installation","text":"Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile","title":"Installation"},{"location":"devops/kubernetes/helm/helmfile/#usage","text":"","title":"Usage"},{"location":"devops/kubernetes/helm/helmfile/#how-to-deploy-a-new-chart","text":"Create a directory with the {{ chart_name }} mkdir {{ chart_name }} Get a copy of the chart values inside that directory helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation Execute the changes helmfile apply","title":"How to deploy a new chart"},{"location":"devops/kubernetes/helm/helmfile/#keep-charts-updated","text":"TBD","title":"Keep charts updated"},{"location":"devops/kubernetes/helm/helmfile/#uninstall-charts","text":"Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands.","title":"Uninstall charts"},{"location":"devops/kubernetes/helm/helmfile/#force-the-reinstallation-of-everything","text":"If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases.","title":"Force the reinstallation of everything"},{"location":"devops/kubernetes/helm/helmfile/#debugging-helmfile","text":"","title":"Debugging helmfile"},{"location":"devops/kubernetes/helm/helmfile/#error-release-name-has-no-deployed-releases","text":"This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again.","title":"Error: \"release-name\" has no deployed releases"},{"location":"devops/kubernetes/helm/helmfile/#error-failed-to-download-stablemetrics-server-hint-running-helm-repo-update-may-help","text":"I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false.","title":"Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help)"},{"location":"devops/kubernetes/helm/helmfile/#links","text":"Git","title":"Links"},{"location":"devops/prometheus/prometheus/","text":"Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. A quick overview of Prometheus would be, as stated in the coreos article : At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented. We are living a shift to the DevOps culture, containers and Kubernetes. So nowadays: Developers need to integrate app and business related metrics as an organic part of the infrastructure. So monitoring needs to be democratized, made more accessible and cover additional layers of the stack. Container based infrastructures are changing how we monitor the resources. Now we have a huge number of volatile software entities, services, virtual network addresses, exposed metrics that suddenly appear or vanish. Traditional monitoring tools are not designed to handle this. These reasons pushed Soundcloud to build a new monitoring system that had the following features Multi-dimensional data model : The model is based on key-value pairs, similar to how Kubernetes itself organizes infrastructure metadata using labels. It allows for flexible and accurate time series data, powering its Prometheus query language. Accessible format and protocols : Exposing prometheus metrics is a pretty straightforward task. Metrics are human readable, are in a self-explanatory format, and are published using a standard HTTP transport. You can check that the metrics are correctly exposed just using your web browser. Service discovery : The Prometheus server is in charge of periodically scraping the targets, so that applications and services don\u2019t need to worry about emitting data (metrics are pulled, not pushed). These Prometheus servers have several methods to auto-discover scrape targets, some of them can be configured to filter and match container metadata, making it an excellent fit for ephemeral Kubernetes workloads. Modular and highly available components : Metric collection, alerting, graphical visualization, etc, are performed by different composable services. All these services are designed to support redundancy and sharding. Installation \u00b6 There are several ways to install prometheus , but I'd recommend using the Kubernetes operator . Debugging \u00b6 Service monitor not being recognized \u00b6 Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work. Links \u00b6 Homepage . Docs . Awesome Prometheus . Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison .","title":"Prometheus"},{"location":"devops/prometheus/prometheus/#installation","text":"There are several ways to install prometheus , but I'd recommend using the Kubernetes operator .","title":"Installation"},{"location":"devops/prometheus/prometheus/#debugging","text":"","title":"Debugging"},{"location":"devops/prometheus/prometheus/#service-monitor-not-being-recognized","text":"Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work.","title":"Service monitor not being recognized"},{"location":"devops/prometheus/prometheus/#links","text":"Homepage . Docs . Awesome Prometheus . Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison .","title":"Links"},{"location":"life_automation/life_automation/","text":"Life Automation is the act of analyzing your interactions with the world to find ways to reduce the time or willpower spent on unwanted processes. Once you've covered some minimum life requirements (health, money or happiness), time is your most valued asset. It's sad to waste it doing stuff that we need but don't increase our happiness. So the idea is to identify which are those processes and find optimizations that allows us to do them in less time or using less willpower. I've also faced the problem of having so much stuff in my mind. Having background processes increase your brain load and are a constant sink of willpower. As a result, when you really need that CPU time, your brain is tired and doesn't work to it's full performance. Automating processes, like life logging and task management, allows you to delegate those worries. Life automation can lead to habit building, which reduces even more the willpower consumption of processes, at the same time it reduces the error rate. Automating life management \u00b6 Week automation , or how to review and plan the week. Automating home chores \u00b6 Using grocy to maintain the house stock, shopping lists and meal plans.","title":"Life Automation"},{"location":"life_automation/life_automation/#automating-life-management","text":"Week automation , or how to review and plan the week.","title":"Automating life management"},{"location":"life_automation/life_automation/#automating-home-chores","text":"Using grocy to maintain the house stock, shopping lists and meal plans.","title":"Automating home chores"},{"location":"life_automation/week_automation/","text":"I've been polishing a week reviewing and planning method that suits my needs. I usually follow it on Wednesdays, as I'm too busy on Mondays and Tuesdays and it gives enough time to plan the weekend. Until I've got pydo ready to natively incorporate all this processes, I heavily use taskwarrior to manage my tasks and logs. To make the process faster and reproducible, I've written small python scripts using tasklib. Week review \u00b6 Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date. Week planning \u00b6 The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. INBOX clean up \u00b6 I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week Automation"},{"location":"life_automation/week_automation/#week-review","text":"Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date.","title":"Week review"},{"location":"life_automation/week_automation/#week-planning","text":"The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode.","title":"Week planning"},{"location":"life_automation/week_automation/#inbox-clean-up","text":"I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"INBOX clean up"},{"location":"linux/rm/","text":"rm definition In computing, rm (short for remove) is a basic command on Unix and Unix-like operating systems used to remove objects such as computer files, directories and symbolic links from file systems and also special files such as device nodes, pipes and sockets Debugging \u00b6 Cannot remove file: \u201cStructure needs cleaning\u201d \u00b6 From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"rm"},{"location":"linux/rm/#debugging","text":"","title":"Debugging"},{"location":"linux/rm/#cannot-remove-file-structure-needs-cleaning","text":"From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"Cannot remove file: \u201cStructure needs cleaning\u201d"},{"location":"linux/luks/luks/","text":"LUKS definition The Linux Unified Key Setup (LUKS) is a disk encryption specification created by Clemens Fruhwirth in 2004 and was originally intended for Linux. While most disk encryption software implements different, incompatible, and undocumented formats, LUKS implements a platform-independent standard on-disk format for use in various tools. This not only facilitates compatibility and interoperability among different programs, but also assures that they all implement password management in a secure and documented manner. The reference implementation for LUKS operates on Linux and is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend. LUKS is designed to conform to the TKS1 secure key setup scheme. LUKS Commands \u00b6 We use the cryptsetup command to interact with LUKS partitions. Header management \u00b6 Get the disk header \u00b6 cryptsetup luksDump /dev/sda3 Backup header \u00b6 cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }} Key management \u00b6 Add a key \u00b6 cryptsetup luksAddKey --key-slot 1 {{ luks_device }} Test if you remember the key \u00b6 Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }} Delete some keys \u00b6 cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }} Delete all keys \u00b6 cryptsetup luksErase {{ device }} Encrypt hard drive \u00b6 Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt LUKS debugging \u00b6 Resource busy \u00b6 Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"LUKS"},{"location":"linux/luks/luks/#luks-commands","text":"We use the cryptsetup command to interact with LUKS partitions.","title":"LUKS Commands"},{"location":"linux/luks/luks/#header-management","text":"","title":"Header management"},{"location":"linux/luks/luks/#get-the-disk-header","text":"cryptsetup luksDump /dev/sda3","title":"Get the disk header"},{"location":"linux/luks/luks/#backup-header","text":"cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }}","title":"Backup header"},{"location":"linux/luks/luks/#key-management","text":"","title":"Key management"},{"location":"linux/luks/luks/#add-a-key","text":"cryptsetup luksAddKey --key-slot 1 {{ luks_device }}","title":"Add a key"},{"location":"linux/luks/luks/#test-if-you-remember-the-key","text":"Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }}","title":"Test if you remember the key"},{"location":"linux/luks/luks/#delete-some-keys","text":"cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }}","title":"Delete some keys"},{"location":"linux/luks/luks/#delete-all-keys","text":"cryptsetup luksErase {{ device }}","title":"Delete all keys"},{"location":"linux/luks/luks/#encrypt-hard-drive","text":"Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt","title":"Encrypt hard drive"},{"location":"linux/luks/luks/#luks-debugging","text":"","title":"LUKS debugging"},{"location":"linux/luks/luks/#resource-busy","text":"Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"Resource busy"},{"location":"meta/meta/","text":"In this book you'll find, in a wiki format, all the notes I made on a huge variety of topics, such as, Linux, DevSecOps, feminism, rationalism, life automation , productivity or programming. The main goal is to store all the knowledge gathered throughout my life in a way that everyone can benefit from reading it or referencing in an easy and quickly way. I will be updating this wiki quite often as I use it myself daily both to keep an account of things I know as well as things I want to know and everything in between. History \u00b6 I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. Furthermore, as I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that I won't have an interesting RSS feed. You could go through the git log but it doesn't make any sense. That's why I'm thinking of generating a monthly newsletter similar to Gwern's Newsletters or Changelog . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born. Book structure \u00b6 Each directory is a topic that can include other subtopics under it related to the parent topic. As sometimes the strict hierarchical structure of the categories doesn't work, I also use tags to link articles. If this is your first time visiting this wiki, you can just start reading from the top entry down and see what sparks your interest. Content Structure \u00b6 Each topic will have a title, some description of it, usually my own thoughts and knowledge on it as well as referencing some resources or links I have liked or used that helped me either understand the topic or gain appreciation of it. The structure of each of the posts will often look roughly like this: Title Description - My thoughts on the topic. Subtopics - Various subtopics related to the main topic. Notes - My own personal notes on the matter as well as things I found interesting on the internet regarding the topic. I often give a link of where I got things from. Links - Links related to the topic. Links \u00b6 My blue book is heavily inspired in this two other second brains: Gwern's website Nikita's wiki","title":"Meta"},{"location":"meta/meta/#history","text":"I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. Furthermore, as I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that I won't have an interesting RSS feed. You could go through the git log but it doesn't make any sense. That's why I'm thinking of generating a monthly newsletter similar to Gwern's Newsletters or Changelog . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born.","title":"History"},{"location":"meta/meta/#book-structure","text":"Each directory is a topic that can include other subtopics under it related to the parent topic. As sometimes the strict hierarchical structure of the categories doesn't work, I also use tags to link articles. If this is your first time visiting this wiki, you can just start reading from the top entry down and see what sparks your interest.","title":"Book structure"},{"location":"meta/meta/#content-structure","text":"Each topic will have a title, some description of it, usually my own thoughts and knowledge on it as well as referencing some resources or links I have liked or used that helped me either understand the topic or gain appreciation of it. The structure of each of the posts will often look roughly like this: Title Description - My thoughts on the topic. Subtopics - Various subtopics related to the main topic. Notes - My own personal notes on the matter as well as things I found interesting on the internet regarding the topic. I often give a link of where I got things from. Links - Links related to the topic.","title":"Content Structure"},{"location":"meta/meta/#links","text":"My blue book is heavily inspired in this two other second brains: Gwern's website Nikita's wiki","title":"Links"},{"location":"projects/projects/","text":"Also known as where I'm spending my spare time. Home Stock inventory \u00b6 I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I'm integrating grocy in my life. Pydo \u00b6 I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo . Blue book \u00b6 I'm refactoring all the knowledge gathered in the past in my cheat sheet repository into the blue book. This means migrating 7422 articles, almost 50 million lines, to the new structure. It's going to be a slow and painful process \u1559(\u21c0\u2038\u21bc\u2036)\u1557 . Clinv \u00b6 As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. I did some research in the past and there was no tool that had: As you can see in how do you document your infrastructure? , there is still a void. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv , Media indexation \u00b6 I've got a music collection of 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to an mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system. Projects I maintain \u00b6 Mediarss \u00b6 I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service . So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor. Playlist_generator \u00b6 When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^). mep \u00b6 I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"Projects"},{"location":"projects/projects/#home-stock-inventory","text":"I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I'm integrating grocy in my life.","title":"Home Stock inventory"},{"location":"projects/projects/#pydo","text":"I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo .","title":"Pydo"},{"location":"projects/projects/#blue-book","text":"I'm refactoring all the knowledge gathered in the past in my cheat sheet repository into the blue book. This means migrating 7422 articles, almost 50 million lines, to the new structure. It's going to be a slow and painful process \u1559(\u21c0\u2038\u21bc\u2036)\u1557 .","title":"Blue book"},{"location":"projects/projects/#clinv","text":"As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. I did some research in the past and there was no tool that had: As you can see in how do you document your infrastructure? , there is still a void. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv ,","title":"Clinv"},{"location":"projects/projects/#media-indexation","text":"I've got a music collection of 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to an mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system.","title":"Media indexation"},{"location":"projects/projects/#projects-i-maintain","text":"","title":"Projects I maintain"},{"location":"projects/projects/#mediarss","text":"I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service . So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor.","title":"Mediarss"},{"location":"projects/projects/#playlist_generator","text":"When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^).","title":"Playlist_generator"},{"location":"projects/projects/#mep","text":"I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"mep"},{"location":"tags.html","text":"Contents grouped by tag \u00b6 Life Automation \u00b6 Life Automation monitoring \u00b6 Prometheus WIP \u00b6 Devops AWS","title":"Tags"},{"location":"tags.html#contents-grouped-by-tag","text":"","title":"Contents grouped by tag"},{"location":"tags.html#life-automation","text":"Life Automation","title":"Life Automation"},{"location":"tags.html#monitoring","text":"Prometheus","title":"monitoring"},{"location":"tags.html#wip","text":"Devops AWS","title":"WIP"}]}