{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is my personal wiki where I share everything I know about this world in form of an online MkDocs book hosted on GitHub . If this is your first time visiting this wiki, take a look at meta , as it describes this wiki, its structure and goals in more detail. Using the wiki well \u00b6 You can quickly search the contents of this wiki above or you can explore the tree view to the left. Start with the first article that grabs your attention and be ready to incrementally read the rest. Or you can use it as a reference, cloning the git repository and using grep. Make your own wiki \u00b6 Don't be afraid to create one of your own and share what you know with the world. If you don't want to build your own, I invite you to use a fork of mine and make contributions . I would love to see the blue-book maintained by several people. You can view other similar continuously updated wikis to get inspiration. Contributing \u00b6 If you find a mistake anywhere in this wiki or want to add new content, I'll be glad to accept your contribution. You can quickly find any entry you wish to edit by searching for the topic or use the edit button on the top right of any article to add your changes with a PR. I also appreciate any ideas you have on how I can improve this wiki. And if you don't want to go through the hassle of building your own, you can use mine Thank you \u00b6 If you liked my book and want to make me happy, please see if you know how could I fulfill any item of my wish list or see if you want to contribute to my other projects .","title":"Introduction"},{"location":"#using-the-wiki-well","text":"You can quickly search the contents of this wiki above or you can explore the tree view to the left. Start with the first article that grabs your attention and be ready to incrementally read the rest. Or you can use it as a reference, cloning the git repository and using grep.","title":"Using the wiki well"},{"location":"#make-your-own-wiki","text":"Don't be afraid to create one of your own and share what you know with the world. If you don't want to build your own, I invite you to use a fork of mine and make contributions . I would love to see the blue-book maintained by several people. You can view other similar continuously updated wikis to get inspiration.","title":"Make your own wiki"},{"location":"#contributing","text":"If you find a mistake anywhere in this wiki or want to add new content, I'll be glad to accept your contribution. You can quickly find any entry you wish to edit by searching for the topic or use the edit button on the top right of any article to add your changes with a PR. I also appreciate any ideas you have on how I can improve this wiki. And if you don't want to go through the hassle of building your own, you can use mine","title":"Contributing"},{"location":"#thank-you","text":"If you liked my book and want to make me happy, please see if you know how could I fulfill any item of my wish list or see if you want to contribute to my other projects .","title":"Thank you"},{"location":"contact/","text":"I'm available through: Email or XMPP at lyz@riseup.net . PGP Key: 6ADA882386CDF9BD1884534C6C7D7C1612CDE02F -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBFhs5wUBEAC289UxruAPfjvJ723AKhUhRI0/fw+cG0IeSUJfOSvWW+HJ7Elo QoPkKYv6E1k4SzIt6AgbEWpL35PQP79aQ5BFog2SbfVvfnq1/gIasFlyeFX1BUTh zxTKrYKwbUdsTeMYw32v5p2Q+D8CZK6/0RCM/GSb5oMPVancOeoZs8IebKpJH2x7 HCniyQbq7xiFU5sUyB6tmgCiXg8INib+oTZqGKW/sVaxmTdH+fF9a2nnH0TN8h2W 5V5XQ9/VQZk/GHQVq/Y0Z73BibOJM5Bv+3r2EIJfozlpWdUblat45lSATBo/sktf YKlxwAztWPtcTavJ58F1ufGcUPjwGW4E92zRaozC+tpzd5QtHeYM7m6fGlXxckua UesZcZLl9pY4Bc8Mw40WvI1ibhA2mP2R5AO8hJ0vJyFfi35lqM/DJVV1900yp+em uY+u6bNJ1gLLb7QnhbV1VYLTSCoWzPQvWHgMHAKpAjO15rKAItXD17BM2eQgJMuX LcoWeOcz/MrMQiGKSkqpmapwgtDZ5t81D2qWv+wsaZgcO/erknugHFmR3kAP8YHp JsIpaYY7kj+yVJb92uzZKQAEaUpq3uRsBDtkoC2MPzKN4fgWa8f4jBpIzxuBTd+6 75sVq5VB5eaq3w4J0Z4kbk1DVyNffv3LeZCv9oC2mb1aXyVD/gWHlPD+6wARAQAB tBZseXouLiA8bHl6QHJpc2V1cC5uZXQ+iQJUBBMBCAA+AhsDBQsJCAcCBhUICQoL AgQWAgMBAh4BAheAFiEEatqII4bN+b0YhFNMbH18FhLN4C8FAl4XCTwFCQeLVbcA CgkQbH18FhLN4C/Jkw//Th/tAagxBchztzA2bAJog7sd3FK4hH2cqGFdBG+yx5TW 2ywfDXjTXVeKhHxkSnZZgxO0U31W2Fv+tLmRKN8MrvGSjIpUlWTmeaIG1W+ftlcG NrR+CDL0lrkKZnyQGJhe675lNoo2FKQ/37B/NIyzfIWw8eZStYabHtj5H40nti1k riwZsk76+kR6FI1EVKCGGmo/Spl/VX9MuWNjg9E0cJvpzKY05gKmFSuMJwxVhrFV ly0MhZS+4xddbCMaBo2OJEDrcFBQgBiUnxS8PcADLK7zn3zpemcJm5/8T/DyQHeY 0Yh76KJ92aIB7eLLnRnRcvCXt0RZ+s3sHqLgrsT3OV0jlC7GLjTBgTe6qGH3Lr/h whiOp6g1k125+v20fKPWDlGar3sdSD/ZjJDeAHedV5I3QVT6zorUYcQYb6vYPlOU aq7k0jLjGuoxHQeXGAZMvlKQfgHDfiwBwyIX6D24wsyr+XDnrVyDoCO654OqYcUH wK1y57NbUOpzvD+ZEO/8aKeBUh0zKz682hsA8HJT8G09UBcs36HAnbTkp+rPxgTH eBVcTYLi/CFy9tXOhBmyPhxrILsPwmOvZA4tg7LLnj2P2qdk2Gz1si/D8s2Afr+c re9pidcYbiXJI+Pnw+e9Pylf/1WM8MS5Z2W9Liyc29/kLsCL8Dp0eJtqzJLAX0y5 Ag0EWGznBQEQALNL9sNc4SytS3fOcS4gHvZpH3TLJ6o0K/Lxg4RfkLMebDJwWvSW mjQLv3GqRfOhGj2Osi2YukFIJb4vxPJFO7wQhCi5LLSVEb5d/z9ZOJUdGdI9JvGW dFDuLEXwDnJaP5Jmjm3DwbvHK+goI7Fn3TKc27iqOVAKVIjWNPaqFZxwIE9o/+1c 3bTk3A8WOBmcv1IaxsUNkRDOFJlQYLM/bFIuDD+cW/CcYro8ouC9aekmvTDoRaU5 xv++fXtesn6Cy+xBgvBGIIXGo5xzd6Y66Yf8uNpuJXo9Dc6rApH1QEQNwZX1cxvG UpQx+9JNF0eptDLvTgmxcCglllrylcw8ZsVEt6BTgrCd2JXMGxUcAnhXpRWRmXNL n97FOBb6OBd6k7DC6QCiVKr7sytq1Ywl8GTtWrTP7sK+/+KDLPJ/oY7+bwV94+N8 Gthr94njNqb5G6t9fqQ/+cJv7oF8DoBvylYGqm2hvYpOH53hMq1y3OTPoFKP6AIx twIWHkdmMALm6a6bxAetGQxiaPZTOduJDehwiF9EUkiNhpESMl3I2+vH86jV2IiT 4BuUqGBU5wrAN/FixIRlmaSUX7e0OkUkDexVlpw5poJbPEbvhOtuj/V9BOxQKWB4 bjXMHEHR5YcJ1lhPjFFM3pqOz6ZaN8Hs70KOBE+/3/c1hS5debWPBMdlABEBAAGJ AjwEGAEIACYCGwwWIQRq2ogjhs35vRiEU0xsfXwWEs3gLwUCXhcJRgUJB4tVwQAK CRBsfXwWEs3gL41DEACYtc6mykbhZh2eWrdNynbYX1TNYFH+4BP+zpN8kNHPwKfX IypLLSSwUhYdZ9kb8WB8n4cH8njk4P1LyGtfUOxbEpKCQNXfW3aWDDsZunxdSkyc 3opaCo2w/Gf2ynxtbJVWoNWYn8fDQJcE3UAz8+rioHGRUCBF//G8VWdqZ4PCARGu TPeurJG5aljJGqlrvAXewqNItGEoARHGC3R9otSC8Y5cd9zL3iKUnBh9xhiqFzjK /7J9uQcDz6GTzZKxDqRQmcs27nGjWFNscZY16dBDj6y2d+v+RJEgFY9uW7KGVfFG Y9kPsSKdKUzeE+TOvwintakMQT26dNWBbUkDkMt08kEFk5SyeoQcjnqWMFJgrav1 RYUwz/UFuWep0y9Rt0PrW40mBZOd4roRdgEX6I65K6CC38u/nIgJRG2I/2LkWIwu n2LROOQ+0O6rn4HObgfoEZE03K6AW1DyNR6BnspbTDt0fRIDk6Rrfw6Xe1AfANrK 9zs95WbKkbydE1xFddOJ10qDleFOOaeCWp7KW1GkvEKfoRXhhAo/xnFpjHbGuvJv bTL4pYkaoOyGriAn3fZ8zOoBLspuAzEENBLtX41XU8PFjwcRu4GfFSrP03svi3km WodDQhjSPW+B/9SmLj+UkaIUlTTqwAs8rHtexkzlIhHGASXc+Iuuz5JuzUlPUw== =9EvG -----END PGP PUBLIC KEY BLOCK----- Through Github by opening an issue .","title":"Contact"},{"location":"emojis/","text":"Curated list of emojis to copy paste. Angry \u00b6 (\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6 Annoyed \u00b6 (>_<) Awesome \u00b6 ( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH Conforting \u00b6 (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) Congratulations \u00b6 ( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f Crying \u00b6 (\u2565\ufe4f\u2565) Excited \u00b6 (((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o) Dance \u00b6 (~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^) Happy \u00b6 \u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a Kisses \u00b6 (\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665 Love \u00b6 \u2764 Pride \u00b6 <(\uffe3\uff3e\uffe3)> Relax \u00b6 _\u3078__(\u203e\u25e1\u25dd )> Sad \u00b6 \uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b) Scared \u00b6 \u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f Sleepy \u00b6 (\u1d17\u02f3\u1d17) Smug \u00b6 \uff08\uffe3\uff5e\uffe3\uff09 Whyyyy? \u00b6 (/\uff9f\u0414\uff9f)/ Surprised \u00b6 (\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*) Who cares \u00b6 \u00af\\_(\u30c4)_/\u00af WTF \u00b6 (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8 Links \u00b6 Japanese Emoticons","title":"Emojis"},{"location":"emojis/#angry","text":"(\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6","title":"Angry"},{"location":"emojis/#annoyed","text":"(>_<)","title":"Annoyed"},{"location":"emojis/#awesome","text":"( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH","title":"Awesome"},{"location":"emojis/#conforting","text":"(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)","title":"Conforting"},{"location":"emojis/#congratulations","text":"( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f","title":"Congratulations"},{"location":"emojis/#crying","text":"(\u2565\ufe4f\u2565)","title":"Crying"},{"location":"emojis/#excited","text":"(((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o)","title":"Excited"},{"location":"emojis/#dance","text":"(~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^)","title":"Dance"},{"location":"emojis/#happy","text":"\u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a","title":"Happy"},{"location":"emojis/#kisses","text":"(\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665","title":"Kisses"},{"location":"emojis/#love","text":"\u2764","title":"Love"},{"location":"emojis/#pride","text":"<(\uffe3\uff3e\uffe3)>","title":"Pride"},{"location":"emojis/#relax","text":"_\u3078__(\u203e\u25e1\u25dd )>","title":"Relax"},{"location":"emojis/#sad","text":"\uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b)","title":"Sad"},{"location":"emojis/#scared","text":"\u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f","title":"Scared"},{"location":"emojis/#sleepy","text":"(\u1d17\u02f3\u1d17)","title":"Sleepy"},{"location":"emojis/#smug","text":"\uff08\uffe3\uff5e\uffe3\uff09","title":"Smug"},{"location":"emojis/#whyyyy","text":"(/\uff9f\u0414\uff9f)/","title":"Whyyyy?"},{"location":"emojis/#surprised","text":"(\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*)","title":"Surprised"},{"location":"emojis/#who-cares","text":"\u00af\\_(\u30c4)_/\u00af","title":"Who cares"},{"location":"emojis/#wtf","text":"(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8","title":"WTF"},{"location":"emojis/#links","text":"Japanese Emoticons","title":"Links"},{"location":"wish_list/","text":"This is a gathering of tools, ideas or services that I'd like to enjoy. If you have any lead, as smallest as it may be on how to fulfill them, please contact me . Self hosted search engine \u00b6 It would be awesome to be able to self host a personal search engine that performs priorized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to priorize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or priorize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe meilisearch or searx could be a solution. Decentralized encrypted end to end VOIP and video software \u00b6 I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own. Others \u00b6 Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A way to store music numeric ratings through the command line compatible with mpd and beets . An e-reader support that could be fixed to the wall.","title":"Wish list"},{"location":"wish_list/#self-hosted-search-engine","text":"It would be awesome to be able to self host a personal search engine that performs priorized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to priorize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or priorize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe meilisearch or searx could be a solution.","title":"Self hosted search engine"},{"location":"wish_list/#decentralized-encrypted-end-to-end-voip-and-video-software","text":"I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own.","title":"Decentralized encrypted end to end VOIP and video software"},{"location":"wish_list/#others","text":"Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A way to store music numeric ratings through the command line compatible with mpd and beets . An e-reader support that could be fixed to the wall.","title":"Others"},{"location":"coding/javascript/javascript/","text":"JavaScript is a multi-paradigm, dynamic language with types and operators, standard built-in objects, and methods. Its syntax is based on the Java and C languages \u2014 many structures from those languages apply to JavaScript as well. JavaScript supports object-oriented programming with object prototypes, instead of classes. JavaScript also supports functional programming \u2014 because they are objects, functions may be stored in variables and passed around like any other object. The basics \u00b6 Javascript types \u00b6 JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined Numbers \u00b6 Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable. Convert a string to an integer \u00b6 Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10 Convert a string into a float \u00b6 Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10. Strings \u00b6 Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods . Find the length of a string \u00b6 'hello' . length ; // 5 Booleans \u00b6 JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: ! Variables \u00b6 New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here Operators \u00b6 Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== . Control structures \u00b6 If conditionals \u00b6 var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ; Switch cases \u00b6 switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional While loops \u00b6 while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input )); For loops \u00b6 It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } * for ... of . for ( let value of array ) { // do something with value } * for ... in . for ( let property in object ) { // do something with object property } Objects \u00b6 Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24. Arrays \u00b6 Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3 Iterate over the values of an array \u00b6 for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] } Append an item to an array \u00b6 Although push() could be used, is better to use concat() as it doesn't mutate the original array. a . concat ( item ); Apply a function to the elements of an array \u00b6 const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6] Functions \u00b6 function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop. Define variable number of arguments \u00b6 function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5 Custom objects \u00b6 JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one. Constructor functions \u00b6 We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\" Split code for readability \u00b6 To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); } Links \u00b6 Re-introduction to JavaScript","title":"Javascript"},{"location":"coding/javascript/javascript/#the-basics","text":"","title":"The basics"},{"location":"coding/javascript/javascript/#javascript-types","text":"JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined","title":"Javascript types"},{"location":"coding/javascript/javascript/#numbers","text":"Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable.","title":"Numbers"},{"location":"coding/javascript/javascript/#convert-a-string-to-an-integer","text":"Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10","title":"Convert a string to an integer"},{"location":"coding/javascript/javascript/#convert-a-string-into-a-float","text":"Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10.","title":"Convert a string into a float"},{"location":"coding/javascript/javascript/#strings","text":"Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods .","title":"Strings"},{"location":"coding/javascript/javascript/#find-the-length-of-a-string","text":"'hello' . length ; // 5","title":"Find the length of a string"},{"location":"coding/javascript/javascript/#booleans","text":"JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: !","title":"Booleans"},{"location":"coding/javascript/javascript/#variables","text":"New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here","title":"Variables"},{"location":"coding/javascript/javascript/#operators","text":"Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== .","title":"Operators"},{"location":"coding/javascript/javascript/#control-structures","text":"","title":"Control structures"},{"location":"coding/javascript/javascript/#if-conditionals","text":"var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ;","title":"If conditionals"},{"location":"coding/javascript/javascript/#switch-cases","text":"switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional","title":"Switch cases"},{"location":"coding/javascript/javascript/#while-loops","text":"while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input ));","title":"While loops"},{"location":"coding/javascript/javascript/#for-loops","text":"It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } * for ... of . for ( let value of array ) { // do something with value } * for ... in . for ( let property in object ) { // do something with object property }","title":"For loops"},{"location":"coding/javascript/javascript/#objects","text":"Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24.","title":"Objects"},{"location":"coding/javascript/javascript/#arrays","text":"Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3","title":"Arrays"},{"location":"coding/javascript/javascript/#iterate-over-the-values-of-an-array","text":"for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] }","title":"Iterate over the values of an array"},{"location":"coding/javascript/javascript/#append-an-item-to-an-array","text":"Although push() could be used, is better to use concat() as it doesn't mutate the original array. a . concat ( item );","title":"Append an item to an array"},{"location":"coding/javascript/javascript/#apply-a-function-to-the-elements-of-an-array","text":"const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6]","title":"Apply a function to the elements of an array"},{"location":"coding/javascript/javascript/#functions","text":"function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop.","title":"Functions"},{"location":"coding/javascript/javascript/#define-variable-number-of-arguments","text":"function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5","title":"Define variable number of arguments"},{"location":"coding/javascript/javascript/#custom-objects","text":"JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one.","title":"Custom objects"},{"location":"coding/javascript/javascript/#constructor-functions","text":"We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\"","title":"Constructor functions"},{"location":"coding/javascript/javascript/#split-code-for-readability","text":"To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); }","title":"Split code for readability"},{"location":"coding/javascript/javascript/#links","text":"Re-introduction to JavaScript","title":"Links"},{"location":"coding/python/alembic/","text":"Alembic is a lightweight database migration tool for SQLAlchemy. It is created by the author of SQLAlchemy and it has become the de-facto standard tool to perform migrations on SQLAlchemy backed databases. Database Migration in SQLAlchemy \u00b6 A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema. Alembic repository initialization \u00b6 It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand. Database Migration \u00b6 When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current Seed database with data \u00b6 Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] ) Database downgrade or rollback \u00b6 If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file. References \u00b6 Git Docs Articles \u00b6 Migrate SQLAlchemy databases with Alembic","title":"Alembic"},{"location":"coding/python/alembic/#database-migration-in-sqlalchemy","text":"A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema.","title":"Database Migration in SQLAlchemy"},{"location":"coding/python/alembic/#alembic-repository-initialization","text":"It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand.","title":"Alembic repository initialization"},{"location":"coding/python/alembic/#database-migration","text":"When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current","title":"Database Migration"},{"location":"coding/python/alembic/#seed-database-with-data","text":"Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] )","title":"Seed database with data"},{"location":"coding/python/alembic/#database-downgrade-or-rollback","text":"If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file.","title":"Database downgrade or rollback"},{"location":"coding/python/alembic/#references","text":"Git Docs","title":"References"},{"location":"coding/python/alembic/#articles","text":"Migrate SQLAlchemy databases with Alembic","title":"Articles"},{"location":"coding/python/factoryboy/","text":"Factoryboy is a fixtures replacement library to generate fake data for your program. As it's designed to work well with different ORMs (Django, SQLAlchemy , Mongo) it serves the purpose of building real objects for your tests. Install \u00b6 pip install factory_boy Or add it to the project requirements.txt . Define a factory class \u00b6 Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py . Define attributes \u00b6 I like to use the faker integration of factory boy to generate most of the attributes. Generate numbers \u00b6 Sequential numbers \u00b6 Ideal for IDs id = factory . Sequence ( lambda n : n ) Random number \u00b6 author_id = factory . Faker ( 'random_number' ) Random float \u00b6 score = factory . Faker ( 'pyfloat' ) ## Generate strings ### Word ``` python default = factory . Faker ( 'word' ) Word from a list \u00b6 user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ]) Sentences \u00b6 description = factory . Faker ( 'sentence' ) Names \u00b6 name = factory . Faker ( 'name' ) Urls \u00b6 url = factory . Faker ( 'url' ) Files \u00b6 file_path = factory . Faker ( 'file_path' ) Generate Datetime \u00b6 factory . Faker ( 'date_time' ) Generate your own attribute \u00b6 Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({}) Define relationships \u00b6 Factory Inheritance \u00b6 class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit' Dependent objects direct ForeignKey \u00b6 When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' References \u00b6 Docs Git Common recipes","title":"FactoryBoy"},{"location":"coding/python/factoryboy/#install","text":"pip install factory_boy Or add it to the project requirements.txt .","title":"Install"},{"location":"coding/python/factoryboy/#define-a-factory-class","text":"Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py .","title":"Define a factory class"},{"location":"coding/python/factoryboy/#define-attributes","text":"I like to use the faker integration of factory boy to generate most of the attributes.","title":"Define attributes"},{"location":"coding/python/factoryboy/#generate-numbers","text":"","title":"Generate numbers"},{"location":"coding/python/factoryboy/#sequential-numbers","text":"Ideal for IDs id = factory . Sequence ( lambda n : n )","title":"Sequential numbers"},{"location":"coding/python/factoryboy/#random-number","text":"author_id = factory . Faker ( 'random_number' )","title":"Random number"},{"location":"coding/python/factoryboy/#random-float","text":"score = factory . Faker ( 'pyfloat' ) ## Generate strings ### Word ``` python default = factory . Faker ( 'word' )","title":"Random float"},{"location":"coding/python/factoryboy/#word-from-a-list","text":"user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ])","title":"Word from a list"},{"location":"coding/python/factoryboy/#sentences","text":"description = factory . Faker ( 'sentence' )","title":"Sentences"},{"location":"coding/python/factoryboy/#names","text":"name = factory . Faker ( 'name' )","title":"Names"},{"location":"coding/python/factoryboy/#urls","text":"url = factory . Faker ( 'url' )","title":"Urls"},{"location":"coding/python/factoryboy/#files","text":"file_path = factory . Faker ( 'file_path' )","title":"Files"},{"location":"coding/python/factoryboy/#generate-datetime","text":"factory . Faker ( 'date_time' )","title":"Generate Datetime"},{"location":"coding/python/factoryboy/#generate-your-own-attribute","text":"Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({})","title":"Generate your own attribute"},{"location":"coding/python/factoryboy/#define-relationships","text":"","title":"Define relationships"},{"location":"coding/python/factoryboy/#factory-inheritance","text":"class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit'","title":"Factory Inheritance"},{"location":"coding/python/factoryboy/#dependent-objects-direct-foreignkey","text":"When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit'","title":"Dependent objects direct ForeignKey"},{"location":"coding/python/factoryboy/#references","text":"Docs Git Common recipes","title":"References"},{"location":"coding/python/faker/","text":"Faker is a Python package that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you. Install \u00b6 If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt . Use \u00b6 Generate fake number \u00b6 fake . random_number () Generate a fake dictionary \u00b6 fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list' Generate a fake date \u00b6 fake . date_time () References \u00b6 Git Docs Faker python providers","title":"Faker"},{"location":"coding/python/faker/#install","text":"If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt .","title":"Install"},{"location":"coding/python/faker/#use","text":"","title":"Use"},{"location":"coding/python/faker/#generate-fake-number","text":"fake . random_number ()","title":"Generate fake number"},{"location":"coding/python/faker/#generate-a-fake-dictionary","text":"fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list'","title":"Generate a fake dictionary"},{"location":"coding/python/faker/#generate-a-fake-date","text":"fake . date_time ()","title":"Generate a fake date"},{"location":"coding/python/faker/#references","text":"Git Docs Faker python providers","title":"References"},{"location":"coding/python/feedparser/","text":"Parse Atom and RSS feeds in Python. Install \u00b6 pip install feedparser Basic Usage \u00b6 Parse a feed from a remote URL \u00b6 >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed' Access common elements \u00b6 The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID. Channel elements \u00b6 >>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ])) Item elements \u00b6 >>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60 Advanced usage \u00b6 It is possible to interact with feeds that are protected with credentials . Links \u00b6 Git Docs","title":"Feedparser"},{"location":"coding/python/feedparser/#install","text":"pip install feedparser","title":"Install"},{"location":"coding/python/feedparser/#basic-usage","text":"","title":"Basic Usage"},{"location":"coding/python/feedparser/#parse-a-feed-from-a-remote-url","text":">>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed'","title":"Parse a feed from a remote URL"},{"location":"coding/python/feedparser/#access-common-elements","text":"The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID.","title":"Access common elements"},{"location":"coding/python/feedparser/#channel-elements","text":">>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ]))","title":"Channel elements"},{"location":"coding/python/feedparser/#item-elements","text":">>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60","title":"Item elements"},{"location":"coding/python/feedparser/#advanced-usage","text":"It is possible to interact with feeds that are protected with credentials .","title":"Advanced usage"},{"location":"coding/python/feedparser/#links","text":"Git Docs","title":"Links"},{"location":"coding/python/python_project_template/","text":"It's hard to correctly define the directory structure to make python programs work as expected. Even more if testing, docs or databases are involved. Basic Python project \u00b6 Create virtualenv mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository cd {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration' Create the tests directories mkdir -p tests/unit touch tests/__init__.py touch tests/unit/__init__.py Create the program module structure ```bash mkdir {{ program_name }} echo \" version = '0.1.0'\" >> {{ program_name }}/ init .py Create the program setup.py file from setuptools import find_packages from setuptools import setup from {{ program_name }} import __version__ setup ( name = '{{ program_name }}' , version = __version__ , description = '{{ program_description }}' , author = '{{ author }}' , author_email = '{{ author_email }}' , license = 'GPLv3' , long_description = open ( 'README.md' ) . read (), packages = find_packages ( exclude = ( 'tests' ,)), package_data = { '{{ program_name }}' : [ 'migrations/*' , 'migrations/versions/*' , ]}, entry_points = { 'console_scripts' : [ '{{ program_name }} = {{ program_name }}:main' ]}, install_requires = [ ] ) Remember to fill up the install_requirements with the dependencies that need to be installed at installation time. Create the requirements.tx file. It should contain the install_requirements in addition to the testing requirements such as: pytest pytest-cov Set up the Continuous Integration \u00b6 To set up the Continuous Integration on a Github workflow create the .github/workflows/pythonpackage.yml file with the following contents: name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov clinv tests And add the Badge to your readme, something like: [![Actions Status](https://github.com/lyz-code/pydo/workflows/Python%20package/badge.svg)](https://github.com/lyz-code/pydo/actions) Set up the documentation \u00b6 I use mkdocs with Github Pages for the documentation. Follow the steps under Installation . Set up sqlalchemy for projects without flask \u00b6 Install Alembic : pip install alembic It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations Create the basic models.py file under the project code. \"\"\" Module to store the models. Classes: Class_name: Class description. ... \"\"\" import os from sqlalchemy import \\ create_engine , \\ Column , \\ Integer from sqlalchemy.ext.declarative import declarative_base db_path = os . path . expanduser ( '{{ path_to_sqlite_file }}' ) engine = create_engine ( os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or 'sqlite:///' + db_path ) Base = declarative_base ( bind = engine ) class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) Create the migrations/env.py file as specified in the alembic article . Create the first alembic revision. alembic \\ -c {{ program_name }} /migrations/alembic.ini \\ revision \\ --autogenerate \\ -m \"Initial schema\" Set up the testing environment for SQLAlchemy","title":"Project Template"},{"location":"coding/python/python_project_template/#basic-python-project","text":"Create virtualenv mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository cd {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration' Create the tests directories mkdir -p tests/unit touch tests/__init__.py touch tests/unit/__init__.py Create the program module structure ```bash mkdir {{ program_name }} echo \" version = '0.1.0'\" >> {{ program_name }}/ init .py Create the program setup.py file from setuptools import find_packages from setuptools import setup from {{ program_name }} import __version__ setup ( name = '{{ program_name }}' , version = __version__ , description = '{{ program_description }}' , author = '{{ author }}' , author_email = '{{ author_email }}' , license = 'GPLv3' , long_description = open ( 'README.md' ) . read (), packages = find_packages ( exclude = ( 'tests' ,)), package_data = { '{{ program_name }}' : [ 'migrations/*' , 'migrations/versions/*' , ]}, entry_points = { 'console_scripts' : [ '{{ program_name }} = {{ program_name }}:main' ]}, install_requires = [ ] ) Remember to fill up the install_requirements with the dependencies that need to be installed at installation time. Create the requirements.tx file. It should contain the install_requirements in addition to the testing requirements such as: pytest pytest-cov","title":"Basic Python project"},{"location":"coding/python/python_project_template/#set-up-the-continuous-integration","text":"To set up the Continuous Integration on a Github workflow create the .github/workflows/pythonpackage.yml file with the following contents: name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov clinv tests And add the Badge to your readme, something like: [![Actions Status](https://github.com/lyz-code/pydo/workflows/Python%20package/badge.svg)](https://github.com/lyz-code/pydo/actions)","title":"Set up the Continuous Integration"},{"location":"coding/python/python_project_template/#set-up-the-documentation","text":"I use mkdocs with Github Pages for the documentation. Follow the steps under Installation .","title":"Set up the documentation"},{"location":"coding/python/python_project_template/#set-up-sqlalchemy-for-projects-without-flask","text":"Install Alembic : pip install alembic It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations Create the basic models.py file under the project code. \"\"\" Module to store the models. Classes: Class_name: Class description. ... \"\"\" import os from sqlalchemy import \\ create_engine , \\ Column , \\ Integer from sqlalchemy.ext.declarative import declarative_base db_path = os . path . expanduser ( '{{ path_to_sqlite_file }}' ) engine = create_engine ( os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or 'sqlite:///' + db_path ) Base = declarative_base ( bind = engine ) class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) Create the migrations/env.py file as specified in the alembic article . Create the first alembic revision. alembic \\ -c {{ program_name }} /migrations/alembic.ini \\ revision \\ --autogenerate \\ -m \"Initial schema\" Set up the testing environment for SQLAlchemy","title":"Set up sqlalchemy for projects without flask"},{"location":"coding/python/sqlalchemy/","text":"SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. Creating an SQL Schema \u00b6 First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward. Creating Tables \u00b6 If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object. class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . It's important to add the different parameters as attributes if you want to access them later. Creating relationships \u00b6 Joined table inheritance \u00b6 In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class. class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , } One to many \u00b6 from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'author' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey . Many to many \u00b6 # Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , ) Self referenced many to many \u00b6 Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side. Testing SQLAlchemy Code \u00b6 The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest . Exporting database to json \u00b6 import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result ) References \u00b6 Home Docs","title":"SQLAlchemy"},{"location":"coding/python/sqlalchemy/#creating-an-sql-schema","text":"First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward.","title":"Creating an SQL Schema"},{"location":"coding/python/sqlalchemy/#creating-tables","text":"If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object. class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . It's important to add the different parameters as attributes if you want to access them later.","title":"Creating Tables"},{"location":"coding/python/sqlalchemy/#creating-relationships","text":"","title":"Creating relationships"},{"location":"coding/python/sqlalchemy/#joined-table-inheritance","text":"In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class. class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , }","title":"Joined table inheritance"},{"location":"coding/python/sqlalchemy/#one-to-many","text":"from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'author' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey .","title":"One to many"},{"location":"coding/python/sqlalchemy/#many-to-many","text":"# Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , )","title":"Many to many"},{"location":"coding/python/sqlalchemy/#self-referenced-many-to-many","text":"Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side.","title":"Self referenced many to many"},{"location":"coding/python/sqlalchemy/#testing-sqlalchemy-code","text":"The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest .","title":"Testing SQLAlchemy Code"},{"location":"coding/python/sqlalchemy/#exporting-database-to-json","text":"import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result )","title":"Exporting database to json"},{"location":"coding/python/sqlalchemy/#references","text":"Home Docs","title":"References"},{"location":"coding/react/react/","text":"React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called \u201ccomponents\u201d. Set up a new project \u00b6 Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start . Start a React + Flask project \u00b6 Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run The basics \u00b6 Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components. Pass data between components \u00b6 Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... } Use of the state \u00b6 React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); } Share the state between parent and children \u00b6 To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components. Handling data change \u00b6 There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering. Function components \u00b6 Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); } Miscellaneous \u00b6 List rendering \u00b6 When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key . Links \u00b6 React tutorial Awesome React Awesome React components Responsive React \u00b6 Responsive react Responsive websites without css react-responsive library With Flask \u00b6 How to create a react + flask project How to deploy a react + flask project","title":"React"},{"location":"coding/react/react/#set-up-a-new-project","text":"Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start .","title":"Set up a new project"},{"location":"coding/react/react/#start-a-react-flask-project","text":"Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run","title":"Start a React + Flask project"},{"location":"coding/react/react/#the-basics","text":"Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components.","title":"The basics"},{"location":"coding/react/react/#pass-data-between-components","text":"Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... }","title":"Pass data between components"},{"location":"coding/react/react/#use-of-the-state","text":"React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); }","title":"Use of the state"},{"location":"coding/react/react/#share-the-state-between-parent-and-children","text":"To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components.","title":"Share the state between parent and children"},{"location":"coding/react/react/#handling-data-change","text":"There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering.","title":"Handling data change"},{"location":"coding/react/react/#function-components","text":"Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); }","title":"Function components"},{"location":"coding/react/react/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"coding/react/react/#list-rendering","text":"When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key .","title":"List rendering"},{"location":"coding/react/react/#links","text":"React tutorial Awesome React Awesome React components","title":"Links"},{"location":"coding/react/react/#responsive-react","text":"Responsive react Responsive websites without css react-responsive library","title":"Responsive React"},{"location":"coding/react/react/#with-flask","text":"How to create a react + flask project How to deploy a react + flask project","title":"With Flask"},{"location":"devops/api_management/","text":"API management is the process of creating and publishing web application programming interfaces (APIs) under a service that: Enforces the usage of policies. Controls access. Collects and analyzes usage statistics. Reports on performance. Components \u00b6 While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","title":"API Management"},{"location":"devops/api_management/#components","text":"While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","title":"Components"},{"location":"devops/devops/","text":"DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops) which aims to shorten the systems development life cycle and provide continuous delivery with high software quality. Learn path \u00b6 DevOps is has become a juicy work, if you want to introduce yourself into this world I suggest you to follow these steps: Learn basic Linux administration, otherwise you'll be lost. Learn how to use Git. If you can host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions .","title":"DevOps"},{"location":"devops/devops/#learn-path","text":"DevOps is has become a juicy work, if you want to introduce yourself into this world I suggest you to follow these steps: Learn basic Linux administration, otherwise you'll be lost. Learn how to use Git. If you can host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions .","title":"Learn path"},{"location":"devops/helmfile/","text":"Helmfile is a declarative spec for deploying Helm charts. It lets you: Keep a directory of chart value files and maintain changes in version control. Apply CI/CD to configuration changes. Environmental chart promotion. Periodically sync to avoid skew in environments. To avoid upgrades for each iteration of helm, the helmfile executable delegates to helm - as a result, helm must be installed. All information is saved in the helmfile.yaml file. In case we need custom yamls, we'll use kustomize . Installation \u00b6 Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile Usage \u00b6 How to deploy a new chart \u00b6 When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are dated, if they are not, run helmfile apply . nfigure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes. Keep charts updated \u00b6 To have your charts updated, this would be my suggested workflow, although the developers haven't confirmed it yet : A periodic CI job would run helmfile deps , once a change is detected in the lock file, the job will run helmfile --environment=staging apply . Developers are notified that the new version is deployed and are prompted to test it. Once it's validated, the developers will manually introduce the new version in the lockfile and run helmfile --environment=production apply . Delegate to the developers the manual introduction of the version in the lockfile isn't the ideal solution, but it's the one I can come up to avoid race conditions on chart releases. Uninstall charts \u00b6 Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands. Force the reinstallation of everything \u00b6 If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases. Debugging helmfile \u00b6 Error: \"release-name\" has no deployed releases \u00b6 This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again. Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help) \u00b6 I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false. Links \u00b6 Git","title":"Helmfile"},{"location":"devops/helmfile/#installation","text":"Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile","title":"Installation"},{"location":"devops/helmfile/#usage","text":"","title":"Usage"},{"location":"devops/helmfile/#how-to-deploy-a-new-chart","text":"When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are dated, if they are not, run helmfile apply . nfigure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes.","title":"How to deploy a new chart"},{"location":"devops/helmfile/#keep-charts-updated","text":"To have your charts updated, this would be my suggested workflow, although the developers haven't confirmed it yet : A periodic CI job would run helmfile deps , once a change is detected in the lock file, the job will run helmfile --environment=staging apply . Developers are notified that the new version is deployed and are prompted to test it. Once it's validated, the developers will manually introduce the new version in the lockfile and run helmfile --environment=production apply . Delegate to the developers the manual introduction of the version in the lockfile isn't the ideal solution, but it's the one I can come up to avoid race conditions on chart releases.","title":"Keep charts updated"},{"location":"devops/helmfile/#uninstall-charts","text":"Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands.","title":"Uninstall charts"},{"location":"devops/helmfile/#force-the-reinstallation-of-everything","text":"If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases.","title":"Force the reinstallation of everything"},{"location":"devops/helmfile/#debugging-helmfile","text":"","title":"Debugging helmfile"},{"location":"devops/helmfile/#error-release-name-has-no-deployed-releases","text":"This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again.","title":"Error: \"release-name\" has no deployed releases"},{"location":"devops/helmfile/#error-failed-to-download-stablemetrics-server-hint-running-helm-repo-update-may-help","text":"I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false.","title":"Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help)"},{"location":"devops/helmfile/#links","text":"Git","title":"Links"},{"location":"devops/aws/aws/","text":"Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. In aggregate, these cloud computing web services provide a set of primitive abstract technical infrastructure and distributed computing building blocks and tools. Learn path \u00b6 TBD","title":"AWS"},{"location":"devops/aws/aws/#learn-path","text":"TBD","title":"Learn path"},{"location":"devops/aws/s3/","text":"S3 is the secure, durable, and scalable object storage infrastructure of AWS. Often used for serving static website content or holding backups or data. Commands \u00b6 Bucket management \u00b6 List buckets \u00b6 aws s3 ls Create bucket \u00b6 aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1 Enable versioning \u00b6 aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled Enable encryption \u00b6 aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' Download bucket \u00b6 aws s3 cp --recursive s3:// {{ bucket_name }} . Audit the S3 bucket policy \u00b6 IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done Object management \u00b6 Remove an object \u00b6 aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }} Upload \u00b6 Upload a local file with the cli \u00b6 aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }} Upload a file unauthenticated \u00b6 curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/ Restore an object \u00b6 First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }} Troubleshooting \u00b6 get_environ_proxies() missing 1 required positional argument: 'no_proxy' \u00b6 sudo pip3 install --upgrade boto3 Links \u00b6 User guide","title":"S3"},{"location":"devops/aws/s3/#commands","text":"","title":"Commands"},{"location":"devops/aws/s3/#bucket-management","text":"","title":"Bucket management"},{"location":"devops/aws/s3/#list-buckets","text":"aws s3 ls","title":"List buckets"},{"location":"devops/aws/s3/#create-bucket","text":"aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1","title":"Create bucket"},{"location":"devops/aws/s3/#enable-versioning","text":"aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled","title":"Enable versioning"},{"location":"devops/aws/s3/#enable-encryption","text":"aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }'","title":"Enable encryption"},{"location":"devops/aws/s3/#download-bucket","text":"aws s3 cp --recursive s3:// {{ bucket_name }} .","title":"Download bucket"},{"location":"devops/aws/s3/#audit-the-s3-bucket-policy","text":"IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done","title":"Audit the S3 bucket policy"},{"location":"devops/aws/s3/#object-management","text":"","title":"Object management"},{"location":"devops/aws/s3/#remove-an-object","text":"aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }}","title":"Remove an object"},{"location":"devops/aws/s3/#upload","text":"","title":"Upload"},{"location":"devops/aws/s3/#upload-a-local-file-with-the-cli","text":"aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }}","title":"Upload a local file with the cli"},{"location":"devops/aws/s3/#upload-a-file-unauthenticated","text":"curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/","title":"Upload a file unauthenticated"},{"location":"devops/aws/s3/#restore-an-object","text":"First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }}","title":"Restore an object"},{"location":"devops/aws/s3/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"devops/aws/s3/#get_environ_proxies-missing-1-required-positional-argument-no_proxy","text":"sudo pip3 install --upgrade boto3","title":"get_environ_proxies() missing 1 required positional argument: 'no_proxy'"},{"location":"devops/aws/s3/#links","text":"User guide","title":"Links"},{"location":"devops/aws/security_groups/","text":"Security groups are the AWS way of defining firewall rules between the resources. If not handled properly they can soon become hard to read, which can lead to an insecure infrastructure. It has helped me to use four types of security groups: Default security groups: Security groups created by AWS per VPC and region, they can't be deleted. Naming security groups: Used to identify an aws resource. They are usually referenced in other security groups. Ingress security groups: Used to define the rules of ingress traffic to the resource. Egress security groups: Used to define the rules of egress traffic to the resource. But what helped most has been using clinv while refactoring all the security groups. With clinv unused I got rid of all the security groups that weren't used by any AWS resource (beware of #16 , 17 , #18 and #19 ), then used the clinv unassigned security_groups to methodically decide if they were correct and add them to my inventory or if I needed to refactor them. Best practices \u00b6 Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them. Naming convention \u00b6 A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources Default security groups \u00b6 There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws . {{ region_id }} } Where the provider must be configured in the `terraform_config . tf` file , for example: ``` terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" } Naming security groups \u00b6 For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$ { 3:vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2 . id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file. Ingress security groups \u00b6 For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Egress security groups \u00b6 For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Instance security group definition \u00b6 When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data . terraform_remote_state . security_groups . outputs . gitea_instance_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_monitoring_at_privatedmz_id , data . terraform_remote_state . security_groups . outputs . ingress_administration_from_bastion_at_connectiondmz_id , data . terraform_remote_state . security_groups . outputs . egress_ldap_to_ldap_instance_from_publicdmz_id , data . terraform_remote_state . security_groups . outputs . egress_https_to_debian_repositories_from_publicdmz_id , ]","title":"Security groups workflow"},{"location":"devops/aws/security_groups/#best-practices","text":"Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them.","title":"Best practices"},{"location":"devops/aws/security_groups/#naming-convention","text":"A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources","title":"Naming convention"},{"location":"devops/aws/security_groups/#default-security-groups","text":"There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws . {{ region_id }} } Where the provider must be configured in the `terraform_config . tf` file , for example: ``` terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" }","title":"Default security groups"},{"location":"devops/aws/security_groups/#naming-security-groups","text":"For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$ { 3:vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2 . id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Naming security groups"},{"location":"devops/aws/security_groups/#ingress-security-groups","text":"For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Ingress security groups"},{"location":"devops/aws/security_groups/#egress-security-groups","text":"For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Egress security groups"},{"location":"devops/aws/security_groups/#instance-security-group-definition","text":"When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data . terraform_remote_state . security_groups . outputs . gitea_instance_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_monitoring_at_privatedmz_id , data . terraform_remote_state . security_groups . outputs . ingress_administration_from_bastion_at_connectiondmz_id , data . terraform_remote_state . security_groups . outputs . egress_ldap_to_ldap_instance_from_publicdmz_id , data . terraform_remote_state . security_groups . outputs . egress_https_to_debian_repositories_from_publicdmz_id , ]","title":"Instance security group definition"},{"location":"devops/aws/iam/iam/","text":"AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization). Configurable AWS access controls: Grant access to AWS Management console, APIs Create individual users Manage permissions with groups Configure a strong password policy Enable Multi-Factor Authentication for privileged users Use IAM roles for EC2 instances Use IAM roles to share access Rotate security credentials regularly Restrict privileged access further with conditions Use your corporate directory system or a third party authentication Links \u00b6 Docs","title":"IAM"},{"location":"devops/aws/iam/iam/#links","text":"Docs","title":"Links"},{"location":"devops/aws/iam/iam_commands/","text":"Information gathering \u00b6 List roles \u00b6 aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table List policies \u00b6 aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table List attached policies \u00b6 aws iam list-attached-role-policies --role-name {{ role_name }} Get role configuration \u00b6 aws iam get-role --role-name {{ role_name }} Get role policies \u00b6 aws iam list-role-policies --role-name {{ role_name }}","title":"IAM Commands"},{"location":"devops/aws/iam/iam_commands/#information-gathering","text":"","title":"Information gathering"},{"location":"devops/aws/iam/iam_commands/#list-roles","text":"aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table","title":"List roles"},{"location":"devops/aws/iam/iam_commands/#list-policies","text":"aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table","title":"List policies"},{"location":"devops/aws/iam/iam_commands/#list-attached-policies","text":"aws iam list-attached-role-policies --role-name {{ role_name }}","title":"List attached policies"},{"location":"devops/aws/iam/iam_commands/#get-role-configuration","text":"aws iam get-role --role-name {{ role_name }}","title":"Get role configuration"},{"location":"devops/aws/iam/iam_commands/#get-role-policies","text":"aws iam list-role-policies --role-name {{ role_name }}","title":"Get role policies"},{"location":"devops/aws/iam/iam_debug/","text":"MFADevice entity at the same path and name already exists \u00b6 It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"IAM Debugging"},{"location":"devops/aws/iam/iam_debug/#mfadevice-entity-at-the-same-path-and-name-already-exists","text":"It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"MFADevice entity at the same path and name already exists"},{"location":"devops/helm/helm/","text":"Helm is the package manager for Kubernetes. Through charts it helps you define, install and upgrade even the most complex Kubernetes applications. The advantages of using helm over kubectl apply are the easiness of: Repeatable application installation. CI integration. Versioning and sharing. Charts are a group of Go templates of kubernetes yaml resource manifests, they are easy to create, version, share, and publish. Helm alone lacks some features, that are satisfied through some external programs: Helmfile is used to declaratively configure your charts, so they can be versioned through git. Helm-secrets is used to remove hardcoded credentials from values.yaml files. Helm has an open issue to integrate it into it's codebase. Links \u00b6 Homepage Docs Git Chart hub Git charts repositories","title":"Helm"},{"location":"devops/helm/helm/#links","text":"Homepage Docs Git Chart hub Git charts repositories","title":"Links"},{"location":"devops/helm/helm_commands/","text":"Small cheatsheet on how to use the helm command. List charts \u00b6 helm ls Get information of chart \u00b6 helm inspect {{ package_name }} Download a chart \u00b6 helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }} Search charts \u00b6 helm search {{ package_name }} Operations you should do with helmfile \u00b6 The following operations can be done with helm, but consider using helmfile instead. Install chart \u00b6 Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }} Give it a name \u00b6 helm install --name {{ release_name }} {{ package_name }} Give it a namespace \u00b6 helm install --namespace {{ namespace }} {{ package_name }} Customize the chart before installing \u00b6 helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }} Upgrade a release \u00b6 If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }} Rollback an upgrade \u00b6 First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }} Delete a release \u00b6 helm delete --purge {{ release_name }} Working with repositories \u00b6 List repositories \u00b6 helm repo list Add repository \u00b6 helm repo add {{ repo_name }} {{ repo_url }} Update repositories \u00b6 helm repo update","title":"Helm Commands"},{"location":"devops/helm/helm_commands/#list-charts","text":"helm ls","title":"List charts"},{"location":"devops/helm/helm_commands/#get-information-of-chart","text":"helm inspect {{ package_name }}","title":"Get information of chart"},{"location":"devops/helm/helm_commands/#download-a-chart","text":"helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }}","title":"Download a chart"},{"location":"devops/helm/helm_commands/#search-charts","text":"helm search {{ package_name }}","title":"Search charts"},{"location":"devops/helm/helm_commands/#operations-you-should-do-with-helmfile","text":"The following operations can be done with helm, but consider using helmfile instead.","title":"Operations you should do with helmfile"},{"location":"devops/helm/helm_commands/#install-chart","text":"Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }}","title":"Install chart"},{"location":"devops/helm/helm_commands/#give-it-a-name","text":"helm install --name {{ release_name }} {{ package_name }}","title":"Give it a name"},{"location":"devops/helm/helm_commands/#give-it-a-namespace","text":"helm install --namespace {{ namespace }} {{ package_name }}","title":"Give it a namespace"},{"location":"devops/helm/helm_commands/#customize-the-chart-before-installing","text":"helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }}","title":"Customize the chart before installing"},{"location":"devops/helm/helm_commands/#upgrade-a-release","text":"If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }}","title":"Upgrade a release"},{"location":"devops/helm/helm_commands/#rollback-an-upgrade","text":"First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }}","title":"Rollback an upgrade"},{"location":"devops/helm/helm_commands/#delete-a-release","text":"helm delete --purge {{ release_name }}","title":"Delete a release"},{"location":"devops/helm/helm_commands/#working-with-repositories","text":"","title":"Working with repositories"},{"location":"devops/helm/helm_commands/#list-repositories","text":"helm repo list","title":"List repositories"},{"location":"devops/helm/helm_commands/#add-repository","text":"helm repo add {{ repo_name }} {{ repo_url }}","title":"Add repository"},{"location":"devops/helm/helm_commands/#update-repositories","text":"helm repo update","title":"Update repositories"},{"location":"devops/helm/helm_installation/","text":"There are two usable versions of Helm, v2 and v3, the latter is quite new so some of the things we need to install as of 2020-01-27 are not yet supported (Prometheus operator), so we are going to stick to the version 2. Helm has a client-server architecture, the server is installed in the Kubernetes cluster and the client is a Go executable installed in the user computer. Helm client \u00b6 You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm Installation"},{"location":"devops/helm/helm_installation/#helm-client","text":"You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm client"},{"location":"devops/helm/helm_secrets/","text":"Helm-secrets is a helm plugin that manages secrets with Git workflow and stores them anywhere. It delegates the cryptographic operations to Mozilla's Sops tool, which supports PGP, AWS KMS and GCP KMS. The configuration is stored in .sops.yaml files. You can find in Mozilla's documentation a detailed configuration guide. For my use case, I'm only going to use a list of PGP keys, so the following contents should be in the .sops.yaml file at the project root directory. creation_rules : - pgp : >- {{ gpg_key_1 }}, {{ gpg_key_2}} Prevent committing decrypted files to git \u00b6 From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit Usage \u00b6 Encrypt secret files \u00b6 Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials. Edit secret files \u00b6 helm secrets edit secrets.yaml Decrypt secret files \u00b6 helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git . Clean all the decrypted files \u00b6 helm secrets clean . Add or remove keys \u00b6 Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options. Links \u00b6 Git","title":"Helm Secrets"},{"location":"devops/helm/helm_secrets/#prevent-committing-decrypted-files-to-git","text":"From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit","title":"Prevent committing decrypted files to git"},{"location":"devops/helm/helm_secrets/#usage","text":"","title":"Usage"},{"location":"devops/helm/helm_secrets/#encrypt-secret-files","text":"Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials.","title":"Encrypt secret files"},{"location":"devops/helm/helm_secrets/#edit-secret-files","text":"helm secrets edit secrets.yaml","title":"Edit secret files"},{"location":"devops/helm/helm_secrets/#decrypt-secret-files","text":"helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git .","title":"Decrypt secret files"},{"location":"devops/helm/helm_secrets/#clean-all-the-decrypted-files","text":"helm secrets clean .","title":"Clean all the decrypted files"},{"location":"devops/helm/helm_secrets/#add-or-remove-keys","text":"Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options.","title":"Add or remove keys"},{"location":"devops/helm/helm_secrets/#links","text":"Git","title":"Links"},{"location":"devops/kong/kong/","text":"Kong is a lua application API platform running in Nginx. Installation \u00b6 Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use. Links \u00b6 Homepage Docs","title":"Kong"},{"location":"devops/kong/kong/#installation","text":"Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use.","title":"Installation"},{"location":"devops/kong/kong/#links","text":"Homepage Docs","title":"Links"},{"location":"devops/kubectl/kubectl/","text":"Kubectl Definition Kubectl is a command line tool for controlling Kubernetes clusters. kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag. Resource types and it's aliases \u00b6 Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources Links \u00b6 Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.","title":"Kubectl"},{"location":"devops/kubectl/kubectl/#resource-types-and-its-aliases","text":"Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources","title":"Resource types and it's aliases"},{"location":"devops/kubectl/kubectl/#links","text":"Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.","title":"Links"},{"location":"devops/kubectl/kubectl_installation/","text":"Kubectl is not yet in the distribution package managers, so we'll need to install it manually. curl -LO \"https://storage.googleapis.com/kubernetes-release/release/ $( \\ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt \\ ) /bin/linux/amd64/kubectl\" chmod +x kubectl mv kubectl ~/.local/bin/kubectl Configure kubectl \u00b6 Set editor \u00b6 # File ~/.bashrc KUBE_EDITOR = \"vim\" Set auto completion \u00b6 # File ~/.bashrc source < ( kubectl completion bash ) Configure EKS cluster \u00b6 To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}","title":"Kubectl Installation"},{"location":"devops/kubectl/kubectl_installation/#configure-kubectl","text":"","title":"Configure kubectl"},{"location":"devops/kubectl/kubectl_installation/#set-editor","text":"# File ~/.bashrc KUBE_EDITOR = \"vim\"","title":"Set editor"},{"location":"devops/kubectl/kubectl_installation/#set-auto-completion","text":"# File ~/.bashrc source < ( kubectl completion bash )","title":"Set auto completion"},{"location":"devops/kubectl/kubectl_installation/#configure-eks-cluster","text":"To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}","title":"Configure EKS cluster"},{"location":"devops/kubernetes/kubernetes/","text":"Kubernetes (commonly stylized as k8s) is an open-source container-orchestration system for automating application deployment, scaling, and management. Developed by Google in Go under the Apache 2.0 license, it was first released on June 7, 2014 reaching 1.0 by July 21, 2015. It works with a range of container tools, including Docker. Many cloud services offer a Kubernetes-based platform or infrastructure as a service ( PaaS or IaaS ) on which Kubernetes can be deployed as a platform-providing service. Many vendors also provide their own branded Kubernetes distributions. It has become the standard infrastructure to manage containers in production environments. Docker Swarm would be an alternative but it falls short in features compared with Kubernetes. These are some of the advantages of using Kubernetes: Widely used in production and actively developed. Ensure high availability of your services with autohealing and autoscaling. Easy, quickly and predictable deployment and promotion of applications. Seamless roll out of features. Optimize hardware use while guaranteeing resource isolation. Easiest way to build multi-cloud and baremetal environments. Several companies have used Kubernetes to release their own PaaS : OpenShift by Red Hat. Tectonic by CoreOS. Rancher labs by Rancher. Learn roadmap \u00b6 K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af . Links \u00b6 Docs Awesome K8s Katacoda playground Comic Diving deeper \u00b6 Architecture Resources Kubectl Additional Components Networking Helm Tools Reference \u00b6 References API conventions","title":"Kubernetes"},{"location":"devops/kubernetes/kubernetes/#learn-roadmap","text":"K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af .","title":"Learn roadmap"},{"location":"devops/kubernetes/kubernetes/#links","text":"Docs Awesome K8s Katacoda playground Comic","title":"Links"},{"location":"devops/kubernetes/kubernetes/#diving-deeper","text":"Architecture Resources Kubectl Additional Components Networking Helm Tools","title":"Diving deeper"},{"location":"devops/kubernetes/kubernetes/#reference","text":"References API conventions","title":"Reference"},{"location":"devops/kubernetes/kubernetes_annotations/","text":"Annotations are non-identifying metadata key/value pairs attached to objects, such as pods. Annotations are intended to give meaningful and relevant information to libraries and tools. Annotations, like labels, are key/value maps: \"annotations\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" } Here are some examples of information that could be recorded in annotations: Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto generated fields and fields set by auto sizing or auto scaling systems. Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address. Pointers to logging, monitoring, analytics, or audit repositories. Client library or tool information that can be used for debugging purposes, for example, name, version, and build information. User or tool/system provenance information, such as URLs of related objects from other ecosystem components. Lightweight rollout tool metadata: for example, config or checkpoints.","title":"Annotations"},{"location":"devops/kubernetes/kubernetes_architecture/","text":"Kubernetes is a combination of components distributed between two kind of nodes, Masters and Workers . Master Nodes \u00b6 Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes. Worker Nodes \u00b6 Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself. kube-proxy operation modes \u00b6 kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode. Kubectl \u00b6 The kubectl is the command line client used to communicate with the Masters. Links \u00b6 Kubernetes components overview","title":"Architecture"},{"location":"devops/kubernetes/kubernetes_architecture/#master-nodes","text":"Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.","title":"Master Nodes"},{"location":"devops/kubernetes/kubernetes_architecture/#worker-nodes","text":"Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself.","title":"Worker Nodes"},{"location":"devops/kubernetes/kubernetes_architecture/#kube-proxy-operation-modes","text":"kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode.","title":"kube-proxy operation modes"},{"location":"devops/kubernetes/kubernetes_architecture/#kubectl","text":"The kubectl is the command line client used to communicate with the Masters.","title":"Kubectl"},{"location":"devops/kubernetes/kubernetes_architecture/#links","text":"Kubernetes components overview","title":"Links"},{"location":"devops/kubernetes/kubernetes_cluster_autoscaler/","text":"While Horizontal pod autoscaling allows a deployment to scale given the resources needed, they are limited to the kubernetes existing working nodes. To autoscale the number of working nodes we need the cluster autoscaler . For AWS, there are the Amazon guidelines to enable it . But I'd use the cluster-autoscaler helm chart.","title":"Cluster Autoscaler"},{"location":"devops/kubernetes/kubernetes_dashboard/","text":"Dashboard definition Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. Deployment \u00b6 The best way to install it is with the stable/kubernetes-dashboard chart with helmfile . Links \u00b6 Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide","title":"Dashboard"},{"location":"devops/kubernetes/kubernetes_dashboard/#deployment","text":"The best way to install it is with the stable/kubernetes-dashboard chart with helmfile .","title":"Deployment"},{"location":"devops/kubernetes/kubernetes_dashboard/#links","text":"Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide","title":"Links"},{"location":"devops/kubernetes/kubernetes_deployments/","text":"The different types of deployments configure a ReplicaSet and a PodSchema for your application. Depending on the type of application we'll use one of the following types. Deployments \u00b6 Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80 StatefulSets \u00b6 StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator . DaemonSet \u00b6 DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd Job \u00b6 Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.","title":"Deployments"},{"location":"devops/kubernetes/kubernetes_deployments/#deployments","text":"Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80","title":"Deployments"},{"location":"devops/kubernetes/kubernetes_deployments/#statefulsets","text":"StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator .","title":"StatefulSets"},{"location":"devops/kubernetes/kubernetes_deployments/#daemonset","text":"DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd","title":"DaemonSet"},{"location":"devops/kubernetes/kubernetes_deployments/#job","text":"Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.","title":"Job"},{"location":"devops/kubernetes/kubernetes_external_dns/","text":"The external-dns resource allows the creation of DNS records from within kubernetes inside the definition of service and ingress resources. It currently supports the following providers: Provider Status Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta AzureDNS Beta CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha Akamai FastDNS Alpha There are two reasons to enable it: If there is any change in the ingress or service load balancer endpoint, due to a deployment, the dns records are automatically changed. It's easier for developers to connect their applications. Deployment in AWS \u00b6 To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .","title":"External DNS"},{"location":"devops/kubernetes/kubernetes_external_dns/#deployment-in-aws","text":"To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .","title":"Deployment in AWS"},{"location":"devops/kubernetes/kubernetes_hpa/","text":"With Horizontal pod autoscaling , Kubernetes automatically scales the number of pods in a deployment or replication controller based on observed CPU utilization or on some other application provided metrics. The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user. To make it work, the definition of pod resource consumption needs to be specified.","title":"Horizontal Pod Autoscaling"},{"location":"devops/kubernetes/kubernetes_ingress/","text":"An Ingress is An API object that manages external access to the services in a cluster, typically HTTP. Ingress provide a centralized way to: Load balancing. SSL termination. Dynamic service discovery. Traffic routing. Authentication. Traffic distribution: canary deployments, A/B testing, mirroring/shadowing. Graphical user interface. JWT validation. WAF and DDOS protection. Requests tracing. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer .","title":"Ingress"},{"location":"devops/kubernetes/kubernetes_ingress_controller/","text":"Ingress controllers monitor the cluster events for the creation or modification of Ingress resources, modifying accordingly the underlying load balancers. They are not part of the master kube-controller-manager , so you'll need to install them manually. There are different Ingress controllers, such as AWS ALB, Nginx, HAProxy or Traefik, using one or other depends on your needs. Almost all controllers are open sourced and support dynamic service discovery, SSL termination or WebSockets. But they differ in: Supported protocols : HTTP, HTTPS, gRPC, HTTP/2.0, TCP (with SNI) or UDP. Underlying software : NGINX, Traefik, HAProxy or Envoy. Traffic routing : host and path, regular expression support. Namespace limitations : supported or not. Upstream probes : active checks, passive checks, retries, circuit breakers, custom health checks... Load balancing algorithms : round-robin, sticky sessions, rdp-cookie... Authentication : Basic, digest, Oauth, external auth, SSL certificate... Traffic distribution : canary deployments, A/B testing, mirroring/shadowing. Paid subscription : extended functionality or technical support. Graphical user interface : JWT validation : Customization of configuration : Basic DDOS protection mechanisms : rate limit, traffic filtering. WAF : Requests tracing : monitor, trace and debug requests via OpenTracing or other options. Both ITNext and Flant provide good ingress controller comparisons, a synoptical resume of both articles follows. Kubernetes Ingress controller \u00b6 The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s. Traefik \u00b6 Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik. HAProxy \u00b6 HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1. Istio Ingress \u00b6 Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio. ALB Ingress controller \u00b6 The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service. ALB ingress deployment \u00b6 This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so. Links \u00b6 ITNext ingress controller comparison Flant ingress controller comparison","title":"Ingress Controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#kubernetes-ingress-controller","text":"The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s.","title":"Kubernetes Ingress controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#traefik","text":"Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik.","title":"Traefik"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#haproxy","text":"HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1.","title":"HAProxy"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#istio-ingress","text":"Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio.","title":"Istio Ingress"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-controller","text":"The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service.","title":"ALB Ingress controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-deployment","text":"This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so.","title":"ALB ingress deployment"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#links","text":"ITNext ingress controller comparison Flant ingress controller comparison","title":"Links"},{"location":"devops/kubernetes/kubernetes_labels/","text":"Labels are identifying metadata key/value pairs attached to objects, such as pods. Labels are intended to give meaningful and relevant information to users, but which do not directly imply semantics to the core system. \"labels\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" }","title":"Labels"},{"location":"devops/kubernetes/kubernetes_metric_server/","text":"The metrics server monitors the resource consumption inside the cluster. It populates the information in kubectl top nodes to get the node status and gives the information to automatically autoscale deployments with Horizontal pod autoscaling . To install it, you can use the metrics-server helm chart. To test that the horizontal pod autoscaling is working, follow the AWS EKS guide .","title":"Metrics Server"},{"location":"devops/kubernetes/kubernetes_namespaces/","text":"Namespaces are virtual clusters backed by the same physical cluster. It's the first level of isolation between applications. When to Use Multiple Namespaces \u00b6 Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","title":"Namespaces"},{"location":"devops/kubernetes/kubernetes_namespaces/#when-to-use-multiple-namespaces","text":"Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","title":"When to Use Multiple Namespaces"},{"location":"devops/kubernetes/kubernetes_networking/","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. If you want to get a quickly grasp on how k8s networking works, I suggest you to read StackRox's Kubernetes networking demystified article . CNI comparison \u00b6 Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis . TL;DR \u00b6 When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast. Flannel \u00b6 Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work. Calico \u00b6 Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide . Canal \u00b6 Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition. Weave Net \u00b6 Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting. AWS CNI \u00b6 AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS. Advantages of the AWS CNI \u00b6 Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive. Disadvantages of the AWS CNI \u00b6 On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet. VPC CNI Pod Density Limitations \u00b6 First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached. Cloud Portability \u00b6 Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea. Links \u00b6 StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Networking"},{"location":"devops/kubernetes/kubernetes_networking/#cni-comparison","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis .","title":"CNI comparison"},{"location":"devops/kubernetes/kubernetes_networking/#tldr","text":"When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast.","title":"TL;DR"},{"location":"devops/kubernetes/kubernetes_networking/#flannel","text":"Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work.","title":"Flannel"},{"location":"devops/kubernetes/kubernetes_networking/#calico","text":"Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide .","title":"Calico"},{"location":"devops/kubernetes/kubernetes_networking/#canal","text":"Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition.","title":"Canal"},{"location":"devops/kubernetes/kubernetes_networking/#weave-net","text":"Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting.","title":"Weave Net"},{"location":"devops/kubernetes/kubernetes_networking/#aws-cni","text":"AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS.","title":"AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#advantages-of-the-aws-cni","text":"Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive.","title":"Advantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#disadvantages-of-the-aws-cni","text":"On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet.","title":"Disadvantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#vpc-cni-pod-density-limitations","text":"First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached.","title":"VPC CNI Pod Density Limitations"},{"location":"devops/kubernetes/kubernetes_networking/#cloud-portability","text":"Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea.","title":"Cloud Portability"},{"location":"devops/kubernetes/kubernetes_networking/#links","text":"StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Links"},{"location":"devops/kubernetes/kubernetes_operators/","text":"Operators are Kubernetes specific applications (pods) that configure, manage and optimize other Kubernetes deployments automatically. A Kubernetes Operator might be able to: Install and provide sane initial configuration and sizing for your deployment, according to the specs of your Kubernetes cluster. Perform live reloading of deployments and pods to accommodate for any user requested parameter modification (hot config reloading). Safe coordination of application upgrades. Automatically scale up or down according to performance metrics. Service discovery via native Kubernetes APIs Application TLS certificate configuration Disaster recovery. Perform backups to offsite storage, integrity checks or any other maintenance task. How do they work? \u00b6 An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1. Links \u00b6 CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Operators"},{"location":"devops/kubernetes/kubernetes_operators/#how-do-they-work","text":"An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1.","title":"How do they work?"},{"location":"devops/kubernetes/kubernetes_operators/#links","text":"CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Links"},{"location":"devops/kubernetes/kubernetes_pods/","text":"Pods are the basic building block of Kubernetes, the smallest and simplest unit in the object model that you create or deploy. A Pod represents a running process on your cluster. A Pod represents a unit of deployment. It encapsulates: An application container (or, in some cases, multiple tightly coupled containers). Storage resources. A unique network IP. Options that govern how the container(s) should run.","title":"Pods"},{"location":"devops/kubernetes/kubernetes_replicasets/","text":"ReplicaSet maintains a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. You'll probably never manually use these resources, as they are defined inside the deployments . The older version of this resource are the Replication controllers .","title":"ReplicaSets"},{"location":"devops/kubernetes/kubernetes_services/","text":"A Service defines a policy to access a logical set of Pods using a reliable endpoint. Users and other programs can access pods running on your cluster seamlessly. Therefore allowing a loose coupling between dependent Pods. When a request arrives the endpoint, the kube-proxy pod of the node forwards the request to the Pods that match the service LabelSelector. Services can be exposed in different ways by specifying a type in the ServiceSpec: ClusterIP (default): Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster. NodePort : Exposes the Service on the same port of each selected Node in the cluster using NAT to the outside. LoadBalancer : Creates an external load balancer in the current cloud and assigns a fixed, external IP to the Service. To create an internal ELB of AWs add to the annotations: annotations : service.beta.kubernetes.io/aws-load-balancer-internal : 0.0.0.0/0 ExternalName : Exposes the Service using an arbitrary name by returning a CNAME record with the name. No proxy is used. If no RBAC or NetworkPolicies are applied, you can call a service of another namespace with the following nomenclature. curl {{ service_name }} . {{ service_namespace }} .svc.cluster.local","title":"Services"},{"location":"devops/kubernetes/kubernetes_storage_driver/","text":"Storage drivers are pods that through the Container Storage Interface or CSI provide an interface to use external storage services from within Kubernetes. Amazon EBS CSI storage driver \u00b6 Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .","title":"Storage Driver"},{"location":"devops/kubernetes/kubernetes_storage_driver/#amazon-ebs-csi-storage-driver","text":"Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .","title":"Amazon EBS CSI storage driver"},{"location":"devops/kubernetes/kubernetes_tools/","text":"There are several tools built to enhance the operation, installation and use of Kubernetes. Tried \u00b6 K3s : Recommended small kubernetes, like hyperkube. To try \u00b6 crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.","title":"Tools"},{"location":"devops/kubernetes/kubernetes_tools/#tried","text":"K3s : Recommended small kubernetes, like hyperkube.","title":"Tried"},{"location":"devops/kubernetes/kubernetes_tools/#to-try","text":"crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.","title":"To try"},{"location":"devops/kubernetes/kubernetes_vertical_pod_autoscaler/","text":"Kubernetes knows the amount of resources a pod needs to operate through some metadata specified in the deployment . Generally this values change and manually maintaining all the resources requested and limits is a nightmare. The Vertical pod autoscaler does data analysis on the pod metrics to automatically adjust these values. Nevertheless it's still not suggested to use it in conjunction with the horizontal pod autoscaler , so we'll need to watch out for future improvements.","title":"Vertical Pod Autoscaler"},{"location":"devops/kubernetes/kubernetes_volumes/","text":"On disk files in a Container are ephemeral by default, which presents the following issues: When a Container crashes, kubelet will restart it, but the files will be lost. When running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems with several types . configMap \u00b6 The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod. emptyDir \u00b6 An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever. hostPath \u00b6 A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys . secret \u00b6 A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage. awsElasticBlockStore \u00b6 An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume. nfs \u00b6 An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously. local \u00b6 A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk. Others \u00b6 glusterfs cephfs","title":"Volumes"},{"location":"devops/kubernetes/kubernetes_volumes/#configmap","text":"The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod.","title":"configMap"},{"location":"devops/kubernetes/kubernetes_volumes/#emptydir","text":"An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever.","title":"emptyDir"},{"location":"devops/kubernetes/kubernetes_volumes/#hostpath","text":"A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys .","title":"hostPath"},{"location":"devops/kubernetes/kubernetes_volumes/#secret","text":"A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.","title":"secret"},{"location":"devops/kubernetes/kubernetes_volumes/#awselasticblockstore","text":"An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume.","title":"awsElasticBlockStore"},{"location":"devops/kubernetes/kubernetes_volumes/#nfs","text":"An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously.","title":"nfs"},{"location":"devops/kubernetes/kubernetes_volumes/#local","text":"A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.","title":"local"},{"location":"devops/kubernetes/kubernetes_volumes/#others","text":"glusterfs cephfs","title":"Others"},{"location":"devops/prometheus/alertmanager/","text":"The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. It is configured through the alertmanager.config key of the values.yaml of the helm chart. As stated in the configuration file , it has four main keys (as templates is handled in alertmanager.config.templateFiles ): global : SMTP and API main configuration, it will be inherited by the other elements. route : Route tree definition. receivers : Notification integrations configuration. inhibit_rules : Alert inhibition configuration. Route \u00b6 A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node. Receivers \u00b6 Notification receivers are the named configurations of one or more notification integrations. Email notifications \u00b6 To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts. Rocketchat Notifications \u00b6 Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }} Inhibit rules \u00b6 Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch Alert rules \u00b6 Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules Links \u00b6 Awesome prometheus alert rules","title":"AlertManager"},{"location":"devops/prometheus/alertmanager/#route","text":"A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node.","title":"Route"},{"location":"devops/prometheus/alertmanager/#receivers","text":"Notification receivers are the named configurations of one or more notification integrations.","title":"Receivers"},{"location":"devops/prometheus/alertmanager/#email-notifications","text":"To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts.","title":"Email notifications"},{"location":"devops/prometheus/alertmanager/#rocketchat-notifications","text":"Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }}","title":"Rocketchat Notifications"},{"location":"devops/prometheus/alertmanager/#inhibit-rules","text":"Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch","title":"Inhibit rules"},{"location":"devops/prometheus/alertmanager/#alert-rules","text":"Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules","title":"Alert rules"},{"location":"devops/prometheus/alertmanager/#links","text":"Awesome prometheus alert rules","title":"Links"},{"location":"devops/prometheus/blackbox_exporter/","text":"The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP. It can be used to test: Website accessibility . Both for availability and security purposes. Website loading time . DNS response times to diagnose network latency issues. SSL certificates expiration . ICMP requests to gather network health information . Security protections such as if and endpoint stops being protected by VPN, WAF or SSL client certificate. Unauthorized read or write S3 buckets . When running, the Blackbox exporter is going to expose a HTTP endpoint that can be used in order to monitor targets over the network. By default, the Blackbox exporter exposes the /probe endpoint that is used to retrieve those metrics. The blackbox exporter is configured with a YAML configuration file made of modules . Installation \u00b6 To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply Blackbox exporter probes \u00b6 Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx HTTP endpoint working correctly \u00b6 http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint working correctly \u00b6 https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint behind client SSL certificate \u00b6 https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation. HTTPS endpoint with an specific error \u00b6 If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*' HTTP endpoint returning an error \u00b6 http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] no_follow_redirects : false HTTPS endpoint through an HTTP proxy \u00b6 https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\" HTTPS endpoint with basic auth \u00b6 https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }} HTTPs endpoint with API key \u00b6 https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }} HTTPS Put file \u00b6 Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" Check open port \u00b6 tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect Ping to the resource \u00b6 Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\" Blackbox exporter alerts \u00b6 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests Availability alerts \u00b6 The most basic probes, test if the service is up and returning. Blackbox probe failed \u00b6 Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0 Blackbox probe HTTP failure \u00b6 HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" description : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Performance alerts \u00b6 Blackbox slow probe \u00b6 Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" description : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow HTTP \u00b6 HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" description : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow ping \u00b6 Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" description : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" SSL certificate alerts \u00b6 Blackbox SSL certificate will expire in a month \u00b6 SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" description : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Blackbox SSL certificate will expire in a few days \u00b6 SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" description : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\" Blackbox SSL certificate expired \u00b6 SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" description : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Security alerts \u00b6 To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx Test endpoints protected with network policies \u00b6 Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" description : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with SSL client certificate \u00b6 Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" description : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with credentials. \u00b6 Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" description : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with WAF. \u00b6 Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" description : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized read of S3 buckets \u00b6 Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" description : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized write of S3 buckets \u00b6 Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" description : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Monitoring external access to internal services \u00b6 There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic. Troubleshooting \u00b6 To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true . Service monitors are not being created \u00b6 When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again. probe_success == 0 when using an http proxy \u00b6 Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. Links \u00b6 Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .","title":"Blackbox Exporter"},{"location":"devops/prometheus/blackbox_exporter/#installation","text":"To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply","title":"Installation"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-exporter-probes","text":"Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx","title":"Blackbox exporter probes"},{"location":"devops/prometheus/blackbox_exporter/#http-endpoint-working-correctly","text":"http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTP endpoint working correctly"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-working-correctly","text":"https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTPS endpoint working correctly"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-behind-client-ssl-certificate","text":"https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation.","title":"HTTPS endpoint behind client SSL certificate"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-an-specific-error","text":"If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*'","title":"HTTPS endpoint with an specific error"},{"location":"devops/prometheus/blackbox_exporter/#http-endpoint-returning-an-error","text":"http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] no_follow_redirects : false","title":"HTTP endpoint returning an error"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-through-an-http-proxy","text":"https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\"","title":"HTTPS endpoint through an HTTP proxy"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-basic-auth","text":"https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }}","title":"HTTPS endpoint with basic auth"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-api-key","text":"https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }}","title":"HTTPs endpoint with API key"},{"location":"devops/prometheus/blackbox_exporter/#https-put-file","text":"Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTPS Put file"},{"location":"devops/prometheus/blackbox_exporter/#check-open-port","text":"tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect","title":"Check open port"},{"location":"devops/prometheus/blackbox_exporter/#ping-to-the-resource","text":"Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\"","title":"Ping to the resource"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-exporter-alerts","text":"Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests","title":"Blackbox exporter alerts"},{"location":"devops/prometheus/blackbox_exporter/#availability-alerts","text":"The most basic probes, test if the service is up and returning.","title":"Availability alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-failed","text":"Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0","title":"Blackbox probe failed"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-http-failure","text":"HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" description : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"","title":"Blackbox probe HTTP failure"},{"location":"devops/prometheus/blackbox_exporter/#performance-alerts","text":"","title":"Performance alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-slow-probe","text":"Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" description : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1","title":"Blackbox slow probe"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-slow-http","text":"HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" description : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1","title":"Blackbox probe slow HTTP"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-slow-ping","text":"Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" description : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\"","title":"Blackbox probe slow ping"},{"location":"devops/prometheus/blackbox_exporter/#ssl-certificate-alerts","text":"","title":"SSL certificate alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-month","text":"SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" description : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"","title":"Blackbox SSL certificate will expire in a month"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-few-days","text":"SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" description : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\"","title":"Blackbox SSL certificate will expire in a few days"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-expired","text":"SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" description : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Blackbox SSL certificate expired"},{"location":"devops/prometheus/blackbox_exporter/#security-alerts","text":"To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx","title":"Security alerts"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-network-policies","text":"Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" description : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with network policies"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-ssl-client-certificate","text":"Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" description : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with SSL client certificate"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-credentials","text":"Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" description : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with credentials."},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-waf","text":"Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" description : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with WAF."},{"location":"devops/prometheus/blackbox_exporter/#unauthorized-read-of-s3-buckets","text":"Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" description : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Unauthorized read of S3 buckets"},{"location":"devops/prometheus/blackbox_exporter/#unauthorized-write-of-s3-buckets","text":"Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" description : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Unauthorized write of S3 buckets"},{"location":"devops/prometheus/blackbox_exporter/#monitoring-external-access-to-internal-services","text":"There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic.","title":"Monitoring external access to internal services"},{"location":"devops/prometheus/blackbox_exporter/#troubleshooting","text":"To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true .","title":"Troubleshooting"},{"location":"devops/prometheus/blackbox_exporter/#service-monitors-are-not-being-created","text":"When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again.","title":"Service monitors are not being created"},{"location":"devops/prometheus/blackbox_exporter/#probe_success-0-when-using-an-http-proxy","text":"Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist.","title":"probe_success == 0 when using an http proxy"},{"location":"devops/prometheus/blackbox_exporter/#links","text":"Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .","title":"Links"},{"location":"devops/prometheus/prometheus/","text":"Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. A quick overview of Prometheus would be, as stated in the coreos article : At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented. Go to the Prometheus architecture post for more details. We are living a shift to the DevOps culture, containers and Kubernetes. So nowadays: Developers need to integrate app and business related metrics as an organic part of the infrastructure. So monitoring needs to be democratized, made more accessible and cover additional layers of the stack. Container based infrastructures are changing how we monitor the resources. Now we have a huge number of volatile software entities, services, virtual network addresses, exposed metrics that suddenly appear or vanish. Traditional monitoring tools are not designed to handle this. These reasons pushed Soundcloud to build a new monitoring system that had the following features Multi-dimensional data model : The model is based on key-value pairs, similar to how Kubernetes itself organizes infrastructure metadata using labels. It allows for flexible and accurate time series data, powering its Prometheus query language. Accessible format and protocols : Exposing prometheus metrics is a pretty straightforward task. Metrics are human readable, are in a self-explanatory format, and are published using a standard HTTP transport. You can check that the metrics are correctly exposed just using your web browser. Service discovery : The Prometheus server is in charge of periodically scraping the targets, so that applications and services don\u2019t need to worry about emitting data (metrics are pulled, not pushed). These Prometheus servers have several methods to auto-discover scrape targets, some of them can be configured to filter and match container metadata, making it an excellent fit for ephemeral Kubernetes workloads. Modular and highly available components : Metric collection, alerting, graphical visualization, etc, are performed by different composable services. All these services are designed to support redundancy and sharding. Pull based metrics : Most monitoring systems are pushing metrics to a centralized collection platform. Prometheus flips this model on it's head with the following advantages: No need to install custom software in the physical servers or containers. Doesn't require applications to use CPU cycles pushing metrics. Handles service failure/unavailability gracefully. If a target goes down, Prometheus can record it was unable to retrieve data. You can use the Pushgateway if pulling metrics is not feasible. Installation \u00b6 There are several ways to install prometheus , but I'd recommend using the Kubernetes Prometheus operator . Exposing your metrics \u00b6 Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics . Metric types \u00b6 There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately. Using labels \u00b6 Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics. Links \u00b6 Homepage . Docs . Awesome Prometheus . Diving deeper \u00b6 Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Prometheus Troubleshooting Introduction posts \u00b6 Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview Books \u00b6 Prometheus Up & Running . Monitoring With Prometheus .","title":"Prometheus"},{"location":"devops/prometheus/prometheus/#installation","text":"There are several ways to install prometheus , but I'd recommend using the Kubernetes Prometheus operator .","title":"Installation"},{"location":"devops/prometheus/prometheus/#exposing-your-metrics","text":"Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics .","title":"Exposing your metrics"},{"location":"devops/prometheus/prometheus/#metric-types","text":"There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately.","title":"Metric types"},{"location":"devops/prometheus/prometheus/#using-labels","text":"Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics.","title":"Using labels"},{"location":"devops/prometheus/prometheus/#links","text":"Homepage . Docs . Awesome Prometheus .","title":"Links"},{"location":"devops/prometheus/prometheus/#diving-deeper","text":"Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Prometheus Troubleshooting","title":"Diving deeper"},{"location":"devops/prometheus/prometheus/#introduction-posts","text":"Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview","title":"Introduction posts"},{"location":"devops/prometheus/prometheus/#books","text":"Prometheus Up & Running . Monitoring With Prometheus .","title":"Books"},{"location":"devops/prometheus/prometheus_architecture/","text":"Prometheus Server \u00b6 Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries. Prometheus Targets \u00b6 Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX . Pushgateway \u00b6 In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server. Service discovery \u00b6 Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring. Alertmanager \u00b6 The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. Data visualization and export \u00b6 There are several ways to visualize or export data from Prometheus. Prometheus web UI \u00b6 Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery. Grafana \u00b6 Grafana is the best way to visually analyze the evolution of the metrics throughout time. API clients \u00b6 Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too. Links \u00b6 Prometheus Overview Open Source for U architecture overview","title":"Architecture"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-server","text":"Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries.","title":"Prometheus Server"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-targets","text":"Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX .","title":"Prometheus Targets"},{"location":"devops/prometheus/prometheus_architecture/#pushgateway","text":"In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server.","title":"Pushgateway"},{"location":"devops/prometheus/prometheus_architecture/#service-discovery","text":"Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring.","title":"Service discovery"},{"location":"devops/prometheus/prometheus_architecture/#alertmanager","text":"The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.","title":"Alertmanager"},{"location":"devops/prometheus/prometheus_architecture/#data-visualization-and-export","text":"There are several ways to visualize or export data from Prometheus.","title":"Data visualization and export"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-web-ui","text":"Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery.","title":"Prometheus web UI"},{"location":"devops/prometheus/prometheus_architecture/#grafana","text":"Grafana is the best way to visually analyze the evolution of the metrics throughout time.","title":"Grafana"},{"location":"devops/prometheus/prometheus_architecture/#api-clients","text":"Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too.","title":"API clients"},{"location":"devops/prometheus/prometheus_architecture/#links","text":"Prometheus Overview Open Source for U architecture overview","title":"Links"},{"location":"devops/prometheus/prometheus_installation/","text":"To install the operator we'll use helmfile to install the stable/prometheus-operator chart . Add the following lines to your helmfile.yaml . - name : prometheus-operator namespace : monitoring chart : stable/prometheus-operator values : - prometheus-operator/values.yaml Edit the chart values. mkdir prometheus-operator helm inspect values stable/prometheus-operator > prometheus-operator/values.yaml vi prometheus-operator/values.yaml I've implemented the following changes: If you are using a managed solution like EKS, the provider will hide kube-scheduler and kube-controller-manager so those metrics will fail. Therefore you need to disable: defaultRules.rules.kubeScheduler: false . kubeScheduler.enabled: false . kubeControllerManager.enabled: false . Configure the alertmanager . Enabled the ingress of alertmanager , grafana and prometheus . Set up the storage of alertmanager and prometheus with storageClassName: gp2 (for AWS). Configure the grafana dashboards: Blackbox grafana dashboard Change additionalPrometheusRules to additionalPrometheusRulesMap as the former is going to be deprecated in future releases. For private clusters, disable the admission webhook . prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false And install. helmfile diff helmfile apply Once it's installed you can check everything is working by accessing the grafana dashboard. First of all get the pod name (we'll asume you've used the monitoring namespace). kubectl get pods -n monitoring | grep grafana Then set up the proxies kubectl port-forward {{ grafana_pod }} -n monitoring 3000 :3000 kubectl port-forward -n monitoring \\ prometheus-prometheus-operator-prometheus-0 9090 :9090 To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090 . If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved. Edit the 127.0.0.1 value to 0.0.0.0 for the key metricsBindAddress in kubectl -n kube-system edit cm kube-proxy-config And restart the DaemonSet: kubectl rollout restart -n kube-system daemonset.apps/kube-proxy Creating alerts \u00b6 Alerts are configured in Installing other exporters \u00b6 Blackbox Exporter","title":"Prometheus Install"},{"location":"devops/prometheus/prometheus_installation/#creating-alerts","text":"Alerts are configured in","title":"Creating alerts"},{"location":"devops/prometheus/prometheus_installation/#installing-other-exporters","text":"Blackbox Exporter","title":"Installing other exporters"},{"location":"devops/prometheus/prometheus_operator/","text":"Prometheus has it's own kubernetes operator , which makes it simple to install with helm, and enables users to configure and manage instances of Prometheus using simple declarative configuration that will, in response, create, configure, and manage Prometheus monitoring instances. Once installed the Prometheus Operator provides the following features: Create/Destroy : Easily launch a Prometheus instance for your Kubernetes namespace, a specific application or team easily using the Operator. Simple Configuration : Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource. Target Services via Labels : Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. How it works \u00b6 The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running. Links \u00b6 Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3","title":"Prometheus Operator"},{"location":"devops/prometheus/prometheus_operator/#how-it-works","text":"The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running.","title":"How it works"},{"location":"devops/prometheus/prometheus_operator/#links","text":"Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3","title":"Links"},{"location":"devops/prometheus/prometheus_troubleshooting/","text":"Solutions for problems with Prometheus. Service monitor not being recognized \u00b6 Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work. Failed calling webhook prometheusrulemutate.monitoring.coreos.com \u00b6 Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.","title":"Prometheus Troubleshooting"},{"location":"devops/prometheus/prometheus_troubleshooting/#service-monitor-not-being-recognized","text":"Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work.","title":"Service monitor not being recognized"},{"location":"devops/prometheus/prometheus_troubleshooting/#failed-calling-webhook-prometheusrulemutatemonitoringcoreoscom","text":"Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.","title":"Failed calling webhook prometheusrulemutate.monitoring.coreos.com"},{"location":"drawing/drawing/","text":"It's really difficult to choose where to start if you want to learn how to draw from scratch as there are too many resources. I'm starting with Drawabox , a free course based on series of practical exercises to teach the basics. I'd probably go to Ctrl+Paint once I'm done. The basics \u00b6 Changing the mindset \u00b6 Start your drawing path with the following guidelines to make your progression smoother: Focus on the experience of drawing instead of the result. Understand that doing something badly does not define who you are. So tackle the I can't draw that feeling with I can't draw that well . If you're afraid the thing you want to draw is out of your reach, draw it anyway. At least half of the time spent drawing must be devoted to drawing purely for its own sake. If you don't have much time, alternate the purpose of your sessions. Don't over control your hand with your brain to try to be absolutely precise and accurate. Doing so will result in numerous course corrections making your strokes wobbly, stiff and erratic. Furthermore, spending all the focus resources in precision, will result in a lack to solve the other problems involved. Once muscle memory is gained, the strokes will be cleaner. Draw exactly what you see, while you see it . Don't trust you memory, as it will simplify things without you noticing it. Draw from your shoulder . We are used to pivot on the wrist as it makes stiff and accurate linework, suitable for writing. But falls apart when making smooth and consistent strokes. So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get yo u in the habit of taking the path of least resistance . So try to use the shoulder. This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow. If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder. Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should: Flow continuously : When making a line between two points, do it with a single continuous stroke even if you miss the end. Flow smoothly : Draw with a confident and persistent pace (enough to keep your brain from interfering and attempting to course correct as you go). Again we favor flow over accuracy , so expect to make your lines less accurate. Maintain a consistent trayectory : Split lines into derivable strokes. Otherwise, you'll make mindless zigzags. Drawing skills \u00b6 The course focuses on these psychological skills and concepts : Confidence : The willingness to push forwards without hesitation once your preparations are complete. Control : The ability to decide ahead of time what kind of mark you wish to puto down on the page, and to execute as intended. Patience : The path is hard. Spatial Reasoning : To be able to understand the things we draw as being three dimensional forms that exist in and relate to one another within the three dimensional world. Construction : The ability to look at a complex object and break it down into simple components that can be drawn individually and combined to reconstruct our complex object on a page. Visual Communication : The ability to take a concept, idea, or amount of information, and to convey it clearly and directly to an audience using visual means. Ghosting \u00b6 Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down. Planning : Lay out the terms of the line you want to draw, paint a dot for the start and another for the end. Rotating the page : Find the most comfortable angle of approach for the line you've planned. Usually it's roughly a 45 degree angle fom left to right (if you're right handed). Ghosting : Go through the motion of drawing your line, over and over, in one direction, without actually touching the page, so as to build muscle memory. Execution : Once you feel comfortable with the motion, without missing a beat or breaking the rhythm of repetition, lower your pen to the page and go through the motion one more time. Drawabox course guidelines \u00b6 When doing the course exercises, try to: Read only the instruction pages that are assigned to each exercise. Don't redo the exercises until you achieve perfection, even when you don't feel satisfied with your results. Accept this now, and it will save you a lot of grief and wasted time in the future. Your only focus should be on following the instructions to the best of your current ability. Read and follow the instructions carefully to ensure you understand them. Each time you finish an exercise incorporate into a pool from which take two or three at the beginning of each session to do for 10 or 15 minutes. Tools to use \u00b6 For the Drawabox course, you need a fineliner , also called felt tips or technical pens . The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5). When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink. For the paper, use the regular printer one. Links \u00b6 Drawabox Ctrl+Paint Dive deeper \u00b6 Pool of drawing exercises","title":"Drawing"},{"location":"drawing/drawing/#the-basics","text":"","title":"The basics"},{"location":"drawing/drawing/#changing-the-mindset","text":"Start your drawing path with the following guidelines to make your progression smoother: Focus on the experience of drawing instead of the result. Understand that doing something badly does not define who you are. So tackle the I can't draw that feeling with I can't draw that well . If you're afraid the thing you want to draw is out of your reach, draw it anyway. At least half of the time spent drawing must be devoted to drawing purely for its own sake. If you don't have much time, alternate the purpose of your sessions. Don't over control your hand with your brain to try to be absolutely precise and accurate. Doing so will result in numerous course corrections making your strokes wobbly, stiff and erratic. Furthermore, spending all the focus resources in precision, will result in a lack to solve the other problems involved. Once muscle memory is gained, the strokes will be cleaner. Draw exactly what you see, while you see it . Don't trust you memory, as it will simplify things without you noticing it. Draw from your shoulder . We are used to pivot on the wrist as it makes stiff and accurate linework, suitable for writing. But falls apart when making smooth and consistent strokes. So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get yo u in the habit of taking the path of least resistance . So try to use the shoulder. This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow. If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder. Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should: Flow continuously : When making a line between two points, do it with a single continuous stroke even if you miss the end. Flow smoothly : Draw with a confident and persistent pace (enough to keep your brain from interfering and attempting to course correct as you go). Again we favor flow over accuracy , so expect to make your lines less accurate. Maintain a consistent trayectory : Split lines into derivable strokes. Otherwise, you'll make mindless zigzags.","title":"Changing the mindset"},{"location":"drawing/drawing/#drawing-skills","text":"The course focuses on these psychological skills and concepts : Confidence : The willingness to push forwards without hesitation once your preparations are complete. Control : The ability to decide ahead of time what kind of mark you wish to puto down on the page, and to execute as intended. Patience : The path is hard. Spatial Reasoning : To be able to understand the things we draw as being three dimensional forms that exist in and relate to one another within the three dimensional world. Construction : The ability to look at a complex object and break it down into simple components that can be drawn individually and combined to reconstruct our complex object on a page. Visual Communication : The ability to take a concept, idea, or amount of information, and to convey it clearly and directly to an audience using visual means.","title":"Drawing skills"},{"location":"drawing/drawing/#ghosting","text":"Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down. Planning : Lay out the terms of the line you want to draw, paint a dot for the start and another for the end. Rotating the page : Find the most comfortable angle of approach for the line you've planned. Usually it's roughly a 45 degree angle fom left to right (if you're right handed). Ghosting : Go through the motion of drawing your line, over and over, in one direction, without actually touching the page, so as to build muscle memory. Execution : Once you feel comfortable with the motion, without missing a beat or breaking the rhythm of repetition, lower your pen to the page and go through the motion one more time.","title":"Ghosting"},{"location":"drawing/drawing/#drawabox-course-guidelines","text":"When doing the course exercises, try to: Read only the instruction pages that are assigned to each exercise. Don't redo the exercises until you achieve perfection, even when you don't feel satisfied with your results. Accept this now, and it will save you a lot of grief and wasted time in the future. Your only focus should be on following the instructions to the best of your current ability. Read and follow the instructions carefully to ensure you understand them. Each time you finish an exercise incorporate into a pool from which take two or three at the beginning of each session to do for 10 or 15 minutes.","title":"Drawabox course guidelines"},{"location":"drawing/drawing/#tools-to-use","text":"For the Drawabox course, you need a fineliner , also called felt tips or technical pens . The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5). When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink. For the paper, use the regular printer one.","title":"Tools to use"},{"location":"drawing/drawing/#links","text":"Drawabox Ctrl+Paint","title":"Links"},{"location":"drawing/drawing/#dive-deeper","text":"Pool of drawing exercises","title":"Dive deeper"},{"location":"drawing/exercise_pool/","text":"Set of exercises to maintain the fundamental skills required for drawing. Before doing a drawing session, spend 10-20 minutes doing one or several of these exercises. Superimposed lines : for different increasing lengths (4cm, 8cm, half the width and full width), draw a line with a ruler and repeat the stroke freehand eight times. Also try some arcing lines, and even some waves. Make sure you fray only at the end. Example: Ghosted lines : Fill up a page with straight lines following the ghosting method . Special things to avoid: Wobbly lines Arcing lines There are several levels of success with this exercise: Level 1 : Line is smooth and consistent without any visible wobbling, but doesn't quite pass through A or B, due to not following the right trajectory. It's a straight shot, but misses the mark a bit. Level 2 : Like level 1 and maintains the correct trajectory. It does however either fall short or overshot one or both points. Level 3 : Like level 2 and also starts at right at one point and ends exactly at the other. Example: Ghosted planes : Fill up a page with planes using the ghosting method . Start with 4 points, join them, then fill in the two diagonals, and then make a cross through the center of the X. Special things to avoid: Wobbly lines Arcing lines Example: As you repeat the exercise, you can start to envision these planes as being three dimensional rectilinear surfaces. The third and fourth steps, where we construct the diagonals and the cross can be treated as being a subdivision of the plane. The cross will require some estimation to find the center of each edge in space.","title":"Exercise Pool"},{"location":"life_automation/life_automation/","text":"Life Automation is the act of analyzing your interactions with the world to find ways to reduce the time or willpower spent on unwanted processes. Once you've covered some minimum life requirements (health, money or happiness), time is your most valued asset. It's sad to waste it doing stuff that we need but don't increase our happiness. So the idea is to identify which are those processes and find optimizations that allows us to do them in less time or using less willpower. I've also faced the problem of having so much stuff in my mind. Having background processes increase your brain load and are a constant sink of willpower. As a result, when you really need that CPU time, your brain is tired and doesn't work to it's full performance. Automating processes, like life logging and task management, allows you to delegate those worries. Life automation can lead to habit building, which reduces even more the willpower consumption of processes, at the same time it reduces the error rate. Automating life management \u00b6 Week automation , or how to review and plan the week. Automating home chores \u00b6 Using grocy to maintain the house stock, shopping lists and meal plans.","title":"Life Automation"},{"location":"life_automation/life_automation/#automating-life-management","text":"Week automation , or how to review and plan the week.","title":"Automating life management"},{"location":"life_automation/life_automation/#automating-home-chores","text":"Using grocy to maintain the house stock, shopping lists and meal plans.","title":"Automating home chores"},{"location":"life_automation/week_automation/","text":"I've been polishing a week reviewing and planning method that suits my needs. I usually follow it on Wednesdays, as I'm too busy on Mondays and Tuesdays and it gives enough time to plan the weekend. Until I've got pydo ready to natively incorporate all this processes, I heavily use taskwarrior to manage my tasks and logs. To make the process faster and reproducible, I've written small python scripts using tasklib. Week review \u00b6 Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date. Week planning \u00b6 The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week Automation"},{"location":"life_automation/week_automation/#week-review","text":"Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date.","title":"Week review"},{"location":"life_automation/week_automation/#week-planning","text":"The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week planning"},{"location":"linux/google_chrome/","text":"Although I hate it, there are web pages that don't work on Firefox or Chromium. In those cases I install google-chrome and uninstall as soon as I don't need to use that service. Installation \u00b6 Debian \u00b6 wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb","title":"google chrome"},{"location":"linux/google_chrome/#installation","text":"","title":"Installation"},{"location":"linux/google_chrome/#debian","text":"wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb","title":"Debian"},{"location":"linux/mkdocs/","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Installation \u00b6 Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin * Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name }} site_author : {{ your_name }} site_url : {{ site_url }} nav : - Introduction : 'index.md' - Basic Usage : 'basic_usage.md' - Configuration : 'configuration.md' - Update : 'update.md' - Advanced Usage : - Projects : \"projects.md\" - Tags : \"tags.md\" plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : \"theme\" logo : \"images/logo.png\" palette : primary : 'blue grey' accent : 'light blue' extra_css : - 'stylesheets/extra.css' - 'stylesheets/links.css' repo_name : {{ repository_name }} # for example: 'lyz-code/pydo' repo_url : {{ repository_url }} # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve Add a github pages hook. \u00b6 Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : 'x64' - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working. Links \u00b6 Homepage . Material theme configuration guide","title":"mkdocs"},{"location":"linux/mkdocs/#installation","text":"Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin * Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name }} site_author : {{ your_name }} site_url : {{ site_url }} nav : - Introduction : 'index.md' - Basic Usage : 'basic_usage.md' - Configuration : 'configuration.md' - Update : 'update.md' - Advanced Usage : - Projects : \"projects.md\" - Tags : \"tags.md\" plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : \"theme\" logo : \"images/logo.png\" palette : primary : 'blue grey' accent : 'light blue' extra_css : - 'stylesheets/extra.css' - 'stylesheets/links.css' repo_name : {{ repository_name }} # for example: 'lyz-code/pydo' repo_url : {{ repository_url }} # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve","title":"Installation"},{"location":"linux/mkdocs/#add-a-github-pages-hook","text":"Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : 'x64' - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working.","title":"Add a github pages hook."},{"location":"linux/mkdocs/#links","text":"Homepage . Material theme configuration guide","title":"Links"},{"location":"linux/nodejs/","text":"Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. Install \u00b6 The debian base repositories are really outdated, so add the NodeSource repository curl -sL https://deb.nodesource.com/setup_12.x | sudo bash - sudo apt-get update sudo apt-get install nodejs npm nodejs --version Links \u00b6 Home","title":"nodejs"},{"location":"linux/nodejs/#install","text":"The debian base repositories are really outdated, so add the NodeSource repository curl -sL https://deb.nodesource.com/setup_12.x | sudo bash - sudo apt-get update sudo apt-get install nodejs npm nodejs --version","title":"Install"},{"location":"linux/nodejs/#links","text":"Home","title":"Links"},{"location":"linux/rm/","text":"rm definition In computing, rm (short for remove) is a basic command on Unix and Unix-like operating systems used to remove objects such as computer files, directories and symbolic links from file systems and also special files such as device nodes, pipes and sockets Debugging \u00b6 Cannot remove file: \u201cStructure needs cleaning\u201d \u00b6 From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"rm"},{"location":"linux/rm/#debugging","text":"","title":"Debugging"},{"location":"linux/rm/#cannot-remove-file-structure-needs-cleaning","text":"From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"Cannot remove file: \u201cStructure needs cleaning\u201d"},{"location":"linux/syncthing/","text":"Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it's transmitted over the internet. Installation \u00b6 Debian or Ubuntu \u00b6 # Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing Docker \u00b6 Use Linuxserver Docker Links \u00b6 Home Getting Started","title":"Syncthing"},{"location":"linux/syncthing/#installation","text":"","title":"Installation"},{"location":"linux/syncthing/#debian-or-ubuntu","text":"# Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing","title":"Debian or Ubuntu"},{"location":"linux/syncthing/#docker","text":"Use Linuxserver Docker","title":"Docker"},{"location":"linux/syncthing/#links","text":"Home Getting Started","title":"Links"},{"location":"linux/zip/","text":"zip is an UNIX command line tool to package and compress files. Usage \u00b6 Create a zip file \u00b6 zip -r {{ zip_file }} {{ files_to_save }} Split files to a specific size \u00b6 zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m Compress with password \u00b6 zip -er {{ zip_file }} {{ files_to_save }} Read files to compress from a file \u00b6 cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@ Uncompress a zip file \u00b6 unzip {{ zip_file }}","title":"zip"},{"location":"linux/zip/#usage","text":"","title":"Usage"},{"location":"linux/zip/#create-a-zip-file","text":"zip -r {{ zip_file }} {{ files_to_save }}","title":"Create a zip file"},{"location":"linux/zip/#split-files-to-a-specific-size","text":"zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m","title":"Split files to a specific size"},{"location":"linux/zip/#compress-with-password","text":"zip -er {{ zip_file }} {{ files_to_save }}","title":"Compress with password"},{"location":"linux/zip/#read-files-to-compress-from-a-file","text":"cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@","title":"Read files to compress from a file"},{"location":"linux/zip/#uncompress-a-zip-file","text":"unzip {{ zip_file }}","title":"Uncompress a zip file"},{"location":"linux/luks/luks/","text":"LUKS definition The Linux Unified Key Setup (LUKS) is a disk encryption specification created by Clemens Fruhwirth in 2004 and was originally intended for Linux. While most disk encryption software implements different, incompatible, and undocumented formats, LUKS implements a platform-independent standard on-disk format for use in various tools. This not only facilitates compatibility and interoperability among different programs, but also assures that they all implement password management in a secure and documented manner. The reference implementation for LUKS operates on Linux and is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend. LUKS is designed to conform to the TKS1 secure key setup scheme. LUKS Commands \u00b6 We use the cryptsetup command to interact with LUKS partitions. Header management \u00b6 Get the disk header \u00b6 cryptsetup luksDump /dev/sda3 Backup header \u00b6 cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }} Key management \u00b6 Add a key \u00b6 cryptsetup luksAddKey --key-slot 1 {{ luks_device }} Test if you remember the key \u00b6 Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }} Delete some keys \u00b6 cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }} Delete all keys \u00b6 cryptsetup luksErase {{ device }} Encrypt hard drive \u00b6 Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt LUKS debugging \u00b6 Resource busy \u00b6 Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"LUKS"},{"location":"linux/luks/luks/#luks-commands","text":"We use the cryptsetup command to interact with LUKS partitions.","title":"LUKS Commands"},{"location":"linux/luks/luks/#header-management","text":"","title":"Header management"},{"location":"linux/luks/luks/#get-the-disk-header","text":"cryptsetup luksDump /dev/sda3","title":"Get the disk header"},{"location":"linux/luks/luks/#backup-header","text":"cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }}","title":"Backup header"},{"location":"linux/luks/luks/#key-management","text":"","title":"Key management"},{"location":"linux/luks/luks/#add-a-key","text":"cryptsetup luksAddKey --key-slot 1 {{ luks_device }}","title":"Add a key"},{"location":"linux/luks/luks/#test-if-you-remember-the-key","text":"Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }}","title":"Test if you remember the key"},{"location":"linux/luks/luks/#delete-some-keys","text":"cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }}","title":"Delete some keys"},{"location":"linux/luks/luks/#delete-all-keys","text":"cryptsetup luksErase {{ device }}","title":"Delete all keys"},{"location":"linux/luks/luks/#encrypt-hard-drive","text":"Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt","title":"Encrypt hard drive"},{"location":"linux/luks/luks/#luks-debugging","text":"","title":"LUKS debugging"},{"location":"linux/luks/luks/#resource-busy","text":"Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"Resource busy"},{"location":"meta/meta/","text":"In this book you'll find, in a wiki format, all the notes I made on a huge variety of topics, such as, Linux, DevOps , DevSecOps, feminism, rationalism, life automation , productivity or programming. The main goal is to store all the knowledge gathered throughout my life in a way that everyone can benefit from reading it or referencing in an easy and quickly way. I will be updating this wiki quite often as I use it myself daily both to keep an account of things I know as well as things I want to know and everything in between. History \u00b6 I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. Furthermore, as I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that I won't have an interesting RSS feed. You could go through the git log but it doesn't make any sense. That's why I'm thinking of generating a monthly newsletter similar to Gwern's Newsletters or Changelog . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born. Book structure \u00b6 Each directory is a topic that can include other subtopics under it related to the parent topic. As sometimes the strict hierarchical structure of the categories doesn't work, I also use tags to link articles. If this is your first time visiting this wiki, you can just start reading from the top entry down and see what sparks your interest. Content Structure \u00b6 Each topic will have a title, some description of it, usually my own thoughts and knowledge on it as well as referencing some resources or links I have liked or used that helped me either understand the topic or gain appreciation of it. The structure of each of the posts will often look roughly like this: Title Description - My thoughts on the topic. Subtopics - Various subtopics related to the main topic. Notes - My own personal notes on the matter as well as things I found interesting on the internet regarding the topic. I often give a link of where I got things from. Links - Links related to the topic. Links \u00b6 My blue book is heavily inspired in this two other second brains: Gwern's website Nikita's wiki","title":"Meta"},{"location":"meta/meta/#history","text":"I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. Furthermore, as I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that I won't have an interesting RSS feed. You could go through the git log but it doesn't make any sense. That's why I'm thinking of generating a monthly newsletter similar to Gwern's Newsletters or Changelog . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born.","title":"History"},{"location":"meta/meta/#book-structure","text":"Each directory is a topic that can include other subtopics under it related to the parent topic. As sometimes the strict hierarchical structure of the categories doesn't work, I also use tags to link articles. If this is your first time visiting this wiki, you can just start reading from the top entry down and see what sparks your interest.","title":"Book structure"},{"location":"meta/meta/#content-structure","text":"Each topic will have a title, some description of it, usually my own thoughts and knowledge on it as well as referencing some resources or links I have liked or used that helped me either understand the topic or gain appreciation of it. The structure of each of the posts will often look roughly like this: Title Description - My thoughts on the topic. Subtopics - Various subtopics related to the main topic. Notes - My own personal notes on the matter as well as things I found interesting on the internet regarding the topic. I often give a link of where I got things from. Links - Links related to the topic.","title":"Content Structure"},{"location":"meta/meta/#links","text":"My blue book is heavily inspired in this two other second brains: Gwern's website Nikita's wiki","title":"Links"},{"location":"projects/projects/","text":"Also known as where I'm spending my spare time. Home Stock inventory \u00b6 I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I'm integrating grocy in my life. Pydo \u00b6 I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo . Blue book \u00b6 I'm refactoring all the knowledge gathered in the past in my cheat sheet repository into the blue book. This means migrating 7422 articles, almost 50 million lines, to the new structure. It's going to be a slow and painful process \u1559(\u21c0\u2038\u21bc\u2036)\u1557 . Clinv \u00b6 As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. I did some research in the past and there was no tool that had: As you can see in how do you document your infrastructure? , there is still a void. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv , Media indexation \u00b6 I've got a music collection of 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to an mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system. Projects I maintain \u00b6 Mediarss \u00b6 I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service . So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor. Playlist_generator \u00b6 When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^). mep \u00b6 I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"Projects"},{"location":"projects/projects/#home-stock-inventory","text":"I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I'm integrating grocy in my life.","title":"Home Stock inventory"},{"location":"projects/projects/#pydo","text":"I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo .","title":"Pydo"},{"location":"projects/projects/#blue-book","text":"I'm refactoring all the knowledge gathered in the past in my cheat sheet repository into the blue book. This means migrating 7422 articles, almost 50 million lines, to the new structure. It's going to be a slow and painful process \u1559(\u21c0\u2038\u21bc\u2036)\u1557 .","title":"Blue book"},{"location":"projects/projects/#clinv","text":"As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. I did some research in the past and there was no tool that had: As you can see in how do you document your infrastructure? , there is still a void. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv ,","title":"Clinv"},{"location":"projects/projects/#media-indexation","text":"I've got a music collection of 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to an mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system.","title":"Media indexation"},{"location":"projects/projects/#projects-i-maintain","text":"","title":"Projects I maintain"},{"location":"projects/projects/#mediarss","text":"I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service . So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor.","title":"Mediarss"},{"location":"projects/projects/#playlist_generator","text":"When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^).","title":"Playlist_generator"},{"location":"projects/projects/#mep","text":"I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"mep"},{"location":"writing/orthography/","text":"My english writing is not so good, this articles is an effort to gather all my common pitfalls. Who vs Whom \u00b6 If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition. A vs An \u00b6 We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a .","title":"Orthography Rules"},{"location":"writing/orthography/#who-vs-whom","text":"If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition.","title":"Who vs Whom"},{"location":"writing/orthography/#a-vs-an","text":"We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a .","title":"A vs An"},{"location":"tags.html","text":"Contents grouped by tag \u00b6 WIP \u00b6 Devops AWS","title":"Tags"},{"location":"tags.html#contents-grouped-by-tag","text":"","title":"Contents grouped by tag"},{"location":"tags.html#wip","text":"Devops AWS","title":"WIP"}]}