{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is my personal wiki where I share everything I know about this world in form of an online MkDocs book hosted on GitHub . If this is your first time visiting this wiki, take a look at meta , as it describes this wiki, its structure and goals in more detail. Using the wiki well \u2691 You can quickly search the contents of this wiki above or you can explore the tree view to the left. Start with the first article that grabs your attention and be ready to incrementally read the rest. Or you can use it as a reference, cloning the git repository and using grep. Make your own wiki \u2691 Don't be afraid to create one of your own and share what you know with the world. If you don't want to build your own, I invite you to use a fork of mine and make contributions . I would love to see the blue-book maintained by several people. You can view other similar continuously updated wikis to get inspiration. Contributing \u2691 If you find a mistake anywhere in this wiki or want to add new content, I'll be glad to accept your contribution. You can quickly find any entry you wish to edit by searching for the topic or use the edit button on the top right of any article to add your changes with a PR. I also appreciate any ideas you have on how I can improve this wiki. And if you don't want to go through the hassle of building your own, you can use mine Thank you \u2691 If you liked my book and want to make me happy, please see if you know how could I fulfill any item of my wish list or see if you want to contribute to my other projects .","title":"Introduction"},{"location":"#using-the-wiki-well","text":"You can quickly search the contents of this wiki above or you can explore the tree view to the left. Start with the first article that grabs your attention and be ready to incrementally read the rest. Or you can use it as a reference, cloning the git repository and using grep.","title":"Using the wiki well"},{"location":"#make-your-own-wiki","text":"Don't be afraid to create one of your own and share what you know with the world. If you don't want to build your own, I invite you to use a fork of mine and make contributions . I would love to see the blue-book maintained by several people. You can view other similar continuously updated wikis to get inspiration.","title":"Make your own wiki"},{"location":"#contributing","text":"If you find a mistake anywhere in this wiki or want to add new content, I'll be glad to accept your contribution. You can quickly find any entry you wish to edit by searching for the topic or use the edit button on the top right of any article to add your changes with a PR. I also appreciate any ideas you have on how I can improve this wiki. And if you don't want to go through the hassle of building your own, you can use mine","title":"Contributing"},{"location":"#thank-you","text":"If you liked my book and want to make me happy, please see if you know how could I fulfill any item of my wish list or see if you want to contribute to my other projects .","title":"Thank you"},{"location":"contact/","text":"I'm available through: Email or XMPP at lyz@riseup.net . PGP Key: 6ADA882386CDF9BD1884534C6C7D7C1612CDE02F -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBFhs5wUBEAC289UxruAPfjvJ723AKhUhRI0/fw+cG0IeSUJfOSvWW+HJ7Elo QoPkKYv6E1k4SzIt6AgbEWpL35PQP79aQ5BFog2SbfVvfnq1/gIasFlyeFX1BUTh zxTKrYKwbUdsTeMYw32v5p2Q+D8CZK6/0RCM/GSb5oMPVancOeoZs8IebKpJH2x7 HCniyQbq7xiFU5sUyB6tmgCiXg8INib+oTZqGKW/sVaxmTdH+fF9a2nnH0TN8h2W 5V5XQ9/VQZk/GHQVq/Y0Z73BibOJM5Bv+3r2EIJfozlpWdUblat45lSATBo/sktf YKlxwAztWPtcTavJ58F1ufGcUPjwGW4E92zRaozC+tpzd5QtHeYM7m6fGlXxckua UesZcZLl9pY4Bc8Mw40WvI1ibhA2mP2R5AO8hJ0vJyFfi35lqM/DJVV1900yp+em uY+u6bNJ1gLLb7QnhbV1VYLTSCoWzPQvWHgMHAKpAjO15rKAItXD17BM2eQgJMuX LcoWeOcz/MrMQiGKSkqpmapwgtDZ5t81D2qWv+wsaZgcO/erknugHFmR3kAP8YHp JsIpaYY7kj+yVJb92uzZKQAEaUpq3uRsBDtkoC2MPzKN4fgWa8f4jBpIzxuBTd+6 75sVq5VB5eaq3w4J0Z4kbk1DVyNffv3LeZCv9oC2mb1aXyVD/gWHlPD+6wARAQAB tBZseXouLiA8bHl6QHJpc2V1cC5uZXQ+iQJUBBMBCAA+AhsDBQsJCAcCBhUICQoL AgQWAgMBAh4BAheAFiEEatqII4bN+b0YhFNMbH18FhLN4C8FAl4XCTwFCQeLVbcA CgkQbH18FhLN4C/Jkw//Th/tAagxBchztzA2bAJog7sd3FK4hH2cqGFdBG+yx5TW 2ywfDXjTXVeKhHxkSnZZgxO0U31W2Fv+tLmRKN8MrvGSjIpUlWTmeaIG1W+ftlcG NrR+CDL0lrkKZnyQGJhe675lNoo2FKQ/37B/NIyzfIWw8eZStYabHtj5H40nti1k riwZsk76+kR6FI1EVKCGGmo/Spl/VX9MuWNjg9E0cJvpzKY05gKmFSuMJwxVhrFV ly0MhZS+4xddbCMaBo2OJEDrcFBQgBiUnxS8PcADLK7zn3zpemcJm5/8T/DyQHeY 0Yh76KJ92aIB7eLLnRnRcvCXt0RZ+s3sHqLgrsT3OV0jlC7GLjTBgTe6qGH3Lr/h whiOp6g1k125+v20fKPWDlGar3sdSD/ZjJDeAHedV5I3QVT6zorUYcQYb6vYPlOU aq7k0jLjGuoxHQeXGAZMvlKQfgHDfiwBwyIX6D24wsyr+XDnrVyDoCO654OqYcUH wK1y57NbUOpzvD+ZEO/8aKeBUh0zKz682hsA8HJT8G09UBcs36HAnbTkp+rPxgTH eBVcTYLi/CFy9tXOhBmyPhxrILsPwmOvZA4tg7LLnj2P2qdk2Gz1si/D8s2Afr+c re9pidcYbiXJI+Pnw+e9Pylf/1WM8MS5Z2W9Liyc29/kLsCL8Dp0eJtqzJLAX0y5 Ag0EWGznBQEQALNL9sNc4SytS3fOcS4gHvZpH3TLJ6o0K/Lxg4RfkLMebDJwWvSW mjQLv3GqRfOhGj2Osi2YukFIJb4vxPJFO7wQhCi5LLSVEb5d/z9ZOJUdGdI9JvGW dFDuLEXwDnJaP5Jmjm3DwbvHK+goI7Fn3TKc27iqOVAKVIjWNPaqFZxwIE9o/+1c 3bTk3A8WOBmcv1IaxsUNkRDOFJlQYLM/bFIuDD+cW/CcYro8ouC9aekmvTDoRaU5 xv++fXtesn6Cy+xBgvBGIIXGo5xzd6Y66Yf8uNpuJXo9Dc6rApH1QEQNwZX1cxvG UpQx+9JNF0eptDLvTgmxcCglllrylcw8ZsVEt6BTgrCd2JXMGxUcAnhXpRWRmXNL n97FOBb6OBd6k7DC6QCiVKr7sytq1Ywl8GTtWrTP7sK+/+KDLPJ/oY7+bwV94+N8 Gthr94njNqb5G6t9fqQ/+cJv7oF8DoBvylYGqm2hvYpOH53hMq1y3OTPoFKP6AIx twIWHkdmMALm6a6bxAetGQxiaPZTOduJDehwiF9EUkiNhpESMl3I2+vH86jV2IiT 4BuUqGBU5wrAN/FixIRlmaSUX7e0OkUkDexVlpw5poJbPEbvhOtuj/V9BOxQKWB4 bjXMHEHR5YcJ1lhPjFFM3pqOz6ZaN8Hs70KOBE+/3/c1hS5debWPBMdlABEBAAGJ AjwEGAEIACYCGwwWIQRq2ogjhs35vRiEU0xsfXwWEs3gLwUCXhcJRgUJB4tVwQAK CRBsfXwWEs3gL41DEACYtc6mykbhZh2eWrdNynbYX1TNYFH+4BP+zpN8kNHPwKfX IypLLSSwUhYdZ9kb8WB8n4cH8njk4P1LyGtfUOxbEpKCQNXfW3aWDDsZunxdSkyc 3opaCo2w/Gf2ynxtbJVWoNWYn8fDQJcE3UAz8+rioHGRUCBF//G8VWdqZ4PCARGu TPeurJG5aljJGqlrvAXewqNItGEoARHGC3R9otSC8Y5cd9zL3iKUnBh9xhiqFzjK /7J9uQcDz6GTzZKxDqRQmcs27nGjWFNscZY16dBDj6y2d+v+RJEgFY9uW7KGVfFG Y9kPsSKdKUzeE+TOvwintakMQT26dNWBbUkDkMt08kEFk5SyeoQcjnqWMFJgrav1 RYUwz/UFuWep0y9Rt0PrW40mBZOd4roRdgEX6I65K6CC38u/nIgJRG2I/2LkWIwu n2LROOQ+0O6rn4HObgfoEZE03K6AW1DyNR6BnspbTDt0fRIDk6Rrfw6Xe1AfANrK 9zs95WbKkbydE1xFddOJ10qDleFOOaeCWp7KW1GkvEKfoRXhhAo/xnFpjHbGuvJv bTL4pYkaoOyGriAn3fZ8zOoBLspuAzEENBLtX41XU8PFjwcRu4GfFSrP03svi3km WodDQhjSPW+B/9SmLj+UkaIUlTTqwAs8rHtexkzlIhHGASXc+Iuuz5JuzUlPUw== =9EvG -----END PGP PUBLIC KEY BLOCK----- Through Github by opening an issue .","title":"Contact"},{"location":"emojis/","text":"Curated list of emojis to copy paste. Angry \u2691 (\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6 Annoyed \u2691 (>_<) Awesome \u2691 ( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH Conforting \u2691 (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) Congratulations \u2691 ( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f Crying \u2691 (\u2565\ufe4f\u2565) Excited \u2691 (((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o) Dance \u2691 (~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^) Happy \u2691 \u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a Kisses \u2691 (\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665 Love \u2691 \u2764 Pride \u2691 <(\uffe3\uff3e\uffe3)> Relax \u2691 _\u3078__(\u203e\u25e1\u25dd )> Sad \u2691 \uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b) Scared \u2691 \u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f Sleepy \u2691 (\u1d17\u02f3\u1d17) Smug \u2691 \uff08\uffe3\uff5e\uffe3\uff09 Whyyyy? \u2691 (/\uff9f\u0414\uff9f)/ Surprised \u2691 (\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*) Who cares \u2691 \u00af\\_(\u30c4)_/\u00af WTF \u2691 (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8 Links \u2691 Japanese Emoticons","title":"Emojis"},{"location":"emojis/#angry","text":"(\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6","title":"Angry"},{"location":"emojis/#annoyed","text":"(>_<)","title":"Annoyed"},{"location":"emojis/#awesome","text":"( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH","title":"Awesome"},{"location":"emojis/#conforting","text":"(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)","title":"Conforting"},{"location":"emojis/#congratulations","text":"( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f","title":"Congratulations"},{"location":"emojis/#crying","text":"(\u2565\ufe4f\u2565)","title":"Crying"},{"location":"emojis/#excited","text":"(((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o)","title":"Excited"},{"location":"emojis/#dance","text":"(~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^)","title":"Dance"},{"location":"emojis/#happy","text":"\u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a","title":"Happy"},{"location":"emojis/#kisses","text":"(\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665","title":"Kisses"},{"location":"emojis/#love","text":"\u2764","title":"Love"},{"location":"emojis/#pride","text":"<(\uffe3\uff3e\uffe3)>","title":"Pride"},{"location":"emojis/#relax","text":"_\u3078__(\u203e\u25e1\u25dd )>","title":"Relax"},{"location":"emojis/#sad","text":"\uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b)","title":"Sad"},{"location":"emojis/#scared","text":"\u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f","title":"Scared"},{"location":"emojis/#sleepy","text":"(\u1d17\u02f3\u1d17)","title":"Sleepy"},{"location":"emojis/#smug","text":"\uff08\uffe3\uff5e\uffe3\uff09","title":"Smug"},{"location":"emojis/#whyyyy","text":"(/\uff9f\u0414\uff9f)/","title":"Whyyyy?"},{"location":"emojis/#surprised","text":"(\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*)","title":"Surprised"},{"location":"emojis/#who-cares","text":"\u00af\\_(\u30c4)_/\u00af","title":"Who cares"},{"location":"emojis/#wtf","text":"(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8","title":"WTF"},{"location":"emojis/#links","text":"Japanese Emoticons","title":"Links"},{"location":"helm_git/","text":"helm-git is a helm downloader plugin that provides GIT protocol support. This fits the following use cases: Need to keep charts private. Doesn't want to package charts before installing. Charts in a sub-path, or with another ref than master. Pull values files directly from (private) Git repository. Installation \u2691 helm plugin install https://github.com/aslafy-z/helm-git --version 0 .10.0 Usage \u2691 helm-git will package any chart that is not so you can directly reference paths to original charts. Here's the Git urls format, followed by examples: git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=0 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=1 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2 git+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&sparse=0&depupdate=0 Add your repository: helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref = v0.6.2 You can use it as any other Helm chart repository. Try: $ helm search cert-manager NAME CHART VERSION APP VERSION DESCRIPTION cert-manager/cert-manager v0.6.6 v0.6.2 A Helm chart for cert-manager $ helm install cert-manager/cert-manager --version \"0.6.6\" Fetching also works: helm fetch cert-manager/cert-manager --version \"0.6.6\" helm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref = v0.6.2 References \u2691 Git","title":"Helm Git"},{"location":"helm_git/#installation","text":"helm plugin install https://github.com/aslafy-z/helm-git --version 0 .10.0","title":"Installation"},{"location":"helm_git/#usage","text":"helm-git will package any chart that is not so you can directly reference paths to original charts. Here's the Git urls format, followed by examples: git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=0 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=1 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2 git+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&sparse=0&depupdate=0 Add your repository: helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref = v0.6.2 You can use it as any other Helm chart repository. Try: $ helm search cert-manager NAME CHART VERSION APP VERSION DESCRIPTION cert-manager/cert-manager v0.6.6 v0.6.2 A Helm chart for cert-manager $ helm install cert-manager/cert-manager --version \"0.6.6\" Fetching also works: helm fetch cert-manager/cert-manager --version \"0.6.6\" helm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref = v0.6.2","title":"Usage"},{"location":"helm_git/#references","text":"Git","title":"References"},{"location":"lazy_loading/","text":"Lazy loading is an programming implementation paradigm which delays the evaluation of an expression until its value is needed and which also avoids repeated evaluations. Lazy evaluation is the preferred implementation when the operation is expensive, requiring either extensive processing time or memory. For example, in Python, one of the best-known techniques involving lazy evaluation is generators. Instead of creating whole sequences for the iteration, which can consume lots of memory, generators lazily evaluate the current need and yield one element at a time when requested. Other example are attributes that take long to compute: class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation self . relatives = self . _get_all_relatives () def _get_all_relatives (): ... # This is an expensive operation This approach may cause initialization to take unnecessarily long, especially when you don't always need to access Person.relatives . A better strategy would be to get relatives when it's needed. class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation self . _relatives = None @property def relatives ( self ): if self . _relatives is None : self . _relatives = ... # Get all relatives return self . _relatives In this case, the list of relatives is computed the first time Person.relatives is accessed. After that, it's stored in Person._relatives to prevent repeated evaluations. A perhaps more Pythonic approach would be to use a decorator that makes a property lazy-evaluated. def lazy_property ( fn ): '''Decorator that makes a property lazy-evaluated. ''' attr_name = '_lazy_' + fn . __name__ @property def _lazy_property ( self ): if not hasattr ( self , attr_name ): setattr ( self , attr_name , fn ( self )) return getattr ( self , attr_name ) return _lazy_property class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation @lazy_property def relatives ( self ): # Get all relatives relatives = ... return relatives This removes a lot of boilerplate, especially when an object has many lazily-evaluated properties. Another approach is to use the getattr special method . References \u2691 Steven Loria article on Lazy Properties Yong Cui article on Lazy attributes","title":"Lazy loading"},{"location":"lazy_loading/#references","text":"Steven Loria article on Lazy Properties Yong Cui article on Lazy attributes","title":"References"},{"location":"meditation/","text":"Meditation is a practice where an individual uses a technique,such as mindfulness, or focusing the mind on a particular object, thought, or activity, to train attention and awareness, and achieve a mentally clear and emotionally calm and stable state. Meditation may reduce stress, anxiety, depression, and pain, and enhance peace, perception, self-concept, and well-being. Types of meditation \u2691 Although there isn't a right or wrong way to meditate, it\u2019s important to find a practice that meets your needs and complements your personality. There are nine popular types of meditation practice: Mindfulness meditation : You pay attention to your thoughts as they pass through your mind. You don't judge the thoughts or become involved with them. You simply observe and take note of any patterns. This practice combines concentration with awareness. You may find it helpful to focus on an object or your breath while you observe any bodily sensations, thoughts, or feelings. This type of meditation is good for people who don\u2019t have a teacher to guide them, as it can be easily practiced alone. Focused meditation : Involves concentration using any of the five senses. For example, you can focus on something internal, like your breath, or you can bring in external influences to help focus your attention. Try counting mala beads, listening to a gong, or staring at a candle flame. This practice may be simple in theory, but it can be difficult for beginners to hold their focus for longer than a few minutes at first. If your mind does wander, it\u2019s important to come back to the practice and refocus. As the name suggests, this practice is ideal for anyone who requires additional focus in their life. Movement meditation : It\u2019s an active form of meditation where the movement guides you. It can be achieved through yoga, martial arts or by walking through the woods, gardening, qigong, and other gentle forms of motion. Movement meditation is good for people who find peace in action and prefer to let their minds wander. Mantra meditation : Uses a repetitive sound to clear the mind. It can be a word, phrase, or sound, such as the popular \u201cOm.\u201d It doesn't matter if your mantra is spoken loudly or quietly. After chanting the mantra for some time, you\u2019ll be more alert and in tune with your environment. This allows you to experience deeper levels of awareness. Some people enjoy mantra meditation because they find it easier to focus on a word than on their breath. This is also a good practice for people who don't like silence and enjoy repetition. Transcendental Meditation : It is more customizable than mantra meditation, using a mantra or series of words that are specific to each practitioner. This practice is for those who like structure and are serious about maintaining a meditation practice. Progressive relaxation : Also known as body scan meditation, it's a practice aimed at reducing tension in the body and promoting relaxation. Oftentimes, this form of meditation involves slowly tightening and relaxing one muscle group at a time throughout the body. In some cases, it may also encourage you to imagine a gentle wave flowing through your body to help release any tension. This form of meditation is often used to relieve stress and unwind before bedtime. Loving-kindness meditation : is used to strengthen feelings of compassion, kindness, and acceptance toward oneself and others. It typically involves opening the mind to receive love from others and then sending a series of well wishes to loved ones, friends, acquaintances, and all living beings. Because this type of meditation is intended to promote compassion and kindness, it may be ideal for those holding feelings of anger or resentment. * Visualization meditation : Is a technique focused on enhancing feelings of relaxation, peace, and calmness by visualizing positive scenes or images. With this practice, it\u2019s important to imagine the scene vividly and use all five senses to add as much detail as possible. Another form of visualization meditation involves imagining yourself succeeding at specific goals, which is intended to increase focus and motivation. Many people use visualization meditation to boost their mood, reduce stress levels, and promote inner peace. Spiritual meditation : Spiritual meditation is used in Eastern religions, such as Hinduism and Daoism, and in Christian faith.. It\u2019s similar to prayer in that you reflect on the silence around you and seek a deeper connection with your God or Universe. How to get started \u2691 The easiest way to begin is to sit quietly and focus on your breath for 20 minutes every day. If it's too much for you, start in small moments of time, even 5 or 10 minutes, and grow from there. References \u2691 healthline article on types of meditation To review \u2691 https://wiki.nikitavoloboev.xyz/mindfulness/meditation https://www.healthline.com/health/4-7-8-breathing#Other-techniques-to-help-you-sleep https://threader.app/thread/1261481222359801856 https://quietkit.com/box-breathing/ https://www.healthline.com/health/mental-health/best-mindfulness-blogs#8 https://www.mindful.org/how-to-meditate/","title":"Meditation"},{"location":"meditation/#types-of-meditation","text":"Although there isn't a right or wrong way to meditate, it\u2019s important to find a practice that meets your needs and complements your personality. There are nine popular types of meditation practice: Mindfulness meditation : You pay attention to your thoughts as they pass through your mind. You don't judge the thoughts or become involved with them. You simply observe and take note of any patterns. This practice combines concentration with awareness. You may find it helpful to focus on an object or your breath while you observe any bodily sensations, thoughts, or feelings. This type of meditation is good for people who don\u2019t have a teacher to guide them, as it can be easily practiced alone. Focused meditation : Involves concentration using any of the five senses. For example, you can focus on something internal, like your breath, or you can bring in external influences to help focus your attention. Try counting mala beads, listening to a gong, or staring at a candle flame. This practice may be simple in theory, but it can be difficult for beginners to hold their focus for longer than a few minutes at first. If your mind does wander, it\u2019s important to come back to the practice and refocus. As the name suggests, this practice is ideal for anyone who requires additional focus in their life. Movement meditation : It\u2019s an active form of meditation where the movement guides you. It can be achieved through yoga, martial arts or by walking through the woods, gardening, qigong, and other gentle forms of motion. Movement meditation is good for people who find peace in action and prefer to let their minds wander. Mantra meditation : Uses a repetitive sound to clear the mind. It can be a word, phrase, or sound, such as the popular \u201cOm.\u201d It doesn't matter if your mantra is spoken loudly or quietly. After chanting the mantra for some time, you\u2019ll be more alert and in tune with your environment. This allows you to experience deeper levels of awareness. Some people enjoy mantra meditation because they find it easier to focus on a word than on their breath. This is also a good practice for people who don't like silence and enjoy repetition. Transcendental Meditation : It is more customizable than mantra meditation, using a mantra or series of words that are specific to each practitioner. This practice is for those who like structure and are serious about maintaining a meditation practice. Progressive relaxation : Also known as body scan meditation, it's a practice aimed at reducing tension in the body and promoting relaxation. Oftentimes, this form of meditation involves slowly tightening and relaxing one muscle group at a time throughout the body. In some cases, it may also encourage you to imagine a gentle wave flowing through your body to help release any tension. This form of meditation is often used to relieve stress and unwind before bedtime. Loving-kindness meditation : is used to strengthen feelings of compassion, kindness, and acceptance toward oneself and others. It typically involves opening the mind to receive love from others and then sending a series of well wishes to loved ones, friends, acquaintances, and all living beings. Because this type of meditation is intended to promote compassion and kindness, it may be ideal for those holding feelings of anger or resentment. * Visualization meditation : Is a technique focused on enhancing feelings of relaxation, peace, and calmness by visualizing positive scenes or images. With this practice, it\u2019s important to imagine the scene vividly and use all five senses to add as much detail as possible. Another form of visualization meditation involves imagining yourself succeeding at specific goals, which is intended to increase focus and motivation. Many people use visualization meditation to boost their mood, reduce stress levels, and promote inner peace. Spiritual meditation : Spiritual meditation is used in Eastern religions, such as Hinduism and Daoism, and in Christian faith.. It\u2019s similar to prayer in that you reflect on the silence around you and seek a deeper connection with your God or Universe.","title":"Types of meditation"},{"location":"meditation/#how-to-get-started","text":"The easiest way to begin is to sit quietly and focus on your breath for 20 minutes every day. If it's too much for you, start in small moments of time, even 5 or 10 minutes, and grow from there.","title":"How to get started"},{"location":"meditation/#references","text":"healthline article on types of meditation","title":"References"},{"location":"meditation/#to-review","text":"https://wiki.nikitavoloboev.xyz/mindfulness/meditation https://www.healthline.com/health/4-7-8-breathing#Other-techniques-to-help-you-sleep https://threader.app/thread/1261481222359801856 https://quietkit.com/box-breathing/ https://www.healthline.com/health/mental-health/best-mindfulness-blogs#8 https://www.mindful.org/how-to-meditate/","title":"To review"},{"location":"python_optimization/","text":"Optimization can be done through different metrics, such as, CPU performance (execution time) or memory footprint. Optimizing your code makes sense when you are sure that the business logic in the code is correct and not going to change soon. \"First make it work. Then make it right. Then make it fast.\" ~ Kent Beck Unless you're developing a performance-intensive product or a code dependency that is going to be used by other projects which might be performance-intensive, optimizing every aspect of the code can be overkill. For most of the scenarios, the 80-20 principle (80 percent of performance benefits may come from optimizing 20 percent of your code) will be more appropriate. Most of the time we make intuitive guesses on what the bottlenecks are, but more often than not, our guesses are either wrong or just approximately correct. So, it's always advisable to use profiling tools to identify how often a resource is used and who is using the resource. For instance, a profiler designed for profiling execution time will measure how often and for how various long parts of the code are executed. Using a profiling mechanism becomes a necessity when the codebase grows large, and you still want to maintain efficiency. References \u2691 Satwik Kansal article on Scout APM","title":"Optimization"},{"location":"python_optimization/#references","text":"Satwik Kansal article on Scout APM","title":"References"},{"location":"python_profiling/","text":"Profiling is to find out where your code spends its time. Profilers can collect several types of information: timing, function calls, interruptions or cache faults. It can be useful to identify bottlenecks, which should be the first step when trying to optimize some code, or to study the evolution of the performance of your code. Profiling types \u2691 There are two types of profiling: Deterministic Profiling All events are monitored. It provides accurate information but has a big impact on performance (overhead). It means the code runs slower under profiling. Its use in production systems is often impractical. This type of profiling is suitable for small functions. Statistical profiling Sampling the execution state at regular intervals to compute indicators. This method is less accurate, but it also reduces the overhead. Profiling tools \u2691 The profiling tools you should use vary with the code you are working on. If you are writing a single algorithm or a small program, you should use a simple profiler like cProfile or even a fine-grained tool like line_profiler . In contrast, when you are optimizing a whole program, you may want to use a statistical profiler to avoid overhead, such as pyinstrument , or if you're debugging a running process, using py-spy . Deterministic Profiling \u2691 cProfile \u2691 Python comes with two built-in modules for deterministic profiling: cProfile and profile. Both are different implementations of the same interface. The former is a C extension with relatively small overhead, and the latter is a pure Python module. As the official documentation says, the module profile would be suitable when we want to extend the profiler in some way. Otherwise, cProfile is preferred for long-running programs. Unfortunately, there is no built-in module for statistical profiling, but we will see some external packages for it. $: python3 -m cProfile script.py 58 function calls in 9 .419 seconds Ordered by: standard namen calls tottime percall cumtime percall filename:lineno ( function ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:1 ( <module> ) 51 9 .419 0 .185 9 .419 0 .185 part1.py:1 ( computation ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:10 ( function1 ) 1 0 .000 0 .000 9 .243 9 .243 part1.py:15 ( function2 ) 1 0 .000 0 .000 0 .176 0 .176 part1.py:20 ( function3 ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:24 ( main ) Where: ncalls Is the number of calls. We should try to optimize functions that have a lot of calls or consume too much time per call. tottime The total time spent in the function itself, excluding sub calls. This is where we should look closely at. We can see that the function computation is called 51 times, and each time consumes 0.185s. cumtime Cumulative time. It includes sub calls. percall We have two \u201cper call\u201d metrics. The first one: total time per call, and the second one: cumulative time per call. Again, we should focus on the total time metric. We can also sort the functions by some criteria, for example python3 -m cProfile -s tottime script.py . Statistical profiling \u2691 Py-spy \u2691 Py-Spy is a statistical (sampling) profiler that lets you visualize the time each function consumes during the execution. An important feature is that you can attach the profiler without restarting the program or modifying the code, and has a low overhead. This makes the tool highly suitable for production code. To install it, just type: pip install py-spy To test the performance of a file use: py-spy top python3 script.py To assess the performance of a runnin process, specify it's PID: py-spy top --pid $PID They will show a top like interface showing the following data: GIL: 100.00%, Active: 100.00%, Threads: 1 %Own %Total OwnTime TotalTime Function (filename:line) 61.00% 61.00% 10.50s 10.50s computation (script.py:7) 39.00% 39.00% 7.50s 7.50s computation (script.py:6) 0.00% 100.00% 0.000s 18.00s <module> (script.py:30) 0.00% 100.00% 0.000s 18.00s function2 (script.py:18) 0.00% 100.00% 0.000s 18.00s main (script.py:26) 0.00% 100.00% 0.000s 18.00s function1 (script.py:12) pyinstrument \u2691 It is similar to cProfile in the sense that we can\u2019t attach the profiler to a running program, but that is where the similarities end, as pyinstrument doesn't track every function call that your program makes. Instead, it's recording the call stack every 1ms. Install it with: pip install pyinstrument Use: The advantages are that: The output is far more attractive. It has less overhead, so it distorts less the results. Doesn't show the internal calls that make cProfiling result reading difficult. It uses wall-clock time instead of CPU time. So it takes into account the IO time. $: pyinstrument script.py _ ._ __/__ _ _ _ _ _/_ Recorded: 15 :45:20 Samples: 51 /_//_/// /_ \\ / //_// / //_ ' / // Duration: 4 .517 CPU time: 4 .516 / _/ v3.3.0 Program: script.py 4 .516 <module> script.py:2 \u2514\u2500 4 .516 main script.py:25 \u2514\u2500 4 .516 function1 script.py:11 \u251c\u2500 4 .425 function2 script.py:16 \u2502 \u2514\u2500 4 .425 computation script.py:2 \u2514\u2500 0 .092 function3 script.py:21 \u2514\u2500 0 .092 computation script.py:2 With the possibility to generate an HTML report. The disadvantages are that it's only easy to profile python script files, not full packages. You can also profile a chunk of code , which can be useful when developing or for writing performance tests. from pyinstrument import Profiler profiler = Profiler () profiler . start () # code you want to profile profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) To explore the profile in a web browser, use profiler.open_in_browser() . To save this HTML for later, use profiler.output_html() . Introduce profiling in your test workflow \u2691 I run out of time, so here are the starting points: Niklas Meinzer post Pypi page of pytest-benchmark , Docs , Git Docs of pytest-profiling uwpce guide on using pstats The idea is to develop the following ideas: How to integrate profiling with pytest. How to compare benchmark results between CI runs. Some guidelines on writing performance tests References \u2691 Antonio Molner article on Python Profiling","title":"Profiling"},{"location":"python_profiling/#profiling-types","text":"There are two types of profiling: Deterministic Profiling All events are monitored. It provides accurate information but has a big impact on performance (overhead). It means the code runs slower under profiling. Its use in production systems is often impractical. This type of profiling is suitable for small functions. Statistical profiling Sampling the execution state at regular intervals to compute indicators. This method is less accurate, but it also reduces the overhead.","title":"Profiling types"},{"location":"python_profiling/#profiling-tools","text":"The profiling tools you should use vary with the code you are working on. If you are writing a single algorithm or a small program, you should use a simple profiler like cProfile or even a fine-grained tool like line_profiler . In contrast, when you are optimizing a whole program, you may want to use a statistical profiler to avoid overhead, such as pyinstrument , or if you're debugging a running process, using py-spy .","title":"Profiling tools"},{"location":"python_profiling/#deterministic-profiling","text":"","title":"Deterministic Profiling"},{"location":"python_profiling/#cprofile","text":"Python comes with two built-in modules for deterministic profiling: cProfile and profile. Both are different implementations of the same interface. The former is a C extension with relatively small overhead, and the latter is a pure Python module. As the official documentation says, the module profile would be suitable when we want to extend the profiler in some way. Otherwise, cProfile is preferred for long-running programs. Unfortunately, there is no built-in module for statistical profiling, but we will see some external packages for it. $: python3 -m cProfile script.py 58 function calls in 9 .419 seconds Ordered by: standard namen calls tottime percall cumtime percall filename:lineno ( function ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:1 ( <module> ) 51 9 .419 0 .185 9 .419 0 .185 part1.py:1 ( computation ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:10 ( function1 ) 1 0 .000 0 .000 9 .243 9 .243 part1.py:15 ( function2 ) 1 0 .000 0 .000 0 .176 0 .176 part1.py:20 ( function3 ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:24 ( main ) Where: ncalls Is the number of calls. We should try to optimize functions that have a lot of calls or consume too much time per call. tottime The total time spent in the function itself, excluding sub calls. This is where we should look closely at. We can see that the function computation is called 51 times, and each time consumes 0.185s. cumtime Cumulative time. It includes sub calls. percall We have two \u201cper call\u201d metrics. The first one: total time per call, and the second one: cumulative time per call. Again, we should focus on the total time metric. We can also sort the functions by some criteria, for example python3 -m cProfile -s tottime script.py .","title":"cProfile"},{"location":"python_profiling/#statistical-profiling","text":"","title":"Statistical profiling"},{"location":"python_profiling/#py-spy","text":"Py-Spy is a statistical (sampling) profiler that lets you visualize the time each function consumes during the execution. An important feature is that you can attach the profiler without restarting the program or modifying the code, and has a low overhead. This makes the tool highly suitable for production code. To install it, just type: pip install py-spy To test the performance of a file use: py-spy top python3 script.py To assess the performance of a runnin process, specify it's PID: py-spy top --pid $PID They will show a top like interface showing the following data: GIL: 100.00%, Active: 100.00%, Threads: 1 %Own %Total OwnTime TotalTime Function (filename:line) 61.00% 61.00% 10.50s 10.50s computation (script.py:7) 39.00% 39.00% 7.50s 7.50s computation (script.py:6) 0.00% 100.00% 0.000s 18.00s <module> (script.py:30) 0.00% 100.00% 0.000s 18.00s function2 (script.py:18) 0.00% 100.00% 0.000s 18.00s main (script.py:26) 0.00% 100.00% 0.000s 18.00s function1 (script.py:12)","title":"Py-spy"},{"location":"python_profiling/#pyinstrument","text":"It is similar to cProfile in the sense that we can\u2019t attach the profiler to a running program, but that is where the similarities end, as pyinstrument doesn't track every function call that your program makes. Instead, it's recording the call stack every 1ms. Install it with: pip install pyinstrument Use: The advantages are that: The output is far more attractive. It has less overhead, so it distorts less the results. Doesn't show the internal calls that make cProfiling result reading difficult. It uses wall-clock time instead of CPU time. So it takes into account the IO time. $: pyinstrument script.py _ ._ __/__ _ _ _ _ _/_ Recorded: 15 :45:20 Samples: 51 /_//_/// /_ \\ / //_// / //_ ' / // Duration: 4 .517 CPU time: 4 .516 / _/ v3.3.0 Program: script.py 4 .516 <module> script.py:2 \u2514\u2500 4 .516 main script.py:25 \u2514\u2500 4 .516 function1 script.py:11 \u251c\u2500 4 .425 function2 script.py:16 \u2502 \u2514\u2500 4 .425 computation script.py:2 \u2514\u2500 0 .092 function3 script.py:21 \u2514\u2500 0 .092 computation script.py:2 With the possibility to generate an HTML report. The disadvantages are that it's only easy to profile python script files, not full packages. You can also profile a chunk of code , which can be useful when developing or for writing performance tests. from pyinstrument import Profiler profiler = Profiler () profiler . start () # code you want to profile profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) To explore the profile in a web browser, use profiler.open_in_browser() . To save this HTML for later, use profiler.output_html() .","title":"pyinstrument"},{"location":"python_profiling/#introduce-profiling-in-your-test-workflow","text":"I run out of time, so here are the starting points: Niklas Meinzer post Pypi page of pytest-benchmark , Docs , Git Docs of pytest-profiling uwpce guide on using pstats The idea is to develop the following ideas: How to integrate profiling with pytest. How to compare benchmark results between CI runs. Some guidelines on writing performance tests","title":"Introduce profiling in your test workflow"},{"location":"python_profiling/#references","text":"Antonio Molner article on Python Profiling","title":"References"},{"location":"remote_work/","text":"Remote working is a work arrangement in which employees do not commute or travel (e.g. by bus, bicycle or car, etc.) to a central place of work, such as an office building, warehouse, or store. As a side effect, we're spending a lot of time in front of our computers, so we should be careful that our working environment helps us to stay healthy. For example we could: Use an external monitor: Your laptop's screen is usually not big enough and will force you to look down instead of look straight which can lead to neck pain. Some prefer super big monitors (48 inches) while others feel that 24 inches is more than enough so you don't have to turn your head to reach each side of the screen. For me the sweet spot is having two terminals with 100 characters of width one beside the other. If you use a tiling window manager like i3wm , that should be enough. Some people valued that the screen was not fixed, so it could be tilted or it's height could be changed. Adjust the screen to your eye level: The center of the monitor should be at eye level, if the monitor height adjustment is not enough, you can use some old books or buy a screen support. Use an external keyboard: Sometimes the keys of the laptop keyboards have a cheap feedback or a weird key disposition, which leads to finger and wrist aches. The use of an external keyboard (better if it's a mechanical one) can help with this issue. The chair should support your back and don't be too hard to hurt your butt, nor too soft. Your legs should not be hanging in the air, that will add unnecessary pressure on your thighs which can lead to tingling. If you're in this situation, a leg support comes handy. The legs shouldn't be crossed either in front or below you, they should be straight with a 90 degree angle between your thighs and your calves, with a waist level separation between the feet. The table height should be enough to have a 90 degree angle between your forearms and your biceps , and your shoulders are in a relaxed stance. Small people may need a table with no drawers between your elbows and your legs, or you wont be able to fulfill the arm's requirement. The table height should be low enough to fulfill the leg's requirement above. Sometimes they are too high to be able to have a 90 degree angle between the thighs and calves even with feet support, in that case, change the desk or cut it's legs. Think about using a standing desk. Desk's with variable height are quite expensive, but there is always the option to buy a laptop support that let's you stand. Your hands should be at the same level as your forearms, you could use a wrist support for that and also to soften the contact of your forearms with the desk. If you're a heavy mouse user, think of using a vertical mouse instead of the traditional to prevent the metacarpal syndrome. And try not to use it! learn how to use a tiling window manager and Vim shortcuts for everything, such as using tridactyl for Firefox. Keep your working place between 19 and 21 degrees centigrades, otherwise you may unawarely contract your body. Use blue filter either in your glasses or in your screen. Have enough light so you don't need to strain your eyes. Having your monitor screen as your only source of light is harmful. Try to promote initiatives that increase the social interaction between your fellow workers. Stand up and walk around at least once each two hours. Meetings are a good moment to do an outside walk. Other tips non related with your work environment but with the consequences of your work experience can be: Don't remain static, doing exercise daily is a must. As you don't need to go out, it's quite easy to fall into the routine of waking up, sit in your computer, eat and go back to sleep. Both anaerobic (pilates, yoga or stretching) and aerobic (running, biking or dancing) give different benefits. Drink enough water, around 8 to 10 glasses per day. Use the extra time that remote working gives you to strengthen your outside work social relationships. Try not to be exposed to any screen light for an hour before you go to sleep. If you use an e-book, don't rely on their builtin light, use an external source instead. If you have a remote work contract, make sure that your employer pays for any upgrades, it's their responsibility.","title":"Remote Working"},{"location":"sleep/","text":"Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, reduced muscle activity and inhibition of nearly all voluntary muscles during rapid eye movement (REM) sleep,and reduced interactions with surroundings. Distinguished from wakefulness by a decreased ability to react to stimuli. Most of the content of this article is extracted from the Why we sleep book by Matthew Walker Consequences of lack of sleep \u2691 Sleeping less than six or seven hours a night can produce these consequences: Demolishing of the immune system. Doubling your risk of cancer. Is a key lifestyle factor determining the development of the Alzheimer's disease. Disruption of blood sugar levels so profoundly that you would be classified as pre-diabetic. Increase the likelihood of block and brittle of your coronary arteries. Setting you on a path toward cardiovascular disease, stroke, and congestive heart failure. Contributes to all major psychiatric conditions, including depression, anxiety, and suicidality. Swelling concentrations of a hormone that makes you feel hungry while suppressing a companion hormone that otherwise signals food satisfaction. A balanced diet and exercise are of vital importance, but we now see sleep as the key factor in health. The physical and mental impairments caused by one night of bad sleep dwarf those caused by an equivalent absence of food or exercise. Therefore, the shorter you sleep, the shorter your life span . Sleep benefits \u2691 We sleep for a lot of nighttime benefits that service both our brains and our body. There does not seem to be one major organ within the body, or process within the brain, that isn't optimally enhanced by sleep. Within the brain, sleep enriches our ability to learn, memorize and make logical decisions and choices. It recalibrates our emotional brain circuits, allowing us to navigate next day social and psychological challenges with cool-headed composture. Downstairs in the body, sleep: Restocks the armory of our immune system: helping fight malignancy, preventing infection, and warding off sickness. Reforms the body's metabolic state by fine-tuning the balance of insulin and circulating glucose. Regulates our appetite, helping control body weight through healthy food selection rather than rash impulsivity. Maintains a flourishing microbiome within your gut essential to our nutritional health being. Is tied to the fitness of our cardiovascular system, lowering blood pressure while keeping our hearts in fine condition. Dreaming produces a neurochemical bath that mollifies painful memories and a virtual reality space in which the brain melds past and present knowledge, inspiring creativity. Therefore, Sleep is the single most effective thing we can do to reset our brain and body health each day . Sleep physiological effects \u2691 There are two main factors that determine when you want to sleep or stay awake: The signal sent by the suprachiasmatic nucleus following the circadian rhythm. Sleep pressure: The brain builds up a chemical substance that creates the \"sleep pressure\". The longer you've been awake, the more that chemical sleep pressure accumulates, and consequentially, the sleepier you feel. The circadian rhythm \u2691 We have an internal clock deep within the brain, called the suprachiasmatic nucleus, that creates a cycling, day-night rhythm, known as circadian rhythm, that makes you feel tired or alert at regular times of night and day, respectively. The circadian rhythm determines: When you want to be awake or asleep. Your timed preferences for eating and drinking. Your moods and emotions The amount of urine you produce. Your core body temperature. Your metabolic rate. The release of numerous hormones. Contrary to common belief, circadian rhythm is not defined by the daylight sun cycle. As Kleitman and Richardson demonstrated in 1938: When cut off from the daily cycle of light and dark, the body keeps on maintaining the rhythm. The period of the circadian rhythm is different for each person, but has an average of 24 hours and 15 minutes. Even if it's not defined by the sun light, it corrects those 15 minutes of delay to stay in sync with it. The suprachiasmatic nucleus can readjust by about one hour each day, that is why jet lag can be spawn through multiple days. That reset does not come free. Studies in airplane cabin crews who frequently fly on long haul routes and have little chance to recover have registered: The part of the brains related to learning and memory had physically shrunk, suggesting the destruction of brain cells caused by the biological stress of timezone travel. Their short term memory was significantly impaired. They had far higher rates of cancer and type 2 diabetes than the general population. The peak and valley points of wakefulness or sleepiness vary too between people, it's known as their chronotype and it's strongly determined by genetics. The chronotype defines three types of people: Morning types : They have their peak of wakefulness early in the day and the sleepiness early at night. They prefer to wake at or around dawn, and function optimally at this time of day. Evening types : They prefer going to bed late and subsequently wake up late the following morning, or even in the afternoon. In between : The remaining people fall somewhere in between, with a slight leaning towards eveningness. Melatonin \u2691 The suprachiasmatic nucleus communicates its repeating signal of day and night to your brain and body by releasing melatonin into the bloodstream from the pineal gland. Soon after dusk, the suprachiasmatic nucleus starts increasing the levels of this hormone, telling the rest of the body that it's time to sleep. But melatonin has little influence on the generation of sleep itself. Once sleep is under way, melatonin decreases in concentration across the night and into the morning hours. With dawn, as sunlight enters the brain through the eyes (even through the closed lids), the pineal gland is instructed to stop releasing melatonin. The absence of circulating melatonin now informs the brain and body that it's time to return to a wakefulness active state for the rest of the day Sleep pressure \u2691 While you are awake, the brain is releasing a chemical called adenosine. One consequence of the increasing accumulation of adenosine is the increase of the desire to sleep by turning down the \"volume\" of wake promoting regions in the brain and turning up the sleep inducing ones. Most people fall to the pressure after twelve to sixteen hours of being awake. Caffeine \u2691 You can artificially mute the sleep signal of adenosine by using a chemical that makes you feel more alert and awake, such as caffeine. Caffeine works by battling with adenosine for the privilege of latching on to adenosine receptors in the brain. Once caffeine occupies these receptors, it does not stimulate them like adenosine, making you sleepy. Rather, caffeine blocks and effectively inactivates the receptors acting as a masking agent. Levels of caffeine peak around thirty minutes after ingestion. What is problematic, though, is the persistence of caffeine in your system. It takes between five to seven hours to remove 50 percent of the caffeine concentration from your body. An enzyme within your liver removes caffeine from your system. Based in large part on genetics, some people have a more efficient version of the enzyme that degrades caffeine, allowing the liver to clear it from the bloodstream. Age is also a variable to take into account, the older we are the longer it takes our brain and body to remove it. When your liver evicts the caffeine from your system, you encounter the caffeine crash . Your energy levels plummet rapidly, finding difficult to function and concentrate, with a strong sense of sleepiness once again. For the entire time that caffeine is in your system, the adenosine keeps on building up. Your brain is not aware of this rising tide of sleep encouraging chemical because the wall of caffeine is holding it back from your perception. Once your liver dismantles the barricade, you feel a vicious backlash: you are hit with the sleepiness you had experienced two or three hours ago before you drank that cup of coffee plus all the extra adenosine that has accumulated in the hours in between. Relationship between the circadian rhythm and the sleep pressure \u2691 The two governing forces that regulate your sleep are ignorant of each other. Although they are not coupled, they are usually aligned. Starting on the far left of the figure, the circadian rhythm begins to increase its activity a few hours before you wake up. It infuses the brain and body with an alerting energy signal. At first, the signal is faint, but gradually it builds with time. By early afternoon, the activating signal from the circadian rhythm peaks. Now let's look at the sleep pressure pattern. By mid to late morning, you have only been awake for a half of hours. As a result, adenosine concentrations have increased a little. Furthermore, the circadian rhythm is on its powerful upswing of alertness. This combination of strong activating output from the circadian rhythm together with low levels of adenosine result in a delightful sensation of being wide awake. The distance between the curved lines above will be a direct reflection of your desire to sleep. By eleven pm, you've been awake for fifteen hours, and your brain is drenched in high concentrations of adenosine. Additionally, the circadian rhythm line is descending, powering down your activity and alertness levels. This powerful combination triggers a strong desire for sleep. During sleep, a mass evacuation of adenosine gets under way as the brain has the chance to degrade and remove it. After eight hours of healthy sleep, the adenosine purge is complete. As this process is ending, the circadian activity rhythm has returned, and its energizing influence starts to approach, therefore naturally waking us up. All-nighters \u2691 Scientists can demonstrate that the two forces determining when you want to be awake and sleep are independent and can be decoupled from their normal lockstep. When you skip one night's sleep and remain awake throughout the following day, By remaining awake, and blocking access to the adenosine drain that sleep opens up, the brain is unable to rid itself of the chemical sleep pressure. The mounting adenosine levels continue to rise. This should mean that the longer you are awake, the sleepier you feel. But that's not true. Though you will feel increasingly sleepy throughout the nighttime phase, hitting a low point in your alertness around five or six in the morning, thereafter, you'll start to be more awake. This effect is answered by the energy return of the circadian rhythm. Unlike sleep pressure, the circadian rhythm pays no attention to whether you are asleep or awake. Am I getting enough sleep? \u2691 If after waking up in the morning you could fall asleep at ten or eleven a.m, it means that you're likely not getting enough sleep quantity or quality. The same can be said if you can't function optimally without caffeine before noon, you'll be most likely self-medicating your state of chronic sleep deprivation. Other sleep indicators can be if you would sleep more if you didn't set an alarm clock, or if you find yourself at your computer screen reading and then rereading the same sentence. Of course, even if you are giving yourself plenty of time to get a full night of shut-eye, next-day fatigue and sleepiness can still occur because you are suffering from an undiagnosed sleep disorder. References \u2691 Why we sleep book by Matthew Walker","title":"Sleep"},{"location":"sleep/#consequences-of-lack-of-sleep","text":"Sleeping less than six or seven hours a night can produce these consequences: Demolishing of the immune system. Doubling your risk of cancer. Is a key lifestyle factor determining the development of the Alzheimer's disease. Disruption of blood sugar levels so profoundly that you would be classified as pre-diabetic. Increase the likelihood of block and brittle of your coronary arteries. Setting you on a path toward cardiovascular disease, stroke, and congestive heart failure. Contributes to all major psychiatric conditions, including depression, anxiety, and suicidality. Swelling concentrations of a hormone that makes you feel hungry while suppressing a companion hormone that otherwise signals food satisfaction. A balanced diet and exercise are of vital importance, but we now see sleep as the key factor in health. The physical and mental impairments caused by one night of bad sleep dwarf those caused by an equivalent absence of food or exercise. Therefore, the shorter you sleep, the shorter your life span .","title":"Consequences of lack of sleep"},{"location":"sleep/#sleep-benefits","text":"We sleep for a lot of nighttime benefits that service both our brains and our body. There does not seem to be one major organ within the body, or process within the brain, that isn't optimally enhanced by sleep. Within the brain, sleep enriches our ability to learn, memorize and make logical decisions and choices. It recalibrates our emotional brain circuits, allowing us to navigate next day social and psychological challenges with cool-headed composture. Downstairs in the body, sleep: Restocks the armory of our immune system: helping fight malignancy, preventing infection, and warding off sickness. Reforms the body's metabolic state by fine-tuning the balance of insulin and circulating glucose. Regulates our appetite, helping control body weight through healthy food selection rather than rash impulsivity. Maintains a flourishing microbiome within your gut essential to our nutritional health being. Is tied to the fitness of our cardiovascular system, lowering blood pressure while keeping our hearts in fine condition. Dreaming produces a neurochemical bath that mollifies painful memories and a virtual reality space in which the brain melds past and present knowledge, inspiring creativity. Therefore, Sleep is the single most effective thing we can do to reset our brain and body health each day .","title":"Sleep benefits"},{"location":"sleep/#sleep-physiological-effects","text":"There are two main factors that determine when you want to sleep or stay awake: The signal sent by the suprachiasmatic nucleus following the circadian rhythm. Sleep pressure: The brain builds up a chemical substance that creates the \"sleep pressure\". The longer you've been awake, the more that chemical sleep pressure accumulates, and consequentially, the sleepier you feel.","title":"Sleep physiological effects"},{"location":"sleep/#the-circadian-rhythm","text":"We have an internal clock deep within the brain, called the suprachiasmatic nucleus, that creates a cycling, day-night rhythm, known as circadian rhythm, that makes you feel tired or alert at regular times of night and day, respectively. The circadian rhythm determines: When you want to be awake or asleep. Your timed preferences for eating and drinking. Your moods and emotions The amount of urine you produce. Your core body temperature. Your metabolic rate. The release of numerous hormones. Contrary to common belief, circadian rhythm is not defined by the daylight sun cycle. As Kleitman and Richardson demonstrated in 1938: When cut off from the daily cycle of light and dark, the body keeps on maintaining the rhythm. The period of the circadian rhythm is different for each person, but has an average of 24 hours and 15 minutes. Even if it's not defined by the sun light, it corrects those 15 minutes of delay to stay in sync with it. The suprachiasmatic nucleus can readjust by about one hour each day, that is why jet lag can be spawn through multiple days. That reset does not come free. Studies in airplane cabin crews who frequently fly on long haul routes and have little chance to recover have registered: The part of the brains related to learning and memory had physically shrunk, suggesting the destruction of brain cells caused by the biological stress of timezone travel. Their short term memory was significantly impaired. They had far higher rates of cancer and type 2 diabetes than the general population. The peak and valley points of wakefulness or sleepiness vary too between people, it's known as their chronotype and it's strongly determined by genetics. The chronotype defines three types of people: Morning types : They have their peak of wakefulness early in the day and the sleepiness early at night. They prefer to wake at or around dawn, and function optimally at this time of day. Evening types : They prefer going to bed late and subsequently wake up late the following morning, or even in the afternoon. In between : The remaining people fall somewhere in between, with a slight leaning towards eveningness.","title":"The circadian rhythm"},{"location":"sleep/#melatonin","text":"The suprachiasmatic nucleus communicates its repeating signal of day and night to your brain and body by releasing melatonin into the bloodstream from the pineal gland. Soon after dusk, the suprachiasmatic nucleus starts increasing the levels of this hormone, telling the rest of the body that it's time to sleep. But melatonin has little influence on the generation of sleep itself. Once sleep is under way, melatonin decreases in concentration across the night and into the morning hours. With dawn, as sunlight enters the brain through the eyes (even through the closed lids), the pineal gland is instructed to stop releasing melatonin. The absence of circulating melatonin now informs the brain and body that it's time to return to a wakefulness active state for the rest of the day","title":"Melatonin"},{"location":"sleep/#sleep-pressure","text":"While you are awake, the brain is releasing a chemical called adenosine. One consequence of the increasing accumulation of adenosine is the increase of the desire to sleep by turning down the \"volume\" of wake promoting regions in the brain and turning up the sleep inducing ones. Most people fall to the pressure after twelve to sixteen hours of being awake.","title":"Sleep pressure"},{"location":"sleep/#caffeine","text":"You can artificially mute the sleep signal of adenosine by using a chemical that makes you feel more alert and awake, such as caffeine. Caffeine works by battling with adenosine for the privilege of latching on to adenosine receptors in the brain. Once caffeine occupies these receptors, it does not stimulate them like adenosine, making you sleepy. Rather, caffeine blocks and effectively inactivates the receptors acting as a masking agent. Levels of caffeine peak around thirty minutes after ingestion. What is problematic, though, is the persistence of caffeine in your system. It takes between five to seven hours to remove 50 percent of the caffeine concentration from your body. An enzyme within your liver removes caffeine from your system. Based in large part on genetics, some people have a more efficient version of the enzyme that degrades caffeine, allowing the liver to clear it from the bloodstream. Age is also a variable to take into account, the older we are the longer it takes our brain and body to remove it. When your liver evicts the caffeine from your system, you encounter the caffeine crash . Your energy levels plummet rapidly, finding difficult to function and concentrate, with a strong sense of sleepiness once again. For the entire time that caffeine is in your system, the adenosine keeps on building up. Your brain is not aware of this rising tide of sleep encouraging chemical because the wall of caffeine is holding it back from your perception. Once your liver dismantles the barricade, you feel a vicious backlash: you are hit with the sleepiness you had experienced two or three hours ago before you drank that cup of coffee plus all the extra adenosine that has accumulated in the hours in between.","title":"Caffeine"},{"location":"sleep/#relationship-between-the-circadian-rhythm-and-the-sleep-pressure","text":"The two governing forces that regulate your sleep are ignorant of each other. Although they are not coupled, they are usually aligned. Starting on the far left of the figure, the circadian rhythm begins to increase its activity a few hours before you wake up. It infuses the brain and body with an alerting energy signal. At first, the signal is faint, but gradually it builds with time. By early afternoon, the activating signal from the circadian rhythm peaks. Now let's look at the sleep pressure pattern. By mid to late morning, you have only been awake for a half of hours. As a result, adenosine concentrations have increased a little. Furthermore, the circadian rhythm is on its powerful upswing of alertness. This combination of strong activating output from the circadian rhythm together with low levels of adenosine result in a delightful sensation of being wide awake. The distance between the curved lines above will be a direct reflection of your desire to sleep. By eleven pm, you've been awake for fifteen hours, and your brain is drenched in high concentrations of adenosine. Additionally, the circadian rhythm line is descending, powering down your activity and alertness levels. This powerful combination triggers a strong desire for sleep. During sleep, a mass evacuation of adenosine gets under way as the brain has the chance to degrade and remove it. After eight hours of healthy sleep, the adenosine purge is complete. As this process is ending, the circadian activity rhythm has returned, and its energizing influence starts to approach, therefore naturally waking us up.","title":"Relationship between the circadian rhythm and the sleep pressure"},{"location":"sleep/#all-nighters","text":"Scientists can demonstrate that the two forces determining when you want to be awake and sleep are independent and can be decoupled from their normal lockstep. When you skip one night's sleep and remain awake throughout the following day, By remaining awake, and blocking access to the adenosine drain that sleep opens up, the brain is unable to rid itself of the chemical sleep pressure. The mounting adenosine levels continue to rise. This should mean that the longer you are awake, the sleepier you feel. But that's not true. Though you will feel increasingly sleepy throughout the nighttime phase, hitting a low point in your alertness around five or six in the morning, thereafter, you'll start to be more awake. This effect is answered by the energy return of the circadian rhythm. Unlike sleep pressure, the circadian rhythm pays no attention to whether you are asleep or awake.","title":"All-nighters"},{"location":"sleep/#am-i-getting-enough-sleep","text":"If after waking up in the morning you could fall asleep at ten or eleven a.m, it means that you're likely not getting enough sleep quantity or quality. The same can be said if you can't function optimally without caffeine before noon, you'll be most likely self-medicating your state of chronic sleep deprivation. Other sleep indicators can be if you would sleep more if you didn't set an alarm clock, or if you find yourself at your computer screen reading and then rereading the same sentence. Of course, even if you are giving yourself plenty of time to get a full night of shut-eye, next-day fatigue and sleepiness can still occur because you are suffering from an undiagnosed sleep disorder.","title":"Am I getting enough sleep?"},{"location":"sleep/#references","text":"Why we sleep book by Matthew Walker","title":"References"},{"location":"sqlite/","text":"SQLite is a relational database management system (RDBMS) contained in a C library. In contrast to many other database management systems, SQLite is not a client\u2013server database engine. Rather, it is embedded into the end program. SQLite is ACID-compliant and implements most of the SQL standard, generally following PostgreSQL syntax. However, SQLite uses a dynamically and weakly typed SQL syntax that does not guarantee the domain integrity.[7] This means that one can, for example, insert a string into a column defined as an integer. SQLite will attempt to convert data between formats where appropriate, the string \"123\" into an integer in this case, but does not guarantee such conversions and will store the data as-is if such a conversion is not possible. SQLite is a popular choice as embedded database software for local/client storage in application software such as web browsers. It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others. Upsert statements \u2691 UPSERT is a special syntax addition to INSERT that causes the INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint. UPSERT is not standard SQL. UPSERT in SQLite follows the syntax established by PostgreSQL. The syntax that occurs in between the \"ON CONFLICT\" and \"DO\" keywords is called the \"conflict target\". The conflict target specifies a specific uniqueness constraint that will trigger the upsert. The conflict target is required for DO UPDATE upserts, but is optional for DO NOTHING. When the conflict target is omitted, the upsert behavior is triggered by a violation of any uniqueness constraint on the table of the INSERT. If the insert operation would cause the uniqueness constraint identified by the conflict-target clause to fail, then the insert is omitted and either the DO NOTHING or DO UPDATE operation is performed instead. In the case of a multi-row insert, this decision is made separately for each row of the insert. The special UPSERT processing happens only for uniqueness constraint on the table that is receiving the INSERT. A \"uniqueness constraint\" is an explicit UNIQUE or PRIMARY KEY constraint within the CREATE TABLE statement, or a unique index. UPSERT does not intervene for failed NOT NULL or foreign key constraints or for constraints that are implemented using triggers. Column names in the expressions of a DO UPDATE refer to the original unchanged value of the column, before the attempted INSERT. To use the value that would have been inserted had the constraint not failed, add the special \"excluded.\" table qualifier to the column name. CREATE TABLE phonebook2 ( name TEXT PRIMARY KEY , phonenumber TEXT , validDate DATE ); INSERT INTO phonebook2 ( name , phonenumber , validDate ) VALUES ( 'Alice' , '704-555-1212' , '2018-05-08' ) ON CONFLICT ( name ) DO UPDATE SET phonenumber = excluded . phonenumber , validDate = excluded . validDate References \u2691 Home","title":"SQLite"},{"location":"sqlite/#upsert-statements","text":"UPSERT is a special syntax addition to INSERT that causes the INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint. UPSERT is not standard SQL. UPSERT in SQLite follows the syntax established by PostgreSQL. The syntax that occurs in between the \"ON CONFLICT\" and \"DO\" keywords is called the \"conflict target\". The conflict target specifies a specific uniqueness constraint that will trigger the upsert. The conflict target is required for DO UPDATE upserts, but is optional for DO NOTHING. When the conflict target is omitted, the upsert behavior is triggered by a violation of any uniqueness constraint on the table of the INSERT. If the insert operation would cause the uniqueness constraint identified by the conflict-target clause to fail, then the insert is omitted and either the DO NOTHING or DO UPDATE operation is performed instead. In the case of a multi-row insert, this decision is made separately for each row of the insert. The special UPSERT processing happens only for uniqueness constraint on the table that is receiving the INSERT. A \"uniqueness constraint\" is an explicit UNIQUE or PRIMARY KEY constraint within the CREATE TABLE statement, or a unique index. UPSERT does not intervene for failed NOT NULL or foreign key constraints or for constraints that are implemented using triggers. Column names in the expressions of a DO UPDATE refer to the original unchanged value of the column, before the attempted INSERT. To use the value that would have been inserted had the constraint not failed, add the special \"excluded.\" table qualifier to the column name. CREATE TABLE phonebook2 ( name TEXT PRIMARY KEY , phonenumber TEXT , validDate DATE ); INSERT INTO phonebook2 ( name , phonenumber , validDate ) VALUES ( 'Alice' , '704-555-1212' , '2018-05-08' ) ON CONFLICT ( name ) DO UPDATE SET phonenumber = excluded . phonenumber , validDate = excluded . validDate","title":"Upsert statements"},{"location":"sqlite/#references","text":"Home","title":"References"},{"location":"sqlite3/","text":"SQLite3 is a python library that provides an SQL interface compliant with the DB-API 2.0 specification described by PEP 249. Usage \u2691 To use the module, you must first create a Connection object that represents the database, and a Cursor one to interact with it. Here the data will be stored in the example.db file: import sqlite3 conn = sqlite3 . connect ( 'example.db' ) cursor = conn . cursor () Once we have a cursor we can execute the different SQL statements and the save them with the commit method of the Connection object. Finally we can close the connection with close . # Create table cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) # Insert a row of data cursor . execute ( \"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\" ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close () Get columns of a query \u2691 cursor = connection . execute ( 'select * from bar' ) names = [ description [ 0 ] for description in cursor . description ] References \u2691 Docs","title":"sqlite3"},{"location":"sqlite3/#usage","text":"To use the module, you must first create a Connection object that represents the database, and a Cursor one to interact with it. Here the data will be stored in the example.db file: import sqlite3 conn = sqlite3 . connect ( 'example.db' ) cursor = conn . cursor () Once we have a cursor we can execute the different SQL statements and the save them with the commit method of the Connection object. Finally we can close the connection with close . # Create table cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) # Insert a row of data cursor . execute ( \"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\" ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close ()","title":"Usage"},{"location":"sqlite3/#get-columns-of-a-query","text":"cursor = connection . execute ( 'select * from bar' ) names = [ description [ 0 ] for description in cursor . description ]","title":"Get columns of a query"},{"location":"sqlite3/#references","text":"Docs","title":"References"},{"location":"strategy/","text":"Strategy is a general plan to achieve one or more long-term or overall goals under conditions of uncertainty. Strategy is important because the resources available to achieve goals are usually limited. Strategy generally involves setting goals and priorities, determining actions to achieve the goals, and mobilizing resources to execute the actions. A strategy describes how the ends (goals) will be achieved by the means (resources). Strategy can be intended or can emerge as a pattern of activity as the person or organization adapts to its environment. It typically involves two major processes: Formulation : Involves analyzing the environment or situation, making a diagnosis, and developing guiding policies. It includes such activities as strategic planning and strategic thinking . Implementation : Refers to the action plans taken to achieve the goals established by the guiding policy. Strategic planning \u2691 Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy. It helps coordinate the two processes required by the strategy, formulation and implementation. However, strategic planning is analytical in nature (i.e., it involves \"finding the dots\"); strategy formation itself involves synthesis (i.e., \"connecting the dots\") via strategic thinking. As such, strategic planning occurs around the strategy formation activity. Strategic thinking \u2691 Strategic thinking is defined as a mental or thinking process applied by an individual in the context of achieving a goal or set of goals in a game or other endeavor. As a cognitive activity, it produces thought. Strategic thinking includes finding and developing a strategic foresight capacity for an organization or individual, by exploring all possible futures, and challenging conventional thinking to foster decision making today. The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself\u2014and the diagnosis in fact amounts to a choice between alternative courses of action.","title":"Strategy"},{"location":"strategy/#strategic-planning","text":"Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy. It helps coordinate the two processes required by the strategy, formulation and implementation. However, strategic planning is analytical in nature (i.e., it involves \"finding the dots\"); strategy formation itself involves synthesis (i.e., \"connecting the dots\") via strategic thinking. As such, strategic planning occurs around the strategy formation activity.","title":"Strategic planning"},{"location":"strategy/#strategic-thinking","text":"Strategic thinking is defined as a mental or thinking process applied by an individual in the context of achieving a goal or set of goals in a game or other endeavor. As a cognitive activity, it produces thought. Strategic thinking includes finding and developing a strategic foresight capacity for an organization or individual, by exploring all possible futures, and challenging conventional thinking to foster decision making today. The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself\u2014and the diagnosis in fact amounts to a choice between alternative courses of action.","title":"Strategic thinking"},{"location":"talkey/","text":"Talkey is a Simple Text-To-Speech (TTS) interface library with multi-language and multi-engine support. Installation \u2691 pip install talkey You need to install the TTS engines by yourself. Talkey supports: Flite SVOX Pico Festival eSpeak mbrola via eSpeak I've tried SVOX Pico, Festival and eSpeak. I've discarded Flite because it's not in the official repositories. Of those three the one that gives the most natural support is SVOX Pico. To install it execute: sudo apt-get install libttspico-utils It also supports the following networked TTS Engines: MaryTTS (needs hosting). Google TTS (cloud hosted) I obviously discard Google for privacy reasons, and MaryTTS too because it needs you to run a server, which is inconvenient for most users and pico gives us enough quality. Usage \u2691 At its simplest use case: import talkey tts = talkey . Talkey () tts . say ( \"I've been really busy being dead. You know, after you murdered me.\" ) It automatically detects languages without any further configuration: tts . say ( \"La cabra siempre tira al monte\" ) References \u2691 Git Docs","title":"Talkey"},{"location":"talkey/#installation","text":"pip install talkey You need to install the TTS engines by yourself. Talkey supports: Flite SVOX Pico Festival eSpeak mbrola via eSpeak I've tried SVOX Pico, Festival and eSpeak. I've discarded Flite because it's not in the official repositories. Of those three the one that gives the most natural support is SVOX Pico. To install it execute: sudo apt-get install libttspico-utils It also supports the following networked TTS Engines: MaryTTS (needs hosting). Google TTS (cloud hosted) I obviously discard Google for privacy reasons, and MaryTTS too because it needs you to run a server, which is inconvenient for most users and pico gives us enough quality.","title":"Installation"},{"location":"talkey/#usage","text":"At its simplest use case: import talkey tts = talkey . Talkey () tts . say ( \"I've been really busy being dead. You know, after you murdered me.\" ) It automatically detects languages without any further configuration: tts . say ( \"La cabra siempre tira al monte\" )","title":"Usage"},{"location":"talkey/#references","text":"Git Docs","title":"References"},{"location":"wake_on_lan/","text":"Wake on LAN (WoL) is a feature to switch on a computer via the network. Usage \u2691 Host configuration \u2691 On the host you want to activate the wake on lan execute: $: ethtool *interface* | grep Wake-on Supports Wake-on: pumbag Wake-on: d The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver: $: ethtool -s interface wol g If it was not enabled check in the Arch wiki how to make the change persistent. To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine: $: ip link 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 2 : enp1s0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000 link/ether 48 :05:ca:09:0e:6a brd ff:ff:ff:ff:ff:ff Here the MAC address is 48:05:ca:09:0e:6a . In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link). If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so. Client trigger \u2691 If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects. If it's firewalled you need to configure the client firewall to allow the outgoing UDP traffic to the port 9. In the simplest case the default broadcast address 255.255.255.255 is used: $ wakeonlan *target_MAC_address* To broadcast the magic packet only to a specific subnet or host, use the -i switch: $ wakeonlan -i *target_IP* *target_MAC_address* References \u2691 Arch wiki post","title":"Wake on Lan"},{"location":"wake_on_lan/#usage","text":"","title":"Usage"},{"location":"wake_on_lan/#host-configuration","text":"On the host you want to activate the wake on lan execute: $: ethtool *interface* | grep Wake-on Supports Wake-on: pumbag Wake-on: d The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver: $: ethtool -s interface wol g If it was not enabled check in the Arch wiki how to make the change persistent. To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine: $: ip link 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 2 : enp1s0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000 link/ether 48 :05:ca:09:0e:6a brd ff:ff:ff:ff:ff:ff Here the MAC address is 48:05:ca:09:0e:6a . In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link). If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so.","title":"Host configuration"},{"location":"wake_on_lan/#client-trigger","text":"If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects. If it's firewalled you need to configure the client firewall to allow the outgoing UDP traffic to the port 9. In the simplest case the default broadcast address 255.255.255.255 is used: $ wakeonlan *target_MAC_address* To broadcast the magic packet only to a specific subnet or host, use the -i switch: $ wakeonlan -i *target_IP* *target_MAC_address*","title":"Client trigger"},{"location":"wake_on_lan/#references","text":"Arch wiki post","title":"References"},{"location":"wesnoth/","text":"The Battle for Wesnoth is an open source, turn-based strategy game with a high fantasy theme. It features both singleplayer and online/hotseat multiplayer combat. Explore the world of Wesnoth and take part in its different adventures! Embark on a desperate quest to reclaim your rightful throne\u2026 Flee the Lich Lords to a new home across the sea\u2026 Delve into the darkest depths of the earth to craft a jewel of fire itself\u2026 Defend your kingdom against the ravaging hordes of a foul necromancer\u2026 Or lead a straggly band of survivors across the blazing sands to confront an unseen evil. References \u2691 Home Wiki Game manual Mainline campaigns","title":"The Battle for Wesnoth"},{"location":"wesnoth/#references","text":"Home Wiki Game manual Mainline campaigns","title":"References"},{"location":"wesnoth_loyalist/","text":"The Loyalist are a faction of Humans who are loyal to the throne of Wesnoth. The race of men is an extremely diverse one. Although they originally came from the Old Continent, men have spread all over the world and split into many different cultures and races. Although they are not imbued with magic like other creatures, humans can learn to wield it and able to learn more types than most others. They have no extra special abilities or aptitudes except their versatility and drive. While often at odds with all races, they can occasionally form alliances with the less aggressive races such as elves and dwarves. The less scrupulous among them do not shrink back from hiring orcish mercenaries, either. They have no natural enemies, although the majority of men, like most people of all races, have an instinctive dislike of the undead. Men are shorter than the elves, but taller still than dwarves. Their skin color can vary, from almost white to dark brown. Humans are a versatile race who specialize in many different areas. Similarly, the Loyalist faction is a very versatile melee oriented faction with important ranged support from bowmen and mages . How to play loyalists \u2691 Loyalists are considered to be the most versatile faction in the game. They have most available unit types to recruit (8), more than any other faction. Loyalists are mostly melee oriented faction, with only two ranged unit types, the mage and the bowman , but their ranged units play a significant role in the loyalists strategy and have to be used effectively. About the melee troops the loyalist player gets to choose among: Heavy Infantrymen : Few strikes, high damage per strike, slow, higher hit-points, good resistances, low defenses, deals impact damage which is good against undead. Spearmen : Average strikes, average damage per strike, average movement, medium hit-points, normal resistances, average defenses, deals pierce damage which is good against drakes, has a weak ranged attack. Fencer : High number of strikes, low damage per strike, quick, low hit-points, bad resistances, good defenses, deals blade damage which is good against elusive foots or wose, deals less total damage than the other two, is a skirmisher. The loyalists unit price ranges from the cheap spearmen and mermen (14 gold) to the expensive horsemen (23 gold). Loyalist units generally have good hit-points and they deal good damage, even the cheap spearmen and bowmen . Even their scouts have high hit-points and better melee attack compared to most of the scouts of the other factions. But on the other hand, the loyalist scouts have lower defenses than most of the other scouts and they do not have a ranged attack. The loyalists have some units with good resistances, like Heavy Infantry or Cavalryman , which can be very good for defending if you notice that your opponent doesn't have the proper units to counter them. Other units, like the bowman , who is more vulnerable than the previous two, but is also good for defense because it has better defense, is cheaper and it deals solid damage back in both melee and range. Similar goes for the spearman . When attacking , Loyalists have units which can be pretty devastating, like the horseman for attacking enemy mages and other ranged units, mages against the entrenched melee units, fencers for killing enemy injured and valuable units trying to heal behind their lines. But the mentioned units are also very vulnerable and they can die very easily if not properly supported. The loyalists as a faction generally have average defenses (they do not get 70% defense anywhere, with the exception of the fencer, which is a special case) and they are not a terrain dependent faction (like the Elves or Knalgans are). Therefore, even if it good to put spearmen and bowmen on mountains, villages and castles where they get 60% defense, you should also take the faction you are facing into account when deciding where to place your units. For example, if you are fighting Knalgans, you should try not to move your units next to hills and mountains and/or take the hills and mountains with your units and leave the forest to the dwarves. All the loyalist units are lawful, which obviously leads to the conclusion that the loyalists should be more aggressive and attack at day (or even start at dawn, because your opponent in most of the cases will be reluctant to attack too aggressively and leave units on bad ground when the morning arrives) while at night the loyalists should at least play conservatively, or in some cases even run away. Of course, that is also dependent upon the faction of your opponent. The greatest weakness of the loyalists is their limited mobility. The loyalists' units have normal movement, some of their units like the horseman , cavalryman , merman fighter and the fencer are pretty quick when they are in their favorable environment, but all their units get slowed over rough terrain a lot compared to the other factions' units . Half of the loyalists units can not even move on/over mountains, and their scouts get significantly slowed in forest, for example. This lack of mobility will often give your opponent an opportunity to outmaneuver you in the game.To prevent that, sometimes it is good to attack hard on a certain point and defend only with minimal amount of units elsewhere, rather than uniformly spreading your units around the front. When you don't know which faction you are facing, it is good to get a mixture of units. On most of the default 1v1 maps, this initial recruitment list would be good : spearman , bowman , fencer , mage , merman fighter , cavalryman . As always, you should carefully consider about the exact placement of each individual unit from the initial recruits on the castle to come up with the most optimal combination that will allow you the quickest village grabbing in the first turns of the game. Loyalists vs. Undead \u2691 When fighting undead, your recruiting pattern will depend on whether your opponent spams skeleton archers or dark adepts for ranged attacks. If you see lots of skeletons, you'll have to focus heavily on heavy infrantrymen and mages . If you see lots of dark adepts , you'll want some cavalry , horsemen and maybe some spearmen . Bowman : ( D- ) Almost entirely useless. You might use them to poke at walking corpses , ghouls , vampire bats and ghosts , but mages are much better at all of these things. I would only buy a bowman to counter a large zombie horde. Cavalryman : ( B+ ) You'll want some cavalry for scouting and dealing initial damage to dark adepts so that a horseman can finish them. They are also decent at fighting Skeletons in the daytime, because cavalry are blade-resistant. However, the cheap skeleton archers will really ruin cavalry quickly at night, so if there are any skeleton archers on the other side you won't be able to use cavalry to hold territory. If your opponent over-recruits dark adepts , however, you can use cold-resistant cavalry to hold territory against them. They are also decent for holding territory against ghouls because they can run away and heal (unlike your heavy infantrymen ). Fencer : ( C- ) Fencers are a bad recruit in this match up because they are vulnerable to the blade and pierce damage of skeletons and cannot damage them much in return. They also are incapable of holding territory against dark adepts , who cut right through the high defense of fencers . However, you may want to have a fencer or two around for trapping dark adepts or getting in that last hit. With luck, they may also be able to frustrate non-magical attacks from skeletons and the like. Heavy Infantryman : ( B+ ) You need heavy infantry to hold territory against skeletons and skeleton archers , and they will be your unit of choice for dealing melee damage, especially to the cheap skeleton archers . Any heavy infantrymen in the daytime can kill a full health skeleton archer in two hits, while a strong heavy infantrymen in the daytime can kill a full health skeleton in two hits, dealing 18 damage per strike (even a mage in daytime cannot kill a skeleton in one round, unless supported by a lieutenant ). A fearless heavy infantryman may be dangerous even at night. If you don't have enough heavy infantryman to go around, you can get your initial hits in on a skeleton archer with them and finish him with a mage . Just keep heavy infantrymen away from dark adepts , and only let ghouls hit heavy infantrymen if they are on a village (or has a white mage or other healer next to him), since they can't easily run away to heal. Also beware walking corpses , which deal a surprising amount of damage at all times of day since heavy infantrymen can't dodge and impact damage goes around their resistances, and the ranged cold attack of ghosts . The biggest problem with heavy infantrymen is they are slow, which means they're hard to retreat at night and hard to advance in day. Without shielding units they'll get trapped and killed, and if you have to shield a unit maybe it should be a mage instead. Horseman : ( B ) - Because they deal pierce damage, horsemen may not be very useful when faced with skeletons . However, if your opponent over-recruits dark adepts , horsemen can be extremely useful, as dark adepts deal no return damage to the normally risky charge attack. horsemen can even be used to finish skeleton archers , their nemesis, in the daytime. However, if your opponent recruits enough skeleton archers you will have a hard time shielding your horsemen from their devastating pierce attacks, and skeleton archers are dirt cheap. horsemen can also one-shot bats and zombies , which can be useful if you need to clear out a lot of level 0 units quickly. I would want to have at least one horseman around to keep my opponent from getting too bold with dark adepts , if not more. Your opponent will be forced to recruit dark adepts if you have heavy infantrymen in the field. Mage : ( A+ ) mages are an absolute necessity against Undead. If you do not have mages it will be almost impossible for you to kill ghosts , but with mages it's a piece of cake. mages are the best unit for killing almost everything Undead can throw at you, and can even be used to finish dark adepts in the daytime. Your main problem is that dark adepts are cheaper and deal almost as much damage, so your opponent can spam dark adepts while you cannot afford to spam mages . You will also have the difficult task of shielding fragile, expensive mages against lots of cheap Undead units. Your opponent will use skeletons and ghouls to attack your mages when he can, but bats , zombies or just about any other unit will do for killing your mages in a pinch. Shield your mages well, surround them with damage soakers and if you can deliver them safely to their targets you'll be able to clear out the Undead quickly. Merman Fighter : ( C- ) Mermen make a decent navy against Undead, since bats and ghosts will have a hard time killing them with their high defense. Even dark adepts will find Mermen annoying because of their 20% cold resistance. However, Mermen will have a hard time hurting anything the Undead have with their lame pierce weapon. Generally Mermen are only good for holding water hexes and scouting, but don't underestimate how useful that can be. Some well-placed Mermen on a water map can prevent bats from sneaking behind your lines and capturing villages or killing injured units. Even on mostly land maps, a Merman in a river can help hold a defensive line, or a quick Merman can use a river to slip behind the enemy to trap dark adepts or other units that are trying to escape at daytime. Spearman : ( C- ) Spearmen are mostly useful as cheap units for holding villages and occupying space when faced with dark adepts or skeleton archers . (You'll want to avoid letting dark adepts hit your heavy infantrymen because of their vulnerability to cold.) However, you don't really want spearmen to take hits from dark adepts , it would be better to let the cold-resistant cavalry absorb the damage. The only units spearmen are good for attacking are dark adepts and walking corpses . spearmen are completely useless against skeletons unless you level one into a swordsman , and even then they're pretty mediocre. However, if there are lots of skeleton archers you won't be able to use much cavalry or horsemen , so a spearman or two may be necessary as defenders and damage soakers even if they are lousy at dealing damage to Undead. Loyalists vs. Rebels \u2691 Rebels are a very versatile faction like the Loyalists. So you need to counter their versatility with yours. Their weakest trait is low hit-points. That is compensated somewhat by relatively higher defenses and mobility, so you'll need a combination of hard-hitters and speed. mages are also nice for their magical attacks. Anyways, the key for a Loyalist general is to effectively use his hard-hitting units and kill as many units as he can at day. And at night, to defend well as elves ignore the time of day. The smartest thing you can do upon discovering that your opponent is playing Rebels, in multiplayer, is to assume that your opponent has woses and recruit accordingly. woses are a necessary counter to Loyalists' daytime attacks and as soon as your opponent realizes that you are Loyalists he will almost certainly recruit woses . Remember that just because you don't see any woses doesn't mean there aren't a couple hiding in the forest! To fight woses you will need cavalry as meat shields, mages to deal damage, and maybe fencers to get the last hit in. Sadly, cavalry are very vulnerable to elvish archers , so you'll need to make sure that the archers die, which can be difficult if they are in forest. Get horsemen which can one-shot archers (despite their high dodge) and keep up with your cavalry . If one horseman misses, follow up with another one, it's a lucky archer that can dodge 4/4 one-shot attempts. heavy infantrymen are also great against everything but woses and mages , and will reduce the effectiveness of archer spam. spearmen are useless against woses , recruit them sparingly and keep them away from unscouted forest. An alternative way of playing this match up is to maximize the mobility offered by the Loyalists' fastest units, namely the two horse units. By using them to outrun the Rebel units you can achieve some devastating rushes. Some matches require use of both styles of playing. Again, their versatility must be countered with yours. Bowman : ( C- ) Rebels in general are effective in both melee and ranged attacks, so recruiting a ranged unit is less beneficial than most factions because most of the Rebels' units can retaliate against you. They can be useful for taking out an Scout or two, but otherwise, this is not really a smart buy. Cavalryman : ( B+ ) A good scout and an effective counter to woses . Watch out for archers, though, they can really tear horses to shreds. And as always, they are effective against ranged oriented units like the mage . ( A ) If you're going for some faster action, cavalrymen are vital in the attack. They do great damage during the day, and combined with their dangerous mobility, they can be a fearsome unit indeed. Just be careful of archers ZoCing you and tearing the unit to shreds. Fencer : ( B+ ) The fencer shares the 70% defense in the forest like most of the elves, but it has negative resistances. Theoretically, they should be effective against woses , but more often than not woses will crush them easily. In spite of this, Fencers still have skirmsher, which means that they can sneak behind an injured unit and finish them off. Do not over-recruit them, as enemy mages will tear though their high defense. Heavy Infantryman : ( C- ) Usefulness similar to an archer. It's too slow to deal with most of the Rebel units, and it is owned by Mages , woses , and even archers. Even though it has a high damage potential and good resistances, because of its inherently low defense shamans can easily get hits on it and turn it into a piece of metal for a turn. Horseman : ( B ) One horseman may be nice, but no more than that. Enemy archers will tear them apart and shamans totally mess them up. They're expensive too. Their greatest use is probably killing mages or an archer that has gone slightly off-track. ( A- ) Again, when mobility is required, these units need to come and do serious damage. Since Rebels are mostly comprised of low-hp units, horsemen at day can usually rip them apart. Again be careful of archers, for this unit is worth 23 gold. Shield him properly if he is damaged. Mage : ( A- ) You'll need these guys to tear though those high elvish 60-70% evasion in the forest; but be careful, archers and shamans will retaliate and some pretty nasty things may come from that. One purely positive thing though, they just absolutely destroy woses . 13-3 at day. Ouch. mages are expensive and fragile though, so keep them protected. Merman Fighter : ( B+ ) If the map has a lot of water, maybe recruit a few to prevent the Rebel's Mermen Hunters from taking over the waters. Otherwise they don't really contribute much else. Spearman - A - A necessity. To defend at night, to kill pretty much anything except for woses , and to be cheap and cost only 14 gold. These guys pretty much tear though most of the Rebel units, if it were not only for the high-defense archer and shaman . Get a bunch, and move them like a wall against the enemy units. At the start of the game, recruit 1-2 cavalrymen , depending on the map size, 1 mage , (1 merman fighter if you need some water coverage), maybe 1 fencer , and the rest spearman . Later on you maybe can recruit some horseman if you opponent recruits mass mages , or more cavalrymen and mages if he masses woses . Otherwise, spearmen and mages should help you get through most of the match. If you're going for speed, recruit 2-3 cavalrymen , depending on the map, 1 horseman , maybe 1 fencer , and the rest spearmen . Use your spearmen to fill the holes and consolidate the territory that the horses took over. Use the horses to outrun the archers and attack when they can. Loyalists vs Northerners \u2691 The main problem you will have when facing northerners are two things: poison and numbers. If northerners didn't have assassins , it wouldn't be much of a big deal; you could out manoeuvre them with your superior scouting units and skirmishers to grab their villages and then finish them off one by one with spearmen / bowmen . Thing is, assassins poison makes defending at night extremely difficult, and by poisoning your units, they often force you to retreat and heal, or die due to retaliation. The key to winning this match up is to use your superior versatility; the northerners greatest weakness is that ALL of their units are very average and don't often have many special abilities; you have masses. Use your first strike in spearmen to defend against grunts , trolls and wolfriders (even at night), use your magic to finish off wounded assassins , and you can charge any of their wounded units and kill them with one hit. Since all of your units are lawful and theirs are chaotic, you'll want to press your advantage at day and kill as much as possible (but don't attack assassins with bowmen ; you don't want to be poisoned and be forced to retreat in your daytime assault). Another problem the northerners have is that all though their units are many in number, they do not have a very high damage per hex ratio. Use this to your advantage; travel in packs and use ZoC lines to prevent your units from being swarmed too easily, and keep your vulnerable units like mages behind damage soakers like spearmen or heavy infantry . If you can, try and make night time defenses near rivers; northerners will hurt themselves when they attack you, as they have low defense in water and are very annoyed by deep water, because it prevents them from easily swarming you. Spearmen : ( A- ) Pretty much your best unit against northerners. Their first strike ability is great in defense and their attack power to cost ratio is quite high. They also have good health and can retaliate in ranged. As in many matchups, the bulk of your army. Mage : ( B+ ) The mage is fragile and expensive and has a weak melee attack, which is the exact opposite of northerners. Not a great defender unless your opponent goes mad with assassins. However, they have lots of attack power at day and are very useful for nocking trolls of their perches and finishing off those damn assassins . You'll want a few of these, but make sure you keep them protected. Bowmen : ( B ) Good unit to have. They can attack grunts and trolls (although you'd want mages to attack trolls ) without retaliation, and can defend against assassins in the night. They can also take out any wolves that stray too far from the main orchish army. They're not as good as mages , but they're cheaper and tougher. Cavalryman : ( B ) cavalryman are superior to their wolves and can hold ground against grunts and trolls reasonably well. It's also a good idea if your opponent likes to spam lots of assassins , to let them be the target of their poison; cavalryman and can run away and heal. Your heavy infantrymen ? Not so much. Beware though, of the cheap orchish archer, as cavalrymen are weak to pierce. Heavy Infantryman : ( B- ) Heavy infantryman are heavily resistant to physical attacks like blade, pierce and to a lesser degree, impact. You'd think this would make them great units against northerners, if the orchish archer didn't have a ranged fire attack. heavy infantrymen are also much too slow too effectively deal with poison, and that's a 19 gold unit that can't fight. However, they are useful in defense if you opponent hasn't spammed many archers, and they can even be useful in attack to crush up injured grunts . Horseman : ( B ) The horseman is a little controversial in this matchup. northerners are cheap and melee orientated, which is exactly what horseman do badly against. They're also quite tough, which means one hit kills are rare. However, they have one very important advantage over the northerners- high damage per hex. A horseman is very useful for softening up units or outright killing them (particularly when paired with a mage) which will be important in breaking ZoC lines, which can be a real pain with all the units northerners can field. Get one or two, but no more, else it quickly produces diminishing returns. Fencer : ( C+ ) The fencer is a melee based unit that is fragile against blade attacks of grunts , which means they don't have an awful lot of fighting effectiveness with them. On the other hand, the fencer's skirmisher ability is really valuable with the many northener units, and can finish off injured archers or go on sneaky village grabbing incursions. One or two might be useful, but no more, this unit is not a serious fighter! Make sure you keep them on 70% terrain if you can as well. Two hitter grunts can have trouble taking them out. Merman Fighter : ( C+ ) Water based melee unit that doesn't do well against the orc nagas , recruit only if there's lots of water and keep them in defendible terrain, else they'll quickly be killed. As usual, recruit cavalrymen and mermen depending on map (3-4 cavalryman for larger maps, 2-3 for medium sized ones and probably no more than one or two for small maps). Recruit plenty of spearmen and attack at day. Speaking of that, make sure you travel in large groups and do not attack with small numbers even at day. Northerners will swarm you too easily. Also, make sure you don't overextend yourself at day, as northerners do a lot more damage at night. Pay careful attention to your mages , they will be the ones the northerners will be going after, so retreat them quickly. When defending, try to avoid attacking units that can retaliate, unless you have a reasonably high chance of killing them that turn. Avoid attacking trolls if you can't heavily damage them or outright kill them, they're regenaration will quickly mean they can heal from any scratches your bowmen might have dealt them.","title":"Loyalist"},{"location":"wesnoth_loyalist/#how-to-play-loyalists","text":"Loyalists are considered to be the most versatile faction in the game. They have most available unit types to recruit (8), more than any other faction. Loyalists are mostly melee oriented faction, with only two ranged unit types, the mage and the bowman , but their ranged units play a significant role in the loyalists strategy and have to be used effectively. About the melee troops the loyalist player gets to choose among: Heavy Infantrymen : Few strikes, high damage per strike, slow, higher hit-points, good resistances, low defenses, deals impact damage which is good against undead. Spearmen : Average strikes, average damage per strike, average movement, medium hit-points, normal resistances, average defenses, deals pierce damage which is good against drakes, has a weak ranged attack. Fencer : High number of strikes, low damage per strike, quick, low hit-points, bad resistances, good defenses, deals blade damage which is good against elusive foots or wose, deals less total damage than the other two, is a skirmisher. The loyalists unit price ranges from the cheap spearmen and mermen (14 gold) to the expensive horsemen (23 gold). Loyalist units generally have good hit-points and they deal good damage, even the cheap spearmen and bowmen . Even their scouts have high hit-points and better melee attack compared to most of the scouts of the other factions. But on the other hand, the loyalist scouts have lower defenses than most of the other scouts and they do not have a ranged attack. The loyalists have some units with good resistances, like Heavy Infantry or Cavalryman , which can be very good for defending if you notice that your opponent doesn't have the proper units to counter them. Other units, like the bowman , who is more vulnerable than the previous two, but is also good for defense because it has better defense, is cheaper and it deals solid damage back in both melee and range. Similar goes for the spearman . When attacking , Loyalists have units which can be pretty devastating, like the horseman for attacking enemy mages and other ranged units, mages against the entrenched melee units, fencers for killing enemy injured and valuable units trying to heal behind their lines. But the mentioned units are also very vulnerable and they can die very easily if not properly supported. The loyalists as a faction generally have average defenses (they do not get 70% defense anywhere, with the exception of the fencer, which is a special case) and they are not a terrain dependent faction (like the Elves or Knalgans are). Therefore, even if it good to put spearmen and bowmen on mountains, villages and castles where they get 60% defense, you should also take the faction you are facing into account when deciding where to place your units. For example, if you are fighting Knalgans, you should try not to move your units next to hills and mountains and/or take the hills and mountains with your units and leave the forest to the dwarves. All the loyalist units are lawful, which obviously leads to the conclusion that the loyalists should be more aggressive and attack at day (or even start at dawn, because your opponent in most of the cases will be reluctant to attack too aggressively and leave units on bad ground when the morning arrives) while at night the loyalists should at least play conservatively, or in some cases even run away. Of course, that is also dependent upon the faction of your opponent. The greatest weakness of the loyalists is their limited mobility. The loyalists' units have normal movement, some of their units like the horseman , cavalryman , merman fighter and the fencer are pretty quick when they are in their favorable environment, but all their units get slowed over rough terrain a lot compared to the other factions' units . Half of the loyalists units can not even move on/over mountains, and their scouts get significantly slowed in forest, for example. This lack of mobility will often give your opponent an opportunity to outmaneuver you in the game.To prevent that, sometimes it is good to attack hard on a certain point and defend only with minimal amount of units elsewhere, rather than uniformly spreading your units around the front. When you don't know which faction you are facing, it is good to get a mixture of units. On most of the default 1v1 maps, this initial recruitment list would be good : spearman , bowman , fencer , mage , merman fighter , cavalryman . As always, you should carefully consider about the exact placement of each individual unit from the initial recruits on the castle to come up with the most optimal combination that will allow you the quickest village grabbing in the first turns of the game.","title":"How to play loyalists"},{"location":"wesnoth_loyalist/#loyalists-vs-undead","text":"When fighting undead, your recruiting pattern will depend on whether your opponent spams skeleton archers or dark adepts for ranged attacks. If you see lots of skeletons, you'll have to focus heavily on heavy infrantrymen and mages . If you see lots of dark adepts , you'll want some cavalry , horsemen and maybe some spearmen . Bowman : ( D- ) Almost entirely useless. You might use them to poke at walking corpses , ghouls , vampire bats and ghosts , but mages are much better at all of these things. I would only buy a bowman to counter a large zombie horde. Cavalryman : ( B+ ) You'll want some cavalry for scouting and dealing initial damage to dark adepts so that a horseman can finish them. They are also decent at fighting Skeletons in the daytime, because cavalry are blade-resistant. However, the cheap skeleton archers will really ruin cavalry quickly at night, so if there are any skeleton archers on the other side you won't be able to use cavalry to hold territory. If your opponent over-recruits dark adepts , however, you can use cold-resistant cavalry to hold territory against them. They are also decent for holding territory against ghouls because they can run away and heal (unlike your heavy infantrymen ). Fencer : ( C- ) Fencers are a bad recruit in this match up because they are vulnerable to the blade and pierce damage of skeletons and cannot damage them much in return. They also are incapable of holding territory against dark adepts , who cut right through the high defense of fencers . However, you may want to have a fencer or two around for trapping dark adepts or getting in that last hit. With luck, they may also be able to frustrate non-magical attacks from skeletons and the like. Heavy Infantryman : ( B+ ) You need heavy infantry to hold territory against skeletons and skeleton archers , and they will be your unit of choice for dealing melee damage, especially to the cheap skeleton archers . Any heavy infantrymen in the daytime can kill a full health skeleton archer in two hits, while a strong heavy infantrymen in the daytime can kill a full health skeleton in two hits, dealing 18 damage per strike (even a mage in daytime cannot kill a skeleton in one round, unless supported by a lieutenant ). A fearless heavy infantryman may be dangerous even at night. If you don't have enough heavy infantryman to go around, you can get your initial hits in on a skeleton archer with them and finish him with a mage . Just keep heavy infantrymen away from dark adepts , and only let ghouls hit heavy infantrymen if they are on a village (or has a white mage or other healer next to him), since they can't easily run away to heal. Also beware walking corpses , which deal a surprising amount of damage at all times of day since heavy infantrymen can't dodge and impact damage goes around their resistances, and the ranged cold attack of ghosts . The biggest problem with heavy infantrymen is they are slow, which means they're hard to retreat at night and hard to advance in day. Without shielding units they'll get trapped and killed, and if you have to shield a unit maybe it should be a mage instead. Horseman : ( B ) - Because they deal pierce damage, horsemen may not be very useful when faced with skeletons . However, if your opponent over-recruits dark adepts , horsemen can be extremely useful, as dark adepts deal no return damage to the normally risky charge attack. horsemen can even be used to finish skeleton archers , their nemesis, in the daytime. However, if your opponent recruits enough skeleton archers you will have a hard time shielding your horsemen from their devastating pierce attacks, and skeleton archers are dirt cheap. horsemen can also one-shot bats and zombies , which can be useful if you need to clear out a lot of level 0 units quickly. I would want to have at least one horseman around to keep my opponent from getting too bold with dark adepts , if not more. Your opponent will be forced to recruit dark adepts if you have heavy infantrymen in the field. Mage : ( A+ ) mages are an absolute necessity against Undead. If you do not have mages it will be almost impossible for you to kill ghosts , but with mages it's a piece of cake. mages are the best unit for killing almost everything Undead can throw at you, and can even be used to finish dark adepts in the daytime. Your main problem is that dark adepts are cheaper and deal almost as much damage, so your opponent can spam dark adepts while you cannot afford to spam mages . You will also have the difficult task of shielding fragile, expensive mages against lots of cheap Undead units. Your opponent will use skeletons and ghouls to attack your mages when he can, but bats , zombies or just about any other unit will do for killing your mages in a pinch. Shield your mages well, surround them with damage soakers and if you can deliver them safely to their targets you'll be able to clear out the Undead quickly. Merman Fighter : ( C- ) Mermen make a decent navy against Undead, since bats and ghosts will have a hard time killing them with their high defense. Even dark adepts will find Mermen annoying because of their 20% cold resistance. However, Mermen will have a hard time hurting anything the Undead have with their lame pierce weapon. Generally Mermen are only good for holding water hexes and scouting, but don't underestimate how useful that can be. Some well-placed Mermen on a water map can prevent bats from sneaking behind your lines and capturing villages or killing injured units. Even on mostly land maps, a Merman in a river can help hold a defensive line, or a quick Merman can use a river to slip behind the enemy to trap dark adepts or other units that are trying to escape at daytime. Spearman : ( C- ) Spearmen are mostly useful as cheap units for holding villages and occupying space when faced with dark adepts or skeleton archers . (You'll want to avoid letting dark adepts hit your heavy infantrymen because of their vulnerability to cold.) However, you don't really want spearmen to take hits from dark adepts , it would be better to let the cold-resistant cavalry absorb the damage. The only units spearmen are good for attacking are dark adepts and walking corpses . spearmen are completely useless against skeletons unless you level one into a swordsman , and even then they're pretty mediocre. However, if there are lots of skeleton archers you won't be able to use much cavalry or horsemen , so a spearman or two may be necessary as defenders and damage soakers even if they are lousy at dealing damage to Undead.","title":"Loyalists vs. Undead"},{"location":"wesnoth_loyalist/#loyalists-vs-rebels","text":"Rebels are a very versatile faction like the Loyalists. So you need to counter their versatility with yours. Their weakest trait is low hit-points. That is compensated somewhat by relatively higher defenses and mobility, so you'll need a combination of hard-hitters and speed. mages are also nice for their magical attacks. Anyways, the key for a Loyalist general is to effectively use his hard-hitting units and kill as many units as he can at day. And at night, to defend well as elves ignore the time of day. The smartest thing you can do upon discovering that your opponent is playing Rebels, in multiplayer, is to assume that your opponent has woses and recruit accordingly. woses are a necessary counter to Loyalists' daytime attacks and as soon as your opponent realizes that you are Loyalists he will almost certainly recruit woses . Remember that just because you don't see any woses doesn't mean there aren't a couple hiding in the forest! To fight woses you will need cavalry as meat shields, mages to deal damage, and maybe fencers to get the last hit in. Sadly, cavalry are very vulnerable to elvish archers , so you'll need to make sure that the archers die, which can be difficult if they are in forest. Get horsemen which can one-shot archers (despite their high dodge) and keep up with your cavalry . If one horseman misses, follow up with another one, it's a lucky archer that can dodge 4/4 one-shot attempts. heavy infantrymen are also great against everything but woses and mages , and will reduce the effectiveness of archer spam. spearmen are useless against woses , recruit them sparingly and keep them away from unscouted forest. An alternative way of playing this match up is to maximize the mobility offered by the Loyalists' fastest units, namely the two horse units. By using them to outrun the Rebel units you can achieve some devastating rushes. Some matches require use of both styles of playing. Again, their versatility must be countered with yours. Bowman : ( C- ) Rebels in general are effective in both melee and ranged attacks, so recruiting a ranged unit is less beneficial than most factions because most of the Rebels' units can retaliate against you. They can be useful for taking out an Scout or two, but otherwise, this is not really a smart buy. Cavalryman : ( B+ ) A good scout and an effective counter to woses . Watch out for archers, though, they can really tear horses to shreds. And as always, they are effective against ranged oriented units like the mage . ( A ) If you're going for some faster action, cavalrymen are vital in the attack. They do great damage during the day, and combined with their dangerous mobility, they can be a fearsome unit indeed. Just be careful of archers ZoCing you and tearing the unit to shreds. Fencer : ( B+ ) The fencer shares the 70% defense in the forest like most of the elves, but it has negative resistances. Theoretically, they should be effective against woses , but more often than not woses will crush them easily. In spite of this, Fencers still have skirmsher, which means that they can sneak behind an injured unit and finish them off. Do not over-recruit them, as enemy mages will tear though their high defense. Heavy Infantryman : ( C- ) Usefulness similar to an archer. It's too slow to deal with most of the Rebel units, and it is owned by Mages , woses , and even archers. Even though it has a high damage potential and good resistances, because of its inherently low defense shamans can easily get hits on it and turn it into a piece of metal for a turn. Horseman : ( B ) One horseman may be nice, but no more than that. Enemy archers will tear them apart and shamans totally mess them up. They're expensive too. Their greatest use is probably killing mages or an archer that has gone slightly off-track. ( A- ) Again, when mobility is required, these units need to come and do serious damage. Since Rebels are mostly comprised of low-hp units, horsemen at day can usually rip them apart. Again be careful of archers, for this unit is worth 23 gold. Shield him properly if he is damaged. Mage : ( A- ) You'll need these guys to tear though those high elvish 60-70% evasion in the forest; but be careful, archers and shamans will retaliate and some pretty nasty things may come from that. One purely positive thing though, they just absolutely destroy woses . 13-3 at day. Ouch. mages are expensive and fragile though, so keep them protected. Merman Fighter : ( B+ ) If the map has a lot of water, maybe recruit a few to prevent the Rebel's Mermen Hunters from taking over the waters. Otherwise they don't really contribute much else. Spearman - A - A necessity. To defend at night, to kill pretty much anything except for woses , and to be cheap and cost only 14 gold. These guys pretty much tear though most of the Rebel units, if it were not only for the high-defense archer and shaman . Get a bunch, and move them like a wall against the enemy units. At the start of the game, recruit 1-2 cavalrymen , depending on the map size, 1 mage , (1 merman fighter if you need some water coverage), maybe 1 fencer , and the rest spearman . Later on you maybe can recruit some horseman if you opponent recruits mass mages , or more cavalrymen and mages if he masses woses . Otherwise, spearmen and mages should help you get through most of the match. If you're going for speed, recruit 2-3 cavalrymen , depending on the map, 1 horseman , maybe 1 fencer , and the rest spearmen . Use your spearmen to fill the holes and consolidate the territory that the horses took over. Use the horses to outrun the archers and attack when they can.","title":"Loyalists vs. Rebels"},{"location":"wesnoth_loyalist/#loyalists-vs-northerners","text":"The main problem you will have when facing northerners are two things: poison and numbers. If northerners didn't have assassins , it wouldn't be much of a big deal; you could out manoeuvre them with your superior scouting units and skirmishers to grab their villages and then finish them off one by one with spearmen / bowmen . Thing is, assassins poison makes defending at night extremely difficult, and by poisoning your units, they often force you to retreat and heal, or die due to retaliation. The key to winning this match up is to use your superior versatility; the northerners greatest weakness is that ALL of their units are very average and don't often have many special abilities; you have masses. Use your first strike in spearmen to defend against grunts , trolls and wolfriders (even at night), use your magic to finish off wounded assassins , and you can charge any of their wounded units and kill them with one hit. Since all of your units are lawful and theirs are chaotic, you'll want to press your advantage at day and kill as much as possible (but don't attack assassins with bowmen ; you don't want to be poisoned and be forced to retreat in your daytime assault). Another problem the northerners have is that all though their units are many in number, they do not have a very high damage per hex ratio. Use this to your advantage; travel in packs and use ZoC lines to prevent your units from being swarmed too easily, and keep your vulnerable units like mages behind damage soakers like spearmen or heavy infantry . If you can, try and make night time defenses near rivers; northerners will hurt themselves when they attack you, as they have low defense in water and are very annoyed by deep water, because it prevents them from easily swarming you. Spearmen : ( A- ) Pretty much your best unit against northerners. Their first strike ability is great in defense and their attack power to cost ratio is quite high. They also have good health and can retaliate in ranged. As in many matchups, the bulk of your army. Mage : ( B+ ) The mage is fragile and expensive and has a weak melee attack, which is the exact opposite of northerners. Not a great defender unless your opponent goes mad with assassins. However, they have lots of attack power at day and are very useful for nocking trolls of their perches and finishing off those damn assassins . You'll want a few of these, but make sure you keep them protected. Bowmen : ( B ) Good unit to have. They can attack grunts and trolls (although you'd want mages to attack trolls ) without retaliation, and can defend against assassins in the night. They can also take out any wolves that stray too far from the main orchish army. They're not as good as mages , but they're cheaper and tougher. Cavalryman : ( B ) cavalryman are superior to their wolves and can hold ground against grunts and trolls reasonably well. It's also a good idea if your opponent likes to spam lots of assassins , to let them be the target of their poison; cavalryman and can run away and heal. Your heavy infantrymen ? Not so much. Beware though, of the cheap orchish archer, as cavalrymen are weak to pierce. Heavy Infantryman : ( B- ) Heavy infantryman are heavily resistant to physical attacks like blade, pierce and to a lesser degree, impact. You'd think this would make them great units against northerners, if the orchish archer didn't have a ranged fire attack. heavy infantrymen are also much too slow too effectively deal with poison, and that's a 19 gold unit that can't fight. However, they are useful in defense if you opponent hasn't spammed many archers, and they can even be useful in attack to crush up injured grunts . Horseman : ( B ) The horseman is a little controversial in this matchup. northerners are cheap and melee orientated, which is exactly what horseman do badly against. They're also quite tough, which means one hit kills are rare. However, they have one very important advantage over the northerners- high damage per hex. A horseman is very useful for softening up units or outright killing them (particularly when paired with a mage) which will be important in breaking ZoC lines, which can be a real pain with all the units northerners can field. Get one or two, but no more, else it quickly produces diminishing returns. Fencer : ( C+ ) The fencer is a melee based unit that is fragile against blade attacks of grunts , which means they don't have an awful lot of fighting effectiveness with them. On the other hand, the fencer's skirmisher ability is really valuable with the many northener units, and can finish off injured archers or go on sneaky village grabbing incursions. One or two might be useful, but no more, this unit is not a serious fighter! Make sure you keep them on 70% terrain if you can as well. Two hitter grunts can have trouble taking them out. Merman Fighter : ( C+ ) Water based melee unit that doesn't do well against the orc nagas , recruit only if there's lots of water and keep them in defendible terrain, else they'll quickly be killed. As usual, recruit cavalrymen and mermen depending on map (3-4 cavalryman for larger maps, 2-3 for medium sized ones and probably no more than one or two for small maps). Recruit plenty of spearmen and attack at day. Speaking of that, make sure you travel in large groups and do not attack with small numbers even at day. Northerners will swarm you too easily. Also, make sure you don't overextend yourself at day, as northerners do a lot more damage at night. Pay careful attention to your mages , they will be the ones the northerners will be going after, so retreat them quickly. When defending, try to avoid attacking units that can retaliate, unless you have a reasonably high chance of killing them that turn. Avoid attacking trolls if you can't heavily damage them or outright kill them, they're regenaration will quickly mean they can heal from any scratches your bowmen might have dealt them.","title":"Loyalists vs Northerners"},{"location":"wesnoth_northerners/","text":"Northerners are a faction of Orcs and their allies who live in the north of the Great Continent, thus their name. Northerners consist of the warrior orcs race, the enslaved goblins, trolls who are tricked into combat by the orcs, and the serpentine naga. The Northerners play best by taking advantage of having many low-cost and high HP soldiers.","title":"Northerners"},{"location":"wesnoth_rebels/","text":"Rebels are a faction of Elves and their various forest-dwelling allies. They get their human name, Rebels, from the time of Heir to the Throne, when they started the rebellion against the evil Queen Asheviere. Elves are a magical race that are masters of the bow and are capable of living many years longer than humans. In harmony with nature, the elves find allies with the human mages, certain merfolk, and tree creatures called woses. Rebels are best played taking advantage of their high forest defense, mastery of ranged attacks, and the elves' neutral alignment.","title":"Rebels"},{"location":"wish_list/","text":"This is a gathering of tools, ideas or services that I'd like to enjoy. If you have any lead, as smallest as it may be on how to fulfill them, please contact me . Self hosted search engine \u2691 It would be awesome to be able to self host a personal search engine that performs priorized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to priorize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage . Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or priorize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe meilisearch or searx could be a solution. Decentralized encrypted end to end VOIP and video software \u2691 I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own. Others \u2691 Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Hiking route classifier and rating self hosted web application. A command line friendly personal CRM like Monica that is able to register the time length and rating of interactions to do data analysis on my relations. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A way to store music numeric ratings through the command line compatible with mpd and beets . A mkdocs plugin to generate RSS feed on new or changed entries. An e-reader support that could be fixed to the wall.","title":"Wish list"},{"location":"wish_list/#self-hosted-search-engine","text":"It would be awesome to be able to self host a personal search engine that performs priorized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to priorize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage . Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or priorize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe meilisearch or searx could be a solution.","title":"Self hosted search engine"},{"location":"wish_list/#decentralized-encrypted-end-to-end-voip-and-video-software","text":"I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own.","title":"Decentralized encrypted end to end VOIP and video software"},{"location":"wish_list/#others","text":"Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Hiking route classifier and rating self hosted web application. A command line friendly personal CRM like Monica that is able to register the time length and rating of interactions to do data analysis on my relations. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A way to store music numeric ratings through the command line compatible with mpd and beets . A mkdocs plugin to generate RSS feed on new or changed entries. An e-reader support that could be fixed to the wall.","title":"Others"},{"location":"yamlfix/","text":"Yamlfix is a simple opinionated yaml formatter that keeps your comments. Install \u2691 pip install yamlfix Usage \u2691 Imagine we've got the following source code: book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky It has the following errors: There is no --- at the top. The indentation is all wrong. After running yamlfix the resulting source code will be: --- book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky yamlfix can be used both as command line tool and as a library. As a command line tool: $: yamlfix file.yaml As a library: from yamlfix import fix_files fix_files ([ 'file.py' ]) References \u2691 Git Docs","title":"Yamlfix"},{"location":"yamlfix/#install","text":"pip install yamlfix","title":"Install"},{"location":"yamlfix/#usage","text":"Imagine we've got the following source code: book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky It has the following errors: There is no --- at the top. The indentation is all wrong. After running yamlfix the resulting source code will be: --- book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky yamlfix can be used both as command line tool and as a library. As a command line tool: $: yamlfix file.yaml As a library: from yamlfix import fix_files fix_files ([ 'file.py' ])","title":"Usage"},{"location":"yamlfix/#references","text":"Git Docs","title":"References"},{"location":"architecture/database_architecture/","text":"Design a table to keep historical changes in database \u2691 The post suggests two ways of storing a history of changed data in a database table. This might be useful for example for undo functionality or to store the evolution of the attributes. Audit table : Record every single change in every field in an audit table. History table : Each time a change is recorded, the whole line is stored in a History table. Using an Audit table \u2691 The Audit table has the following schema. Column Name Data Type ID int Table varchar(50) Field varchar(50) RecordId int OldValue varchar(255) NewValue varchar(255) AddBy int AddDate date For example, there is a transaction looks like this: Id Description TransactionDate DeliveryDate Status 100 A short text 2019-09-15 2019-09-28 Shipping And now, another user with id 20 modifies the description to A not long text and DeliveryDate to 2019-10-01 . Id Description TransactionDate DeliveryDate Status 100 A not long text 2019-09-15 2019-10-01 Shipping The Audit table entries will look: Id Table Field RecordId OldValue NewValue AddBy AddDate 1 Transaction Description 100 A short text A not long text 20 2019-09-17 2 Transaction DeliveryDate 100 2019-09-28 2019-10-01 20 2019-09-17 And we'll update the original record in the Transaction table. Pros: It's easy to query for field changes. No redundant information is stored. Cons: Possible huge increase of records. Since every change in different fields is one record in the Audit table, it may grow drastically fast. In this case, table indexing plays a vital role for enhancing the querying performance. Suitable for tables with many fields where often only a few change. Using a history table \u2691 The History table has the same schema as the table we are saving the history from. Imagine a Transaction table with the following schema. Column Name Data Type ID int Description text TransactionDate date DeliveryDate date Status varchar(50) AddDate date When doing the same changes as the previous example, we'll introduce the old record into the History table, and update the record in the Transaction table. Pros: Simple query to get the complete history. Cons: Redundant information is stored. Suitable for: A lot of fields are changed in one time. Generating a change report with full record history is needed References \u2691 Decoupling database migrations from server startup","title":"Database Architecture"},{"location":"architecture/database_architecture/#design-a-table-to-keep-historical-changes-in-database","text":"The post suggests two ways of storing a history of changed data in a database table. This might be useful for example for undo functionality or to store the evolution of the attributes. Audit table : Record every single change in every field in an audit table. History table : Each time a change is recorded, the whole line is stored in a History table.","title":"Design a table to keep historical changes in database"},{"location":"architecture/database_architecture/#using-an-audit-table","text":"The Audit table has the following schema. Column Name Data Type ID int Table varchar(50) Field varchar(50) RecordId int OldValue varchar(255) NewValue varchar(255) AddBy int AddDate date For example, there is a transaction looks like this: Id Description TransactionDate DeliveryDate Status 100 A short text 2019-09-15 2019-09-28 Shipping And now, another user with id 20 modifies the description to A not long text and DeliveryDate to 2019-10-01 . Id Description TransactionDate DeliveryDate Status 100 A not long text 2019-09-15 2019-10-01 Shipping The Audit table entries will look: Id Table Field RecordId OldValue NewValue AddBy AddDate 1 Transaction Description 100 A short text A not long text 20 2019-09-17 2 Transaction DeliveryDate 100 2019-09-28 2019-10-01 20 2019-09-17 And we'll update the original record in the Transaction table. Pros: It's easy to query for field changes. No redundant information is stored. Cons: Possible huge increase of records. Since every change in different fields is one record in the Audit table, it may grow drastically fast. In this case, table indexing plays a vital role for enhancing the querying performance. Suitable for tables with many fields where often only a few change.","title":"Using an Audit table"},{"location":"architecture/database_architecture/#using-a-history-table","text":"The History table has the same schema as the table we are saving the history from. Imagine a Transaction table with the following schema. Column Name Data Type ID int Description text TransactionDate date DeliveryDate date Status varchar(50) AddDate date When doing the same changes as the previous example, we'll introduce the old record into the History table, and update the record in the Transaction table. Pros: Simple query to get the complete history. Cons: Redundant information is stored. Suitable for: A lot of fields are changed in one time. Generating a change report with full record history is needed","title":"Using a history table"},{"location":"architecture/database_architecture/#references","text":"Decoupling database migrations from server startup","title":"References"},{"location":"architecture/domain_driven_design/","text":"Domain-driven Design (DDD) is the concept that the structure and language of your code (class names, class methods, class variables) should match the business domain. Domain-driven design is predicated on the following goals: Placing the project's primary focus on the core domain and domain logic. Basing complex designs on a model of the domain. Initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems. It aims to fix these common pitfalls: When asked to design a new system, most developers will start to build a database schema, with the object model treated as an afterthought. Instead, behaviour should come first and drive our storage requirements . Business logic comes spread throughout the layers of our application, making it hard to identify, understand and change. The feared big ball of mud . They are avoided through: Encapsulation an abstraction : understanding behavior encapsulation as identifying a task that needs to be done in our code and giving that task to an abstraction, a well defined object or function. Encapsulating behavior with abstractions is a powerful decoupling tool by hiding details and protecting the consistency of our data, making code more expressive, more testable and easier to maintain. Layering : When one function, module or object uses another, we say that one depends on the other creating a dependency graph. In the big ball of mud the dependencies are out of control, so changing one node becomes difficult because it has the potential to affect many other parts of the system. Layered architectures are one way of tackling this problem by dividing our code into discrete categories or roles, and introducing rules about which categories of code can call each other. By following the Dependency Inversion Principle (the D in SOLID ), we must ensure that our business code doesn't depend on technical details, instead, both should use abstractions. We don't want high-level modules ,which respond to business needs, to be slowed down because they are closely coupled to low-level infrastructure details, which are often harder to change. Similarly, it is important to be able to change the infrastructure details when we need to without needing to make changes to the business layer. Domain modeling \u2691 Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses. It's set in a context and it's defined through ubiquitous language . A language structured around the domain model and used by all team members to connect all the activities of the team with the software. To successfully build a domain model we need to: Explore the domain language : Have an initial conversation with the business expert and agree on a glossary and some rules for the first minimal version of the domain model. Wherever possible, ask for concrete examples to illustrate each rule. Testing the domain models : Translate each of the rules gathered in the exploration phase into tests. Keeping in mind: The name of our tests should describe the behaviour that we want to see from the system. The test level in the testing pyramid should be chosen following the high and low gear metaphor . Code the domain modeling object : Choose the objects that match the behavior you are testing keeping in mind: The names of the classes, methods, functions and variables should be taken from the business jargon. Domain modeling objects \u2691 Value object : Any domain object that is uniquely identified by the data it holds, so it has no conceptual identity. They should be treated as immutable. We can still have complex behaviour in value objects. In fact, it's common to support operations, for example, mathematical operators. dataclasses are great for value objects because: They follow the value equality property (two objects with the same attributes are treated as equal). Can be defined as immutable with the frozen=True decorator argument. They define the __hash__ magic method based on the attribute values. __hash__ is used by Python to control the behaviour of objects when you add them to sets or use them as dict keys. @dataclass ( frozen = True ) class Name : first_name : str surname : str assert Name ( 'Harry' , 'Percival' ) == Name ( 'Harry' , 'Percival' ) assert Name ( 'Harry' , 'Percival' ) != Name ( 'Bob' , 'Gregory' ) Entity : An object that is not defined by it's attributes, but rather by a thread of continuity and it's identity. Unlike values, they have identity equality . We can change their values, and they are still recognizably the same thing. class Person : def __init__ ( self , name : Name ): self . name = name def test_barry_is_harry (): harry = Person ( Name ( \"Harry\" , \"Percival\" )) barry = harry barry . name = Namew ( \"Barry\" , \"Percival\" ) assert harry is barry and barry is harry We usually make this explicit in code by implementing equality operators on entities: class Person : ... def __eq__ ( self , other ): if not isinstance ( other , Person ): return False return other . identifier == self . identifier def __hash__ ( self ): return hash ( self . identifier ) Python's __eq__ magic method defines the behavior of the class for the == operator. For entities, the simplest option is to say that the hash is None , meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing __hash__ without modifying __eq__ is tricky business . Service : Functions that hold operations that don't conceptually belong to any object. We take advantage of the fact that Python is a multiparadigm language. Exceptions : Hold constrains imposed over the objects by the business. Domain modeling patterns \u2691 To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain so we can refactor aggressively. This is achieved through these design patterns: Repository pattern : An abstraction over the idea of persistent storage. Service Layer pattern : Clearly define where our use case begins and ends. Unit of work pattern : Provides atomic operations. Aggregate pattern : Enforces integrity of our data. Unconnected thoughts \u2691 Domain model refactor \u2691 Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed: If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the .add and .get methods to persist and read the models from the persistent storage solution. Create a FakeRepository with similar functionality to start building the Service Layer. Inspect the entrypoints of your program and for each orchestration action create a service (always tests first). Building blocks \u2691 Aggregate : A collection of objects that are bound together by a root entity, otherwise known as an aggregate root. The aggregate root guarantees the consistency of changes being made within the aggregate by forbidding external objects from holding references to it's members. Domain Event : A domain object that defines an event. Repository : Methods for retrieving domain objects should delegate to a specialized Repository object such that alternative storage implementations may be easily interchanged. Factory : Methods for creating domain objects should delegate to a specialized Factory object such that alternative implementations may be easily interchanged. Injection of fakes in edge to edge tests \u2691 If you are developing your program with this design pattern, you'll have fake versions of your adapters. When testing the edge to edge tests, you're going to use the fakes when there is no easy way to do a correct end to end test (if for example you need to bring up a service that is complex to configure). I've been banging my head against the keyboard until I've figured how to do it for click command line tests . References \u2691 Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Wikipedia article Further reading \u2691 awesome domain driven design Books \u2691 Domain-Driven Design by Eric Evans. Implementing Domain-Driven Design by Vaughn Vermon.","title":"Domain Driven Design"},{"location":"architecture/domain_driven_design/#domain-modeling","text":"Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses. It's set in a context and it's defined through ubiquitous language . A language structured around the domain model and used by all team members to connect all the activities of the team with the software. To successfully build a domain model we need to: Explore the domain language : Have an initial conversation with the business expert and agree on a glossary and some rules for the first minimal version of the domain model. Wherever possible, ask for concrete examples to illustrate each rule. Testing the domain models : Translate each of the rules gathered in the exploration phase into tests. Keeping in mind: The name of our tests should describe the behaviour that we want to see from the system. The test level in the testing pyramid should be chosen following the high and low gear metaphor . Code the domain modeling object : Choose the objects that match the behavior you are testing keeping in mind: The names of the classes, methods, functions and variables should be taken from the business jargon.","title":"Domain modeling"},{"location":"architecture/domain_driven_design/#domain-modeling-objects","text":"Value object : Any domain object that is uniquely identified by the data it holds, so it has no conceptual identity. They should be treated as immutable. We can still have complex behaviour in value objects. In fact, it's common to support operations, for example, mathematical operators. dataclasses are great for value objects because: They follow the value equality property (two objects with the same attributes are treated as equal). Can be defined as immutable with the frozen=True decorator argument. They define the __hash__ magic method based on the attribute values. __hash__ is used by Python to control the behaviour of objects when you add them to sets or use them as dict keys. @dataclass ( frozen = True ) class Name : first_name : str surname : str assert Name ( 'Harry' , 'Percival' ) == Name ( 'Harry' , 'Percival' ) assert Name ( 'Harry' , 'Percival' ) != Name ( 'Bob' , 'Gregory' ) Entity : An object that is not defined by it's attributes, but rather by a thread of continuity and it's identity. Unlike values, they have identity equality . We can change their values, and they are still recognizably the same thing. class Person : def __init__ ( self , name : Name ): self . name = name def test_barry_is_harry (): harry = Person ( Name ( \"Harry\" , \"Percival\" )) barry = harry barry . name = Namew ( \"Barry\" , \"Percival\" ) assert harry is barry and barry is harry We usually make this explicit in code by implementing equality operators on entities: class Person : ... def __eq__ ( self , other ): if not isinstance ( other , Person ): return False return other . identifier == self . identifier def __hash__ ( self ): return hash ( self . identifier ) Python's __eq__ magic method defines the behavior of the class for the == operator. For entities, the simplest option is to say that the hash is None , meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing __hash__ without modifying __eq__ is tricky business . Service : Functions that hold operations that don't conceptually belong to any object. We take advantage of the fact that Python is a multiparadigm language. Exceptions : Hold constrains imposed over the objects by the business.","title":"Domain modeling objects"},{"location":"architecture/domain_driven_design/#domain-modeling-patterns","text":"To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain so we can refactor aggressively. This is achieved through these design patterns: Repository pattern : An abstraction over the idea of persistent storage. Service Layer pattern : Clearly define where our use case begins and ends. Unit of work pattern : Provides atomic operations. Aggregate pattern : Enforces integrity of our data.","title":"Domain modeling patterns"},{"location":"architecture/domain_driven_design/#unconnected-thoughts","text":"","title":"Unconnected thoughts"},{"location":"architecture/domain_driven_design/#domain-model-refactor","text":"Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed: If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the .add and .get methods to persist and read the models from the persistent storage solution. Create a FakeRepository with similar functionality to start building the Service Layer. Inspect the entrypoints of your program and for each orchestration action create a service (always tests first).","title":"Domain model refactor"},{"location":"architecture/domain_driven_design/#building-blocks","text":"Aggregate : A collection of objects that are bound together by a root entity, otherwise known as an aggregate root. The aggregate root guarantees the consistency of changes being made within the aggregate by forbidding external objects from holding references to it's members. Domain Event : A domain object that defines an event. Repository : Methods for retrieving domain objects should delegate to a specialized Repository object such that alternative storage implementations may be easily interchanged. Factory : Methods for creating domain objects should delegate to a specialized Factory object such that alternative implementations may be easily interchanged.","title":"Building blocks"},{"location":"architecture/domain_driven_design/#injection-of-fakes-in-edge-to-edge-tests","text":"If you are developing your program with this design pattern, you'll have fake versions of your adapters. When testing the edge to edge tests, you're going to use the fakes when there is no easy way to do a correct end to end test (if for example you need to bring up a service that is complex to configure). I've been banging my head against the keyboard until I've figured how to do it for click command line tests .","title":"Injection of fakes in edge to edge tests"},{"location":"architecture/domain_driven_design/#references","text":"Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Wikipedia article","title":"References"},{"location":"architecture/domain_driven_design/#further-reading","text":"awesome domain driven design","title":"Further reading"},{"location":"architecture/domain_driven_design/#books","text":"Domain-Driven Design by Eric Evans. Implementing Domain-Driven Design by Vaughn Vermon.","title":"Books"},{"location":"architecture/microservices/","text":"Microservices are an application architecture style where independent, self-contained programs with a single purpose each can communicate with each other over a network. Typically, these microservices are able to be deployed independently because they have strong separation of responsibilities via a well-defined specification with significant backwards compatibility to avoid sudden dependency breakage. References \u2691 Fullstackpython introduction to microservices Books \u2691 Hand-On Docker for Microservices with Python by Jaime Buelta : Does a good job defining the whole process of building a microservices python application, from the microservice concept to the definition of the CI, integration testing, deployment in Kubernetes, definition of logging and metrics. But it doesn't help much with the project layout definition or if you want to build your application while following it. Hands-On RESTful Python Web Services by Gaston C.Hillar : I didn't like it at all.","title":"Microservices"},{"location":"architecture/microservices/#references","text":"Fullstackpython introduction to microservices","title":"References"},{"location":"architecture/microservices/#books","text":"Hand-On Docker for Microservices with Python by Jaime Buelta : Does a good job defining the whole process of building a microservices python application, from the microservice concept to the definition of the CI, integration testing, deployment in Kubernetes, definition of logging and metrics. But it doesn't help much with the project layout definition or if you want to build your application while following it. Hands-On RESTful Python Web Services by Gaston C.Hillar : I didn't like it at all.","title":"Books"},{"location":"architecture/orm_builder_query_or_raw_sql/","text":"Databases are the core of storing state for almost all web applications. There are three ways for a programming application to interact with the database. After reading this article, you'll know which are the advantages and disadvantages of using the different solutions. Raw SQL \u2691 Raw SQL, sometimes also called native SQL, is the most basic, most low-level form of database interaction. You tell the database what to do in the language of the database. Most developers should know basics of SQL. This means how to CREATE tables and views, how to SELECT and JOIN data, how to UPDATE and DELETE data. Excels: Flexibility : As you are writing raw SQL code, you are not constrained by higher level abstractions. Performance : You can use engine specific tricks to increase the performance and your queries will probably be simpler than the higher abstraction ones. Magic free : It's easier to understand what your code does, as you scale up in the abstraction level, magic starts to appear which is nice if everything goes well, but it backfires when you encounter problems. No logic coupling : As your models are not linked to the way you interact with the storage solution, it's easier to define a clean software architecture that follows the SOLID principles, which also allows to switch between different storage approaches. Cons: SQL Injections : As you are manually writing the queries, it's easier to fall into these vulnerabilities. Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. Query Extension : If you have an analytical query, it's nice if you can apply slight modifications to it. It\u2019s possible to extend a query when you have raw SQL, but it\u2019s cumbersome. You need to touch the original query and add placeholders. Editor support : As it's interpreted as a string in the editor, your editor is not able to detect typos, syntax highlight or auto complete the SQL code. SQL knowledge : You need to know SQL to interact with the database. Database Locking : You might use features which are specific to that database, which makes a future database switch harder. Query builder \u2691 Query builders are libraries which are written in the programming language you use and use native classes and functions to build SQL queries. Query builders typically have a fluent interface , so the queries are built by an object-oriented interface which uses method chaining. query = Query . from_ ( books ) \\ . select ( \"*\" ) \\ . where ( books . author_id == aid ) Pypika is an example for a Query Builder in Python. Note that the resulting query is still the same as in the raw code, built in another way, so abstraction level over using raw SQL is small. Excels: Performance : Same performance as using raw SQL. Magic free : Same comprehension as using raw SQL. No logic coupling : Same coupling as using raw SQL. Query Extension : Given the fluent interface, it's easier to build, extend and reuse queries. Mitigates: Flexibility : You depend on the builder implementation of the language you are trying to use, but if the functionality you are trying to use is not there, you can always fall back to raw SQL. SQL Injections : Query builders have mechanism to insert parameters into the queries in a safe way. Editor support : The query builder prevents typos in the offered parts \u2014 .select , .from_ , .where , and as it's object oriented you have better syntax highlight and auto completion. Database Locking : Query builders support different databases make database switch easier. Cons: Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. SQL knowledge : You need to know SQL to interact with the database. Query builder knowledge : You need to know the library to interact with the database. ORM \u2691 ORMs create an object for each database table and allows you to interact between related objects, in a way that you can use your object oriented programming to interact with the database even without knowing SQL. SQLAlchemy is an example for an ORM in Python. This way, there is a language-native representation and thus the languages ecosystem features such as autocomplete and syntax-highlighting work. Excels: Change management : ORM come with helper programs like Alembic which can automatically detect when your models changed compared to the last known state of the database, thus it's able to create schema migration files for you. Query Extension : They have a fluent interface used and developed by a lot of people, so it may have better support than query builders. SQL Injections : As the ORM builds the queries by itself and it maintained by a large community, you're less prone to suffer from this vulnerabilities. Editor support : As you are interacting with Python objects, you have full editor support for highlighting and auto-formatting, which reduces the maintenance by making the queries easier to read Database Locking : ORM fully support different databases, so it's easy to switch between different database solutions. Mitigates: SQL knowledge : In theory you don't need to know SQL, in reality, you need to have some basic knowledge to build the tables and relationships, as well as while debugging. Cons: Flexibility : Being the highest level of abstraction, you are constrained by what the ORM solution offers, allowing you to write raw SQL and try to give enough features, so you don't notice it unless you're writing complex queries. Performance : When you run queries with ORMs, you tend to get more than you need. This is translated in fetching more information and executing more queries than the other solutions. You can try to tweak it but it can be tricky, making it easy to create queries which are wrong in a subtle way. They also encounter the N+1 problem, where you potentially run more queries than you need to fetch the same result. It's all magic : ORMs are complex high level abstractions, so when you encounter errors or want to change the default behaviour, you're going to have a bad time. Big coupling : ORM models already contain all the data you need, so you will be tempted to use it outside of database related code, which introduces a tight coupling between your business model and the storage solution, which decreases flexibility when changing storage drivers, makes testing harder, leads to software architectures that induce the big ball of mud by getting further from the SOLID principles. SQLAlchemy still supports the use of classical mappings between object models and ORM definitions, but I've read that it's a feature that it's not going to be maintained, as it's not the intended way of using it. Even with them I've had issues when trying to integrate the models with pydantic( 1 , 2 . Learn the ORM : ORMs are complex to learn, they have lots of features and different ways to achieve the same result, so it's hard to learn how to use them well, and usually there is no way to fulfill all your needs. Configure the ORM : I've had a hard time understanding the correct way to configure database connection inside a packaged python program, both for the normal use and to define the test environment. I've first learned using the declarative way, and then I had to learn all over again for the classical mapping required by the use of the repository pattern . Conclusion \u2691 Query Builders live in the sweet spot in the abstraction plane. They give enough abstraction to ease the interaction with the database and mitigating security vulnerabilities while retaining the flexibility, performance and architecture cleanness of using raw SQL. Although they require you to learn SQL and the query builder library, it will pay off as you develop your programs. In the end, if you don't expect to use this knowledge in the future, you may better use pandas to your small project than a SQL solution. Query builders should be used when: You don't mind spending some time learning SQL. You plan to develop and maintain complex or different projects that use SQL to store data. Raw SQL should be used when: You don't want to learn SQL and need to create a small script that needs to perform a specific task. ORMs should be used when: Small projects where the developers are already familiar with the ORM. Maintaining existing ORM code, although migrating to query builders should be evaluated. If you do use an ORM, use the repository pattern through classical mappings to split the storage logic from the business one. References \u2691 Raw SQL vs Query Builder vs ORM by Martin Thoma ORMs vs Plain SQL in Python by Koby Bass","title":"ORM, Query Builder or Raw SQL"},{"location":"architecture/orm_builder_query_or_raw_sql/#raw-sql","text":"Raw SQL, sometimes also called native SQL, is the most basic, most low-level form of database interaction. You tell the database what to do in the language of the database. Most developers should know basics of SQL. This means how to CREATE tables and views, how to SELECT and JOIN data, how to UPDATE and DELETE data. Excels: Flexibility : As you are writing raw SQL code, you are not constrained by higher level abstractions. Performance : You can use engine specific tricks to increase the performance and your queries will probably be simpler than the higher abstraction ones. Magic free : It's easier to understand what your code does, as you scale up in the abstraction level, magic starts to appear which is nice if everything goes well, but it backfires when you encounter problems. No logic coupling : As your models are not linked to the way you interact with the storage solution, it's easier to define a clean software architecture that follows the SOLID principles, which also allows to switch between different storage approaches. Cons: SQL Injections : As you are manually writing the queries, it's easier to fall into these vulnerabilities. Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. Query Extension : If you have an analytical query, it's nice if you can apply slight modifications to it. It\u2019s possible to extend a query when you have raw SQL, but it\u2019s cumbersome. You need to touch the original query and add placeholders. Editor support : As it's interpreted as a string in the editor, your editor is not able to detect typos, syntax highlight or auto complete the SQL code. SQL knowledge : You need to know SQL to interact with the database. Database Locking : You might use features which are specific to that database, which makes a future database switch harder.","title":"Raw SQL"},{"location":"architecture/orm_builder_query_or_raw_sql/#query-builder","text":"Query builders are libraries which are written in the programming language you use and use native classes and functions to build SQL queries. Query builders typically have a fluent interface , so the queries are built by an object-oriented interface which uses method chaining. query = Query . from_ ( books ) \\ . select ( \"*\" ) \\ . where ( books . author_id == aid ) Pypika is an example for a Query Builder in Python. Note that the resulting query is still the same as in the raw code, built in another way, so abstraction level over using raw SQL is small. Excels: Performance : Same performance as using raw SQL. Magic free : Same comprehension as using raw SQL. No logic coupling : Same coupling as using raw SQL. Query Extension : Given the fluent interface, it's easier to build, extend and reuse queries. Mitigates: Flexibility : You depend on the builder implementation of the language you are trying to use, but if the functionality you are trying to use is not there, you can always fall back to raw SQL. SQL Injections : Query builders have mechanism to insert parameters into the queries in a safe way. Editor support : The query builder prevents typos in the offered parts \u2014 .select , .from_ , .where , and as it's object oriented you have better syntax highlight and auto completion. Database Locking : Query builders support different databases make database switch easier. Cons: Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. SQL knowledge : You need to know SQL to interact with the database. Query builder knowledge : You need to know the library to interact with the database.","title":"Query builder"},{"location":"architecture/orm_builder_query_or_raw_sql/#orm","text":"ORMs create an object for each database table and allows you to interact between related objects, in a way that you can use your object oriented programming to interact with the database even without knowing SQL. SQLAlchemy is an example for an ORM in Python. This way, there is a language-native representation and thus the languages ecosystem features such as autocomplete and syntax-highlighting work. Excels: Change management : ORM come with helper programs like Alembic which can automatically detect when your models changed compared to the last known state of the database, thus it's able to create schema migration files for you. Query Extension : They have a fluent interface used and developed by a lot of people, so it may have better support than query builders. SQL Injections : As the ORM builds the queries by itself and it maintained by a large community, you're less prone to suffer from this vulnerabilities. Editor support : As you are interacting with Python objects, you have full editor support for highlighting and auto-formatting, which reduces the maintenance by making the queries easier to read Database Locking : ORM fully support different databases, so it's easy to switch between different database solutions. Mitigates: SQL knowledge : In theory you don't need to know SQL, in reality, you need to have some basic knowledge to build the tables and relationships, as well as while debugging. Cons: Flexibility : Being the highest level of abstraction, you are constrained by what the ORM solution offers, allowing you to write raw SQL and try to give enough features, so you don't notice it unless you're writing complex queries. Performance : When you run queries with ORMs, you tend to get more than you need. This is translated in fetching more information and executing more queries than the other solutions. You can try to tweak it but it can be tricky, making it easy to create queries which are wrong in a subtle way. They also encounter the N+1 problem, where you potentially run more queries than you need to fetch the same result. It's all magic : ORMs are complex high level abstractions, so when you encounter errors or want to change the default behaviour, you're going to have a bad time. Big coupling : ORM models already contain all the data you need, so you will be tempted to use it outside of database related code, which introduces a tight coupling between your business model and the storage solution, which decreases flexibility when changing storage drivers, makes testing harder, leads to software architectures that induce the big ball of mud by getting further from the SOLID principles. SQLAlchemy still supports the use of classical mappings between object models and ORM definitions, but I've read that it's a feature that it's not going to be maintained, as it's not the intended way of using it. Even with them I've had issues when trying to integrate the models with pydantic( 1 , 2 . Learn the ORM : ORMs are complex to learn, they have lots of features and different ways to achieve the same result, so it's hard to learn how to use them well, and usually there is no way to fulfill all your needs. Configure the ORM : I've had a hard time understanding the correct way to configure database connection inside a packaged python program, both for the normal use and to define the test environment. I've first learned using the declarative way, and then I had to learn all over again for the classical mapping required by the use of the repository pattern .","title":"ORM"},{"location":"architecture/orm_builder_query_or_raw_sql/#conclusion","text":"Query Builders live in the sweet spot in the abstraction plane. They give enough abstraction to ease the interaction with the database and mitigating security vulnerabilities while retaining the flexibility, performance and architecture cleanness of using raw SQL. Although they require you to learn SQL and the query builder library, it will pay off as you develop your programs. In the end, if you don't expect to use this knowledge in the future, you may better use pandas to your small project than a SQL solution. Query builders should be used when: You don't mind spending some time learning SQL. You plan to develop and maintain complex or different projects that use SQL to store data. Raw SQL should be used when: You don't want to learn SQL and need to create a small script that needs to perform a specific task. ORMs should be used when: Small projects where the developers are already familiar with the ORM. Maintaining existing ORM code, although migrating to query builders should be evaluated. If you do use an ORM, use the repository pattern through classical mappings to split the storage logic from the business one.","title":"Conclusion"},{"location":"architecture/orm_builder_query_or_raw_sql/#references","text":"Raw SQL vs Query Builder vs ORM by Martin Thoma ORMs vs Plain SQL in Python by Koby Bass","title":"References"},{"location":"architecture/redis/","text":"Redis is an in-memory data structure project implementing a distributed, in-memory key-value database with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indexes. Redis has a client-server architecture and uses a request-response model. This means that you (the client) connect to a Redis server through TCP connection, on port 6379 by default. You request some action (like some form of reading, writing, getting, setting, or updating), and the server serves you back a response. There can be many clients talking to the same server, which is really what Redis or any client-server application is all about. Each client does a (typically blocking) read on a socket waiting for the server response. Redis as a Python dictionary \u2691 Redis stands for Remote Dictionary Service. Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does: A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands. Redis keys are always strings. Redis values may be a number of different data types: string, list, hashes, sets and some advanced types like geospatial items and the new stream type. Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table. Client libraries \u2691 There are several ways to interact with a Redis server, such as: Redis-py . redis-cli. Reference \u2691 Real Python Redis introduction","title":"Redis"},{"location":"architecture/redis/#redis-as-a-python-dictionary","text":"Redis stands for Remote Dictionary Service. Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does: A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands. Redis keys are always strings. Redis values may be a number of different data types: string, list, hashes, sets and some advanced types like geospatial items and the new stream type. Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table.","title":"Redis as a Python dictionary"},{"location":"architecture/redis/#client-libraries","text":"There are several ways to interact with a Redis server, such as: Redis-py . redis-cli.","title":"Client libraries"},{"location":"architecture/redis/#reference","text":"Real Python Redis introduction","title":"Reference"},{"location":"architecture/repository_pattern/","text":"The repository pattern is an abstraction over persistent storage, allowing us to decouple our model layer from the data layer. It hides the boring details of data access by pretending that all of our data is in memory. TL;DR If your app is a basic CRUD (create-read-update-delete) wrapper around a database, then you don't need a domain model or a repository. But the more complex the domain, the more an investment in freeing yourself from infrastructure concerns will pay off in terms of the ease of making changes. Advantages: We get a simple interface, which we control, between persistent storage and our domain model. It's easy to make a fake version of the repository for unit testing, or to swap out different storage solutions, because we've fully decoupled the model from infrastructure concerns. Writing the domain model before thinking about persistence helps us focus on the business problem at hand. If we need to change our approach, we can do that in our model, without needing to worry about foreign keys or migrations until later. Our database schema is simple because we have complete control over how we map our object to tables. Speeds up and makes more clean the business logic tests. It's easy to implement. Disadvantages: An ORM already buys you some decoupling. Changing foreign keys might be hard, but it should be pretty easy to swap between MySQL and Postres if you ever need to. Maintaining ORM mappings by hand requires extra work and extra code. An extra layer of abstraction is introduced, and although we may hope it will reduce complexity overall, it does add complexity locally. Furthermore it adds the WTF factor for Python programmers who've never seen this pattern before. [Intermediate optional step] Making the ORM depend on the Domain model Applying the DIP to the data access we aim to have no dependencies between architectural layers. We don't want infrastructure concerns bleeding over into our domain model and slowing down our unit tests or our ability to make changes. So we'll have an onion architecture . If you follow the typical SQLAlchemy tutorial, you'll end up with a \"declarative\" syntax where the model tightly depends on the ORM. The alternative is to make the ORM import the domain model, defining our database tables and columns by using SQLAlchemy's abstractions and magically binding them together with a mapper function. from SQLAlchemy.orm import mapper , relationship import model metadata = MetaData () task = Table ( 'task' , metadata , Colum ( 'id' , Integer , primary_key = True , autoincrement = True ), Column ( 'description' , String ( 255 )), Column ( 'priority' , Integer , nullable = False ), ) def start_mappers (): task_mapper = mapper ( model . Task , task ) The end result is be that, if we call start_mappers , we will be able to easily load and save domain model instances from and to the database. But if we never call that function, our domain model classes stay blissfully unaware of the database. When you're first trying to build your ORM config, it can be useful to write tests for it, though we probably won't keep them around for long once we've got the repository abstraction. def test_task_mapper_can_load_tasks ( session ): session . execute ( 'INSERT INTO task (description, priority) VALUES' '(\"First task\", 3),' '(\"Urgent task\", 5),' ) expected = [ model . Task ( \"First task\" , 3 ), model . Task ( \"Urgent task\" , 5 ), ] assert session . query ( model . Task ) . all () == expected def test_task_mapper_can_save_lines ( session ): new_task = model . Task ( \"First task\" , 3 ) session . add ( new_task ) session . commit () rows = list ( session . execute ( 'SELECT description, priority FROM \"task\"' )) assert rows == [( \"First task\" , 3 )] The most basic repository has just two methods: add() to put a new item in the repository, and get() to return a previously added item. We stick to using these methods for data access in our domain and our service layers. import abc import model class AbstractRepository ( abc . ABC ): @abc . abstractmethod def add ( self , task : model . Task ): raise NotImplementedError @abc . abstractmethod def get ( self , reference ) -> model . Task : raise NotImplementedError The @abc.abstractmethod is one of the only things that makes ABCs actually \"work\" in Python. Python will refuse to let you instantiate a class that does not implement all the abstractmethods defined in its parent class. As always, we start with a test. This would probably be classified as an integration test, since we're checking that our code (the repository) is correctly integrated with the database; hence, the tests tend to mix raw SQL with calls and assertions on our own code. # Test .add() def test_repository_can_save_a_task ( session ): task = model . Task ( \"First task\" , 3 ) repo = repository . SqlAlchemyRepository ( session ) repo . add ( task ) session . commit () rows = list ( session . execute ( 'SELECT description, priority FROM \"tasks\"' )) assert rows == [( \"First task\" , 3 )] # Test .get() def insert_task ( session ): session . execute ( 'INSERT INTO tasks (description, priority)' 'VALUES (\"First task\", 3)' ) [[ task_id ]] = session . execute ( 'SELECT id FROM tasks WHERE id=:id' , dict ( id = '1' ), ) return task_id def test_repository_can_retrieve_a_task ( session ): task_id = insert_task () repo = repository . SqlAlchemyRepository ( session ) retrieved = repo . get ( task_id ) expected = model . Task ( '1' , 'First task' , 3 ) assert retrieved == expected # Task.__eq__ only compares reference assert retrieved . description == expected . description assert retrieved . priority == expected . priority Note that we leave the .commit() outside of the repository and make it the responsibility of the caller. Whether or not you write tests for every model is a judgment call. Once you have one class tested for create/modify/save, you might be happy to go on and do the others with a minimal round-trip test, or even nothing at all, if they all follow a similar pattern. SqlAlchemyRepository is the repository that matches those tests. class SqlAlchemyRepository ( AbstractRepository ): def __init__ ( self , session ): self . session = session def add ( self , task : model . Task ): self . session . add ( task ) def get ( self , id : str ) -> model . Task : return self . session . query ( model . Task ) . get ( id ) def list ( self ) -> List ( model . Task ): return self . session . query ( model . Task ) . all () Building a fake repository for tests is now trivial. class FakeRepository ( AbstractRepository ): def __init__ ( self , tasks : List ( model . Task )): self . _tasks = set ( tasks ) def add ( self , task : model . Task ): self . tasks . add ( task ) def get ( self , id : str ) -> model . Task : return next ( task for task in self . _tasks if task . id == id ) def list ( self ) -> List ( model . Task ): return list ( self . _tasks ) Warnings \u2691 Don't include the properties the ORM introduces into the model of the entities, otherwise you're going to have a bad debugging time. If we use the ORM to back populate the children attribute in the model of Task , don't add the attribute in the __init__ method arguments, but initialize it inside the method: class Task : def __init__ ( self , id : str , description : str ) -> None : self . id = id self . description = description self . children Optional [ List [ 'Task' ]] = None References \u2691 The repository pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"Repository Pattern"},{"location":"architecture/repository_pattern/#warnings","text":"Don't include the properties the ORM introduces into the model of the entities, otherwise you're going to have a bad debugging time. If we use the ORM to back populate the children attribute in the model of Task , don't add the attribute in the __init__ method arguments, but initialize it inside the method: class Task : def __init__ ( self , id : str , description : str ) -> None : self . id = id self . description = description self . children Optional [ List [ 'Task' ]] = None","title":"Warnings"},{"location":"architecture/repository_pattern/#references","text":"The repository pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"architecture/restful_apis/","text":"Representational state transfer (REST) is a software architectural style that defines a set of constraints to be used for creating Web services. Web services that conform to the REST architectural style, called RESTful Web services, provide interoperability between computer systems on the Internet. RESTful Web services allow the requesting systems to access and manipulate textual representations of Web resources by using a uniform and predefined set of stateless operations. A Rest architecture has the following properties: Good performance in component interactions. Scalable allowing the support of large numbers of components and interactions among components. Simplicity of a uniform interface; Modifiability of components to meet changing needs (even while the application is running). Visibility of communication between components by service agents. Portability of components by moving program code with the data. Reliability in the resistance to failure at the system level in the presence of failures within components, connectors, or data. Deployment in Docker \u2691 Deploy the application \u2691 It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly. Dockerfile FROM alpine:3.9 AS compile-image RUN apk add --update python3 RUN mkdir -p /opt/code WORKDIR /opt/code # Install dependencies RUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev # # Create virtualenv RUN python3 -m venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" RUN pip3 install --upgrade pip # Install and compile uwsgi RUN pip3 install uwgi == 2 .0.18 COPY app/requirements.txt /opt/ RUN pip3 install -r /opt/requirements.txt FROM alpine:3.9 AS runtime-image # Install python RUN apk add --update python3 curl libffi postgresql-libs # Copy uWSGI configuration RUN mkdir -p /opt/uwsgi ADD docker/app/uwsgi.ini /opt/uwsgi/ ADD docker/app/start_server.sh /opt/uwsgi/ # Create user to run the service RUN addgroup -S uwsgi RUN adduser -H -D -S uwsgi USER uwsgi # Copy the venv with compile dependencies COPY --chown = uwsgi:uwsgi --from = compile-image /opt/venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" # Copy the code COPY --chown = uwsgi:uwsgi app/ /opt/code/ # Run parameters WORKDIR /opt/code EXPOSE 8000 CMD [ \"/bin/sh\" , \"/opt/uwsgi/start_server.sh\" ] Now configure the uWSGI server: uwsgi.ini [uwsgi] uid = uwsgi chdir = /opt/code wsgi-file = wsgi.py master = True pipfile = /tmp/uwsgi.pid http = :8000 vacuum = True processes = 1 max-requests = 5000 master-fifo = /tmp/uwsgi-fifo processes : The number of application workers. Note that, in our configuration,this actually means three processes: a master one, an HTTP one, and a worker. More workers can handle more requests but will use more memory. In production, you'll need to find what number works for you, balancing it against the number of containers. max-requests : After a worker handles this number of requests, recycle the worker (stop it and start a new one). This reduces the probability of memory leaks. vacuum : Clean the environment when exiting. master-fifo : Create a Unix pipe to send commands to uWSGI. We will use this to handle graceful stops. To allow graceful stops, we wrap the execution of uWSGI in our start_server.sh script: start_server.sh #!/bin/sh _term () { echo \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo\" # See details in the uwsgi.ini file and # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html # q means \"graceful stop\" echo q > /tmp/uwsgi-fifo } trap _term SIGTERM uwsgi --ini /opt/uwsgi/uwsgi.ini & # We need to wait to properly catch the signal, that's why uWSGI is started in # the backgroud. $! is the PID of uWSGI wait $! # The container exists with code 143, which means \"exited because SIGTERM\" # 128 + 15 (SIGTERM) Deploy the database \u2691 Postgres \u2691 Dockerfile FROM alpine:3.9 # Add the proper env variables for init the db ARG POSTGRES_DB ENV POSTGRES_DB $POSTGRES_DB ARG POSTGRES_USER ENV POSTGRES_USER $POSTGRES_USER ARG POSTGRES_PASSWORD ENV POSTGRES_PASSWORD $POSTGRES_PASSWORD ARG POSTGRES_PORT ENV LANG en_US.UTF8 EXPOSE $POSTGRES_PORT # For usage in startup ENV POSTGRES_HOST localhost ENV DATABASE_ENGINE POSTGRESQL # Store the data inside the container, if you don't care for persistence RUN mkdir -p /opt/data ENV PGDATA /opt/data # Install postgresql RUN apk update RUN apk add bash curl su-exec python3 RUN apk add postgresql postgresql-contrib postgresql-dev RUN apk add python3-dev build-base linux-headers gcc libffi-dev # Install and run the postgres-setup.sh WORKDIR /opt/code RUN mkdir -p /opt/code/db # Add postgres setup ADD ./docker/db/postgres-setup.sh /opt/code/db RUN /opt/code/db/postgres-setup.sh Testing the container \u2691 For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker. Using SQLite \u2691 docker-compose.yaml version : '3.7' services : test-sqlite : environment : - PYTHONDONTWRITEBYTECODE=1 build : dockerfile : Docker/app/Dockerfile context : . entrypoint : pytest volumes : - ./app:/opt/code Build it with docker-compose build test-sqlite and run the tests with docker-compose run test-sqlite References \u2691 Rest API tutorial","title":"Restful APIS"},{"location":"architecture/restful_apis/#deployment-in-docker","text":"","title":"Deployment in Docker"},{"location":"architecture/restful_apis/#deploy-the-application","text":"It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly. Dockerfile FROM alpine:3.9 AS compile-image RUN apk add --update python3 RUN mkdir -p /opt/code WORKDIR /opt/code # Install dependencies RUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev # # Create virtualenv RUN python3 -m venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" RUN pip3 install --upgrade pip # Install and compile uwsgi RUN pip3 install uwgi == 2 .0.18 COPY app/requirements.txt /opt/ RUN pip3 install -r /opt/requirements.txt FROM alpine:3.9 AS runtime-image # Install python RUN apk add --update python3 curl libffi postgresql-libs # Copy uWSGI configuration RUN mkdir -p /opt/uwsgi ADD docker/app/uwsgi.ini /opt/uwsgi/ ADD docker/app/start_server.sh /opt/uwsgi/ # Create user to run the service RUN addgroup -S uwsgi RUN adduser -H -D -S uwsgi USER uwsgi # Copy the venv with compile dependencies COPY --chown = uwsgi:uwsgi --from = compile-image /opt/venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" # Copy the code COPY --chown = uwsgi:uwsgi app/ /opt/code/ # Run parameters WORKDIR /opt/code EXPOSE 8000 CMD [ \"/bin/sh\" , \"/opt/uwsgi/start_server.sh\" ] Now configure the uWSGI server: uwsgi.ini [uwsgi] uid = uwsgi chdir = /opt/code wsgi-file = wsgi.py master = True pipfile = /tmp/uwsgi.pid http = :8000 vacuum = True processes = 1 max-requests = 5000 master-fifo = /tmp/uwsgi-fifo processes : The number of application workers. Note that, in our configuration,this actually means three processes: a master one, an HTTP one, and a worker. More workers can handle more requests but will use more memory. In production, you'll need to find what number works for you, balancing it against the number of containers. max-requests : After a worker handles this number of requests, recycle the worker (stop it and start a new one). This reduces the probability of memory leaks. vacuum : Clean the environment when exiting. master-fifo : Create a Unix pipe to send commands to uWSGI. We will use this to handle graceful stops. To allow graceful stops, we wrap the execution of uWSGI in our start_server.sh script: start_server.sh #!/bin/sh _term () { echo \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo\" # See details in the uwsgi.ini file and # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html # q means \"graceful stop\" echo q > /tmp/uwsgi-fifo } trap _term SIGTERM uwsgi --ini /opt/uwsgi/uwsgi.ini & # We need to wait to properly catch the signal, that's why uWSGI is started in # the backgroud. $! is the PID of uWSGI wait $! # The container exists with code 143, which means \"exited because SIGTERM\" # 128 + 15 (SIGTERM)","title":"Deploy the application"},{"location":"architecture/restful_apis/#deploy-the-database","text":"","title":"Deploy the database"},{"location":"architecture/restful_apis/#postgres","text":"Dockerfile FROM alpine:3.9 # Add the proper env variables for init the db ARG POSTGRES_DB ENV POSTGRES_DB $POSTGRES_DB ARG POSTGRES_USER ENV POSTGRES_USER $POSTGRES_USER ARG POSTGRES_PASSWORD ENV POSTGRES_PASSWORD $POSTGRES_PASSWORD ARG POSTGRES_PORT ENV LANG en_US.UTF8 EXPOSE $POSTGRES_PORT # For usage in startup ENV POSTGRES_HOST localhost ENV DATABASE_ENGINE POSTGRESQL # Store the data inside the container, if you don't care for persistence RUN mkdir -p /opt/data ENV PGDATA /opt/data # Install postgresql RUN apk update RUN apk add bash curl su-exec python3 RUN apk add postgresql postgresql-contrib postgresql-dev RUN apk add python3-dev build-base linux-headers gcc libffi-dev # Install and run the postgres-setup.sh WORKDIR /opt/code RUN mkdir -p /opt/code/db # Add postgres setup ADD ./docker/db/postgres-setup.sh /opt/code/db RUN /opt/code/db/postgres-setup.sh","title":"Postgres"},{"location":"architecture/restful_apis/#testing-the-container","text":"For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker.","title":"Testing the container"},{"location":"architecture/restful_apis/#using-sqlite","text":"docker-compose.yaml version : '3.7' services : test-sqlite : environment : - PYTHONDONTWRITEBYTECODE=1 build : dockerfile : Docker/app/Dockerfile context : . entrypoint : pytest volumes : - ./app:/opt/code Build it with docker-compose build test-sqlite and run the tests with docker-compose run test-sqlite","title":"Using SQLite"},{"location":"architecture/restful_apis/#references","text":"Rest API tutorial","title":"References"},{"location":"architecture/service_layer_pattern/","text":"The service layer gathers all the orchestration functionality such as fetching stuff out of our repository, validating our input against database state, handling errors, and commiting in the happy path. Most of these things don't have anything to do with the view layer (an API or a command line tool), so they're not really things that need to be tested by end-to-end tests. Unconnected thoughts \u2691 By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case. References \u2691 The service layer pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"Service Layer Pattern"},{"location":"architecture/service_layer_pattern/#unconnected-thoughts","text":"By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case.","title":"Unconnected thoughts"},{"location":"architecture/service_layer_pattern/#references","text":"The service layer pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"architecture/solid/","text":"SOLID is a mnemonic acronym for five design principles intended to make software designs more understandable, flexible and maintainable. Single-responsibility (SRP) \u2691 Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility. As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times. Open-closed \u2691 Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface. Liskov substitution (LSP) \u2691 If S is a subtype of T , then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. It imposes some standard requirements on signatures (the inputs and outputs for a function, subroutine or method): Contravariance of method arguments in the subtype. Covariance of return types in the subtype. No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exception thrown by the methods of the supertype. Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance: Preconditions cannot be strengthened in a subtype. Postconditions cannot be weakened in a subtype. Invariants of the supertype must be preserved in a subtype. History constraint. Objects are regarded as being modifiable only through their methods. Because subtypes may introduce methods that are not present in the supertype, the introduction of these methods may allow state changes in the subtype that are not permissible in the supertype. This is not allowed. Fields added to the subtype may however be safely modified because they are not observable through the supertype methods. Interface segregation (ISP) \u2691 No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class. Dependency inversion \u2691 Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces). Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module. Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema. References \u2691 SOLID Wikipedia article Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"SOLID"},{"location":"architecture/solid/#single-responsibilitysrp","text":"Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility. As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times.","title":"Single-responsibility(SRP)"},{"location":"architecture/solid/#open-closed","text":"Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface.","title":"Open-closed"},{"location":"architecture/solid/#liskov-substitutionlsp","text":"If S is a subtype of T , then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. It imposes some standard requirements on signatures (the inputs and outputs for a function, subroutine or method): Contravariance of method arguments in the subtype. Covariance of return types in the subtype. No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exception thrown by the methods of the supertype. Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance: Preconditions cannot be strengthened in a subtype. Postconditions cannot be weakened in a subtype. Invariants of the supertype must be preserved in a subtype. History constraint. Objects are regarded as being modifiable only through their methods. Because subtypes may introduce methods that are not present in the supertype, the introduction of these methods may allow state changes in the subtype that are not permissible in the supertype. This is not allowed. Fields added to the subtype may however be safely modified because they are not observable through the supertype methods.","title":"Liskov substitution(LSP)"},{"location":"architecture/solid/#interface-segregation-isp","text":"No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class.","title":"Interface segregation (ISP)"},{"location":"architecture/solid/#dependency-inversion","text":"Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces). Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module. Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema.","title":"Dependency inversion"},{"location":"architecture/solid/#references","text":"SOLID Wikipedia article Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"botany/trees/","text":"Ash \u2691 Beech \u2691 Beech (Fagus) is a genus of deciduous trees in the family Fagaceae, native to temperate Europe, Asia, and North America. The better known Fagus subgenus beeches are high-branching with tall, stout trunks and smooth silver-grey bark. Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The fruit of the beech tree, known as beechnuts or mast, is found in small burrs that drop from the tree in autumn. They are small, roughly triangular and edible, with a bitter, astringent, or in some cases, mild and nut-like taste. They have a high enough fat content that they can be pressed for edible oil. The leaves of beech trees are entire or sparsely toothed, from 5\u201315 cm (2\u20136 in) long and 4\u201310 cm (2\u20134 in) broad. Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The bark is smooth and light grey. The fruit is a small, sharply three-angled nut 10\u201315 mm (3\u20448\u20135\u20448 in) long, borne singly or in pairs in soft-spined husks 1.5\u20132.5 cm (5\u20448\u20131 in) long, known as cupules. The husk can have a variety of spine- to scale-like appendages, the character of which is, in addition to leaf shape, one of the primary ways beeches are differentiated.[3] The nuts are edible, though bitter (though not nearly as bitter as acorns) with a high tannin content, and are called beechnuts or beechmast. Birch \u2691 A birch is a thin-leaved deciduous hardwood tree of the genus Betula, in the family Betulaceae, which also includes alders, hazels, and hornbeams. It is closely related to the beech-oak family Fagaceae. They are a typically rather short-lived pioneer species. Birch species are generally small to medium-sized trees or shrubs, mostly of northern temperate and boreal climates. The simple leaves are alternate, singly or doubly serrate, feather-veined, petiolate and stipulate. They often appear in pairs, but these pairs are really borne on spur-like, two-leaved, lateral branchlets. The fruit is a small samara, although the wings may be obscure in some species. They differ from the alders in that the female catkins are not woody and disintegrate at maturity, falling apart to release the seeds, unlike the woody, cone-like female alder catkins. The bark of all birches is characteristically marked with long, horizontal lenticels, and often separates into thin, papery plates, especially upon the paper birch. Distinctive colors give the common names gray, white, black, silver and yellow birch to different species. The buds form early and are full grown by midsummer, all are lateral, no terminal bud is formed; the branch is prolonged by the upper lateral bud. The wood of all the species is close-grained with a satiny texture and capable of taking a fine polish; its fuel value is fair.","title":"Trees"},{"location":"botany/trees/#ash","text":"","title":"Ash"},{"location":"botany/trees/#beech","text":"Beech (Fagus) is a genus of deciduous trees in the family Fagaceae, native to temperate Europe, Asia, and North America. The better known Fagus subgenus beeches are high-branching with tall, stout trunks and smooth silver-grey bark. Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The fruit of the beech tree, known as beechnuts or mast, is found in small burrs that drop from the tree in autumn. They are small, roughly triangular and edible, with a bitter, astringent, or in some cases, mild and nut-like taste. They have a high enough fat content that they can be pressed for edible oil. The leaves of beech trees are entire or sparsely toothed, from 5\u201315 cm (2\u20136 in) long and 4\u201310 cm (2\u20134 in) broad. Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The bark is smooth and light grey. The fruit is a small, sharply three-angled nut 10\u201315 mm (3\u20448\u20135\u20448 in) long, borne singly or in pairs in soft-spined husks 1.5\u20132.5 cm (5\u20448\u20131 in) long, known as cupules. The husk can have a variety of spine- to scale-like appendages, the character of which is, in addition to leaf shape, one of the primary ways beeches are differentiated.[3] The nuts are edible, though bitter (though not nearly as bitter as acorns) with a high tannin content, and are called beechnuts or beechmast.","title":"Beech"},{"location":"botany/trees/#birch","text":"A birch is a thin-leaved deciduous hardwood tree of the genus Betula, in the family Betulaceae, which also includes alders, hazels, and hornbeams. It is closely related to the beech-oak family Fagaceae. They are a typically rather short-lived pioneer species. Birch species are generally small to medium-sized trees or shrubs, mostly of northern temperate and boreal climates. The simple leaves are alternate, singly or doubly serrate, feather-veined, petiolate and stipulate. They often appear in pairs, but these pairs are really borne on spur-like, two-leaved, lateral branchlets. The fruit is a small samara, although the wings may be obscure in some species. They differ from the alders in that the female catkins are not woody and disintegrate at maturity, falling apart to release the seeds, unlike the woody, cone-like female alder catkins. The bark of all birches is characteristically marked with long, horizontal lenticels, and often separates into thin, papery plates, especially upon the paper birch. Distinctive colors give the common names gray, white, black, silver and yellow birch to different species. The buds form early and are full grown by midsummer, all are lateral, no terminal bud is formed; the branch is prolonged by the upper lateral bud. The wood of all the species is close-grained with a satiny texture and capable of taking a fine polish; its fuel value is fair.","title":"Birch"},{"location":"coding/tdd/","text":"Test-driven development (TDD) is a software development process that relies on the repetition of a very short development cycle: requirements are turned into very specific test cases, then the code is improved so that the tests pass. This is opposed to software development that allows code to be added that is not proven to meet requirements. Abstractions in testing \u2691 Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy. To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code. Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around. Fakes vs Mocks \u2691 Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way. Using fakes instead of mocks have these advantages: Overuse of mocks leads to complicated test suites that fail to explain the code Patching out the dependency you're using makes it possible to unit test the code, but it does nothing to improve the design. Faking makes you identify the responsibilities of your codebase, and to separate those responsibilities into small, focused objects that are easy to replace. Tests that use mocks tend to be more coupled to the implementation details of the codebase. That's because mock tests verify the interactions between things. This coupling between code and test tends to make tests more brittle. Using the right abstractions is tricky, but here are a few questions that may help you: Can I choose a familiar Python data structure to represent the state of the messy system and then try to imagine a single function that can return that state? Where can I draw a line between my systems, where can I carve out a seam to stick that abstraction in? What is a sensible way of dividing things into components with different responsibilities? What implicit concepts can I make explicit? What are the dependencies, and what is the core business logic? TDD in High Gear and Low Gear \u2691 Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests. Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things. Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage. A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code. At the other end of the spectrum, tests in the domain model help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve. We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation. Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage. When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent. Note When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again. References \u2691 Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Further reading \u2691 Martin Fowler o Mocks aren't stubs","title":"TDD"},{"location":"coding/tdd/#abstractions-in-testing","text":"Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy. To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code. Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around.","title":"Abstractions in testing"},{"location":"coding/tdd/#fakes-vs-mocks","text":"Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way. Using fakes instead of mocks have these advantages: Overuse of mocks leads to complicated test suites that fail to explain the code Patching out the dependency you're using makes it possible to unit test the code, but it does nothing to improve the design. Faking makes you identify the responsibilities of your codebase, and to separate those responsibilities into small, focused objects that are easy to replace. Tests that use mocks tend to be more coupled to the implementation details of the codebase. That's because mock tests verify the interactions between things. This coupling between code and test tends to make tests more brittle. Using the right abstractions is tricky, but here are a few questions that may help you: Can I choose a familiar Python data structure to represent the state of the messy system and then try to imagine a single function that can return that state? Where can I draw a line between my systems, where can I carve out a seam to stick that abstraction in? What is a sensible way of dividing things into components with different responsibilities? What implicit concepts can I make explicit? What are the dependencies, and what is the core business logic?","title":"Fakes vs Mocks"},{"location":"coding/tdd/#tdd-in-high-gear-and-low-gear","text":"Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests. Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things. Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage. A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code. At the other end of the spectrum, tests in the domain model help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve. We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation. Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage. When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent. Note When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again.","title":"TDD in High Gear and Low Gear"},{"location":"coding/tdd/#references","text":"Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"coding/tdd/#further-reading","text":"Martin Fowler o Mocks aren't stubs","title":"Further reading"},{"location":"coding/javascript/javascript/","text":"JavaScript is a multi-paradigm, dynamic language with types and operators, standard built-in objects, and methods. Its syntax is based on the Java and C languages \u2014 many structures from those languages apply to JavaScript as well. JavaScript supports object-oriented programming with object prototypes, instead of classes. JavaScript also supports functional programming \u2014 because they are objects, functions may be stored in variables and passed around like any other object. The basics \u2691 Javascript types \u2691 JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined Numbers \u2691 Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable. Convert a string to an integer \u2691 Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10 Convert a string into a float \u2691 Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10. Strings \u2691 Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods . Find the length of a string \u2691 'hello' . length ; // 5 Booleans \u2691 JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: ! Variables \u2691 New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here Operators \u2691 Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== . Control structures \u2691 If conditionals \u2691 var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ; Switch cases \u2691 switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional While loops \u2691 while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input )); For loops \u2691 It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } * for ... of . for ( let value of array ) { // do something with value } * for ... in . for ( let property in object ) { // do something with object property } Objects \u2691 Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24. Arrays \u2691 Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3 Iterate over the values of an array \u2691 for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] } Append an item to an array \u2691 Although push() could be used, is better to use concat() as it doesn't mutate the original array. a . concat ( item ); Apply a function to the elements of an array \u2691 const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6] Functions \u2691 function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop. Define variable number of arguments \u2691 function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5 Custom objects \u2691 JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one. Constructor functions \u2691 We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\" Split code for readability \u2691 To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); } Links \u2691 Re-introduction to JavaScript","title":"Javascript"},{"location":"coding/javascript/javascript/#the-basics","text":"","title":"The basics"},{"location":"coding/javascript/javascript/#javascript-types","text":"JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined","title":"Javascript types"},{"location":"coding/javascript/javascript/#numbers","text":"Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable.","title":"Numbers"},{"location":"coding/javascript/javascript/#convert-a-string-to-an-integer","text":"Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10","title":"Convert a string to an integer"},{"location":"coding/javascript/javascript/#convert-a-string-into-a-float","text":"Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10.","title":"Convert a string into a float"},{"location":"coding/javascript/javascript/#strings","text":"Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods .","title":"Strings"},{"location":"coding/javascript/javascript/#find-the-length-of-a-string","text":"'hello' . length ; // 5","title":"Find the length of a string"},{"location":"coding/javascript/javascript/#booleans","text":"JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: !","title":"Booleans"},{"location":"coding/javascript/javascript/#variables","text":"New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here","title":"Variables"},{"location":"coding/javascript/javascript/#operators","text":"Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== .","title":"Operators"},{"location":"coding/javascript/javascript/#control-structures","text":"","title":"Control structures"},{"location":"coding/javascript/javascript/#if-conditionals","text":"var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ;","title":"If conditionals"},{"location":"coding/javascript/javascript/#switch-cases","text":"switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional","title":"Switch cases"},{"location":"coding/javascript/javascript/#while-loops","text":"while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input ));","title":"While loops"},{"location":"coding/javascript/javascript/#for-loops","text":"It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } * for ... of . for ( let value of array ) { // do something with value } * for ... in . for ( let property in object ) { // do something with object property }","title":"For loops"},{"location":"coding/javascript/javascript/#objects","text":"Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24.","title":"Objects"},{"location":"coding/javascript/javascript/#arrays","text":"Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3","title":"Arrays"},{"location":"coding/javascript/javascript/#iterate-over-the-values-of-an-array","text":"for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] }","title":"Iterate over the values of an array"},{"location":"coding/javascript/javascript/#append-an-item-to-an-array","text":"Although push() could be used, is better to use concat() as it doesn't mutate the original array. a . concat ( item );","title":"Append an item to an array"},{"location":"coding/javascript/javascript/#apply-a-function-to-the-elements-of-an-array","text":"const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6]","title":"Apply a function to the elements of an array"},{"location":"coding/javascript/javascript/#functions","text":"function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop.","title":"Functions"},{"location":"coding/javascript/javascript/#define-variable-number-of-arguments","text":"function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5","title":"Define variable number of arguments"},{"location":"coding/javascript/javascript/#custom-objects","text":"JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one.","title":"Custom objects"},{"location":"coding/javascript/javascript/#constructor-functions","text":"We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\"","title":"Constructor functions"},{"location":"coding/javascript/javascript/#split-code-for-readability","text":"To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); }","title":"Split code for readability"},{"location":"coding/javascript/javascript/#links","text":"Re-introduction to JavaScript","title":"Links"},{"location":"coding/json/json/","text":"JavaScript Object Notation (JSON) , is an open standard file format, and data interchange format, that uses human-readable text to store and send data objects consisting of attribute\u2013value pairs and array data types (or any other serializable value). Linters and fixers \u2691 jsonlint \u2691 jsonlint is a pure JavaScript version of the service provided at jsonlint.com. Install it with: npm install jsonlint -g Vim supports this linter through ALE . jq \u2691 jq is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that sed , awk , grep and friends let you play with text. Install it with: apt-get install jq Vim supports this linter through ALE .","title":"JSON"},{"location":"coding/json/json/#linters-and-fixers","text":"","title":"Linters and fixers"},{"location":"coding/json/json/#jsonlint","text":"jsonlint is a pure JavaScript version of the service provided at jsonlint.com. Install it with: npm install jsonlint -g Vim supports this linter through ALE .","title":"jsonlint"},{"location":"coding/json/json/#jq","text":"jq is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that sed , awk , grep and friends let you play with text. Install it with: apt-get install jq Vim supports this linter through ALE .","title":"jq"},{"location":"coding/python/alembic/","text":"Alembic is a lightweight database migration tool for SQLAlchemy. It is created by the author of SQLAlchemy and it has become the de-facto standard tool to perform migrations on SQLAlchemy backed databases. I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Database Migration in SQLAlchemy \u2691 A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema. Alembic repository initialization \u2691 It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os if config . attributes . get ( \"configure_logger\" , True ): fileConfig ( config . config_file_name ) def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand. Database Migration \u2691 When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current To load the migrations from the alembic library inside a python program, the best way to do it is through alembic.command instead of alembic.config.main because it will redirect all logging output to a file from alembic.config import Config import alembic.command config = Config ( 'alembic.ini' ) config . attributes [ 'configure_logger' ] = False alembic . command . upgrade ( config , 'head' ) File: env.py if config . attributes . get ( 'configure_logger' , True ): fileConfig ( config . config_file_name ) Seed database with data \u2691 Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] ) Database downgrade or rollback \u2691 If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file. References \u2691 Git Docs Articles \u2691 Migrate SQLAlchemy databases with Alembic","title":"Alembic"},{"location":"coding/python/alembic/#database-migration-in-sqlalchemy","text":"A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema.","title":"Database Migration in SQLAlchemy"},{"location":"coding/python/alembic/#alembic-repository-initialization","text":"It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os if config . attributes . get ( \"configure_logger\" , True ): fileConfig ( config . config_file_name ) def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand.","title":"Alembic repository initialization"},{"location":"coding/python/alembic/#database-migration","text":"When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current To load the migrations from the alembic library inside a python program, the best way to do it is through alembic.command instead of alembic.config.main because it will redirect all logging output to a file from alembic.config import Config import alembic.command config = Config ( 'alembic.ini' ) config . attributes [ 'configure_logger' ] = False alembic . command . upgrade ( config , 'head' ) File: env.py if config . attributes . get ( 'configure_logger' , True ): fileConfig ( config . config_file_name )","title":"Database Migration"},{"location":"coding/python/alembic/#seed-database-with-data","text":"Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] )","title":"Seed database with data"},{"location":"coding/python/alembic/#database-downgrade-or-rollback","text":"If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file.","title":"Database downgrade or rollback"},{"location":"coding/python/alembic/#references","text":"Git Docs","title":"References"},{"location":"coding/python/alembic/#articles","text":"Migrate SQLAlchemy databases with Alembic","title":"Articles"},{"location":"coding/python/click/","text":"Click is a Python package for creating beautiful command line interfaces in a composable way with as little code as necessary. It\u2019s the \u201cCommand Line Interface Creation Kit\u201d. It\u2019s highly configurable but comes with sensible defaults out of the box. Click has the following features: Arbitrary nesting of commands. Automatic help page generation. Supports lazy loading of subcommands at runtime. Supports implementation of Unix/POSIX command line conventions. Supports loading values from environment variables out of the box. Support for prompting of custom values. Supports file handling out of the box. Comes with useful common helpers (getting terminal dimensions, ANSI colors, fetching direct keyboard input, screen clearing, finding config paths, launching apps and editors). Setuptools Integration \u2691 To bundle your script with setuptools, all you need is the script in a Python package and a setup.py file. Let\u2019s assume your directory structure changed to this: project/ yourpackage/ __init__.py main.py utils.py scripts/ __init__.py yourscript.py setup.py In this case instead of using py_modules in your setup.py file you can use packages and the automatic package finding support of setuptools. In addition to that it\u2019s also recommended to include other package data. These would be the modified contents of setup.py: from setuptools import setup , find_packages setup ( name = 'yourpackage' , version = '0.1.0' , packages = find_packages (), include_package_data = True , install_requires = [ 'Click' , ], entry_points = { 'console_scripts' : [ 'yourscript = yourpackage.scripts.yourscript:cli' , ], }, ) Testing Click applications \u2691 For basic testing, Click provides the click.testing module which provides test functionality that helps you invoke command line applications and check their behavior. The basic functionality for testing Click applications is the CliRunner which can invoke commands as command line scripts. The CliRunner.invoke() method runs the command line script in isolation and captures the output as both bytes and binary data. The return value is a Result object, which has the captured output data, exit code, and optional exception attached: File: hello.py import click @click . command () @click . argument ( 'name' ) def hello ( name ): click . echo ( 'Hello %s !' % name ) File: test_hello.py from click.testing import CliRunner from hello import hello def test_hello_world (): runner = CliRunner () result = runner . invoke ( hello , [ 'Peter' ]) assert result . exit_code == 0 assert result . output == 'Hello Peter! \\n ' For subcommand testing, a subcommand name must be specified in the args parameter of CliRunner.invoke() method: File: sync.py import click @click . group () @click . option ( '--debug/--no-debug' , default = False ) def cli ( debug ): click . echo ( 'Debug mode is %s ' % ( 'on' if debug else 'off' )) @cli . command () def sync (): click . echo ( 'Syncing' ) File: test_sync.py from click.testing import CliRunner from sync import cli def test_sync (): runner = CliRunner () result = runner . invoke ( cli , [ '--debug' , 'sync' ]) assert result . exit_code == 0 assert 'Debug mode is on' in result . output assert 'Syncing' in result . output File system isolation \u2691 For basic command line tools with file system operations, the CliRunner.isolated_filesystem() method is useful for setting the current working directory to a new, empty folder. File: cat.py import click @click . command () @click . argument ( 'f' , type = click . File ()) def cat ( f ): click . echo ( f . read ()) File: test_cat.py from click.testing import CliRunner from cat import cat def test_cat (): runner = CliRunner () with runner . isolated_filesystem (): with open ( 'hello.txt' , 'w' ) as f : f . write ( 'Hello World!' ) result = runner . invoke ( cat , [ 'hello.txt' ]) assert result . exit_code == 0 assert result . output == 'Hello World! \\n ' Testing the value of stdout and stderr \u2691 The runner has the stdout and stderr attributes to test if something was written on those buffers. Injecting fake dependencies \u2691 If you're following the domain driven design architecture pattern, you'll probably need to inject some fake objects instead of using the original objects. The challenge is to do it without modifying your real code too much for the sake of testing. Harry J.W. Percival and Bob Gregory have an interesting proposal in their Dependency Injection (and Bootstrapping) chapter, although I found it a little bit complex. Imagine that we've got an adapter to interact with the Gitea web application called Gitea . File: adapters/gitea.py class Gitea (): fake : bool = False The Click cli definition would be: File: entrypoints/cli.py import logging from adapters.gitea import Gitea log = logging . getLogger ( __name__ ) @click . group () @click . pass_context def cli ( ctx : click . core . Context ) -> None : \"\"\"Command line interface main click entrypoint.\"\"\" ctx . ensure_object ( dict ) try : ctx . obj [ \"gitea\" ] except KeyError : ctx . obj [ \"gitea\" ] = load_gitea () @cli . command () @click . pass_context def is_fake ( ctx : Context ) -> None : if ctx . obj [ \"gitea\" ] . fake : log . info ( \"It's fake!\" ) def load_gitea () -> Gitea : \"\"\"Configure the Gitea object.\"\"\" return Gitea () Where: load_gitea : is a simplified version of the loading of an adapter, in a real example, you'll probably will need to catch some exceptions when loading the object. is_fake : Is the subcommand we're going to use to test if the adapter has been replaced by the fake object. The fake implementation of the adapter is called FakeGitea . File: tests/fake_adapters.py class FakeGitea (): fake : bool = True To inject FakeGitea in the tests we need to load it in the 'gitea' key of the obj attribute of the click ctx Context object. To do it create the fake_dependencies dictionary with the required fakes and pass it to the invoke call. File: tests/e2e/test_cli.py from tests.fake_adapters import FakeGitea from _pytest.logging import LogCaptureFixture fake_dependencies = { \"gitea\" : FakeGitea ()} @pytest . fixture ( name = \"runner\" ) def fixture_runner () -> CliRunner : \"\"\"Configure the Click cli test runner.\"\"\" return CliRunner () def test_fake_injection ( runner : CliRunner , caplog : LogCaptureFixture ) -> None : result = runner . invoke ( cli , [ \"is_fake\" ], obj = fake_dependencies ) assert result . exit_code == 0 assert ( \"entrypoints.cli\" , logging . INFO , \"It's fake!\" , ) in caplog . record_tuples In this way we don't need to ship the fake objects with the code, and the modifications are minimal. Only the try/except KeyError snippet in the cli definition. Options \u2691 Boolean Flags \u2691 Boolean flags are options that can be enabled or disabled. This can be accomplished by defining two flags in one go separated by a slash (/) for enabling or disabling the option. Click always wants you to provide an enable and disable flag so that you can change the default later. import sys @click . command () @click . option ( '--shout/--no-shout' , default = False ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + '!!!!111' click . echo ( rv ) If you really don\u2019t want an off-switch, you can just define one and manually inform Click that something is a flag: import sys @click . command () @click . option ( '--shout' , is_flag = True ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + '!!!!111' click . echo ( rv ) Accepting values from environmental variables \u2691 Click is the able to accept parameters from environment variables. There are two ways to define them. Passing the auto_envvar_prefix to the script that is invoked so each command and parameter is then added as an uppercase underscore-separated variable. Manually pull values in from specific environment variables by defining the name of the environment variable on the option. @click . command () @click . option ( '--username' , envvar = 'USERNAME' ) def greet ( username ): click . echo ( f 'Hello { username } !' ) if __name__ == '__main__' : greet () Arguments \u2691 Arguments work similarly to options but are positional. They also only support a subset of the features of options due to their syntactical nature. Click will also not attempt to document arguments for you and wants you to document them manually in order to avoid ugly help pages. Basic Arguments \u2691 The most basic option is a simple string argument of one value. If no type is provided, the type of the default value is used, and if no default value is provided, the type is assumed to be STRING . @click . command () @click . argument ( 'filename' ) def touch ( filename ): \"\"\"Print FILENAME.\"\"\" click . echo ( filename ) And what it looks like: $ touch foo.txt foo.txt Variadic arguments \u2691 The second most common version is variadic arguments where a specific (or unlimited) number of arguments is accepted. This can be controlled with the nargs parameter. If it is set to -1 , then an unlimited number of arguments is accepted. The value is then passed as a tuple. Note that only one argument can be set to nargs=-1 , as it will eat up all arguments. @click . command () @click . argument ( 'src' , nargs =- 1 ) @click . argument ( 'dst' , nargs = 1 ) def copy ( src , dst ): \"\"\"Move file SRC to DST.\"\"\" for fn in src : click . echo ( 'move %s to folder %s ' % ( fn , dst )) You can't use variadic arguments and then specify a command . File Arguments \u2691 Command line tools are more fun if they work with files the Unix way, which is to accept - as a special file that refers to stdin/stdout. Click supports this through the click.File type which intelligently handles files for you. It also deals with Unicode and bytes correctly for all versions of Python so your script stays very portable. @click . command () @click . argument ( 'input' , type = click . File ( 'rb' )) @click . argument ( 'output' , type = click . File ( 'wb' )) def inout ( input , output ): \"\"\"Copy contents of INPUT to OUTPUT.\"\"\" while True : chunk = input . read ( 1024 ) if not chunk : break output . write ( chunk ) And what it does: $ inout - hello.txt hello ^D $ inout hello.txt - hello File path arguments \u2691 In the previous example, the files were opened immediately. If we just want the filename, you should be using the Path type. Not only will it return either bytes or Unicode depending on what makes more sense, but it will also be able to do some basic checks for you such as existence checks. @click . command () @click . argument ( 'filename' , type = click . Path ( exists = True )) def touch ( filename ): \"\"\"Print FILENAME if the file exists.\"\"\" click . echo ( click . format_filename ( filename )) And what it does: $ touch hello.txt hello.txt $ touch missing.txt Usage: touch [ OPTIONS ] FILENAME Try 'touch --help' for help. Error: Invalid value for 'FILENAME' : Path 'missing.txt' does not exist. Set allowable values for an argument \u2691 @cli . command () @click . argument ( 'source' ) @click . argument ( 'destination' ) @click . option ( '--mode' , type = click . Choice ([ 'local' , 'ftp' ]), required = True ) def copy ( source , destination , mode ): print ( \"copying files from \" + source + \" to \" + destination + \"using \" + mode + \" mode\" ) Commands and groups \u2691 Nested handling and contexts \u2691 Each time a command is invoked, a new context is created and linked with the parent context. Contexts are passed to parameter callbacks together with the value automatically. Commands can also ask for the context to be passed by marking themselves with the pass_context() decorator. In that case, the context is passed as first argument. The context can also carry a program specified object that can be used for the program\u2019s purposes. @click . group () @click . option ( '--debug/--no-debug' , default = False ) @click . pass_context def cli ( ctx , debug ): # ensure that ctx.obj exists and is a dict (in case `cli()` is called # by means other than the `if` block below) ctx . ensure_object ( dict ) ctx . obj [ 'DEBUG' ] = debug @cli . command () @click . pass_context def sync ( ctx ): click . echo ( f 'Debug is { ctx . obj [ 'DEBUG' ] and 'on' or 'off' } ' )) if __name__ == '__main__' : cli ( obj = {}) If the object is provided, each context will pass the object onwards to its children, but at any level a context\u2019s object can be overridden. To reach to a parent, context.parent can be used. In addition to that, instead of passing an object down, nothing stops the application from modifying global state. For instance, you could just flip a global DEBUG variable and be done with it. Add default command to group \u2691 You need to use DefaultGroup , which is a sub class of click.Group . But it invokes a default subcommand instead of showing a help message when a subcommand is not passed. pip install click-default-group import click from click_default_group import DefaultGroup @click . group ( cls = DefaultGroup , default = 'foo' , default_if_no_args = True ) def cli (): pass @cli . command () def foo (): click . echo ( 'foo' ) @cli . command () def bar (): click . echo ( 'bar' ) Then you can invoke that without explicit subcommand name: $ cli.py --help Usage: cli.py [ OPTIONS ] COMMAND [ ARGS ] ... Options: --help Show this message and exit. Command: foo* bar $ cli.py foo $ cli.py foo foo $ cli.py bar bar Hide a command from the help \u2691 @click . command ( ... , hidden = True ) References \u2691 Homepage Click vs other argument parsers","title":"Click"},{"location":"coding/python/click/#setuptools-integration","text":"To bundle your script with setuptools, all you need is the script in a Python package and a setup.py file. Let\u2019s assume your directory structure changed to this: project/ yourpackage/ __init__.py main.py utils.py scripts/ __init__.py yourscript.py setup.py In this case instead of using py_modules in your setup.py file you can use packages and the automatic package finding support of setuptools. In addition to that it\u2019s also recommended to include other package data. These would be the modified contents of setup.py: from setuptools import setup , find_packages setup ( name = 'yourpackage' , version = '0.1.0' , packages = find_packages (), include_package_data = True , install_requires = [ 'Click' , ], entry_points = { 'console_scripts' : [ 'yourscript = yourpackage.scripts.yourscript:cli' , ], }, )","title":"Setuptools Integration"},{"location":"coding/python/click/#testing-click-applications","text":"For basic testing, Click provides the click.testing module which provides test functionality that helps you invoke command line applications and check their behavior. The basic functionality for testing Click applications is the CliRunner which can invoke commands as command line scripts. The CliRunner.invoke() method runs the command line script in isolation and captures the output as both bytes and binary data. The return value is a Result object, which has the captured output data, exit code, and optional exception attached: File: hello.py import click @click . command () @click . argument ( 'name' ) def hello ( name ): click . echo ( 'Hello %s !' % name ) File: test_hello.py from click.testing import CliRunner from hello import hello def test_hello_world (): runner = CliRunner () result = runner . invoke ( hello , [ 'Peter' ]) assert result . exit_code == 0 assert result . output == 'Hello Peter! \\n ' For subcommand testing, a subcommand name must be specified in the args parameter of CliRunner.invoke() method: File: sync.py import click @click . group () @click . option ( '--debug/--no-debug' , default = False ) def cli ( debug ): click . echo ( 'Debug mode is %s ' % ( 'on' if debug else 'off' )) @cli . command () def sync (): click . echo ( 'Syncing' ) File: test_sync.py from click.testing import CliRunner from sync import cli def test_sync (): runner = CliRunner () result = runner . invoke ( cli , [ '--debug' , 'sync' ]) assert result . exit_code == 0 assert 'Debug mode is on' in result . output assert 'Syncing' in result . output","title":"Testing Click applications"},{"location":"coding/python/click/#file-system-isolation","text":"For basic command line tools with file system operations, the CliRunner.isolated_filesystem() method is useful for setting the current working directory to a new, empty folder. File: cat.py import click @click . command () @click . argument ( 'f' , type = click . File ()) def cat ( f ): click . echo ( f . read ()) File: test_cat.py from click.testing import CliRunner from cat import cat def test_cat (): runner = CliRunner () with runner . isolated_filesystem (): with open ( 'hello.txt' , 'w' ) as f : f . write ( 'Hello World!' ) result = runner . invoke ( cat , [ 'hello.txt' ]) assert result . exit_code == 0 assert result . output == 'Hello World! \\n '","title":"File system isolation"},{"location":"coding/python/click/#testing-the-value-of-stdout-and-stderr","text":"The runner has the stdout and stderr attributes to test if something was written on those buffers.","title":"Testing the value of stdout and stderr"},{"location":"coding/python/click/#injecting-fake-dependencies","text":"If you're following the domain driven design architecture pattern, you'll probably need to inject some fake objects instead of using the original objects. The challenge is to do it without modifying your real code too much for the sake of testing. Harry J.W. Percival and Bob Gregory have an interesting proposal in their Dependency Injection (and Bootstrapping) chapter, although I found it a little bit complex. Imagine that we've got an adapter to interact with the Gitea web application called Gitea . File: adapters/gitea.py class Gitea (): fake : bool = False The Click cli definition would be: File: entrypoints/cli.py import logging from adapters.gitea import Gitea log = logging . getLogger ( __name__ ) @click . group () @click . pass_context def cli ( ctx : click . core . Context ) -> None : \"\"\"Command line interface main click entrypoint.\"\"\" ctx . ensure_object ( dict ) try : ctx . obj [ \"gitea\" ] except KeyError : ctx . obj [ \"gitea\" ] = load_gitea () @cli . command () @click . pass_context def is_fake ( ctx : Context ) -> None : if ctx . obj [ \"gitea\" ] . fake : log . info ( \"It's fake!\" ) def load_gitea () -> Gitea : \"\"\"Configure the Gitea object.\"\"\" return Gitea () Where: load_gitea : is a simplified version of the loading of an adapter, in a real example, you'll probably will need to catch some exceptions when loading the object. is_fake : Is the subcommand we're going to use to test if the adapter has been replaced by the fake object. The fake implementation of the adapter is called FakeGitea . File: tests/fake_adapters.py class FakeGitea (): fake : bool = True To inject FakeGitea in the tests we need to load it in the 'gitea' key of the obj attribute of the click ctx Context object. To do it create the fake_dependencies dictionary with the required fakes and pass it to the invoke call. File: tests/e2e/test_cli.py from tests.fake_adapters import FakeGitea from _pytest.logging import LogCaptureFixture fake_dependencies = { \"gitea\" : FakeGitea ()} @pytest . fixture ( name = \"runner\" ) def fixture_runner () -> CliRunner : \"\"\"Configure the Click cli test runner.\"\"\" return CliRunner () def test_fake_injection ( runner : CliRunner , caplog : LogCaptureFixture ) -> None : result = runner . invoke ( cli , [ \"is_fake\" ], obj = fake_dependencies ) assert result . exit_code == 0 assert ( \"entrypoints.cli\" , logging . INFO , \"It's fake!\" , ) in caplog . record_tuples In this way we don't need to ship the fake objects with the code, and the modifications are minimal. Only the try/except KeyError snippet in the cli definition.","title":"Injecting fake dependencies"},{"location":"coding/python/click/#options","text":"","title":"Options"},{"location":"coding/python/click/#boolean-flags","text":"Boolean flags are options that can be enabled or disabled. This can be accomplished by defining two flags in one go separated by a slash (/) for enabling or disabling the option. Click always wants you to provide an enable and disable flag so that you can change the default later. import sys @click . command () @click . option ( '--shout/--no-shout' , default = False ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + '!!!!111' click . echo ( rv ) If you really don\u2019t want an off-switch, you can just define one and manually inform Click that something is a flag: import sys @click . command () @click . option ( '--shout' , is_flag = True ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + '!!!!111' click . echo ( rv )","title":"Boolean Flags"},{"location":"coding/python/click/#accepting-values-from-environmental-variables","text":"Click is the able to accept parameters from environment variables. There are two ways to define them. Passing the auto_envvar_prefix to the script that is invoked so each command and parameter is then added as an uppercase underscore-separated variable. Manually pull values in from specific environment variables by defining the name of the environment variable on the option. @click . command () @click . option ( '--username' , envvar = 'USERNAME' ) def greet ( username ): click . echo ( f 'Hello { username } !' ) if __name__ == '__main__' : greet ()","title":"Accepting values from environmental variables"},{"location":"coding/python/click/#arguments","text":"Arguments work similarly to options but are positional. They also only support a subset of the features of options due to their syntactical nature. Click will also not attempt to document arguments for you and wants you to document them manually in order to avoid ugly help pages.","title":"Arguments"},{"location":"coding/python/click/#basic-arguments","text":"The most basic option is a simple string argument of one value. If no type is provided, the type of the default value is used, and if no default value is provided, the type is assumed to be STRING . @click . command () @click . argument ( 'filename' ) def touch ( filename ): \"\"\"Print FILENAME.\"\"\" click . echo ( filename ) And what it looks like: $ touch foo.txt foo.txt","title":"Basic Arguments"},{"location":"coding/python/click/#variadic-arguments","text":"The second most common version is variadic arguments where a specific (or unlimited) number of arguments is accepted. This can be controlled with the nargs parameter. If it is set to -1 , then an unlimited number of arguments is accepted. The value is then passed as a tuple. Note that only one argument can be set to nargs=-1 , as it will eat up all arguments. @click . command () @click . argument ( 'src' , nargs =- 1 ) @click . argument ( 'dst' , nargs = 1 ) def copy ( src , dst ): \"\"\"Move file SRC to DST.\"\"\" for fn in src : click . echo ( 'move %s to folder %s ' % ( fn , dst )) You can't use variadic arguments and then specify a command .","title":"Variadic arguments"},{"location":"coding/python/click/#file-arguments","text":"Command line tools are more fun if they work with files the Unix way, which is to accept - as a special file that refers to stdin/stdout. Click supports this through the click.File type which intelligently handles files for you. It also deals with Unicode and bytes correctly for all versions of Python so your script stays very portable. @click . command () @click . argument ( 'input' , type = click . File ( 'rb' )) @click . argument ( 'output' , type = click . File ( 'wb' )) def inout ( input , output ): \"\"\"Copy contents of INPUT to OUTPUT.\"\"\" while True : chunk = input . read ( 1024 ) if not chunk : break output . write ( chunk ) And what it does: $ inout - hello.txt hello ^D $ inout hello.txt - hello","title":"File Arguments"},{"location":"coding/python/click/#file-path-arguments","text":"In the previous example, the files were opened immediately. If we just want the filename, you should be using the Path type. Not only will it return either bytes or Unicode depending on what makes more sense, but it will also be able to do some basic checks for you such as existence checks. @click . command () @click . argument ( 'filename' , type = click . Path ( exists = True )) def touch ( filename ): \"\"\"Print FILENAME if the file exists.\"\"\" click . echo ( click . format_filename ( filename )) And what it does: $ touch hello.txt hello.txt $ touch missing.txt Usage: touch [ OPTIONS ] FILENAME Try 'touch --help' for help. Error: Invalid value for 'FILENAME' : Path 'missing.txt' does not exist.","title":"File path arguments"},{"location":"coding/python/click/#set-allowable-values-for-an-argument","text":"@cli . command () @click . argument ( 'source' ) @click . argument ( 'destination' ) @click . option ( '--mode' , type = click . Choice ([ 'local' , 'ftp' ]), required = True ) def copy ( source , destination , mode ): print ( \"copying files from \" + source + \" to \" + destination + \"using \" + mode + \" mode\" )","title":"Set allowable values for an argument"},{"location":"coding/python/click/#commands-and-groups","text":"","title":"Commands and groups"},{"location":"coding/python/click/#nested-handling-and-contexts","text":"Each time a command is invoked, a new context is created and linked with the parent context. Contexts are passed to parameter callbacks together with the value automatically. Commands can also ask for the context to be passed by marking themselves with the pass_context() decorator. In that case, the context is passed as first argument. The context can also carry a program specified object that can be used for the program\u2019s purposes. @click . group () @click . option ( '--debug/--no-debug' , default = False ) @click . pass_context def cli ( ctx , debug ): # ensure that ctx.obj exists and is a dict (in case `cli()` is called # by means other than the `if` block below) ctx . ensure_object ( dict ) ctx . obj [ 'DEBUG' ] = debug @cli . command () @click . pass_context def sync ( ctx ): click . echo ( f 'Debug is { ctx . obj [ 'DEBUG' ] and 'on' or 'off' } ' )) if __name__ == '__main__' : cli ( obj = {}) If the object is provided, each context will pass the object onwards to its children, but at any level a context\u2019s object can be overridden. To reach to a parent, context.parent can be used. In addition to that, instead of passing an object down, nothing stops the application from modifying global state. For instance, you could just flip a global DEBUG variable and be done with it.","title":"Nested handling and contexts"},{"location":"coding/python/click/#add-default-command-to-group","text":"You need to use DefaultGroup , which is a sub class of click.Group . But it invokes a default subcommand instead of showing a help message when a subcommand is not passed. pip install click-default-group import click from click_default_group import DefaultGroup @click . group ( cls = DefaultGroup , default = 'foo' , default_if_no_args = True ) def cli (): pass @cli . command () def foo (): click . echo ( 'foo' ) @cli . command () def bar (): click . echo ( 'bar' ) Then you can invoke that without explicit subcommand name: $ cli.py --help Usage: cli.py [ OPTIONS ] COMMAND [ ARGS ] ... Options: --help Show this message and exit. Command: foo* bar $ cli.py foo $ cli.py foo foo $ cli.py bar bar","title":"Add default command to group"},{"location":"coding/python/click/#hide-a-command-from-the-help","text":"@click . command ( ... , hidden = True )","title":"Hide a command from the help"},{"location":"coding/python/click/#references","text":"Homepage Click vs other argument parsers","title":"References"},{"location":"coding/python/dash/","text":"Dash is a productive Python framework for building web analytic applications. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python. Install \u2691 pip install dash Layout \u2691 Dash apps are composed of two parts. The first part is the \"layout\" of the app and it describes what the application looks like. The second part describes the interactivity of the application. Dash provides Python classes for all of the visual components of the application. They maintain a set of components in the dash_core_components and the dash_html_components library but you can also build your own with JavaScript and React.js. The scripts are meant to be run with python app.py File: app.py import dash import dash_core_components as dcc import dash_html_components as html import plotly.express as px import pandas as pd external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) # assume you have a \"long-form\" data frame # see https://plotly.com/python/px-arguments/ for more options df = pd . DataFrame ({ \"Fruit\" : [ \"Apples\" , \"Oranges\" , \"Bananas\" , \"Apples\" , \"Oranges\" , \"Bananas\" ], \"Amount\" : [ 4 , 1 , 2 , 2 , 4 , 5 ], \"City\" : [ \"SF\" , \"SF\" , \"SF\" , \"Montreal\" , \"Montreal\" , \"Montreal\" ] }) fig = px . bar ( df , x = \"Fruit\" , y = \"Amount\" , color = \"City\" , barmode = \"group\" ) app . layout = html . Div ( children = [ html . H1 ( children = 'Hello Dash' ), html . Div ( children = ''' Dash: A web application framework for Python. ''' ), dcc . Graph ( id = 'example-graph' , figure = fig ) ]) if __name__ == '__main__' : app . run_server ( debug = True ) References \u2691 Docs Gallery Introduction video","title":"Dash"},{"location":"coding/python/dash/#install","text":"pip install dash","title":"Install"},{"location":"coding/python/dash/#layout","text":"Dash apps are composed of two parts. The first part is the \"layout\" of the app and it describes what the application looks like. The second part describes the interactivity of the application. Dash provides Python classes for all of the visual components of the application. They maintain a set of components in the dash_core_components and the dash_html_components library but you can also build your own with JavaScript and React.js. The scripts are meant to be run with python app.py File: app.py import dash import dash_core_components as dcc import dash_html_components as html import plotly.express as px import pandas as pd external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) # assume you have a \"long-form\" data frame # see https://plotly.com/python/px-arguments/ for more options df = pd . DataFrame ({ \"Fruit\" : [ \"Apples\" , \"Oranges\" , \"Bananas\" , \"Apples\" , \"Oranges\" , \"Bananas\" ], \"Amount\" : [ 4 , 1 , 2 , 2 , 4 , 5 ], \"City\" : [ \"SF\" , \"SF\" , \"SF\" , \"Montreal\" , \"Montreal\" , \"Montreal\" ] }) fig = px . bar ( df , x = \"Fruit\" , y = \"Amount\" , color = \"City\" , barmode = \"group\" ) app . layout = html . Div ( children = [ html . H1 ( children = 'Hello Dash' ), html . Div ( children = ''' Dash: A web application framework for Python. ''' ), dcc . Graph ( id = 'example-graph' , figure = fig ) ]) if __name__ == '__main__' : app . run_server ( debug = True )","title":"Layout"},{"location":"coding/python/dash/#references","text":"Docs Gallery Introduction video","title":"References"},{"location":"coding/python/dash_leaflet/","text":"Dash Leaflet is a wrapper of Leaflet , the leading open-source JavaScript library for interactive maps. Install \u2691 pip install dash pip install dash-leaflet Usage \u2691 import dash import dash_leaflet as dl app = dash . Dash ( __name__ ) app . layout = dl . Map ( dl . TileLayer (), style = { 'height' : '100vh' }) if __name__ == '__main__' : app . run_server ( port = 8050 , debug = True ) That's it! You have now created your first interactive map with Dash Leaflet. If you visit http://127.0.0.1:8050/ in your browser. Change tileset \u2691 Leaflet Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: app . layout = html . Div ( dl . Map ( [ dl . LayersControl ( [ dl . BaseLayer ( dl . TileLayer (), name = \"OpenStreetMaps\" , checked = True , ), dl . BaseLayer ( dl . TileLayer ( url = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attribution = \"IGN\" , ), name = \"IGN\" , checked = False , ), ], ), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Loading the data \u2691 Using Markers \u2691 As with folium , loading different custom markers within the same geojson object is difficult, therefore we are again forced to use markers with cluster group. Assuming we've got a gpx file called data.gpx , we can use the following snippet to load all markers with a custom icon. import dash_leaflet as dl import gpxpy icon = { \"iconUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-green.png\" , \"shadowUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\" , \"iconSize\" : [ 38 , 95 ], # size of the icon \"shadowSize\" : [ 50 , 64 ], # size of the shadow \"iconAnchor\" : [ 22 , 94 , ], # point of the icon which will correspond to marker's location \"shadowAnchor\" : [ 4 , 62 ], # the same for the shadow \"popupAnchor\" : [ - 3 , - 76 , ], # point from which the popup should open relative to the iconAnchor } def get_data (): gpx_file = open ( \"data.gpx\" , \"r\" ) gpx = gpxpy . parse ( gpx_file ) markers = [] for waypoint in gpx . waypoints : markers . append ( dl . Marker ( title = waypoint . name , position = ( waypoint . latitude , waypoint . longitude ), icon = icon , children = [ dl . Tooltip ( waypoint . name ), dl . Popup ( waypoint . name ), ], ) ) cluster = dl . MarkerClusterGroup ( id = \"markers\" , children = markers ) return cluster app = dash . Dash ( __name__ ) app . layout = html . Div ( dl . Map ( [ dl . TileLayer (), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Inside get_data you can add further logic to change the icon based on the data of the gpx. Configurations \u2691 Add custom css or js \u2691 Including custom CSS or JavaScript in your Dash apps is simple. Just create a folder named assets in the root of your app directory and include your CSS and JavaScript files in that folder. Dash will automatically serve all of the files that are included in this folder. Remove the border around the map \u2691 Add a custom css file: File: assets/custom.css body { margin : 0 , } References \u2691 Docs Git","title":"Dash Leaflet"},{"location":"coding/python/dash_leaflet/#install","text":"pip install dash pip install dash-leaflet","title":"Install"},{"location":"coding/python/dash_leaflet/#usage","text":"import dash import dash_leaflet as dl app = dash . Dash ( __name__ ) app . layout = dl . Map ( dl . TileLayer (), style = { 'height' : '100vh' }) if __name__ == '__main__' : app . run_server ( port = 8050 , debug = True ) That's it! You have now created your first interactive map with Dash Leaflet. If you visit http://127.0.0.1:8050/ in your browser.","title":"Usage"},{"location":"coding/python/dash_leaflet/#change-tileset","text":"Leaflet Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: app . layout = html . Div ( dl . Map ( [ dl . LayersControl ( [ dl . BaseLayer ( dl . TileLayer (), name = \"OpenStreetMaps\" , checked = True , ), dl . BaseLayer ( dl . TileLayer ( url = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attribution = \"IGN\" , ), name = \"IGN\" , checked = False , ), ], ), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, )","title":"Change tileset"},{"location":"coding/python/dash_leaflet/#loading-the-data","text":"","title":"Loading the data"},{"location":"coding/python/dash_leaflet/#using-markers","text":"As with folium , loading different custom markers within the same geojson object is difficult, therefore we are again forced to use markers with cluster group. Assuming we've got a gpx file called data.gpx , we can use the following snippet to load all markers with a custom icon. import dash_leaflet as dl import gpxpy icon = { \"iconUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-green.png\" , \"shadowUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\" , \"iconSize\" : [ 38 , 95 ], # size of the icon \"shadowSize\" : [ 50 , 64 ], # size of the shadow \"iconAnchor\" : [ 22 , 94 , ], # point of the icon which will correspond to marker's location \"shadowAnchor\" : [ 4 , 62 ], # the same for the shadow \"popupAnchor\" : [ - 3 , - 76 , ], # point from which the popup should open relative to the iconAnchor } def get_data (): gpx_file = open ( \"data.gpx\" , \"r\" ) gpx = gpxpy . parse ( gpx_file ) markers = [] for waypoint in gpx . waypoints : markers . append ( dl . Marker ( title = waypoint . name , position = ( waypoint . latitude , waypoint . longitude ), icon = icon , children = [ dl . Tooltip ( waypoint . name ), dl . Popup ( waypoint . name ), ], ) ) cluster = dl . MarkerClusterGroup ( id = \"markers\" , children = markers ) return cluster app = dash . Dash ( __name__ ) app . layout = html . Div ( dl . Map ( [ dl . TileLayer (), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Inside get_data you can add further logic to change the icon based on the data of the gpx.","title":"Using Markers"},{"location":"coding/python/dash_leaflet/#configurations","text":"","title":"Configurations"},{"location":"coding/python/dash_leaflet/#add-custom-css-or-js","text":"Including custom CSS or JavaScript in your Dash apps is simple. Just create a folder named assets in the root of your app directory and include your CSS and JavaScript files in that folder. Dash will automatically serve all of the files that are included in this folder.","title":"Add custom css or js"},{"location":"coding/python/dash_leaflet/#remove-the-border-around-the-map","text":"Add a custom css file: File: assets/custom.css body { margin : 0 , }","title":"Remove the border around the map"},{"location":"coding/python/dash_leaflet/#references","text":"Docs Git","title":"References"},{"location":"coding/python/data_classes/","text":"A data class is a regular Python class that has basic data model methods like __init__() , __repr__() , and __eq__() implemented for you. Introduced in Python 3.7 , they typically containing mainly data, although there aren\u2019t really any restrictions. from dataclasses import dataclass @dataclass class DataClassCard : rank : str suit : str They behave similar to named tuples but come with many more features. At the same time, named tuples have some other features that are not necessarily desirable, such as: By design it's a regular tuple, which can lead to subtle and hard to find bugs. It's hard to add default values to some fields. It's by nature immutable. That being said, if you need your data structure to behave like a tuple, then a named tuple is a great alternative. Advantages over regular classes \u2691 Simplify the class definition @dataclass class DataClassCard : rank : str suit : str # Versus class RegularCard def __init__ ( self , rank , suit ): self . rank = rank self . suit = suit More descriptive object representation through a better default __repr__() method. >>> queen_of_hearts = DataClassCard ( 'Q' , 'Hearts' ) >>> queen_of_hearts DataClassCard ( rank = 'Q' , suit = 'Hearts' ) # Versus >>> queen_of_spades = RegularCard ( 'Q' , 'Spades' ) >>> queen_of_spades < __main__ . RegularCard object at 0x7fb6eee35d30 > * Instance comparison out of the box through a better default __eq__() method. >>> queen_of_hearts == DataClassCard ( 'Q' , 'Hearts' ) True # Versus >>> queen_of_spades == RegularCard ( 'Q' , 'Spades' ) False Usage \u2691 Definition \u2691 from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float What makes this a data class is the @dataclass decorator. Beneath the class Position: , simply list the fields you want in your data class. The data class decorator support the following parameters : init : Add .__init__() method? (Default is True). repr : Add .__repr__() method? (Default is True). eq : Add .__eq__() method? (Default is True). order : Add ordering methods? (Default is False). unsafe_hash : Force the addition of a .__hash__() method? (Default is False). frozen : If True , assigning to fields raise an exception. (Default is False). Default values \u2691 It's easy to add default values to the fields of your data class: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck: from dataclasses import dataclass , field from typing import List RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () def make_french_deck (): return [ PlayingCard ( r , s ) for s in SUITS for r in RANKS ] @dataclass class PlayingCard : rank : str suit : str @dataclass class Deck : cards : List [ PlayingCard ] = field ( default_factory = make_french_deck ) Using cards: List[PlayingCard] = make_french_deck() introduces the using mutable default arguments anti-pattern. Instead, data classes use the default_factory to handle mutable default values. To use it, you need to use the field() specifier which is used to customize each field of a data class individually. It supports the following parameters: default : Default value of the field. default_factory : Function that returns the initial value of the field. init : Use field in .__init__() method? (Default is True ). repr : Use field in repr of the object? (Default is True ). For example to hide a parameter from the repr , use lat: float = field(default=0.0, repr=False) . compare : Include the field in comparisons? (Default is True ). hash : Include the field when calculating hash() ? (Default is to use the same as compare ). metadata : A mapping with information about the field. It's not used by the data classes themselves but is available for you to attach information to fields. For example: from dataclasses import dataclass , field @dataclass class Position : name : str lon : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) lat : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) To retrieve the information use the fields() function. >>> from dataclasses import fields >>> fields ( Position ) ( Field ( name = 'name' , type =< class ' str '>,...,metadata= {} ), Field ( name = 'lon' , type =< class ' float '>,...,metadata={' unit ': ' degrees '}), Field ( name = 'lat' , type =< class ' float '>,...,metadata={' unit ': ' degrees '})) >>> lat_unit = fields ( Position )[ 2 ] . metadata [ 'unit' ] >>> lat_unit 'degrees' Type hints \u2691 They support typing out of the box. Without a type hint, the field will not be a part of the data class. While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language . Adding methods \u2691 Same as with a normal class. Adding complex order comparison logic \u2691 from dataclasses import dataclass @dataclass ( order = True ) class PlayingCard : rank : str suit : str def __str__ ( self ): return f ' { self . suit }{ self . rank } ' After setting order=True in the decorator definition the instances of PlayingCard can be compared. >>> queen_of_hearts = PlayingCard ( 'Q' , '\u2661' ) >>> ace_of_spades = PlayingCard ( 'A' , '\u2660' ) >>> ace_of_spades > queen_of_hearts False Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because Q comes after A in the alphabet. >>> ( 'A' , '\u2660' ) > ( 'Q' , '\u2661' ) False To use more complex comparisons, we need to add the field .sort_index to the class. However, this field should be calculated from the other fields automatically. That's what the special method .__post_init__() is for. It allows for special processing after the regular .__init__() method is called. from dataclasses import dataclass , field RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () @dataclass ( order = True ) class PlayingCard : sort_index : int = field ( init = False , repr = False ) rank : str suit : str def __post_init__ ( self ): self . sort_index = ( RANKS . index ( self . rank ) * len ( SUITS ) + SUITS . index ( self . suit )) def __str__ ( self ): return f ' { self . suit }{ self . rank } ' Note that .sort_index is added as the first field of the class. That way, the comparison is first done using .sort_index and only if there are ties are the other fields used. Using field() , you must also specify that .sort_index should not be included as a parameter in the .__init__() method (because it is calculated from the .rank and .suit fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the repr of the class. Immutable data classes \u2691 To make a data class immutable, set frozen=True when you create it. from dataclasses import dataclass @dataclass ( frozen = True ) class Position : name : str lon : float = 0.0 lat : float = 0.0 In a frozen data class, you can not assign values to the fields after creation: >>> pos = Position ( 'Oslo' , 10.8 , 59.9 ) >>> pos . name 'Oslo' >>> pos . name = 'Stockholm' dataclasses . FrozenInstanceError : cannot assign to field 'name' Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python: from dataclasses import dataclass from typing import List @dataclass ( frozen = True ) class ImmutableCard : rank : str suit : str @dataclass ( frozen = True ) class ImmutableDeck : cards : List [ PlayingCard ] Even though both ImmutableCard and ImmutableDeck are immutable, the list holding cards is not. You can therefore still change the cards in the deck: >>> queen_of_hearts = ImmutableCard ( 'Q' , '\u2661' ) >>> ace_of_spades = ImmutableCard ( 'A' , '\u2660' ) >>> deck = ImmutableDeck ([ queen_of_hearts , ace_of_spades ]) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = 'Q' , suit = '\u2661' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) >>> deck . cards [ 0 ] = ImmutableCard ( '7' , '\u2662' ) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = '7' , suit = '\u2662' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The ImmutableDeck should be implemented using a tuple instead of a list. Inheritance \u2691 You can subclass data classes quite freely. from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float @dataclass class Capital ( Position ): country : str >>> Capital ( 'Oslo' , 10.8 , 59.9 , 'Norway' ) Capital ( name = 'Oslo' , lon = 10.8 , lat = 59.9 , country = 'Norway' ) Warning This won't work if the base class have default values unless all the subclass parameters also have default values. Warning If you redefine a base class field, you need to keep the fields order after the subclass new fields: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 @dataclass class Capital ( Position ): country : str = 'Unknown' lat : float = 40.0 Optimizing Data Classes \u2691 Slots can be used to make classes faster and use less memory. from dataclasses import dataclass @dataclass class SimplePosition : name : str lon : float lat : float @dataclass class SlotPosition : __slots__ = [ 'name' , 'lon' , 'lat' ] name : str lon : float lat : float Essentially, slots are defined using .__slots__ to list the variables on a class. Variables or attributes not present in .__slots__ may not be defined. Furthermore, a slots class may not have default values . References \u2691 Real Python Data classes article","title":"Data Classes"},{"location":"coding/python/data_classes/#advantages-over-regular-classes","text":"Simplify the class definition @dataclass class DataClassCard : rank : str suit : str # Versus class RegularCard def __init__ ( self , rank , suit ): self . rank = rank self . suit = suit More descriptive object representation through a better default __repr__() method. >>> queen_of_hearts = DataClassCard ( 'Q' , 'Hearts' ) >>> queen_of_hearts DataClassCard ( rank = 'Q' , suit = 'Hearts' ) # Versus >>> queen_of_spades = RegularCard ( 'Q' , 'Spades' ) >>> queen_of_spades < __main__ . RegularCard object at 0x7fb6eee35d30 > * Instance comparison out of the box through a better default __eq__() method. >>> queen_of_hearts == DataClassCard ( 'Q' , 'Hearts' ) True # Versus >>> queen_of_spades == RegularCard ( 'Q' , 'Spades' ) False","title":"Advantages over regular classes"},{"location":"coding/python/data_classes/#usage","text":"","title":"Usage"},{"location":"coding/python/data_classes/#definition","text":"from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float What makes this a data class is the @dataclass decorator. Beneath the class Position: , simply list the fields you want in your data class. The data class decorator support the following parameters : init : Add .__init__() method? (Default is True). repr : Add .__repr__() method? (Default is True). eq : Add .__eq__() method? (Default is True). order : Add ordering methods? (Default is False). unsafe_hash : Force the addition of a .__hash__() method? (Default is False). frozen : If True , assigning to fields raise an exception. (Default is False).","title":"Definition"},{"location":"coding/python/data_classes/#default-values","text":"It's easy to add default values to the fields of your data class: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck: from dataclasses import dataclass , field from typing import List RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () def make_french_deck (): return [ PlayingCard ( r , s ) for s in SUITS for r in RANKS ] @dataclass class PlayingCard : rank : str suit : str @dataclass class Deck : cards : List [ PlayingCard ] = field ( default_factory = make_french_deck ) Using cards: List[PlayingCard] = make_french_deck() introduces the using mutable default arguments anti-pattern. Instead, data classes use the default_factory to handle mutable default values. To use it, you need to use the field() specifier which is used to customize each field of a data class individually. It supports the following parameters: default : Default value of the field. default_factory : Function that returns the initial value of the field. init : Use field in .__init__() method? (Default is True ). repr : Use field in repr of the object? (Default is True ). For example to hide a parameter from the repr , use lat: float = field(default=0.0, repr=False) . compare : Include the field in comparisons? (Default is True ). hash : Include the field when calculating hash() ? (Default is to use the same as compare ). metadata : A mapping with information about the field. It's not used by the data classes themselves but is available for you to attach information to fields. For example: from dataclasses import dataclass , field @dataclass class Position : name : str lon : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) lat : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) To retrieve the information use the fields() function. >>> from dataclasses import fields >>> fields ( Position ) ( Field ( name = 'name' , type =< class ' str '>,...,metadata= {} ), Field ( name = 'lon' , type =< class ' float '>,...,metadata={' unit ': ' degrees '}), Field ( name = 'lat' , type =< class ' float '>,...,metadata={' unit ': ' degrees '})) >>> lat_unit = fields ( Position )[ 2 ] . metadata [ 'unit' ] >>> lat_unit 'degrees'","title":"Default values"},{"location":"coding/python/data_classes/#type-hints","text":"They support typing out of the box. Without a type hint, the field will not be a part of the data class. While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language .","title":"Type hints"},{"location":"coding/python/data_classes/#adding-methods","text":"Same as with a normal class.","title":"Adding methods"},{"location":"coding/python/data_classes/#adding-complex-order-comparison-logic","text":"from dataclasses import dataclass @dataclass ( order = True ) class PlayingCard : rank : str suit : str def __str__ ( self ): return f ' { self . suit }{ self . rank } ' After setting order=True in the decorator definition the instances of PlayingCard can be compared. >>> queen_of_hearts = PlayingCard ( 'Q' , '\u2661' ) >>> ace_of_spades = PlayingCard ( 'A' , '\u2660' ) >>> ace_of_spades > queen_of_hearts False Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because Q comes after A in the alphabet. >>> ( 'A' , '\u2660' ) > ( 'Q' , '\u2661' ) False To use more complex comparisons, we need to add the field .sort_index to the class. However, this field should be calculated from the other fields automatically. That's what the special method .__post_init__() is for. It allows for special processing after the regular .__init__() method is called. from dataclasses import dataclass , field RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () @dataclass ( order = True ) class PlayingCard : sort_index : int = field ( init = False , repr = False ) rank : str suit : str def __post_init__ ( self ): self . sort_index = ( RANKS . index ( self . rank ) * len ( SUITS ) + SUITS . index ( self . suit )) def __str__ ( self ): return f ' { self . suit }{ self . rank } ' Note that .sort_index is added as the first field of the class. That way, the comparison is first done using .sort_index and only if there are ties are the other fields used. Using field() , you must also specify that .sort_index should not be included as a parameter in the .__init__() method (because it is calculated from the .rank and .suit fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the repr of the class.","title":"Adding complex order comparison logic"},{"location":"coding/python/data_classes/#immutable-data-classes","text":"To make a data class immutable, set frozen=True when you create it. from dataclasses import dataclass @dataclass ( frozen = True ) class Position : name : str lon : float = 0.0 lat : float = 0.0 In a frozen data class, you can not assign values to the fields after creation: >>> pos = Position ( 'Oslo' , 10.8 , 59.9 ) >>> pos . name 'Oslo' >>> pos . name = 'Stockholm' dataclasses . FrozenInstanceError : cannot assign to field 'name' Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python: from dataclasses import dataclass from typing import List @dataclass ( frozen = True ) class ImmutableCard : rank : str suit : str @dataclass ( frozen = True ) class ImmutableDeck : cards : List [ PlayingCard ] Even though both ImmutableCard and ImmutableDeck are immutable, the list holding cards is not. You can therefore still change the cards in the deck: >>> queen_of_hearts = ImmutableCard ( 'Q' , '\u2661' ) >>> ace_of_spades = ImmutableCard ( 'A' , '\u2660' ) >>> deck = ImmutableDeck ([ queen_of_hearts , ace_of_spades ]) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = 'Q' , suit = '\u2661' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) >>> deck . cards [ 0 ] = ImmutableCard ( '7' , '\u2662' ) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = '7' , suit = '\u2662' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The ImmutableDeck should be implemented using a tuple instead of a list.","title":"Immutable data classes"},{"location":"coding/python/data_classes/#inheritance","text":"You can subclass data classes quite freely. from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float @dataclass class Capital ( Position ): country : str >>> Capital ( 'Oslo' , 10.8 , 59.9 , 'Norway' ) Capital ( name = 'Oslo' , lon = 10.8 , lat = 59.9 , country = 'Norway' ) Warning This won't work if the base class have default values unless all the subclass parameters also have default values. Warning If you redefine a base class field, you need to keep the fields order after the subclass new fields: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 @dataclass class Capital ( Position ): country : str = 'Unknown' lat : float = 40.0","title":"Inheritance"},{"location":"coding/python/data_classes/#optimizing-data-classes","text":"Slots can be used to make classes faster and use less memory. from dataclasses import dataclass @dataclass class SimplePosition : name : str lon : float lat : float @dataclass class SlotPosition : __slots__ = [ 'name' , 'lon' , 'lat' ] name : str lon : float lat : float Essentially, slots are defined using .__slots__ to list the variables on a class. Variables or attributes not present in .__slots__ may not be defined. Furthermore, a slots class may not have default values .","title":"Optimizing Data Classes"},{"location":"coding/python/data_classes/#references","text":"Real Python Data classes article","title":"References"},{"location":"coding/python/deepdiff/","text":"The DeepDiff library is used to perform search and differences in Python objects. It comes with three operations: DeepDiff: Deep Difference of dictionaries, iterables, strings and other objects. It will recursively look for all the changes. DeepSearch: Search for objects within other objects. DeepHash: Hash any object based on their content even if they are not \u201chashable\u201d. Install \u2691 Install from PyPi: pip install deepdiff DeepDiff prefers to use Murmur3 for hashing. However you have to manually install Murmur3 by running: pip install 'deepdiff[murmur]' Otherwise DeepDiff will be using SHA256 for hashing which is a cryptographic hash and is considerably slower. DeepSearch \u2691 Deep Search inside objects to find the item matching your criteria. Note that is searches for either the path to match your criteria or the word in an item. Examples: Importing from deepdiff import DeepSearch , grep DeepSearch comes with grep function which is easier to remember! Search in list for string >>> obj = [ \"long somewhere\" , \"string\" , 0 , \"somewhere great!\" ] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> print ( ds ) { 'matched_values' : { 'root[3]' : 'somewhere great!' , 'root[0]' : 'long somewhere' }} Search in nested data for string >>> obj = [ \"something somewhere\" , { \"long\" : \"somewhere\" , \"string\" : 2 , 0 : 0 , \"somewhere\" : \"around\" }] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> pprint ( ds , indent = 2 ) { 'matched_paths' : { \"root[1]['somewhere']\" : 'around' }, 'matched_values' : { 'root[0]' : 'something somewhere' , \"root[1]['long']\" : 'somewhere' }} To obtain the keys and values of the matched objects, you can use the Extract object. >>> from deepdiff import grep >>> obj = { 1 : [{ '2' : 'b' }, 3 ], 2 : [ 4 , 5 ]} >>> result = obj | grep ( 5 ) >>> result { 'matched_values' : OrderedSet ([ 'root[2][1]' ])} >>> result [ 'matched_values' ][ 0 ] 'root[2][1]' >>> path = result [ 'matched_values' ][ 0 ] >>> extract ( obj , path ) 5 References \u2691 Homepage/Docs Old Docs","title":"DeepDiff"},{"location":"coding/python/deepdiff/#install","text":"Install from PyPi: pip install deepdiff DeepDiff prefers to use Murmur3 for hashing. However you have to manually install Murmur3 by running: pip install 'deepdiff[murmur]' Otherwise DeepDiff will be using SHA256 for hashing which is a cryptographic hash and is considerably slower.","title":"Install"},{"location":"coding/python/deepdiff/#deepsearch","text":"Deep Search inside objects to find the item matching your criteria. Note that is searches for either the path to match your criteria or the word in an item. Examples: Importing from deepdiff import DeepSearch , grep DeepSearch comes with grep function which is easier to remember! Search in list for string >>> obj = [ \"long somewhere\" , \"string\" , 0 , \"somewhere great!\" ] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> print ( ds ) { 'matched_values' : { 'root[3]' : 'somewhere great!' , 'root[0]' : 'long somewhere' }} Search in nested data for string >>> obj = [ \"something somewhere\" , { \"long\" : \"somewhere\" , \"string\" : 2 , 0 : 0 , \"somewhere\" : \"around\" }] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> pprint ( ds , indent = 2 ) { 'matched_paths' : { \"root[1]['somewhere']\" : 'around' }, 'matched_values' : { 'root[0]' : 'something somewhere' , \"root[1]['long']\" : 'somewhere' }} To obtain the keys and values of the matched objects, you can use the Extract object. >>> from deepdiff import grep >>> obj = { 1 : [{ '2' : 'b' }, 3 ], 2 : [ 4 , 5 ]} >>> result = obj | grep ( 5 ) >>> result { 'matched_values' : OrderedSet ([ 'root[2][1]' ])} >>> result [ 'matched_values' ][ 0 ] 'root[2][1]' >>> path = result [ 'matched_values' ][ 0 ] >>> extract ( obj , path ) 5","title":"DeepSearch"},{"location":"coding/python/deepdiff/#references","text":"Homepage/Docs Old Docs","title":"References"},{"location":"coding/python/docstrings/","text":"Docstrings are strings that define the purpose and use of a module, class, function or method. They are accessible from the doc attribute __doc__ and with the built-in help() function. Documenting your code is very important as it's more often read than written . Documenting vs commenting \u2691 Commenting is describing your code to/for developers. The intended audience is the maintainers and developers of the Python code. In conjuction with well-written code, comments help to guide the reader to better understand your code and its purpose and design. Documenting is describing its use and functionality to your users. While it may be helpful in the development process, the main intended audience are the users. Docstring format \u2691 Docstrings should use the triple-double quote ( \"\"\" ) string format. This should be done whether the docstring is multi-lined or not. At a bare minimum, a docstring should be a quick summary of whatever is it you\u2019re describing and should be contained within a single line. Multi-lined docstrings are used to further elaborate on the object beyond the summary. All multi-lined docstrings have the following parts: A one-line summary line A blank line proceeding the summary Any further elaboration for the docstring Another blank line To ensure your docstrings follow these practices, configure flakehell with the flake8-docstrings extension. Docstring types \u2691 Class docstrings \u2691 Class Docstrings are created for the class itself, as well as any class methods. The docstrings are placed immediately following the class or class method indented by one level: class SimpleClass: \"\"\"Class docstrings go here.\"\"\" def say_hello(self, name: str): \"\"\"Class method docstrings go here.\"\"\" print(f'Hello {name}') Class docstrings should contain the following information: A brief summary of its purpose and behavior Any public methods, along with a brief description Any class properties (attributes) Anything related to the interface for subclassers, if the class is intended to be subclassed The class constructor parameters should be documented within the init class method docstring. Individual methods should be documented using their individual docstrings. Class method docstrings should contain the following: A brief description of what the method is and what it\u2019s used for Any arguments (both required and optional) that are passed including keyword arguments Label any arguments that are considered optional or have a default value Any side effects that occur when executing the method Any exceptions that are raised Any restrictions on when the method can be called Package and module docstrings \u2691 Package docstrings should be placed at the top of the package\u2019s __init__.py file. This docstring should list the modules and sub-packages that are exported by the package. Module docstrings should include the following: A brief description of the module and its purpose A list of any classes, exception, functions, and any other objects exported by the module. Only needed if they are not defined in the same file, otherwise help() will get them automatically. The docstring for a module function should include the same items as a class method. Docstring formats \u2691 Google docstrings : the most user friendly. reStructured Text : The official ones, but super ugly to write. Numpy docstrings . Google Docstrings \u2691 Napoleon gathered a nice cheatsheet with examples . Functions and methods \u2691 A method that overrides a method from a base class may have a simple docstring sending the reader to its overridden method\u2019s docstring, such as \"\"\"See base class.\"\"\" . The rationale is that there is no need to repeat in many places documentation that is already present in the base method\u2019s docstring. However, if the overriding method\u2019s behavior is substantially different from the overridden method, or details need to be provided (e.g., documenting additional side effects), a docstring with at least those differences is required on the overriding method. Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function\u2019s name and signature are informative enough that it can be aptly described using a one-line docstring. Args List each parameter by name. A description should follow the name, and be separated by a colon followed by either a space or newline. If the description is too long to fit on a single 80-character line, use a hanging indent of 2 or 4 spaces more than the parameter name (be consistent with the rest of the docstrings in the file). The description should include required type(s) if the code does not contain a corresponding type annotation. If a function accepts *foo (variable length argument lists) and/or **bar (arbitrary keyword arguments), they should be listed as *foo and **bar . Returns (or Yields: for generators) Describe the type and semantics of the return value. If the function only returns None, this section is not required. It may also be omitted if the docstring starts with Returns or Yields (e.g. \"\"\"Returns row from API as a tuple of strings.\"\"\" ) and the opening sentence is sufficient to describe return value. Raises List all exceptions that are relevant to the interface followed by a description. Use a similar exception name + colon + space or newline and hanging indent style as described in Args:. You should not document exceptions that get raised if the API specified in the docstring is violated (because this would paradoxically make behavior under violation of the API part of the API). Automatic documentation generation \u2691 Use the mkdocstrings plugin to automatically generate the documentation. References \u2691 Real Python post on docstrings by James Mertz","title":"Docstrings"},{"location":"coding/python/docstrings/#documenting-vs-commenting","text":"Commenting is describing your code to/for developers. The intended audience is the maintainers and developers of the Python code. In conjuction with well-written code, comments help to guide the reader to better understand your code and its purpose and design. Documenting is describing its use and functionality to your users. While it may be helpful in the development process, the main intended audience are the users.","title":"Documenting vs commenting"},{"location":"coding/python/docstrings/#docstring-format","text":"Docstrings should use the triple-double quote ( \"\"\" ) string format. This should be done whether the docstring is multi-lined or not. At a bare minimum, a docstring should be a quick summary of whatever is it you\u2019re describing and should be contained within a single line. Multi-lined docstrings are used to further elaborate on the object beyond the summary. All multi-lined docstrings have the following parts: A one-line summary line A blank line proceeding the summary Any further elaboration for the docstring Another blank line To ensure your docstrings follow these practices, configure flakehell with the flake8-docstrings extension.","title":"Docstring format"},{"location":"coding/python/docstrings/#docstring-types","text":"","title":"Docstring types"},{"location":"coding/python/docstrings/#class-docstrings","text":"Class Docstrings are created for the class itself, as well as any class methods. The docstrings are placed immediately following the class or class method indented by one level: class SimpleClass: \"\"\"Class docstrings go here.\"\"\" def say_hello(self, name: str): \"\"\"Class method docstrings go here.\"\"\" print(f'Hello {name}') Class docstrings should contain the following information: A brief summary of its purpose and behavior Any public methods, along with a brief description Any class properties (attributes) Anything related to the interface for subclassers, if the class is intended to be subclassed The class constructor parameters should be documented within the init class method docstring. Individual methods should be documented using their individual docstrings. Class method docstrings should contain the following: A brief description of what the method is and what it\u2019s used for Any arguments (both required and optional) that are passed including keyword arguments Label any arguments that are considered optional or have a default value Any side effects that occur when executing the method Any exceptions that are raised Any restrictions on when the method can be called","title":"Class docstrings"},{"location":"coding/python/docstrings/#package-and-module-docstrings","text":"Package docstrings should be placed at the top of the package\u2019s __init__.py file. This docstring should list the modules and sub-packages that are exported by the package. Module docstrings should include the following: A brief description of the module and its purpose A list of any classes, exception, functions, and any other objects exported by the module. Only needed if they are not defined in the same file, otherwise help() will get them automatically. The docstring for a module function should include the same items as a class method.","title":"Package and module docstrings"},{"location":"coding/python/docstrings/#docstring-formats","text":"Google docstrings : the most user friendly. reStructured Text : The official ones, but super ugly to write. Numpy docstrings .","title":"Docstring formats"},{"location":"coding/python/docstrings/#google-docstrings","text":"Napoleon gathered a nice cheatsheet with examples .","title":"Google Docstrings"},{"location":"coding/python/docstrings/#functions-and-methods","text":"A method that overrides a method from a base class may have a simple docstring sending the reader to its overridden method\u2019s docstring, such as \"\"\"See base class.\"\"\" . The rationale is that there is no need to repeat in many places documentation that is already present in the base method\u2019s docstring. However, if the overriding method\u2019s behavior is substantially different from the overridden method, or details need to be provided (e.g., documenting additional side effects), a docstring with at least those differences is required on the overriding method. Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function\u2019s name and signature are informative enough that it can be aptly described using a one-line docstring. Args List each parameter by name. A description should follow the name, and be separated by a colon followed by either a space or newline. If the description is too long to fit on a single 80-character line, use a hanging indent of 2 or 4 spaces more than the parameter name (be consistent with the rest of the docstrings in the file). The description should include required type(s) if the code does not contain a corresponding type annotation. If a function accepts *foo (variable length argument lists) and/or **bar (arbitrary keyword arguments), they should be listed as *foo and **bar . Returns (or Yields: for generators) Describe the type and semantics of the return value. If the function only returns None, this section is not required. It may also be omitted if the docstring starts with Returns or Yields (e.g. \"\"\"Returns row from API as a tuple of strings.\"\"\" ) and the opening sentence is sufficient to describe return value. Raises List all exceptions that are relevant to the interface followed by a description. Use a similar exception name + colon + space or newline and hanging indent style as described in Args:. You should not document exceptions that get raised if the API specified in the docstring is violated (because this would paradoxically make behavior under violation of the API part of the API).","title":"Functions and methods"},{"location":"coding/python/docstrings/#automatic-documentation-generation","text":"Use the mkdocstrings plugin to automatically generate the documentation.","title":"Automatic documentation generation"},{"location":"coding/python/docstrings/#references","text":"Real Python post on docstrings by James Mertz","title":"References"},{"location":"coding/python/factoryboy/","text":"Factoryboy is a fixtures replacement library to generate fake data for your program. As it's designed to work well with different ORMs (Django, SQLAlchemy , Mongo) it serves the purpose of building real objects for your tests. Install \u2691 pip install factory_boy Or add it to the project requirements.txt . Define a factory class \u2691 Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py . Use the factory class \u2691 Create an instance. UserFactory . create () Create an instance with a defined attribute. UserFactory . create ( name = 'John' ) Create 100 instances of objects with an attribute defined. UserFactory . create_batch ( 100 , name = 'John' ) Define attributes \u2691 I like to use the faker integration of factory boy to generate most of the attributes. Generate numbers \u2691 Sequential numbers \u2691 Ideal for IDs id = factory . Sequence ( lambda n : n ) Random number or integer \u2691 author_id = factory . Faker ( 'random_number' ) Random float \u2691 score = factory . Faker ( 'pyfloat' ) Generate strings \u2691 Word \u2691 default = factory . Faker ( 'word' ) Word from a list \u2691 user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ]) Sentences \u2691 description = factory . Faker ( 'sentence' ) Names \u2691 name = factory . Faker ( 'name' ) Urls \u2691 url = factory . Faker ( 'url' ) Files \u2691 file_path = factory . Faker ( 'file_path' ) Generate Datetime \u2691 factory . Faker ( 'date_time' ) Generate bool \u2691 factory . Faker ( 'pybool' ) Generate your own attribute \u2691 In newer versions of Factoryboy this method doesn't work As the Faker object doesn't have the generate method. Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({}) Define relationships \u2691 Factory Inheritance \u2691 class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit' Dependent objects direct ForeignKey \u2691 When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' References \u2691 Docs Git Common recipes","title":"FactoryBoy"},{"location":"coding/python/factoryboy/#install","text":"pip install factory_boy Or add it to the project requirements.txt .","title":"Install"},{"location":"coding/python/factoryboy/#define-a-factory-class","text":"Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py .","title":"Define a factory class"},{"location":"coding/python/factoryboy/#use-the-factory-class","text":"Create an instance. UserFactory . create () Create an instance with a defined attribute. UserFactory . create ( name = 'John' ) Create 100 instances of objects with an attribute defined. UserFactory . create_batch ( 100 , name = 'John' )","title":"Use the factory class"},{"location":"coding/python/factoryboy/#define-attributes","text":"I like to use the faker integration of factory boy to generate most of the attributes.","title":"Define attributes"},{"location":"coding/python/factoryboy/#generate-numbers","text":"","title":"Generate numbers"},{"location":"coding/python/factoryboy/#sequential-numbers","text":"Ideal for IDs id = factory . Sequence ( lambda n : n )","title":"Sequential numbers"},{"location":"coding/python/factoryboy/#random-number-or-integer","text":"author_id = factory . Faker ( 'random_number' )","title":"Random number or integer"},{"location":"coding/python/factoryboy/#random-float","text":"score = factory . Faker ( 'pyfloat' )","title":"Random float"},{"location":"coding/python/factoryboy/#generate-strings","text":"","title":"Generate strings"},{"location":"coding/python/factoryboy/#word","text":"default = factory . Faker ( 'word' )","title":"Word"},{"location":"coding/python/factoryboy/#word-from-a-list","text":"user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ])","title":"Word from a list"},{"location":"coding/python/factoryboy/#sentences","text":"description = factory . Faker ( 'sentence' )","title":"Sentences"},{"location":"coding/python/factoryboy/#names","text":"name = factory . Faker ( 'name' )","title":"Names"},{"location":"coding/python/factoryboy/#urls","text":"url = factory . Faker ( 'url' )","title":"Urls"},{"location":"coding/python/factoryboy/#files","text":"file_path = factory . Faker ( 'file_path' )","title":"Files"},{"location":"coding/python/factoryboy/#generate-datetime","text":"factory . Faker ( 'date_time' )","title":"Generate Datetime"},{"location":"coding/python/factoryboy/#generate-bool","text":"factory . Faker ( 'pybool' )","title":"Generate bool"},{"location":"coding/python/factoryboy/#generate-your-own-attribute","text":"In newer versions of Factoryboy this method doesn't work As the Faker object doesn't have the generate method. Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({})","title":"Generate your own attribute"},{"location":"coding/python/factoryboy/#define-relationships","text":"","title":"Define relationships"},{"location":"coding/python/factoryboy/#factory-inheritance","text":"class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit'","title":"Factory Inheritance"},{"location":"coding/python/factoryboy/#dependent-objects-direct-foreignkey","text":"When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit'","title":"Dependent objects direct ForeignKey"},{"location":"coding/python/factoryboy/#references","text":"Docs Git Common recipes","title":"References"},{"location":"coding/python/faker/","text":"Faker is a Python package that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you. Install \u2691 If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt . Use \u2691 Faker includes a faker fixture for pytest. def test_faker ( faker ): assert isinstance ( faker . name (), str ) By default it's populated with a seed of 0 , to set a random seed add the following to your test configuration. File: conftest.py from random import SystemRandom @pytest . fixture ( scope = \"session\" , autouse = True ) def faker_seed (): return SystemRandom () . randint ( 0 , 999999 ) Generate fake number \u2691 fake . random_number () If you want to specify max and min values use: faker . pyint ( min_value = 0 , max_value = 99 ) Generate a fake dictionary \u2691 fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list' Generate a fake date \u2691 fake . date_time () Generate a random string \u2691 faker . pystr () References \u2691 Git Docs Faker python providers","title":"Faker"},{"location":"coding/python/faker/#install","text":"If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt .","title":"Install"},{"location":"coding/python/faker/#use","text":"Faker includes a faker fixture for pytest. def test_faker ( faker ): assert isinstance ( faker . name (), str ) By default it's populated with a seed of 0 , to set a random seed add the following to your test configuration. File: conftest.py from random import SystemRandom @pytest . fixture ( scope = \"session\" , autouse = True ) def faker_seed (): return SystemRandom () . randint ( 0 , 999999 )","title":"Use"},{"location":"coding/python/faker/#generate-fake-number","text":"fake . random_number () If you want to specify max and min values use: faker . pyint ( min_value = 0 , max_value = 99 )","title":"Generate fake number"},{"location":"coding/python/faker/#generate-a-fake-dictionary","text":"fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list'","title":"Generate a fake dictionary"},{"location":"coding/python/faker/#generate-a-fake-date","text":"fake . date_time ()","title":"Generate a fake date"},{"location":"coding/python/faker/#generate-a-random-string","text":"faker . pystr ()","title":"Generate a random string"},{"location":"coding/python/faker/#references","text":"Git Docs Faker python providers","title":"References"},{"location":"coding/python/feedparser/","text":"Parse Atom and RSS feeds in Python. Install \u2691 pip install feedparser Basic Usage \u2691 Parse a feed from a remote URL \u2691 >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed' Access common elements \u2691 The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID. Channel elements \u2691 >>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ])) Item elements \u2691 >>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60 Advanced usage \u2691 It is possible to interact with feeds that are protected with credentials . Links \u2691 Git Docs","title":"Feedparser"},{"location":"coding/python/feedparser/#install","text":"pip install feedparser","title":"Install"},{"location":"coding/python/feedparser/#basic-usage","text":"","title":"Basic Usage"},{"location":"coding/python/feedparser/#parse-a-feed-from-a-remote-url","text":">>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed'","title":"Parse a feed from a remote URL"},{"location":"coding/python/feedparser/#access-common-elements","text":"The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID.","title":"Access common elements"},{"location":"coding/python/feedparser/#channel-elements","text":">>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ]))","title":"Channel elements"},{"location":"coding/python/feedparser/#item-elements","text":">>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60","title":"Item elements"},{"location":"coding/python/feedparser/#advanced-usage","text":"It is possible to interact with feeds that are protected with credentials .","title":"Advanced usage"},{"location":"coding/python/feedparser/#links","text":"Git Docs","title":"Links"},{"location":"coding/python/flask/","text":"Flask is a micro web framework written in Python. It has no database abstraction layer, form validation, or any other components where pre-existing third-party libraries provide common functions. However, Flask supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies and several common framework related tools. Extensions are updated far more frequently than the core Flask program. Install \u2691 pip install flask How flask blueprints work \u2691 A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint. Making a Flask Blueprint \u2691 References \u2691 Docs Tutorials \u2691 Miguel's Flask Mega-Tutorial Patrick's Software Flask Tutorial Blueprints \u2691 Flask docs on blueprints Explore flask article on Blueprints","title":"Flask"},{"location":"coding/python/flask/#install","text":"pip install flask","title":"Install"},{"location":"coding/python/flask/#how-flask-blueprints-work","text":"A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint.","title":"How flask blueprints work"},{"location":"coding/python/flask/#making-a-flask-blueprint","text":"","title":"Making a Flask Blueprint"},{"location":"coding/python/flask/#references","text":"Docs","title":"References"},{"location":"coding/python/flask/#tutorials","text":"Miguel's Flask Mega-Tutorial Patrick's Software Flask Tutorial","title":"Tutorials"},{"location":"coding/python/flask/#blueprints","text":"Flask docs on blueprints Explore flask article on Blueprints","title":"Blueprints"},{"location":"coding/python/folium/","text":"folium makes it easy to visualize data that\u2019s been manipulated in Python on an interactive leaflet map. It enables both the binding of data to a map for choropleth visualizations as well as passing rich vector/raster/HTML visualizations as markers on the map. The library has a number of built-in tilesets from OpenStreetMap, Mapbox, and Stamen, and supports custom tilesets with Mapbox or Cloudmade API keys. folium supports both Image, Video, GeoJSON and TopoJSON overlays. Use dash-leaflet if you want to do complex stuff. If you want to do multiple filters on the plotted data or connect the map with other graphics, use dash-leaflet instead. Install \u2691 Although you can install it with pip install folium their release pace is slow, therefore I recommend pulling it directly from master pip install git+https://github.com/python-visualization/folium.git@master It's a heavy repo, so it might take some time. Usage \u2691 Use the following snippet to create an empty map centered in Spain import folium m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , ) # Map configuration goes here m . save ( \"map.html\" ) From now on, those lines are going to be assumed, and all the snippets are supposed to go in between the definition and the save. If you need to center the map elsewhere, I suggest that you configure the map to show the coordinates of the mouse with from folium.plugins import MousePosition MousePosition () . add_to ( m ) Change tileset \u2691 Folium Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , tiles = None , ) folium . raster_layers . TileLayer ( name = \"OpenStreetMaps\" , tiles = \"OpenStreetMap\" , attr = \"OpenStreetMaps\" , ) . add_to ( m ) folium . raster_layers . TileLayer ( name = \"IGN\" , tiles = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attr = \"IGN\" , ) . add_to ( m ) folium . LayerControl () . add_to ( m ) We need to set the tiles=None in the Map definition so both are shown in the LayerControl menu. Loading the data \u2691 Using geojson \u2691 folium . GeoJson ( \"my_map.geojson\" , name = \"geojson\" ) . add_to ( m ) If you don't want the data to be embedded in the html use embed=False , this can be handy if you don't want to redeploy the web application on each change of data. You'll need to supply a valid url to the data file. The downside (as of today) of using geojson is that you can't have different markers for the data . The solution is to load it and iterate over the elements. See the issue for more information. Using gpx \u2691 Another popular data format is gpx, which is the one that OsmAnd uses. To import it we'll use the gpxpy library. import gpxpy import gpxpy.gpx gpx_file = open ( 'map.gpx' , 'r' ) gpx = gpxpy . parse ( gpx_file ) References \u2691 Git Docs Quickstart Examples , more examples Use examples \u2691 Flask example Build Interactive GPS activity maps from GPX files Use Open Data to build interactive maps Search examples \u2691 Official docs Leafleft search control Leafleft search examples ( source code ) Searching in OSM data It seems that it doesn't yet support searching for multiple attributes in the geojson data","title":"Folium"},{"location":"coding/python/folium/#install","text":"Although you can install it with pip install folium their release pace is slow, therefore I recommend pulling it directly from master pip install git+https://github.com/python-visualization/folium.git@master It's a heavy repo, so it might take some time.","title":"Install"},{"location":"coding/python/folium/#usage","text":"Use the following snippet to create an empty map centered in Spain import folium m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , ) # Map configuration goes here m . save ( \"map.html\" ) From now on, those lines are going to be assumed, and all the snippets are supposed to go in between the definition and the save. If you need to center the map elsewhere, I suggest that you configure the map to show the coordinates of the mouse with from folium.plugins import MousePosition MousePosition () . add_to ( m )","title":"Usage"},{"location":"coding/python/folium/#change-tileset","text":"Folium Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , tiles = None , ) folium . raster_layers . TileLayer ( name = \"OpenStreetMaps\" , tiles = \"OpenStreetMap\" , attr = \"OpenStreetMaps\" , ) . add_to ( m ) folium . raster_layers . TileLayer ( name = \"IGN\" , tiles = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attr = \"IGN\" , ) . add_to ( m ) folium . LayerControl () . add_to ( m ) We need to set the tiles=None in the Map definition so both are shown in the LayerControl menu.","title":"Change tileset"},{"location":"coding/python/folium/#loading-the-data","text":"","title":"Loading the data"},{"location":"coding/python/folium/#using-geojson","text":"folium . GeoJson ( \"my_map.geojson\" , name = \"geojson\" ) . add_to ( m ) If you don't want the data to be embedded in the html use embed=False , this can be handy if you don't want to redeploy the web application on each change of data. You'll need to supply a valid url to the data file. The downside (as of today) of using geojson is that you can't have different markers for the data . The solution is to load it and iterate over the elements. See the issue for more information.","title":"Using geojson"},{"location":"coding/python/folium/#using-gpx","text":"Another popular data format is gpx, which is the one that OsmAnd uses. To import it we'll use the gpxpy library. import gpxpy import gpxpy.gpx gpx_file = open ( 'map.gpx' , 'r' ) gpx = gpxpy . parse ( gpx_file )","title":"Using gpx"},{"location":"coding/python/folium/#references","text":"Git Docs Quickstart Examples , more examples","title":"References"},{"location":"coding/python/folium/#use-examples","text":"Flask example Build Interactive GPS activity maps from GPX files Use Open Data to build interactive maps","title":"Use examples"},{"location":"coding/python/folium/#search-examples","text":"Official docs Leafleft search control Leafleft search examples ( source code ) Searching in OSM data It seems that it doesn't yet support searching for multiple attributes in the geojson data","title":"Search examples"},{"location":"coding/python/mkdocstrings/","text":"mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings. Install \u2691 pip install mkdocstrings Activate the plugin by adding it to the plugin section in the mkdocs.yml configuration file: plugins : - mkdocstrings Usage \u2691 MkDocstrings works by processing special expressions in your Markdown files. The syntax is as follow: ::: identifier YAML block The identifier is a string identifying the object you want to document. The format of an identifier can vary from one handler to another. For example, the Python handler expects the full dotted-path to a Python object: my_package.my_module.MyClass.my_method . The YAML block is optional, and contains some configuration options: handler : the name of the handler to use to collect and render this object. selection : a dictionary of options passed to the handler's collector. The collector is responsible for collecting the documentation from the source code. Therefore, selection options change how the documentation is collected from the source code. rendering : a dictionary of options passed to the handler's renderer. The renderer is responsible for rendering the documentation with Jinja2 templates. Therefore, rendering options affect how the selected object's documentation is rendered. Example with the Python handler docs/my_page.md # Documentation for `MyClass` ::: my_package.my_module.MyClass handler: python selection: members: - method_a - method_b rendering: show_root_heading: false show_source: false mkdocs.yml nav : - \"My page\" : my_page.md src/my_package/my_module.py class MyClass : \"\"\"Print print print!\"\"\" def method_a ( self ): \"\"\"Print A!\"\"\" print ( \"A!\" ) def method_b ( self ): \"\"\"Print B!\"\"\" print ( \"B!\" ) def method_c ( self ): \"\"\"Print C!\"\"\" print ( \"C!\" ) Result Documentation for MyClass Print print print! method_a ( self ) Print A! method_b ( self ) Print B! Reference the objects in the documentation \u2691 With a custom title: [`Object 1`][full.path.object1] With the identifier as title: [full.path.object2][] Global options \u2691 MkDocstrings accept a few top-level configuration options in mkdocs.yml : watch : a list of directories to watch while serving the documentation. So if any file is changed in those directories, the documentation is rebuilt. default_handler : the handler that is used by default when no handler is specified. custom_templates : the path to a directory containing custom templates. The path is relative to the docs directory. See Customization . handlers : the handlers global configuration. Example: plugins : - mkdocstrings : default_handler : python handlers : python : rendering : show_source : false custom_templates : templates watch : - src/my_package The handlers global configuration can then be overridden by local configurations: :: : my_package.my_module.MyClass rendering : show_source : true Check the Python handler options for more details. References \u2691 Docs","title":"mkdocstrings"},{"location":"coding/python/mkdocstrings/#install","text":"pip install mkdocstrings Activate the plugin by adding it to the plugin section in the mkdocs.yml configuration file: plugins : - mkdocstrings","title":"Install"},{"location":"coding/python/mkdocstrings/#usage","text":"MkDocstrings works by processing special expressions in your Markdown files. The syntax is as follow: ::: identifier YAML block The identifier is a string identifying the object you want to document. The format of an identifier can vary from one handler to another. For example, the Python handler expects the full dotted-path to a Python object: my_package.my_module.MyClass.my_method . The YAML block is optional, and contains some configuration options: handler : the name of the handler to use to collect and render this object. selection : a dictionary of options passed to the handler's collector. The collector is responsible for collecting the documentation from the source code. Therefore, selection options change how the documentation is collected from the source code. rendering : a dictionary of options passed to the handler's renderer. The renderer is responsible for rendering the documentation with Jinja2 templates. Therefore, rendering options affect how the selected object's documentation is rendered. Example with the Python handler docs/my_page.md # Documentation for `MyClass` ::: my_package.my_module.MyClass handler: python selection: members: - method_a - method_b rendering: show_root_heading: false show_source: false mkdocs.yml nav : - \"My page\" : my_page.md src/my_package/my_module.py class MyClass : \"\"\"Print print print!\"\"\" def method_a ( self ): \"\"\"Print A!\"\"\" print ( \"A!\" ) def method_b ( self ): \"\"\"Print B!\"\"\" print ( \"B!\" ) def method_c ( self ): \"\"\"Print C!\"\"\" print ( \"C!\" ) Result","title":"Usage"},{"location":"coding/python/mkdocstrings/#reference-the-objects-in-the-documentation","text":"With a custom title: [`Object 1`][full.path.object1] With the identifier as title: [full.path.object2][]","title":"Reference the objects in the documentation"},{"location":"coding/python/mkdocstrings/#global-options","text":"MkDocstrings accept a few top-level configuration options in mkdocs.yml : watch : a list of directories to watch while serving the documentation. So if any file is changed in those directories, the documentation is rebuilt. default_handler : the handler that is used by default when no handler is specified. custom_templates : the path to a directory containing custom templates. The path is relative to the docs directory. See Customization . handlers : the handlers global configuration. Example: plugins : - mkdocstrings : default_handler : python handlers : python : rendering : show_source : false custom_templates : templates watch : - src/my_package The handlers global configuration can then be overridden by local configurations: :: : my_package.my_module.MyClass rendering : show_source : true Check the Python handler options for more details.","title":"Global options"},{"location":"coding/python/mkdocstrings/#references","text":"Docs","title":"References"},{"location":"coding/python/pandas/","text":"Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Install \u2691 pip3 install pandas Import import pandas as pd Snippets \u2691 Load csv \u2691 data = pd . read_csv ( \"filename.csv\" ) If you want to parse the dates of the start column give read_csv the argument parse_dates=['start'] . Do operation on column data and save it in other column \u2691 # make a simple dataframe df = pd . DataFrame ({ 'a' :[ 1 , 2 ], 'b' :[ 3 , 4 ]}) df # a b # 0 1 3 # 1 2 4 # create an unattached column with an index df . apply ( lambda row : row . a + row . b , axis = 1 ) # 0 4 # 1 6 # do same but attach it to the dataframe df [ 'c' ] = df . apply ( lambda row : row . a + row . b , axis = 1 ) df # a b c # 0 1 3 4 # 1 2 4 6 Get unique values of column \u2691 If we want to get the unique values of the name column: df . name . unique () Extract columns of dataframe \u2691 df1 = df [[ 'a' , 'b' ]] Remove dumplicate rows \u2691 df = df . drop_duplicates () Remove column from dataframe \u2691 del df [ 'name' ] Count unique combinations of values in selected columns \u2691 df1 . groupby ([ 'A' , 'B' ]) . size () . reset_index () . rename ( columns = { 0 : 'count' }) A B count 0 no no 1 1 no yes 2 2 yes no 4 3 yes yes 3 Get row that contains the maximum value of a column \u2691 df . loc [ df [ 'Value' ] . idxmax ()] References \u2691 Homepage","title":"Pandas"},{"location":"coding/python/pandas/#install","text":"pip3 install pandas Import import pandas as pd","title":"Install"},{"location":"coding/python/pandas/#snippets","text":"","title":"Snippets"},{"location":"coding/python/pandas/#load-csv","text":"data = pd . read_csv ( \"filename.csv\" ) If you want to parse the dates of the start column give read_csv the argument parse_dates=['start'] .","title":"Load csv"},{"location":"coding/python/pandas/#do-operation-on-column-data-and-save-it-in-other-column","text":"# make a simple dataframe df = pd . DataFrame ({ 'a' :[ 1 , 2 ], 'b' :[ 3 , 4 ]}) df # a b # 0 1 3 # 1 2 4 # create an unattached column with an index df . apply ( lambda row : row . a + row . b , axis = 1 ) # 0 4 # 1 6 # do same but attach it to the dataframe df [ 'c' ] = df . apply ( lambda row : row . a + row . b , axis = 1 ) df # a b c # 0 1 3 4 # 1 2 4 6","title":"Do operation on column data and save it in other column"},{"location":"coding/python/pandas/#get-unique-values-of-column","text":"If we want to get the unique values of the name column: df . name . unique ()","title":"Get unique values of column"},{"location":"coding/python/pandas/#extract-columns-of-dataframe","text":"df1 = df [[ 'a' , 'b' ]]","title":"Extract columns of dataframe"},{"location":"coding/python/pandas/#remove-dumplicate-rows","text":"df = df . drop_duplicates ()","title":"Remove dumplicate rows"},{"location":"coding/python/pandas/#remove-column-from-dataframe","text":"del df [ 'name' ]","title":"Remove column from dataframe"},{"location":"coding/python/pandas/#count-unique-combinations-of-values-in-selected-columns","text":"df1 . groupby ([ 'A' , 'B' ]) . size () . reset_index () . rename ( columns = { 0 : 'count' }) A B count 0 no no 1 1 no yes 2 2 yes no 4 3 yes yes 3","title":"Count unique combinations of values in selected columns"},{"location":"coding/python/pandas/#get-row-that-contains-the-maximum-value-of-a-column","text":"df . loc [ df [ 'Value' ] . idxmax ()]","title":"Get row that contains the maximum value of a column"},{"location":"coding/python/pandas/#references","text":"Homepage","title":"References"},{"location":"coding/python/passpy/","text":"passpy a platform independent library and cli that is compatible with ZX2C4's pass . Installation \u2691 pip install passpy Usage \u2691 To use passpy in your Python project, we will first have to create a new passpy.store.Store object. import passpy store = passpy . Store () If git or gpg2 are not in your PATH you will have to specify them via git_bin and gpg_bin when creating the store object. You can also create the store on a different folder, be passing store_dir along. To initialize the password store at store_dir , if it isn't already, use store . init_store ( 'store gpg id' ) Where store gpg id is the name of a GPG ID. Optionally, git can be initialized in very much the same way. store . init_git () You are now ready to interact with the password store. You can set and get keys using passpy.store.Store.set_key and passpy.store.Store.get_key . passpy.store.Store.gen_key generates a new password for a new or existing key. To delete a key or directory, use passpy.store.Store.remove_path . For a full overview over all available methods see store-module-label . References \u2691 Docs Git","title":"Passpy"},{"location":"coding/python/passpy/#installation","text":"pip install passpy","title":"Installation"},{"location":"coding/python/passpy/#usage","text":"To use passpy in your Python project, we will first have to create a new passpy.store.Store object. import passpy store = passpy . Store () If git or gpg2 are not in your PATH you will have to specify them via git_bin and gpg_bin when creating the store object. You can also create the store on a different folder, be passing store_dir along. To initialize the password store at store_dir , if it isn't already, use store . init_store ( 'store gpg id' ) Where store gpg id is the name of a GPG ID. Optionally, git can be initialized in very much the same way. store . init_git () You are now ready to interact with the password store. You can set and get keys using passpy.store.Store.set_key and passpy.store.Store.get_key . passpy.store.Store.gen_key generates a new password for a new or existing key. To delete a key or directory, use passpy.store.Store.remove_path . For a full overview over all available methods see store-module-label .","title":"Usage"},{"location":"coding/python/passpy/#references","text":"Docs Git","title":"References"},{"location":"coding/python/plotly/","text":"Plotly is a Python graphing library that makes interactive, publication-quality graphs. Install \u2691 pip3 install plotly Import import plotly.graph_objects as go Snippets \u2691 Select graph source using dropdown \u2691 # imports import plotly.graph_objects as go import numpy as np # data x = list ( np . linspace ( - np . pi , np . pi , 100 )) values_1 = list ( np . sin ( x )) values_1b = [ elem *- 1 for elem in values_1 ] values_2 = list ( np . tan ( x )) values_2b = [ elem *- 1 for elem in values_2 ] # plotly setup] fig = go . Figure () # Add one ore more traces fig . add_traces ( go . Scatter ( x = x , y = values_1 )) fig . add_traces ( go . Scatter ( x = x , y = values_1b )) # construct menus updatemenus = [{ 'buttons' : [{ 'method' : 'update' , 'label' : 'Val 1' , 'args' : [{ 'y' : [ values_1 , values_1b ]},] }, { 'method' : 'update' , 'label' : 'Val 2' , 'args' : [{ 'y' : [ values_2 , values_2b ]},]}], 'direction' : 'down' , 'showactive' : True ,}] # update layout with buttons, and show the figure fig . update_layout ( updatemenus = updatemenus ) fig . show () References \u2691 Homepage Git","title":"Plotly"},{"location":"coding/python/plotly/#install","text":"pip3 install plotly Import import plotly.graph_objects as go","title":"Install"},{"location":"coding/python/plotly/#snippets","text":"","title":"Snippets"},{"location":"coding/python/plotly/#select-graph-source-using-dropdown","text":"# imports import plotly.graph_objects as go import numpy as np # data x = list ( np . linspace ( - np . pi , np . pi , 100 )) values_1 = list ( np . sin ( x )) values_1b = [ elem *- 1 for elem in values_1 ] values_2 = list ( np . tan ( x )) values_2b = [ elem *- 1 for elem in values_2 ] # plotly setup] fig = go . Figure () # Add one ore more traces fig . add_traces ( go . Scatter ( x = x , y = values_1 )) fig . add_traces ( go . Scatter ( x = x , y = values_1b )) # construct menus updatemenus = [{ 'buttons' : [{ 'method' : 'update' , 'label' : 'Val 1' , 'args' : [{ 'y' : [ values_1 , values_1b ]},] }, { 'method' : 'update' , 'label' : 'Val 2' , 'args' : [{ 'y' : [ values_2 , values_2b ]},]}], 'direction' : 'down' , 'showactive' : True ,}] # update layout with buttons, and show the figure fig . update_layout ( updatemenus = updatemenus ) fig . show ()","title":"Select graph source using dropdown"},{"location":"coding/python/plotly/#references","text":"Homepage Git","title":"References"},{"location":"coding/python/pydantic/","text":"Pydantic is a data validation and settings management using python type annotations. pydantic enforces type hints at runtime, and provides user friendly errors when data is invalid. Define how data should be in pure, canonical python; check it with pydantic. Install \u2691 pip install pydantic Models \u2691 The primary means of defining objects in pydantic is via models (models are simply classes which inherit from BaseModel ). You can think of models as similar to types in strictly typed languages, or as the requirements of a single endpoint in an API. Untrusted data can be passed to a model, and after parsing and validation pydantic guarantees that the fields of the resultant model instance will conform to the field types defined on the model. Basic model usage \u2691 from pydantic import BaseModel class User ( BaseModel ): id : int name = 'Jane Doe' User here is a model with two fields id which is an integer and is required, and name which is a string and is not required (it has a default value). The type of name is inferred from the default value, and so a type annotation is not required. user = User ( id = '123' ) user here is an instance of User . Initialisation of the object will perform all parsing and validation, if no ValidationError is raised, you know the resulting model instance is valid. Model properties \u2691 Models possess the following methods and attributes: dict() returns a dictionary of the model's fields and values. json() returns a JSON string representation dict() . copy() returns a deep copy of the model. parse_obj() very similar to the __init__ method of the model, except it takes a dict rather than keyword arguments. If the object passed is not a dict a ValidationError will be raised. parse_raw() takes a str or bytes and parses it as json , then passes the result to parse_obj . parse_file() reads a file and passes the contents to parse_raw . If content_type is omitted, it is inferred from the file's extension. from_orm() loads data into a model from an arbitrary class. schema() returns a dictionary representing the model as JSON Schema. schema_json() returns a JSON string representation of schema() . Recursive Models \u2691 More complex hierarchical data structures can be defined using models themselves as types in annotations. from typing import List from pydantic import BaseModel class Foo ( BaseModel ): count : int size : float = None class Bar ( BaseModel ): apple = 'x' banana = 'y' class Spam ( BaseModel ): foo : Foo bars : List [ Bar ] m = Spam ( foo = { 'count' : 4 }, bars = [{ 'apple' : 'x1' }, { 'apple' : 'x2' }]) print ( m ) #> foo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'), #> Bar(apple='x2', banana='y')] print ( m . dict ()) \"\"\" { 'foo': {'count': 4, 'size': None}, 'bars': [ {'apple': 'x1', 'banana': 'y'}, {'apple': 'x2', 'banana': 'y'}, ], } \"\"\" For self-referencing models, use postponed annotations. Error Handling \u2691 pydantic will raise ValidationError whenever it finds an error in the data it's validating. Note Validation code should not raise ValidationError itself, but rather raise ValueError , TypeError or AssertionError (or subclasses of ValueError or TypeError ) which will be caught and used to populate ValidationError . One exception will be raised regardless of the number of errors found, that ValidationError will contain information about all the errors and how they happened. You can access these errors in a several ways: e.errors() method will return list of errors found in the input data. e.json() method will return a JSON representation of errors . str(e) method will return a human readable representation of the errors. Each error object contains: loc the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model, subsequent items will be present to indicate the nested location of the error. type a computer-readable identifier of the error type. msg a human readable explanation of the error. ctx an optional object which contains values required to render the error message. Custom Errors \u2691 You can also define your own error classes, which can specify a custom error code, message template, and context: from pydantic import BaseModel , PydanticValueError , ValidationError , validator class NotABarError ( PydanticValueError ): code = 'not_a_bar' msg_template = 'value is not \"bar\", got \" {wrong_value} \"' class Model ( BaseModel ): foo : str @validator ( 'foo' ) def name_must_contain_space ( cls , v ): if v != 'bar' : raise NotABarError ( wrong_value = v ) return v try : Model ( foo = 'ber' ) except ValidationError as e : print ( e . json ()) \"\"\" [ { \"loc\": [ \"foo\" ], \"msg\": \"value is not \\\"bar\\\", got \\\"ber\\\"\", \"type\": \"value_error.not_a_bar\", \"ctx\": { \"wrong_value\": \"ber\" } } ] \"\"\" Dynamic model creation \u2691 There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the create_model method to allow models to be created on the fly. from pydantic import BaseModel , create_model DynamicFoobarModel = create_model ( 'DynamicFoobarModel' , foo = ( str , ... ), bar = 123 ) class StaticFoobarModel ( BaseModel ): foo : str bar : int = 123 Here StaticFoobarModel and DynamicFoobarModel are identical. Warning See the note in Required Optional Fields for the distinct between an ellipsis as a field default and annotation only fields. See samuelcolvin/pydantic#1047 for more details. Fields are defined by either a tuple of the form (<type>, <default value>) or just a default value. The special key word arguments __config__ and __base__ can be used to customize the new model. This includes extending a base model with extra fields. from pydantic import BaseModel , create_model class FooModel ( BaseModel ): foo : str bar : int = 123 BarModel = create_model ( 'BarModel' , apple = 'russet' , banana = 'yellow' , __base__ = FooModel , ) print ( BarModel ) #> <class 'BarModel'> print ( BarModel . __fields__ . keys ()) #> dict_keys(['foo', 'bar', 'apple', 'banana']) Abstract Base Classes \u2691 Pydantic models can be used alongside Python's Abstract Base Classes (ABCs). import abc from pydantic import BaseModel class FooBarModel ( BaseModel , abc . ABC ): a : str b : int @abc . abstractmethod def my_abstract_method ( self ): pass Field Ordering \u2691 Field order is important in models for the following reasons: Validation is performed in the order fields are defined; fields validators can access the values of earlier fields, but not later ones Field order is preserved in the model schema Field order is preserved in validation errors Field order is preserved by .dict() and .json() etc. As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined. Field with dynamic default value \u2691 When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a default_factory . In Beta The default_factory argument is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its signature or behaviour will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue. Example of usage: from datetime import datetime from uuid import UUID , uuid4 from pydantic import BaseModel , Field class Model ( BaseModel ): uid : UUID = Field ( default_factory = uuid4 ) updated : datetime = Field ( default_factory = datetime . utcnow ) m1 = Model () m2 = Model () print ( f ' { m1 . uid } != { m2 . uid } ' ) #> 3b187763-a19c-4ed8-9588-387e224e04f1 != 0c58f97b-c8a7-4fe8-8550-e9b2b8026574 print ( f ' { m1 . updated } != { m2 . updated } ' ) #> 2020-07-15 20:01:48.451066 != 2020-07-15 20:01:48.451083 Warning The default_factory expects the field type to be set. Moreover if you want to validate default values with validate_all , pydantic will need to call the default_factory , which could lead to side effects! Parsing data into a specified type \u2691 Pydantic includes a standalone utility function parse_obj_as that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to BaseModel.parse_obj , but works with arbitrary pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of BaseModel . For example: from typing import List from pydantic import BaseModel , parse_obj_as class Item ( BaseModel ): id : int name : str # `item_data` could come from an API call, eg., via something like: # item_data = requests.get('https://my-api.com/items').json() item_data = [{ 'id' : 1 , 'name' : 'My Item' }] items = parse_obj_as ( List [ Item ], item_data ) print ( items ) #> [Item(id=1, name='My Item')] This function is capable of parsing data into any of the types pydantic can handle as fields of a BaseModel . Pydantic also includes a similar standalone function called parse_file_as , which is analogous to BaseModel.parse_file . Data Conversion \u2691 pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example: from pydantic import BaseModel class Model ( BaseModel ): a : int b : float c : str print ( Model ( a = 3.1415 , b = ' 2.72 ' , c = 123 ) . dict ()) #> {'a': 3, 'b': 2.72, 'c': '123'} This is a deliberate decision of pydantic , and in general it's the most useful approach. See here for a longer discussion on the subject. Initialize attributes at object creation \u2691 If you want to initialize attributes of the object automatically at object creation, similar of what you'd do with the __init__ method of the class, you need to use root_validators . class PypikaRepository ( BaseModel ): \"\"\"Implement the repository pattern using the Pypika query builder.\"\"\" connection : sqlite3 . Connection cursor : sqlite3 . Cursor class Config : \"\"\"Configure the pydantic model.\"\"\" arbitrary_types_allowed = True @root_validator ( pre = True ) @classmethod def set_connection ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Set the connection to the database. Raises: ConnectionError: If there is no database file. \"\"\" database_file = values [ \"database_url\" ] . replace ( \"sqlite:///\" , \"\" ) if not os . path . isfile ( database_file ): raise ConnectionError ( f \"There is no database file: { database_file } \" ) connection = sqlite3 . connect ( database_file ) values [ \"connection\" ] = connection values [ \"cursor\" ] = connection . cursor () return values I had to set the arbitrary_types_allowed because the sqlite3 objects are not between the pydantic object types. Troubleshooting \u2691 E0611: No name 'BaseModel' in module 'pydantic' \u2691 Add to your pyproject.toml the following lines: # --------- Pylint ------------- [tool.pylint] extension-pkg-whitelist = 'pydantic' Or if it fails, add to the line # pylint: extension-pkg-whitelist . References \u2691 Docs","title":"Pydantic"},{"location":"coding/python/pydantic/#install","text":"pip install pydantic","title":"Install"},{"location":"coding/python/pydantic/#models","text":"The primary means of defining objects in pydantic is via models (models are simply classes which inherit from BaseModel ). You can think of models as similar to types in strictly typed languages, or as the requirements of a single endpoint in an API. Untrusted data can be passed to a model, and after parsing and validation pydantic guarantees that the fields of the resultant model instance will conform to the field types defined on the model.","title":"Models"},{"location":"coding/python/pydantic/#basic-model-usage","text":"from pydantic import BaseModel class User ( BaseModel ): id : int name = 'Jane Doe' User here is a model with two fields id which is an integer and is required, and name which is a string and is not required (it has a default value). The type of name is inferred from the default value, and so a type annotation is not required. user = User ( id = '123' ) user here is an instance of User . Initialisation of the object will perform all parsing and validation, if no ValidationError is raised, you know the resulting model instance is valid.","title":"Basic model usage"},{"location":"coding/python/pydantic/#model-properties","text":"Models possess the following methods and attributes: dict() returns a dictionary of the model's fields and values. json() returns a JSON string representation dict() . copy() returns a deep copy of the model. parse_obj() very similar to the __init__ method of the model, except it takes a dict rather than keyword arguments. If the object passed is not a dict a ValidationError will be raised. parse_raw() takes a str or bytes and parses it as json , then passes the result to parse_obj . parse_file() reads a file and passes the contents to parse_raw . If content_type is omitted, it is inferred from the file's extension. from_orm() loads data into a model from an arbitrary class. schema() returns a dictionary representing the model as JSON Schema. schema_json() returns a JSON string representation of schema() .","title":"Model properties"},{"location":"coding/python/pydantic/#recursive-models","text":"More complex hierarchical data structures can be defined using models themselves as types in annotations. from typing import List from pydantic import BaseModel class Foo ( BaseModel ): count : int size : float = None class Bar ( BaseModel ): apple = 'x' banana = 'y' class Spam ( BaseModel ): foo : Foo bars : List [ Bar ] m = Spam ( foo = { 'count' : 4 }, bars = [{ 'apple' : 'x1' }, { 'apple' : 'x2' }]) print ( m ) #> foo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'), #> Bar(apple='x2', banana='y')] print ( m . dict ()) \"\"\" { 'foo': {'count': 4, 'size': None}, 'bars': [ {'apple': 'x1', 'banana': 'y'}, {'apple': 'x2', 'banana': 'y'}, ], } \"\"\" For self-referencing models, use postponed annotations.","title":"Recursive Models"},{"location":"coding/python/pydantic/#error-handling","text":"pydantic will raise ValidationError whenever it finds an error in the data it's validating. Note Validation code should not raise ValidationError itself, but rather raise ValueError , TypeError or AssertionError (or subclasses of ValueError or TypeError ) which will be caught and used to populate ValidationError . One exception will be raised regardless of the number of errors found, that ValidationError will contain information about all the errors and how they happened. You can access these errors in a several ways: e.errors() method will return list of errors found in the input data. e.json() method will return a JSON representation of errors . str(e) method will return a human readable representation of the errors. Each error object contains: loc the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model, subsequent items will be present to indicate the nested location of the error. type a computer-readable identifier of the error type. msg a human readable explanation of the error. ctx an optional object which contains values required to render the error message.","title":"Error Handling"},{"location":"coding/python/pydantic/#custom-errors","text":"You can also define your own error classes, which can specify a custom error code, message template, and context: from pydantic import BaseModel , PydanticValueError , ValidationError , validator class NotABarError ( PydanticValueError ): code = 'not_a_bar' msg_template = 'value is not \"bar\", got \" {wrong_value} \"' class Model ( BaseModel ): foo : str @validator ( 'foo' ) def name_must_contain_space ( cls , v ): if v != 'bar' : raise NotABarError ( wrong_value = v ) return v try : Model ( foo = 'ber' ) except ValidationError as e : print ( e . json ()) \"\"\" [ { \"loc\": [ \"foo\" ], \"msg\": \"value is not \\\"bar\\\", got \\\"ber\\\"\", \"type\": \"value_error.not_a_bar\", \"ctx\": { \"wrong_value\": \"ber\" } } ] \"\"\"","title":"Custom Errors"},{"location":"coding/python/pydantic/#dynamic-model-creation","text":"There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the create_model method to allow models to be created on the fly. from pydantic import BaseModel , create_model DynamicFoobarModel = create_model ( 'DynamicFoobarModel' , foo = ( str , ... ), bar = 123 ) class StaticFoobarModel ( BaseModel ): foo : str bar : int = 123 Here StaticFoobarModel and DynamicFoobarModel are identical. Warning See the note in Required Optional Fields for the distinct between an ellipsis as a field default and annotation only fields. See samuelcolvin/pydantic#1047 for more details. Fields are defined by either a tuple of the form (<type>, <default value>) or just a default value. The special key word arguments __config__ and __base__ can be used to customize the new model. This includes extending a base model with extra fields. from pydantic import BaseModel , create_model class FooModel ( BaseModel ): foo : str bar : int = 123 BarModel = create_model ( 'BarModel' , apple = 'russet' , banana = 'yellow' , __base__ = FooModel , ) print ( BarModel ) #> <class 'BarModel'> print ( BarModel . __fields__ . keys ()) #> dict_keys(['foo', 'bar', 'apple', 'banana'])","title":"Dynamic model creation"},{"location":"coding/python/pydantic/#abstract-base-classes","text":"Pydantic models can be used alongside Python's Abstract Base Classes (ABCs). import abc from pydantic import BaseModel class FooBarModel ( BaseModel , abc . ABC ): a : str b : int @abc . abstractmethod def my_abstract_method ( self ): pass","title":"Abstract Base Classes"},{"location":"coding/python/pydantic/#field-ordering","text":"Field order is important in models for the following reasons: Validation is performed in the order fields are defined; fields validators can access the values of earlier fields, but not later ones Field order is preserved in the model schema Field order is preserved in validation errors Field order is preserved by .dict() and .json() etc. As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined.","title":"Field Ordering"},{"location":"coding/python/pydantic/#field-with-dynamic-default-value","text":"When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a default_factory . In Beta The default_factory argument is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its signature or behaviour will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue. Example of usage: from datetime import datetime from uuid import UUID , uuid4 from pydantic import BaseModel , Field class Model ( BaseModel ): uid : UUID = Field ( default_factory = uuid4 ) updated : datetime = Field ( default_factory = datetime . utcnow ) m1 = Model () m2 = Model () print ( f ' { m1 . uid } != { m2 . uid } ' ) #> 3b187763-a19c-4ed8-9588-387e224e04f1 != 0c58f97b-c8a7-4fe8-8550-e9b2b8026574 print ( f ' { m1 . updated } != { m2 . updated } ' ) #> 2020-07-15 20:01:48.451066 != 2020-07-15 20:01:48.451083 Warning The default_factory expects the field type to be set. Moreover if you want to validate default values with validate_all , pydantic will need to call the default_factory , which could lead to side effects!","title":"Field with dynamic default value"},{"location":"coding/python/pydantic/#parsing-data-into-a-specified-type","text":"Pydantic includes a standalone utility function parse_obj_as that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to BaseModel.parse_obj , but works with arbitrary pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of BaseModel . For example: from typing import List from pydantic import BaseModel , parse_obj_as class Item ( BaseModel ): id : int name : str # `item_data` could come from an API call, eg., via something like: # item_data = requests.get('https://my-api.com/items').json() item_data = [{ 'id' : 1 , 'name' : 'My Item' }] items = parse_obj_as ( List [ Item ], item_data ) print ( items ) #> [Item(id=1, name='My Item')] This function is capable of parsing data into any of the types pydantic can handle as fields of a BaseModel . Pydantic also includes a similar standalone function called parse_file_as , which is analogous to BaseModel.parse_file .","title":"Parsing data into a specified type"},{"location":"coding/python/pydantic/#data-conversion","text":"pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example: from pydantic import BaseModel class Model ( BaseModel ): a : int b : float c : str print ( Model ( a = 3.1415 , b = ' 2.72 ' , c = 123 ) . dict ()) #> {'a': 3, 'b': 2.72, 'c': '123'} This is a deliberate decision of pydantic , and in general it's the most useful approach. See here for a longer discussion on the subject.","title":"Data Conversion"},{"location":"coding/python/pydantic/#initialize-attributes-at-object-creation","text":"If you want to initialize attributes of the object automatically at object creation, similar of what you'd do with the __init__ method of the class, you need to use root_validators . class PypikaRepository ( BaseModel ): \"\"\"Implement the repository pattern using the Pypika query builder.\"\"\" connection : sqlite3 . Connection cursor : sqlite3 . Cursor class Config : \"\"\"Configure the pydantic model.\"\"\" arbitrary_types_allowed = True @root_validator ( pre = True ) @classmethod def set_connection ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Set the connection to the database. Raises: ConnectionError: If there is no database file. \"\"\" database_file = values [ \"database_url\" ] . replace ( \"sqlite:///\" , \"\" ) if not os . path . isfile ( database_file ): raise ConnectionError ( f \"There is no database file: { database_file } \" ) connection = sqlite3 . connect ( database_file ) values [ \"connection\" ] = connection values [ \"cursor\" ] = connection . cursor () return values I had to set the arbitrary_types_allowed because the sqlite3 objects are not between the pydantic object types.","title":"Initialize attributes at object creation"},{"location":"coding/python/pydantic/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"coding/python/pydantic/#e0611-no-name-basemodel-in-module-pydantic","text":"Add to your pyproject.toml the following lines: # --------- Pylint ------------- [tool.pylint] extension-pkg-whitelist = 'pydantic' Or if it fails, add to the line # pylint: extension-pkg-whitelist .","title":"E0611: No name 'BaseModel' in module 'pydantic'"},{"location":"coding/python/pydantic/#references","text":"Docs","title":"References"},{"location":"coding/python/pydantic_exporting/","text":"As well as accessing model attributes directly via their names (e.g. model.foobar ), models can be converted and exported in a number of ways: model.dict(...) \u2691 This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not explicitly set when creating the model should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) # returns a dictionary: print ( m . dict ()) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': 123}, } \"\"\" print ( m . dict ( include = { 'foo' , 'bar' })) #> {'foo': 'hello', 'bar': {'whatever': 123}} print ( m . dict ( exclude = { 'foo' , 'bar' })) #> {'banana': 3.14} dict(model) and iteration \u2691 pydantic models can also be converted to dictionaries using dict(model) , and you can also iterate over a model's field using for field_name, value in model: . With this approach the raw field values are returned, so sub-models will not be converted to dictionaries. Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( dict ( m )) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': BarModel( whatever=123, ), } \"\"\" for name , value in m : print ( f ' { name } : { value } ' ) #> banana: 3.14 #> foo: hello #> bar: whatever=123 model.copy(...) \u2691 copy() allows models to be duplicated, which is particularly useful for immutable models. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. update : A dictionary of values to change when creating the copied model deep : whether to make a deep copy of the new model; default False Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( m . copy ( include = { 'foo' , 'bar' })) #> foo='hello' bar=BarModel(whatever=123) print ( m . copy ( exclude = { 'foo' , 'bar' })) #> banana=3.14 print ( m . copy ( update = { 'banana' : 0 })) #> banana=0 foo='hello' bar=BarModel(whatever=123) print ( id ( m . bar ), id ( m . copy () . bar )) #> 139868119420992 139868119420992 # normal copy gives the same object reference for `bar` print ( id ( m . bar ), id ( m . copy ( deep = True ) . bar )) #> 139868119420992 139868119423296 # deep copy gives a new object reference for `bar` model.json(...) \u2691 The .json() method will serialise a model to JSON. Typically, .json() in turn calls .dict() and serialises its result. (For models with a custom root type, after calling .dict() , only the value for the __root__ key is serialised). Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not set when creating the model and have their default values should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . encoder : A custom encoder function passed to the default argument of json.dumps() ; defaults to a custom encoder designed to take care of all common types. **dumps_kwargs : Any other keyword arguments are passed to json.dumps() , e.g. indent . pydantic can serialise many commonly used types to JSON (e.g. datetime , date or UUID ) which would normally fail with a simple json.dumps(foobar) . from datetime import datetime from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): foo : datetime bar : BarModel m = FooBarModel ( foo = datetime ( 2032 , 6 , 1 , 12 , 13 , 14 ), bar = { 'whatever' : 123 }) print ( m . json ()) #> {\"foo\": \"2032-06-01T12:13:14\", \"bar\": {\"whatever\": 123}} Advanced include and exclude \u2691 The dict , json , and copy methods support include and exclude arguments which can either be sets or dictionaries. This allows nested selection of which fields to export: from pydantic import BaseModel , SecretStr class User ( BaseModel ): id : int username : str password : SecretStr class Transaction ( BaseModel ): id : str user : User value : int t = Transaction ( id = '1234567890' , user = User ( id = 42 , username = 'JohnDoe' , password = 'hashedpassword' ), value = 9876543210 , ) # using a set: print ( t . dict ( exclude = { 'user' , 'value' })) #> {'id': '1234567890'} # using a dict: print ( t . dict ( exclude = { 'user' : { 'username' , 'password' }, 'value' : ... })) #> {'id': '1234567890', 'user': {'id': 42}} print ( t . dict ( include = { 'id' : ... , 'user' : { 'id' }})) #> {'id': '1234567890', 'user': {'id': 42}}``` The ellipsis ( `` ... `` ) indicates that we want to exclude or include an entire key , just as if we included it in a set . Of course , the same can be done at any depth level . Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries . In this scenario , ` dict ` and related methods expect integer keys for element - wise inclusion or exclusion . To exclude a field from ** every ** member of a list or tuple , the dictionary key ` '__all__' ` can be used as follows : ``` python import datetime from typing import List from pydantic import BaseModel , SecretStr class Country ( BaseModel ): name : str phone_code : int class Address ( BaseModel ): post_code : int country : Country class CardDetails ( BaseModel ): number : SecretStr expires : datetime . date class Hobby ( BaseModel ): name : str info : str class User ( BaseModel ): first_name : str second_name : str address : Address card_details : CardDetails hobbies : List [ Hobby ] user = User ( first_name = 'John' , second_name = 'Doe' , address = Address ( post_code = 123456 , country = Country ( name = 'USA' , phone_code = 1 ) ), card_details = CardDetails ( number = 4212934504460000 , expires = datetime . date ( 2020 , 5 , 1 ) ), hobbies = [ Hobby ( name = 'Programming' , info = 'Writing code and stuff' ), Hobby ( name = 'Gaming' , info = 'Hell Yeah!!!' ), ], ) exclude_keys = { 'second_name' : ... , 'address' : { 'post_code' : ... , 'country' : { 'phone_code' }}, 'card_details' : ... , # You can exclude fields from specific members of a tuple/list by index: 'hobbies' : { - 1 : { 'info' }}, } include_keys = { 'first_name' : ... , 'address' : { 'country' : { 'name' }}, 'hobbies' : { 0 : ... , - 1 : { 'name' }}, } # would be the same as user.dict(exclude=exclude_keys) in this case: print ( user . dict ( include = include_keys )) \"\"\" { 'first_name': 'John', 'address': {'country': {'name': 'USA'}}, 'hobbies': [ { 'name': 'Programming', 'info': 'Writing code and stuff', }, {'name': 'Gaming'}, ], } \"\"\" # To exclude a field from all members of a nested list or tuple, use \"__all__\": print ( user . dict ( exclude = { 'hobbies' : { '__all__' : { 'info' }}})) \"\"\" { 'first_name': 'John', 'second_name': 'Doe', 'address': { 'post_code': 123456, 'country': {'name': 'USA', 'phone_code': 1}, }, 'card_details': { 'number': SecretStr('**********'), 'expires': datetime.date(2020, 5, 1), }, 'hobbies': [{'name': 'Programming'}, {'name': 'Gaming'}], } \"\"\" The same holds for the json and copy methods. References \u2691 Pydantic exporting models","title":"Pydantic Exporting Models"},{"location":"coding/python/pydantic_exporting/#modeldict","text":"This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not explicitly set when creating the model should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) # returns a dictionary: print ( m . dict ()) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': 123}, } \"\"\" print ( m . dict ( include = { 'foo' , 'bar' })) #> {'foo': 'hello', 'bar': {'whatever': 123}} print ( m . dict ( exclude = { 'foo' , 'bar' })) #> {'banana': 3.14}","title":"model.dict(...)"},{"location":"coding/python/pydantic_exporting/#dictmodel-and-iteration","text":"pydantic models can also be converted to dictionaries using dict(model) , and you can also iterate over a model's field using for field_name, value in model: . With this approach the raw field values are returned, so sub-models will not be converted to dictionaries. Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( dict ( m )) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': BarModel( whatever=123, ), } \"\"\" for name , value in m : print ( f ' { name } : { value } ' ) #> banana: 3.14 #> foo: hello #> bar: whatever=123","title":"dict(model) and iteration"},{"location":"coding/python/pydantic_exporting/#modelcopy","text":"copy() allows models to be duplicated, which is particularly useful for immutable models. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. update : A dictionary of values to change when creating the copied model deep : whether to make a deep copy of the new model; default False Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( m . copy ( include = { 'foo' , 'bar' })) #> foo='hello' bar=BarModel(whatever=123) print ( m . copy ( exclude = { 'foo' , 'bar' })) #> banana=3.14 print ( m . copy ( update = { 'banana' : 0 })) #> banana=0 foo='hello' bar=BarModel(whatever=123) print ( id ( m . bar ), id ( m . copy () . bar )) #> 139868119420992 139868119420992 # normal copy gives the same object reference for `bar` print ( id ( m . bar ), id ( m . copy ( deep = True ) . bar )) #> 139868119420992 139868119423296 # deep copy gives a new object reference for `bar`","title":"model.copy(...)"},{"location":"coding/python/pydantic_exporting/#modeljson","text":"The .json() method will serialise a model to JSON. Typically, .json() in turn calls .dict() and serialises its result. (For models with a custom root type, after calling .dict() , only the value for the __root__ key is serialised). Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not set when creating the model and have their default values should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . encoder : A custom encoder function passed to the default argument of json.dumps() ; defaults to a custom encoder designed to take care of all common types. **dumps_kwargs : Any other keyword arguments are passed to json.dumps() , e.g. indent . pydantic can serialise many commonly used types to JSON (e.g. datetime , date or UUID ) which would normally fail with a simple json.dumps(foobar) . from datetime import datetime from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): foo : datetime bar : BarModel m = FooBarModel ( foo = datetime ( 2032 , 6 , 1 , 12 , 13 , 14 ), bar = { 'whatever' : 123 }) print ( m . json ()) #> {\"foo\": \"2032-06-01T12:13:14\", \"bar\": {\"whatever\": 123}}","title":"model.json(...)"},{"location":"coding/python/pydantic_exporting/#advanced-include-and-exclude","text":"The dict , json , and copy methods support include and exclude arguments which can either be sets or dictionaries. This allows nested selection of which fields to export: from pydantic import BaseModel , SecretStr class User ( BaseModel ): id : int username : str password : SecretStr class Transaction ( BaseModel ): id : str user : User value : int t = Transaction ( id = '1234567890' , user = User ( id = 42 , username = 'JohnDoe' , password = 'hashedpassword' ), value = 9876543210 , ) # using a set: print ( t . dict ( exclude = { 'user' , 'value' })) #> {'id': '1234567890'} # using a dict: print ( t . dict ( exclude = { 'user' : { 'username' , 'password' }, 'value' : ... })) #> {'id': '1234567890', 'user': {'id': 42}} print ( t . dict ( include = { 'id' : ... , 'user' : { 'id' }})) #> {'id': '1234567890', 'user': {'id': 42}}``` The ellipsis ( `` ... `` ) indicates that we want to exclude or include an entire key , just as if we included it in a set . Of course , the same can be done at any depth level . Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries . In this scenario , ` dict ` and related methods expect integer keys for element - wise inclusion or exclusion . To exclude a field from ** every ** member of a list or tuple , the dictionary key ` '__all__' ` can be used as follows : ``` python import datetime from typing import List from pydantic import BaseModel , SecretStr class Country ( BaseModel ): name : str phone_code : int class Address ( BaseModel ): post_code : int country : Country class CardDetails ( BaseModel ): number : SecretStr expires : datetime . date class Hobby ( BaseModel ): name : str info : str class User ( BaseModel ): first_name : str second_name : str address : Address card_details : CardDetails hobbies : List [ Hobby ] user = User ( first_name = 'John' , second_name = 'Doe' , address = Address ( post_code = 123456 , country = Country ( name = 'USA' , phone_code = 1 ) ), card_details = CardDetails ( number = 4212934504460000 , expires = datetime . date ( 2020 , 5 , 1 ) ), hobbies = [ Hobby ( name = 'Programming' , info = 'Writing code and stuff' ), Hobby ( name = 'Gaming' , info = 'Hell Yeah!!!' ), ], ) exclude_keys = { 'second_name' : ... , 'address' : { 'post_code' : ... , 'country' : { 'phone_code' }}, 'card_details' : ... , # You can exclude fields from specific members of a tuple/list by index: 'hobbies' : { - 1 : { 'info' }}, } include_keys = { 'first_name' : ... , 'address' : { 'country' : { 'name' }}, 'hobbies' : { 0 : ... , - 1 : { 'name' }}, } # would be the same as user.dict(exclude=exclude_keys) in this case: print ( user . dict ( include = include_keys )) \"\"\" { 'first_name': 'John', 'address': {'country': {'name': 'USA'}}, 'hobbies': [ { 'name': 'Programming', 'info': 'Writing code and stuff', }, {'name': 'Gaming'}, ], } \"\"\" # To exclude a field from all members of a nested list or tuple, use \"__all__\": print ( user . dict ( exclude = { 'hobbies' : { '__all__' : { 'info' }}})) \"\"\" { 'first_name': 'John', 'second_name': 'Doe', 'address': { 'post_code': 123456, 'country': {'name': 'USA', 'phone_code': 1}, }, 'card_details': { 'number': SecretStr('**********'), 'expires': datetime.date(2020, 5, 1), }, 'hobbies': [{'name': 'Programming'}, {'name': 'Gaming'}], } \"\"\" The same holds for the json and copy methods.","title":"Advanced include and exclude"},{"location":"coding/python/pydantic_exporting/#references","text":"Pydantic exporting models","title":"References"},{"location":"coding/python/pydantic_functions/","text":"The validate_arguments decorator allows the arguments passed to a function to be parsed and validated using the function's annotations before the function is called. While under the hood this uses the same approach of model creation and initialisation; it provides an extremely easy way to apply validation to your code with minimal boilerplate. In Beta The validate_arguments decorator is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its interface will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #1205 or create a new issue. Be sure you understand it's limitations . Example of usage: from pydantic import validate_arguments , ValidationError @validate_arguments def repeat ( s : str , count : int , * , separator : bytes = b '' ) -> bytes : b = s . encode () return separator . join ( b for _ in range ( count )) a = repeat ( 'hello' , 3 ) print ( a ) #> b'hellohellohello' b = repeat ( 'x' , '4' , separator = ' ' ) print ( b ) #> b'x x x x' try : c = repeat ( 'hello' , 'wrong' ) except ValidationError as exc : print ( exc ) \"\"\" 1 validation error for Repeat count value is not a valid integer (type=type_error.integer) \"\"\" Usage with mypy \u2691 The validate_arguments decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require type: ignore . References \u2691 Pydantic validation decorator docs","title":"Pydantic Validating Functions"},{"location":"coding/python/pydantic_functions/#usage-with-mypy","text":"The validate_arguments decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require type: ignore .","title":"Usage with mypy"},{"location":"coding/python/pydantic_functions/#references","text":"Pydantic validation decorator docs","title":"References"},{"location":"coding/python/pydantic_mypy_plugin/","text":"Pydantic works well with mypy right out of the box. However, Pydantic also ships with a mypy plugin that adds a number of important pydantic-specific features to mypy that improve its ability to type-check your code. Enabling the Plugin \u2691 To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file (this could be mypy.ini or setup.cfg ). To get started, all you need to do is create a mypy.ini file with following contents: [mypy] plugins = pydantic.mypy See the mypy usage and plugin configuration docs for more details. References \u2691 Pydantic mypy plugin docs","title":"Pydantic Mypy Plugin"},{"location":"coding/python/pydantic_mypy_plugin/#enabling-the-plugin","text":"To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file (this could be mypy.ini or setup.cfg ). To get started, all you need to do is create a mypy.ini file with following contents: [mypy] plugins = pydantic.mypy See the mypy usage and plugin configuration docs for more details.","title":"Enabling the Plugin"},{"location":"coding/python/pydantic_mypy_plugin/#references","text":"Pydantic mypy plugin docs","title":"References"},{"location":"coding/python/pydantic_types/","text":"Where possible pydantic uses standard library types to define fields, thus smoothing the learning curve. For many useful applications, however, no standard library type exists, so pydantic implements many commonly used types . If no existing type suits your purpose you can also implement your own pydantic-compatible types with custom properties and validation. Standard Library Types \u2691 pydantic supports many common types from the python standard library. If you need stricter processing see Strict Types ; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types . bool see Booleans for details on how bools are validated and what values are permitted. int pydantic uses int(v) to coerce types to an int ; see this warning on loss of information during data conversion. float similarly, float(v) is used to coerce values to floats. str strings are accepted as-is, int float and Decimal are coerced using str(v) , bytes and bytearray are converted using v.decode() , enums inheriting from str are converted using v.value , and all other types cause an error. list allows list , tuple , set , frozenset , or generators and casts to a list. tuple allows list , tuple , set , frozenset , or generators and casts to a tuple. dict dict(v) is used to attempt to convert a dictionary. set allows list , tuple , set , frozenset , or generators and casts to a set. frozenset allows list , tuple , set , frozenset , or generators and casts to a frozen set. datetime.date see Datetime Types below for more detail on parsing and validation. datetime.time see Datetime Types below for more detail on parsing and validation. datetime.datetime see Datetime Types below for more detail on parsing and validation. datetime.timedelta see Datetime Types below for more detail on parsing and validation. typing.Any allows any value include None , thus an Any field is optional. typing.TypeVar constrains the values allowed based on constraints or bound , see TypeVar . typing.Union see Unions below for more detail on parsing and validation. typing.Optional Optional[x] is simply short hand for Union[x, None] ; see Unions below for more detail on parsing and validation. typing.List : typing.Tuple : typing.Dict : typing.Set : typing.FrozenSet : typing.Sequence : typing.Iterable this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation. typing.Type see Type below for more detail on parsing and validation. typing.Callable see Callable for more detail on parsing and validation. typing.Pattern will cause the input value to be passed to re.compile(v) to create a regex pattern. ipaddress.IPv4Address simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Interface simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Network simply uses the type itself for validation by passing the value to IPv4Network(v) . enum.Enum checks that the value is a valid member of the enum; see Enums and Choices for more details. enum.IntEnum checks that the value is a valid member of the integer enum; see Enums and Choices for more details. decimal.Decimal pydantic attempts to convert the value to a string, then passes the string to Decimal(v) . pathlib.Path simply uses the type itself for validation by passing the value to Path(v) . Iterables \u2691 Define default value for an iterable \u2691 If you want to define an empty list, dictionary, set or other iterable as a model attribute, you can use the default_factory . from typing import Sequence from pydantic import BaseModel , Field class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = Field ( default_factory = list ) It might be tempting to do class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = [] # Bad! But you'll follow the mutable default argument anti-pattern. Unions \u2691 The Union type allows a model attribute to accept different types, e.g.: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ int , str , UUID ] name : str user_01 = User ( id = 123 , name = 'John Doe' ) print ( user_01 ) #> id=123 name='John Doe' print ( user_01 . id ) #> 123 user_02 = User ( id = '1234' , name = 'John Doe' ) print ( user_02 ) #> id=1234 name='John Doe' print ( user_02 . id ) #> 1234 user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=275603287559914445491632874575877060712 name='John Doe' print ( user_03 . id ) #> 275603287559914445491632874575877060712 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 However, as can be seen above, pydantic will attempt to 'match' any of the types defined under Union and will use the first one that matches. In the above example the id of user_03 was defined as a uuid.UUID class (which is defined under the attribute's Union annotation) but as the uuid.UUID can be marshalled into an int it chose to match against the int type and disregarded the other types. As such, it is recommended that, when defining Union annotations, the most specific type is included first and followed by less specific types. In the above example, the UUID class should precede the int and str classes to preclude the unexpected representation as such: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ UUID , int , str ] name : str user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe' print ( user_03 . id ) #> cf57432e-809e-4353-adbd-9d5c0d733868 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 Enums and Choices \u2691 pydantic uses python's standard enum classes to define choices. from enum import Enum , IntEnum from pydantic import BaseModel , ValidationError class FruitEnum ( str , Enum ): pear = 'pear' banana = 'banana' class ToolEnum ( IntEnum ): spanner = 1 wrench = 2 class CookingModel ( BaseModel ): fruit : FruitEnum = FruitEnum . pear tool : ToolEnum = ToolEnum . spanner print ( CookingModel ()) #> fruit=<FruitEnum.pear: 'pear'> tool=<ToolEnum.spanner: 1> print ( CookingModel ( tool = 2 , fruit = 'banana' )) #> fruit=<FruitEnum.banana: 'banana'> tool=<ToolEnum.wrench: 2> try : CookingModel ( fruit = 'other' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for CookingModel fruit value is not a valid enumeration member; permitted: 'pear', 'banana' (type=type_error.enum; enum_values=[<FruitEnum.pear: 'pear'>, <FruitEnum.banana: 'banana'>]) \"\"\" Datetime Types \u2691 Pydantic supports the following datetime types: datetime fields can be: datetime , existing datetime object int or float , assumed as Unix time, i.e. seconds (if >= -2e10 or <= 2e10 ) or milliseconds (if < -2e10 or > 2e10 ) since 1 January 1970 str , following formats work: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z[\u00b1]HH[:]MM]]] int or float as a string (assumed as Unix time) date fields can be: date , existing date object int or float , see datetime str , following formats work: YYYY-MM-DD int or float , see datetime time fields can be: time , existing time object str , following formats work: HH:MM[:SS[.ffffff]] timedelta fields can be: timedelta , existing timedelta object int or float , assumed as seconds str , following formats work: [-][DD ][HH:MM]SS[.ffffff] [\u00b1]P[DD]DT[HH]H[MM]M[SS]S (ISO 8601 format for timedelta) Type \u2691 pydantic supports the use of Type[T] to specify that a field may only accept classes (not instances) that are subclasses of T . from typing import Type from pydantic import BaseModel from pydantic import ValidationError class Foo : pass class Bar ( Foo ): pass class Other : pass class SimpleModel ( BaseModel ): just_subclasses : Type [ Foo ] SimpleModel ( just_subclasses = Foo ) SimpleModel ( just_subclasses = Bar ) try : SimpleModel ( just_subclasses = Other ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for SimpleModel just_subclasses subclass of Foo expected (type=type_error.subclass; expected_class=Foo) \"\"\" TypeVar \u2691 TypeVar is supported either unconstrained, constrained or with a bound. from typing import TypeVar from pydantic import BaseModel Foobar = TypeVar ( 'Foobar' ) BoundFloat = TypeVar ( 'BoundFloat' , bound = float ) IntStr = TypeVar ( 'IntStr' , int , str ) class Model ( BaseModel ): a : Foobar # equivalent of \": Any\" b : BoundFloat # equivalent of \": float\" c : IntStr # equivalent of \": Union[int, str]\" print ( Model ( a = [ 1 ], b = 4.2 , c = 'x' )) #> a=[1] b=4.2 c='x' # a may be None and is therefore optional print ( Model ( b = 1 , c = 1 )) #> a=None b=1.0 c=1 Pydantic Types \u2691 pydantic also provides a variety of other useful types: FilePath like Path , but the path must exist and be a file. DirectoryPath like Path , but the path must exist and be a directory. Color for parsing HTML and CSS colors; see Color Type . Json a special type wrapper which loads JSON before parsing; see JSON Type . AnyUrl any URL; see URLs . AnyHttpUrl an HTTP URL; see URLs . HttpUrl a stricter HTTP URL; see URLs . PostgresDsn a postgres DSN style URL; see URLs . RedisDsn a redis DSN style URL; see URLs . SecretStr string where the value is kept partially secret; see Secrets . IPvAnyAddress allows either an IPv4Address or an IPv6Address . IPvAnyInterface allows either an IPv4Interface or an IPv6Interface . IPvAnyNetwork allows either an IPv4Network or an IPv6Network . NegativeFloat allows a float which is negative; uses standard float parsing then checks the value is less than 0; see Constrained Types . NegativeInt allows an int which is negative; uses standard int parsing then checks the value is less than 0; see Constrained Types . PositiveFloat allows a float which is positive; uses standard float parsing then checks the value is greater than 0; see Constrained Types . PositiveInt allows an int which is positive; uses standard int parsing then checks the value is greater than 0; see Constrained Types . condecimal type method for constraining Decimals; see Constrained Types . confloat type method for constraining floats; see Constrained Types . conint type method for constraining ints; see Constrained Types . conlist type method for constraining lists; see Constrained Types . conset type method for constraining sets; see Constrained Types . constr type method for constraining strs; see Constrained Types . Custom Data Types \u2691 You can also define your own custom data types. There are several ways to achieve it. Classes with __get_validators__ \u2691 You use a custom class with a classmethod __get_validators__ . It will be called to get validators to parse and validate the input data. Tip These validators have the same semantics as in Validators , you can declare a parameter config , field , etc. import re from pydantic import BaseModel # https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom#Validation post_code_regex = re . compile ( r '(?:' r '([A-Z]{1,2}[0-9][A-Z0-9]?|ASCN|STHL|TDCU|BBND|[BFS]IQQ|PCRN|TKCA) ?' r '([0-9][A-Z] {2} )|' r '(BFPO) ?([0-9]{1,4})|' r '(KY[0-9]|MSR|VG|AI)[ -]?[0-9] {4} |' r '([A-Z] {2} ) ?([0-9] {2} )|' r '(GE) ?(CX)|' r '(GIR) ?(0A {2} )|' r '(SAN) ?(TA1)' r ')' ) class PostCode ( str ): \"\"\" Partial UK postcode validation. Note: this is just an example, and is not intended for use in production; in particular this does NOT guarantee a postcode exists, just that it has a valid format. \"\"\" @classmethod def __get_validators__ ( cls ): # one or more validators may be yielded which will be called in the # order to validate the input, each validator will receive as an input # the value returned from the previous validator yield cls . validate @classmethod def __modify_schema__ ( cls , field_schema ): # __modify_schema__ should mutate the dict it receives in place, # the returned value will be ignored field_schema . update ( # simplified regex here for brevity, see the wikipedia link above pattern = '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z] {2} $' , # some example postcodes examples = [ 'SP11 9DG' , 'w1j7bu' ], ) @classmethod def validate ( cls , v ): if not isinstance ( v , str ): raise TypeError ( 'string required' ) m = post_code_regex . fullmatch ( v . upper ()) if not m : raise ValueError ( 'invalid postcode format' ) # you could also return a string here which would mean model.post_code # would be a string, pydantic won't care but you could end up with some # confusion since the value's type won't match the type annotation # exactly return cls ( f ' { m . group ( 1 ) } { m . group ( 2 ) } ' ) def __repr__ ( self ): return f 'PostCode( { super () . __repr__ () } )' class Model ( BaseModel ): post_code : PostCode model = Model ( post_code = 'sw8 5el' ) print ( model ) #> post_code=PostCode('SW8 5EL') print ( model . post_code ) #> SW8 5EL print ( Model . schema ()) \"\"\" { 'title': 'Model', 'type': 'object', 'properties': { 'post_code': { 'title': 'Post Code', 'pattern': '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}$', 'examples': ['SP11 9DG', 'w1j7bu'], 'type': 'string', }, }, 'required': ['post_code'], } \"\"\" Generic Classes as Types \u2691 Warning This is an advanced technique that you might not need in the beginning. In most of the cases you will probably be fine with standard pydantic models. You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with __get_validators__ . If the Generic class that you are using as a sub-type has a classmethod __get_validators__ you don't need to use arbitrary_types_allowed for it to work. Because you can declare validators that receive the current field , you can extract the sub_fields (from the generic class type parameters) and validate data with them. from pydantic import BaseModel , ValidationError from pydantic.fields import ModelField from typing import TypeVar , Generic AgedType = TypeVar ( 'AgedType' ) QualityType = TypeVar ( 'QualityType' ) # This is not a pydantic model, it's an arbitrary generic class class TastingModel ( Generic [ AgedType , QualityType ]): def __init__ ( self , name : str , aged : AgedType , quality : QualityType ): self . name = name self . aged = aged self . quality = quality @classmethod def __get_validators__ ( cls ): yield cls . validate @classmethod # You don't need to add the \"ModelField\", but it will help your # editor give you completion and catch errors def validate ( cls , v , field : ModelField ): if not isinstance ( v , cls ): # The value is not even a TastingModel raise TypeError ( 'Invalid value' ) if not field . sub_fields : # Generic parameters were not provided so we don't try to validate # them and just return the value as is return v aged_f = field . sub_fields [ 0 ] quality_f = field . sub_fields [ 1 ] errors = [] # Here we don't need the validated value, but we want the errors valid_value , error = aged_f . validate ( v . aged , {}, loc = 'aged' ) if error : errors . append ( error ) # Here we don't need the validated value, but we want the errors valid_value , error = quality_f . validate ( v . quality , {}, loc = 'quality' ) if error : errors . append ( error ) if errors : raise ValidationError ( errors , cls ) # Validation passed without errors, return the same instance received return v class Model ( BaseModel ): # for wine, \"aged\" is an int with years, \"quality\" is a float wine : TastingModel [ int , float ] # for cheese, \"aged\" is a bool, \"quality\" is a str cheese : TastingModel [ bool , str ] # for thing, \"aged\" is a Any, \"quality\" is Any thing : TastingModel model = Model ( # This wine was aged for 20 years and has a quality of 85.6 wine = TastingModel ( name = 'Cabernet Sauvignon' , aged = 20 , quality = 85.6 ), # This cheese is aged (is mature) and has \"Good\" quality cheese = TastingModel ( name = 'Gouda' , aged = True , quality = 'Good' ), # This Python thing has aged \"Not much\" and has a quality \"Awesome\" thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) print ( model ) \"\"\" wine=<types_generics.TastingModel object at 0x7f3593a4eee0> cheese=<types_generics.TastingModel object at 0x7f3593a46100> thing=<types_generics.TastingModel object at 0x7f3593a464c0> \"\"\" print ( model . wine . aged ) #> 20 print ( model . wine . quality ) #> 85.6 print ( model . cheese . aged ) #> True print ( model . cheese . quality ) #> Good print ( model . thing . aged ) #> Not much try : # If the values of the sub-types are invalid, we get an error Model ( # For wine, aged should be an int with the years, and quality a float wine = TastingModel ( name = 'Merlot' , aged = True , quality = 'Kinda good' ), # For cheese, aged should be a bool, and quality a str cheese = TastingModel ( name = 'Gouda' , aged = 'yeah' , quality = 5 ), # For thing, no type parameters are declared, and we skipped validation # in those cases in the Assessment.validate() function thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for Model wine -> quality value is not a valid float (type=type_error.float) cheese -> aged value could not be parsed to a boolean (type=type_error.bool) \"\"\" References \u2691 Field types","title":"Pydantic Field Types"},{"location":"coding/python/pydantic_types/#standard-library-types","text":"pydantic supports many common types from the python standard library. If you need stricter processing see Strict Types ; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types . bool see Booleans for details on how bools are validated and what values are permitted. int pydantic uses int(v) to coerce types to an int ; see this warning on loss of information during data conversion. float similarly, float(v) is used to coerce values to floats. str strings are accepted as-is, int float and Decimal are coerced using str(v) , bytes and bytearray are converted using v.decode() , enums inheriting from str are converted using v.value , and all other types cause an error. list allows list , tuple , set , frozenset , or generators and casts to a list. tuple allows list , tuple , set , frozenset , or generators and casts to a tuple. dict dict(v) is used to attempt to convert a dictionary. set allows list , tuple , set , frozenset , or generators and casts to a set. frozenset allows list , tuple , set , frozenset , or generators and casts to a frozen set. datetime.date see Datetime Types below for more detail on parsing and validation. datetime.time see Datetime Types below for more detail on parsing and validation. datetime.datetime see Datetime Types below for more detail on parsing and validation. datetime.timedelta see Datetime Types below for more detail on parsing and validation. typing.Any allows any value include None , thus an Any field is optional. typing.TypeVar constrains the values allowed based on constraints or bound , see TypeVar . typing.Union see Unions below for more detail on parsing and validation. typing.Optional Optional[x] is simply short hand for Union[x, None] ; see Unions below for more detail on parsing and validation. typing.List : typing.Tuple : typing.Dict : typing.Set : typing.FrozenSet : typing.Sequence : typing.Iterable this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation. typing.Type see Type below for more detail on parsing and validation. typing.Callable see Callable for more detail on parsing and validation. typing.Pattern will cause the input value to be passed to re.compile(v) to create a regex pattern. ipaddress.IPv4Address simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Interface simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Network simply uses the type itself for validation by passing the value to IPv4Network(v) . enum.Enum checks that the value is a valid member of the enum; see Enums and Choices for more details. enum.IntEnum checks that the value is a valid member of the integer enum; see Enums and Choices for more details. decimal.Decimal pydantic attempts to convert the value to a string, then passes the string to Decimal(v) . pathlib.Path simply uses the type itself for validation by passing the value to Path(v) .","title":"Standard Library Types"},{"location":"coding/python/pydantic_types/#iterables","text":"","title":"Iterables"},{"location":"coding/python/pydantic_types/#define-default-value-for-an-iterable","text":"If you want to define an empty list, dictionary, set or other iterable as a model attribute, you can use the default_factory . from typing import Sequence from pydantic import BaseModel , Field class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = Field ( default_factory = list ) It might be tempting to do class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = [] # Bad! But you'll follow the mutable default argument anti-pattern.","title":"Define default value for an iterable"},{"location":"coding/python/pydantic_types/#unions","text":"The Union type allows a model attribute to accept different types, e.g.: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ int , str , UUID ] name : str user_01 = User ( id = 123 , name = 'John Doe' ) print ( user_01 ) #> id=123 name='John Doe' print ( user_01 . id ) #> 123 user_02 = User ( id = '1234' , name = 'John Doe' ) print ( user_02 ) #> id=1234 name='John Doe' print ( user_02 . id ) #> 1234 user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=275603287559914445491632874575877060712 name='John Doe' print ( user_03 . id ) #> 275603287559914445491632874575877060712 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 However, as can be seen above, pydantic will attempt to 'match' any of the types defined under Union and will use the first one that matches. In the above example the id of user_03 was defined as a uuid.UUID class (which is defined under the attribute's Union annotation) but as the uuid.UUID can be marshalled into an int it chose to match against the int type and disregarded the other types. As such, it is recommended that, when defining Union annotations, the most specific type is included first and followed by less specific types. In the above example, the UUID class should precede the int and str classes to preclude the unexpected representation as such: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ UUID , int , str ] name : str user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe' print ( user_03 . id ) #> cf57432e-809e-4353-adbd-9d5c0d733868 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712","title":"Unions"},{"location":"coding/python/pydantic_types/#enums-and-choices","text":"pydantic uses python's standard enum classes to define choices. from enum import Enum , IntEnum from pydantic import BaseModel , ValidationError class FruitEnum ( str , Enum ): pear = 'pear' banana = 'banana' class ToolEnum ( IntEnum ): spanner = 1 wrench = 2 class CookingModel ( BaseModel ): fruit : FruitEnum = FruitEnum . pear tool : ToolEnum = ToolEnum . spanner print ( CookingModel ()) #> fruit=<FruitEnum.pear: 'pear'> tool=<ToolEnum.spanner: 1> print ( CookingModel ( tool = 2 , fruit = 'banana' )) #> fruit=<FruitEnum.banana: 'banana'> tool=<ToolEnum.wrench: 2> try : CookingModel ( fruit = 'other' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for CookingModel fruit value is not a valid enumeration member; permitted: 'pear', 'banana' (type=type_error.enum; enum_values=[<FruitEnum.pear: 'pear'>, <FruitEnum.banana: 'banana'>]) \"\"\"","title":"Enums and Choices"},{"location":"coding/python/pydantic_types/#datetime-types","text":"Pydantic supports the following datetime types: datetime fields can be: datetime , existing datetime object int or float , assumed as Unix time, i.e. seconds (if >= -2e10 or <= 2e10 ) or milliseconds (if < -2e10 or > 2e10 ) since 1 January 1970 str , following formats work: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z[\u00b1]HH[:]MM]]] int or float as a string (assumed as Unix time) date fields can be: date , existing date object int or float , see datetime str , following formats work: YYYY-MM-DD int or float , see datetime time fields can be: time , existing time object str , following formats work: HH:MM[:SS[.ffffff]] timedelta fields can be: timedelta , existing timedelta object int or float , assumed as seconds str , following formats work: [-][DD ][HH:MM]SS[.ffffff] [\u00b1]P[DD]DT[HH]H[MM]M[SS]S (ISO 8601 format for timedelta)","title":"Datetime Types"},{"location":"coding/python/pydantic_types/#type","text":"pydantic supports the use of Type[T] to specify that a field may only accept classes (not instances) that are subclasses of T . from typing import Type from pydantic import BaseModel from pydantic import ValidationError class Foo : pass class Bar ( Foo ): pass class Other : pass class SimpleModel ( BaseModel ): just_subclasses : Type [ Foo ] SimpleModel ( just_subclasses = Foo ) SimpleModel ( just_subclasses = Bar ) try : SimpleModel ( just_subclasses = Other ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for SimpleModel just_subclasses subclass of Foo expected (type=type_error.subclass; expected_class=Foo) \"\"\"","title":"Type"},{"location":"coding/python/pydantic_types/#typevar","text":"TypeVar is supported either unconstrained, constrained or with a bound. from typing import TypeVar from pydantic import BaseModel Foobar = TypeVar ( 'Foobar' ) BoundFloat = TypeVar ( 'BoundFloat' , bound = float ) IntStr = TypeVar ( 'IntStr' , int , str ) class Model ( BaseModel ): a : Foobar # equivalent of \": Any\" b : BoundFloat # equivalent of \": float\" c : IntStr # equivalent of \": Union[int, str]\" print ( Model ( a = [ 1 ], b = 4.2 , c = 'x' )) #> a=[1] b=4.2 c='x' # a may be None and is therefore optional print ( Model ( b = 1 , c = 1 )) #> a=None b=1.0 c=1","title":"TypeVar"},{"location":"coding/python/pydantic_types/#pydantic-types","text":"pydantic also provides a variety of other useful types: FilePath like Path , but the path must exist and be a file. DirectoryPath like Path , but the path must exist and be a directory. Color for parsing HTML and CSS colors; see Color Type . Json a special type wrapper which loads JSON before parsing; see JSON Type . AnyUrl any URL; see URLs . AnyHttpUrl an HTTP URL; see URLs . HttpUrl a stricter HTTP URL; see URLs . PostgresDsn a postgres DSN style URL; see URLs . RedisDsn a redis DSN style URL; see URLs . SecretStr string where the value is kept partially secret; see Secrets . IPvAnyAddress allows either an IPv4Address or an IPv6Address . IPvAnyInterface allows either an IPv4Interface or an IPv6Interface . IPvAnyNetwork allows either an IPv4Network or an IPv6Network . NegativeFloat allows a float which is negative; uses standard float parsing then checks the value is less than 0; see Constrained Types . NegativeInt allows an int which is negative; uses standard int parsing then checks the value is less than 0; see Constrained Types . PositiveFloat allows a float which is positive; uses standard float parsing then checks the value is greater than 0; see Constrained Types . PositiveInt allows an int which is positive; uses standard int parsing then checks the value is greater than 0; see Constrained Types . condecimal type method for constraining Decimals; see Constrained Types . confloat type method for constraining floats; see Constrained Types . conint type method for constraining ints; see Constrained Types . conlist type method for constraining lists; see Constrained Types . conset type method for constraining sets; see Constrained Types . constr type method for constraining strs; see Constrained Types .","title":"Pydantic Types"},{"location":"coding/python/pydantic_types/#custom-data-types","text":"You can also define your own custom data types. There are several ways to achieve it.","title":"Custom Data Types"},{"location":"coding/python/pydantic_types/#classes-with-__get_validators__","text":"You use a custom class with a classmethod __get_validators__ . It will be called to get validators to parse and validate the input data. Tip These validators have the same semantics as in Validators , you can declare a parameter config , field , etc. import re from pydantic import BaseModel # https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom#Validation post_code_regex = re . compile ( r '(?:' r '([A-Z]{1,2}[0-9][A-Z0-9]?|ASCN|STHL|TDCU|BBND|[BFS]IQQ|PCRN|TKCA) ?' r '([0-9][A-Z] {2} )|' r '(BFPO) ?([0-9]{1,4})|' r '(KY[0-9]|MSR|VG|AI)[ -]?[0-9] {4} |' r '([A-Z] {2} ) ?([0-9] {2} )|' r '(GE) ?(CX)|' r '(GIR) ?(0A {2} )|' r '(SAN) ?(TA1)' r ')' ) class PostCode ( str ): \"\"\" Partial UK postcode validation. Note: this is just an example, and is not intended for use in production; in particular this does NOT guarantee a postcode exists, just that it has a valid format. \"\"\" @classmethod def __get_validators__ ( cls ): # one or more validators may be yielded which will be called in the # order to validate the input, each validator will receive as an input # the value returned from the previous validator yield cls . validate @classmethod def __modify_schema__ ( cls , field_schema ): # __modify_schema__ should mutate the dict it receives in place, # the returned value will be ignored field_schema . update ( # simplified regex here for brevity, see the wikipedia link above pattern = '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z] {2} $' , # some example postcodes examples = [ 'SP11 9DG' , 'w1j7bu' ], ) @classmethod def validate ( cls , v ): if not isinstance ( v , str ): raise TypeError ( 'string required' ) m = post_code_regex . fullmatch ( v . upper ()) if not m : raise ValueError ( 'invalid postcode format' ) # you could also return a string here which would mean model.post_code # would be a string, pydantic won't care but you could end up with some # confusion since the value's type won't match the type annotation # exactly return cls ( f ' { m . group ( 1 ) } { m . group ( 2 ) } ' ) def __repr__ ( self ): return f 'PostCode( { super () . __repr__ () } )' class Model ( BaseModel ): post_code : PostCode model = Model ( post_code = 'sw8 5el' ) print ( model ) #> post_code=PostCode('SW8 5EL') print ( model . post_code ) #> SW8 5EL print ( Model . schema ()) \"\"\" { 'title': 'Model', 'type': 'object', 'properties': { 'post_code': { 'title': 'Post Code', 'pattern': '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}$', 'examples': ['SP11 9DG', 'w1j7bu'], 'type': 'string', }, }, 'required': ['post_code'], } \"\"\"","title":"Classes with __get_validators__"},{"location":"coding/python/pydantic_types/#generic-classes-as-types","text":"Warning This is an advanced technique that you might not need in the beginning. In most of the cases you will probably be fine with standard pydantic models. You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with __get_validators__ . If the Generic class that you are using as a sub-type has a classmethod __get_validators__ you don't need to use arbitrary_types_allowed for it to work. Because you can declare validators that receive the current field , you can extract the sub_fields (from the generic class type parameters) and validate data with them. from pydantic import BaseModel , ValidationError from pydantic.fields import ModelField from typing import TypeVar , Generic AgedType = TypeVar ( 'AgedType' ) QualityType = TypeVar ( 'QualityType' ) # This is not a pydantic model, it's an arbitrary generic class class TastingModel ( Generic [ AgedType , QualityType ]): def __init__ ( self , name : str , aged : AgedType , quality : QualityType ): self . name = name self . aged = aged self . quality = quality @classmethod def __get_validators__ ( cls ): yield cls . validate @classmethod # You don't need to add the \"ModelField\", but it will help your # editor give you completion and catch errors def validate ( cls , v , field : ModelField ): if not isinstance ( v , cls ): # The value is not even a TastingModel raise TypeError ( 'Invalid value' ) if not field . sub_fields : # Generic parameters were not provided so we don't try to validate # them and just return the value as is return v aged_f = field . sub_fields [ 0 ] quality_f = field . sub_fields [ 1 ] errors = [] # Here we don't need the validated value, but we want the errors valid_value , error = aged_f . validate ( v . aged , {}, loc = 'aged' ) if error : errors . append ( error ) # Here we don't need the validated value, but we want the errors valid_value , error = quality_f . validate ( v . quality , {}, loc = 'quality' ) if error : errors . append ( error ) if errors : raise ValidationError ( errors , cls ) # Validation passed without errors, return the same instance received return v class Model ( BaseModel ): # for wine, \"aged\" is an int with years, \"quality\" is a float wine : TastingModel [ int , float ] # for cheese, \"aged\" is a bool, \"quality\" is a str cheese : TastingModel [ bool , str ] # for thing, \"aged\" is a Any, \"quality\" is Any thing : TastingModel model = Model ( # This wine was aged for 20 years and has a quality of 85.6 wine = TastingModel ( name = 'Cabernet Sauvignon' , aged = 20 , quality = 85.6 ), # This cheese is aged (is mature) and has \"Good\" quality cheese = TastingModel ( name = 'Gouda' , aged = True , quality = 'Good' ), # This Python thing has aged \"Not much\" and has a quality \"Awesome\" thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) print ( model ) \"\"\" wine=<types_generics.TastingModel object at 0x7f3593a4eee0> cheese=<types_generics.TastingModel object at 0x7f3593a46100> thing=<types_generics.TastingModel object at 0x7f3593a464c0> \"\"\" print ( model . wine . aged ) #> 20 print ( model . wine . quality ) #> 85.6 print ( model . cheese . aged ) #> True print ( model . cheese . quality ) #> Good print ( model . thing . aged ) #> Not much try : # If the values of the sub-types are invalid, we get an error Model ( # For wine, aged should be an int with the years, and quality a float wine = TastingModel ( name = 'Merlot' , aged = True , quality = 'Kinda good' ), # For cheese, aged should be a bool, and quality a str cheese = TastingModel ( name = 'Gouda' , aged = 'yeah' , quality = 5 ), # For thing, no type parameters are declared, and we skipped validation # in those cases in the Assessment.validate() function thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for Model wine -> quality value is not a valid float (type=type_error.float) cheese -> aged value could not be parsed to a boolean (type=type_error.bool) \"\"\"","title":"Generic Classes as Types"},{"location":"coding/python/pydantic_types/#references","text":"Field types","title":"References"},{"location":"coding/python/pydantic_validators/","text":"Custom validation and complex relationships between objects can be achieved using the validator decorator. from pydantic import BaseModel , ValidationError , validator class UserModel ( BaseModel ): name : str username : str password1 : str password2 : str @validator ( 'name' ) def name_must_contain_space ( cls , v ): if ' ' not in v : raise ValueError ( 'must contain a space' ) return v . title () @validator ( 'password2' ) def passwords_match ( cls , v , values , ** kwargs ): if 'password1' in values and v != values [ 'password1' ]: raise ValueError ( 'passwords do not match' ) return v @validator ( 'username' ) def username_alphanumeric ( cls , v ): assert v . isalnum (), 'must be alphanumeric' return v user = UserModel ( name = 'samuel colvin' , username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , ) print ( user ) #> name='Samuel Colvin' username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( name = 'samuel' , username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' , ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for UserModel name must contain a space (type=value_error) password2 passwords do not match (type=value_error) \"\"\" You need to be aware of these validator behaviours. Validators are \"class methods\", so the first argument value they receive is the UserModel class, not an instance of UserModel . The second argument is always the field value to validate; it can be named as you please. You can also add any subset of the following arguments to the signature (the names must match): values : a dict containing the name-to-value mapping of any previously-validated fields. config : the model config. field : the field being validated. **kwargs : if provided, this will include the arguments above not explicitly listed in the signature. Validators should either return the parsed value or raise a ValueError , TypeError , or AssertionError ( assert statements may be used). Where validators rely on other values, you should be aware that: Validation is done in the order fields are defined. If validation fails on another field (or that field is missing) it will not be included in values , hence if 'password1' in values and ... in this example. Pre and per-item validators \u2691 Validators can do a few more complex things: A single validator can be applied to multiple fields by passing it multiple field names. A single validator can also be called on all fields by passing the special value '*' . The keyword argument pre will cause the validator to be called prior to other validation. Passing each_item=True will result in the validator being applied to individual values (e.g. of List , Dict , Set , etc.), rather than the whole object. from typing import List from pydantic import BaseModel , ValidationError , validator class DemoModel ( BaseModel ): square_numbers : List [ int ] = [] cube_numbers : List [ int ] = [] # '*' is the same as 'cube_numbers', 'square_numbers' here: @validator ( '*' , pre = True ) def split_str ( cls , v ): if isinstance ( v , str ): return v . split ( '|' ) return v @validator ( 'cube_numbers' , 'square_numbers' ) def check_sum ( cls , v ): if sum ( v ) > 42 : raise ValueError ( 'sum of numbers greater than 42' ) return v @validator ( 'square_numbers' , each_item = True ) def check_squares ( cls , v ): assert v ** 0.5 % 1 == 0 , f ' { v } is not a square number' return v @validator ( 'cube_numbers' , each_item = True ) def check_cubes ( cls , v ): # 64 ** (1 / 3) == 3.9999999999999996 (!) # this is not a good way of checking cubes assert v ** ( 1 / 3 ) % 1 == 0 , f ' { v } is not a cubed number' return v print ( DemoModel ( square_numbers = [ 1 , 4 , 9 ])) #> square_numbers=[1, 4, 9] cube_numbers=[] print ( DemoModel ( square_numbers = '1|4|16' )) #> square_numbers=[1, 4, 16] cube_numbers=[] print ( DemoModel ( square_numbers = [ 16 ], cube_numbers = [ 8 , 27 ])) #> square_numbers=[16] cube_numbers=[8, 27] try : DemoModel ( square_numbers = [ 1 , 4 , 2 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel square_numbers -> 2 2 is not a square number (type=assertion_error) \"\"\" try : DemoModel ( cube_numbers = [ 27 , 27 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel cube_numbers sum of numbers greater than 42 (type=value_error) \"\"\" Subclass Validators and each_item \u2691 If using a validator with a subclass that references a List type field on a parent class, using each_item=True will cause the validator not to run; instead, the list must be iterated over programatically. from typing import List from pydantic import BaseModel , ValidationError , validator class ParentModel ( BaseModel ): names : List [ str ] class ChildModel ( ParentModel ): @validator ( 'names' , each_item = True ) def check_names_not_empty ( cls , v ): assert v != '' , 'Empty strings are not allowed.' return v # This will NOT raise a ValidationError because the validator was not called try : child = ChildModel ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) else : print ( 'No ValidationError caught.' ) #> No ValidationError caught. class ChildModel2 ( ParentModel ): @validator ( 'names' ) def check_names_not_empty ( cls , v ): for name in v : assert name != '' , 'Empty strings are not allowed.' return v try : child = ChildModel2 ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for ChildModel2 names Empty strings are not allowed. (type=assertion_error) \"\"\" Validate Always \u2691 For performance reasons, by default validators are not called for fields when a value is not supplied. However there are situations where it may be useful or required to always call the validator, e.g. to set a dynamic default value. from datetime import datetime from pydantic import BaseModel , validator class DemoModel ( BaseModel ): ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoModel ()) #> ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 966302) print ( DemoModel ( ts = '2017-11-08T14:00' )) #> ts=datetime.datetime(2017, 11, 8, 14, 0) You'll often want to use this together with pre , since otherwise with always=True pydantic would try to validate the default None which would cause an error. Reuse validators \u2691 Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators. Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the allow_reuse parameter has been added to pydantic.validator in v1.2 ( False by default): from pydantic import BaseModel , validator def normalize ( name : str ) -> str : return ' ' . join (( word . capitalize ()) for word in name . split ( ' ' )) class Producer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) class Consumer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) jane_doe = Producer ( name = 'JaNe DOE' ) john_doe = Consumer ( name = 'joHN dOe' ) assert jane_doe . name == 'Jane Doe' assert john_doe . name == 'John Doe' As it is obvious, repetition has been reduced and the models become again almost declarative. Tip If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting allow_reuse=True over and over again. Root Validators \u2691 Validation can also be performed on the entire model's data. from pydantic import BaseModel , ValidationError , root_validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator ( pre = True ) def check_card_number_omitted ( cls , values ): assert 'card_number' not in values , 'card_number should not be included' return values @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values print ( UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' )) #> username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ passwords do not match (type=value_error) \"\"\" try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , card_number = '1234' , ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ card_number should not be included (type=assertion_error) \"\"\" As with field validators, root validators can have pre=True , in which case they're called before field validation occurs (and are provided with the raw input data), or pre=False (the default), in which case they're called after field validation. Field validation will not occur if pre=True root validators raise an error. As with field validators, \"post\" (i.e. pre=False ) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the skip_on_failure=True keyword argument to the validator. The values argument will be a dict containing the values which passed field validation and field defaults where applicable. Field Checks \u2691 On class creation, validators are checked to confirm that the fields they specify actually exist on the model. Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models. In this case you should set check_fields=False on the validator. Dataclass Validators \u2691 Validators also work with pydantic dataclasses. from datetime import datetime from pydantic import validator from pydantic.dataclasses import dataclass @dataclass class DemoDataclass : ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoDataclass ()) #> DemoDataclass(ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 969037)) print ( DemoDataclass ( ts = '2017-11-08T14:00' )) #> DemoDataclass(ts=datetime.datetime(2017, 11, 8, 14, 0)) Troubleshooting validators \u2691 pylint complains on the validators \u2691 Pylint complains that R0201: Method could be a function and N805: first argument of a method should be named 'self' . Seems to be an error of pylint, people have solved it by specifying @classmethod between the definition and the validator decorator. References \u2691 Pydantic validators","title":"Pydantic Validators"},{"location":"coding/python/pydantic_validators/#pre-and-per-item-validators","text":"Validators can do a few more complex things: A single validator can be applied to multiple fields by passing it multiple field names. A single validator can also be called on all fields by passing the special value '*' . The keyword argument pre will cause the validator to be called prior to other validation. Passing each_item=True will result in the validator being applied to individual values (e.g. of List , Dict , Set , etc.), rather than the whole object. from typing import List from pydantic import BaseModel , ValidationError , validator class DemoModel ( BaseModel ): square_numbers : List [ int ] = [] cube_numbers : List [ int ] = [] # '*' is the same as 'cube_numbers', 'square_numbers' here: @validator ( '*' , pre = True ) def split_str ( cls , v ): if isinstance ( v , str ): return v . split ( '|' ) return v @validator ( 'cube_numbers' , 'square_numbers' ) def check_sum ( cls , v ): if sum ( v ) > 42 : raise ValueError ( 'sum of numbers greater than 42' ) return v @validator ( 'square_numbers' , each_item = True ) def check_squares ( cls , v ): assert v ** 0.5 % 1 == 0 , f ' { v } is not a square number' return v @validator ( 'cube_numbers' , each_item = True ) def check_cubes ( cls , v ): # 64 ** (1 / 3) == 3.9999999999999996 (!) # this is not a good way of checking cubes assert v ** ( 1 / 3 ) % 1 == 0 , f ' { v } is not a cubed number' return v print ( DemoModel ( square_numbers = [ 1 , 4 , 9 ])) #> square_numbers=[1, 4, 9] cube_numbers=[] print ( DemoModel ( square_numbers = '1|4|16' )) #> square_numbers=[1, 4, 16] cube_numbers=[] print ( DemoModel ( square_numbers = [ 16 ], cube_numbers = [ 8 , 27 ])) #> square_numbers=[16] cube_numbers=[8, 27] try : DemoModel ( square_numbers = [ 1 , 4 , 2 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel square_numbers -> 2 2 is not a square number (type=assertion_error) \"\"\" try : DemoModel ( cube_numbers = [ 27 , 27 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel cube_numbers sum of numbers greater than 42 (type=value_error) \"\"\"","title":"Pre and per-item validators"},{"location":"coding/python/pydantic_validators/#subclass-validators-and-each_item","text":"If using a validator with a subclass that references a List type field on a parent class, using each_item=True will cause the validator not to run; instead, the list must be iterated over programatically. from typing import List from pydantic import BaseModel , ValidationError , validator class ParentModel ( BaseModel ): names : List [ str ] class ChildModel ( ParentModel ): @validator ( 'names' , each_item = True ) def check_names_not_empty ( cls , v ): assert v != '' , 'Empty strings are not allowed.' return v # This will NOT raise a ValidationError because the validator was not called try : child = ChildModel ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) else : print ( 'No ValidationError caught.' ) #> No ValidationError caught. class ChildModel2 ( ParentModel ): @validator ( 'names' ) def check_names_not_empty ( cls , v ): for name in v : assert name != '' , 'Empty strings are not allowed.' return v try : child = ChildModel2 ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for ChildModel2 names Empty strings are not allowed. (type=assertion_error) \"\"\"","title":"Subclass Validators and each_item"},{"location":"coding/python/pydantic_validators/#validate-always","text":"For performance reasons, by default validators are not called for fields when a value is not supplied. However there are situations where it may be useful or required to always call the validator, e.g. to set a dynamic default value. from datetime import datetime from pydantic import BaseModel , validator class DemoModel ( BaseModel ): ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoModel ()) #> ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 966302) print ( DemoModel ( ts = '2017-11-08T14:00' )) #> ts=datetime.datetime(2017, 11, 8, 14, 0) You'll often want to use this together with pre , since otherwise with always=True pydantic would try to validate the default None which would cause an error.","title":"Validate Always"},{"location":"coding/python/pydantic_validators/#reuse-validators","text":"Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators. Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the allow_reuse parameter has been added to pydantic.validator in v1.2 ( False by default): from pydantic import BaseModel , validator def normalize ( name : str ) -> str : return ' ' . join (( word . capitalize ()) for word in name . split ( ' ' )) class Producer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) class Consumer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) jane_doe = Producer ( name = 'JaNe DOE' ) john_doe = Consumer ( name = 'joHN dOe' ) assert jane_doe . name == 'Jane Doe' assert john_doe . name == 'John Doe' As it is obvious, repetition has been reduced and the models become again almost declarative. Tip If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting allow_reuse=True over and over again.","title":"Reuse validators"},{"location":"coding/python/pydantic_validators/#root-validators","text":"Validation can also be performed on the entire model's data. from pydantic import BaseModel , ValidationError , root_validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator ( pre = True ) def check_card_number_omitted ( cls , values ): assert 'card_number' not in values , 'card_number should not be included' return values @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values print ( UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' )) #> username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ passwords do not match (type=value_error) \"\"\" try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , card_number = '1234' , ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ card_number should not be included (type=assertion_error) \"\"\" As with field validators, root validators can have pre=True , in which case they're called before field validation occurs (and are provided with the raw input data), or pre=False (the default), in which case they're called after field validation. Field validation will not occur if pre=True root validators raise an error. As with field validators, \"post\" (i.e. pre=False ) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the skip_on_failure=True keyword argument to the validator. The values argument will be a dict containing the values which passed field validation and field defaults where applicable.","title":"Root Validators"},{"location":"coding/python/pydantic_validators/#field-checks","text":"On class creation, validators are checked to confirm that the fields they specify actually exist on the model. Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models. In this case you should set check_fields=False on the validator.","title":"Field Checks"},{"location":"coding/python/pydantic_validators/#dataclass-validators","text":"Validators also work with pydantic dataclasses. from datetime import datetime from pydantic import validator from pydantic.dataclasses import dataclass @dataclass class DemoDataclass : ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoDataclass ()) #> DemoDataclass(ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 969037)) print ( DemoDataclass ( ts = '2017-11-08T14:00' )) #> DemoDataclass(ts=datetime.datetime(2017, 11, 8, 14, 0))","title":"Dataclass Validators"},{"location":"coding/python/pydantic_validators/#troubleshooting-validators","text":"","title":"Troubleshooting validators"},{"location":"coding/python/pydantic_validators/#pylint-complains-on-the-validators","text":"Pylint complains that R0201: Method could be a function and N805: first argument of a method should be named 'self' . Seems to be an error of pylint, people have solved it by specifying @classmethod between the definition and the validator decorator.","title":"pylint complains on the validators"},{"location":"coding/python/pydantic_validators/#references","text":"Pydantic validators","title":"References"},{"location":"coding/python/pypika/","text":"Pypika is a Python API for building SQL queries. The motivation behind PyPika is to provide a simple interface for building SQL queries without limiting the flexibility of handwritten SQL. PyPika is a fast, expressive and flexible way to replace handwritten SQL. Validation of SQL correctness is not an explicit goal of the project. Instead you are encouraged to check inputs you provide to PyPika or appropriately handle errors raised from your SQL database. After the queries have been built you need to interact with the database with other libraries. Installation \u2691 pip install pypika Usage \u2691 The main classes in pypika are pypika.Query , pypika.Table , and pypika.Field . from pypika import Query , Table , Field Creating Tables \u2691 The entry point for creating tables is pypika.Query.create_table , which is used with the class pypika.Column . As with selecting data, first the table should be specified. This can be either a string or a pypika.Table . Then the columns, and constraints.. stmt = Query \\ . create_table ( \"person\" ) \\ . columns ( Column ( \"id\" , \"INT\" , nullable = False ), Column ( \"first_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"last_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"phone_number\" , \"VARCHAR(20)\" , nullable = True ), Column ( \"status\" , \"VARCHAR(20)\" , nullable = False , default = ValueWrapper ( \"NEW\" )), Column ( \"date_of_birth\" , \"DATETIME\" )) \\ . unique ( \"last_name\" , \"first_name\" ) \\ . primary_key ( \"id\" ) This produces: CREATE TABLE \"person\" ( \"id\" INT NOT NULL , \"first_name\" VARCHAR ( 100 ) NOT NULL , \"last_name\" VARCHAR ( 100 ) NOT NULL , \"phone_number\" VARCHAR ( 20 ) NULL , \"status\" VARCHAR ( 20 ) NOT NULL DEFAULT 'NEW' , \"date_of_birth\" DATETIME , UNIQUE ( \"last_name\" , \"first_name\" ), PRIMARY KEY ( \"id\" ) ) It seems that they don't yet support the definition of FOREIGN KEYS when creating a new table. Selecting Data \u2691 The entry point for building queries is pypika.Query . In order to select columns from a table, the table must first be added to the query. For simple queries with only one table, tables and columns can be references using strings. For more sophisticated queries a pypika.Table must be used. q = Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) To convert the query into raw SQL, it can be cast to a string. str ( q ) Alternatively, you can use the Query.get_sql() function: q . get_sql () Filtering \u2691 Queries can be filtered with pypika.Criterion by using equality or inequality operators. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE lname = 'Mustermann' Query methods such as select, where, groupby, and orderby can be called multiple times. Multiple calls to the where method will add additional conditions as: customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . fname == 'Max' ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE fname = 'Max' AND lname = 'Mustermann' Filters such as IN and BETWEEN are also supported. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname ) . where ( customers . age [ 18 : 65 ] & customers . status . isin ([ 'new' , 'active' ]) ) SELECT id , fname FROM customers WHERE age BETWEEN 18 AND 65 AND status IN ( 'new' , 'active' ) Filtering with complex criteria can be created using boolean symbols & , | , and ^ . AND customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) & ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 AND lname = 'Mustermann' OR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) | ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 OR lname = 'Mustermann' XOR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) ^ customers . is_registered ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 XOR is_registered Deleting data \u2691 Query . from_ ( table ) . delete () . where ( table . id == id ) References \u2691 Docs Source","title":"Pypika"},{"location":"coding/python/pypika/#installation","text":"pip install pypika","title":"Installation"},{"location":"coding/python/pypika/#usage","text":"The main classes in pypika are pypika.Query , pypika.Table , and pypika.Field . from pypika import Query , Table , Field","title":"Usage"},{"location":"coding/python/pypika/#creating-tables","text":"The entry point for creating tables is pypika.Query.create_table , which is used with the class pypika.Column . As with selecting data, first the table should be specified. This can be either a string or a pypika.Table . Then the columns, and constraints.. stmt = Query \\ . create_table ( \"person\" ) \\ . columns ( Column ( \"id\" , \"INT\" , nullable = False ), Column ( \"first_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"last_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"phone_number\" , \"VARCHAR(20)\" , nullable = True ), Column ( \"status\" , \"VARCHAR(20)\" , nullable = False , default = ValueWrapper ( \"NEW\" )), Column ( \"date_of_birth\" , \"DATETIME\" )) \\ . unique ( \"last_name\" , \"first_name\" ) \\ . primary_key ( \"id\" ) This produces: CREATE TABLE \"person\" ( \"id\" INT NOT NULL , \"first_name\" VARCHAR ( 100 ) NOT NULL , \"last_name\" VARCHAR ( 100 ) NOT NULL , \"phone_number\" VARCHAR ( 20 ) NULL , \"status\" VARCHAR ( 20 ) NOT NULL DEFAULT 'NEW' , \"date_of_birth\" DATETIME , UNIQUE ( \"last_name\" , \"first_name\" ), PRIMARY KEY ( \"id\" ) ) It seems that they don't yet support the definition of FOREIGN KEYS when creating a new table.","title":"Creating Tables"},{"location":"coding/python/pypika/#selecting-data","text":"The entry point for building queries is pypika.Query . In order to select columns from a table, the table must first be added to the query. For simple queries with only one table, tables and columns can be references using strings. For more sophisticated queries a pypika.Table must be used. q = Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) To convert the query into raw SQL, it can be cast to a string. str ( q ) Alternatively, you can use the Query.get_sql() function: q . get_sql ()","title":"Selecting Data"},{"location":"coding/python/pypika/#filtering","text":"Queries can be filtered with pypika.Criterion by using equality or inequality operators. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE lname = 'Mustermann' Query methods such as select, where, groupby, and orderby can be called multiple times. Multiple calls to the where method will add additional conditions as: customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . fname == 'Max' ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE fname = 'Max' AND lname = 'Mustermann' Filters such as IN and BETWEEN are also supported. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname ) . where ( customers . age [ 18 : 65 ] & customers . status . isin ([ 'new' , 'active' ]) ) SELECT id , fname FROM customers WHERE age BETWEEN 18 AND 65 AND status IN ( 'new' , 'active' ) Filtering with complex criteria can be created using boolean symbols & , | , and ^ . AND customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) & ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 AND lname = 'Mustermann' OR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) | ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 OR lname = 'Mustermann' XOR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) ^ customers . is_registered ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 XOR is_registered","title":"Filtering"},{"location":"coding/python/pypika/#deleting-data","text":"Query . from_ ( table ) . delete () . where ( table . id == id )","title":"Deleting data"},{"location":"coding/python/pypika/#references","text":"Docs Source","title":"References"},{"location":"coding/python/pytest/","text":"pytest is a Python framework to makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. Pytest stands out over other test frameworks in: Simple tests are simple to write in pytest. Complex tests are still simple to write. Tests are easy to read. You can get started in seconds. You use assert to fail a test, not things like self.assertEqual() or self.assertLessThan() . Just assert . You can use pytest to run tests written for unittest or nose. You can use this cookiecutter template to create a python project with pytest already configured. Install \u2691 pip install pytest Usage \u2691 Run in the project directory. pytest If you need more information run it with -v . Pytest automatically finds which tests to run in a phase called test discovery . It will get the tests that match one of the following conditions: Test files that are named test_{{ something }}.py or {{ something }}_test.py . Test methods and functions named test_{{ something }} . Test classes named Test{{ Something }} . There are several possible outcomes of a test function: PASSED (.) : The test ran successfully. FAILED (F) : The test did not run usccessfully (or XPASS + strict). SKIPPED (s) : The test was skipped. You can tell pytest to skip a test by using enter the @pytest.mark.skip() or pytest.mark.skipif() decorators. xfail (x) : The test was not supposed to pass, ran, and failed. You can tell pytest that a test is expected to fail by using the @pytest.mark.xfail() decorator. XPASS (X) : The tests was not supposed to pass, ran, and passed. ERROR (E) : An exception happened outside of the test function, in either a fixture or a hook function. Pytest supports several cool flags like: -k EXPRESSION : Used to select a subset of tests to run. For example pytest -k \"asdict or defaults\" will run both test_asdict() and test_defaults() . --lf or --last-failed : Just run the tests that have failed in the previous run. -x , or --exitfirst : Exit on first failed test. -l or --showlocals : Print out the local variables in a test if the test fails. -s Allows any output that normally would be printed to stdout to actually be printed to stdout . It's an alias of --capture=no , so the output is not captured when the tests are run, which is the default behavior. This is useful to debug with print() statements. --durations=N : It reports the slowest N number of tests/setups/teardowns after the test run. If you pass in --durations=0 , it reports everything in order of slowest to fastest. Fixtures \u2691 Fixtures are functions that are run by pytest before (and sometimes after) the actual test functions. You can use fixtures to get a data set for the tests to work on, or use them to get a system into a known state before running a test. They are also used to get data ready for multiple tests. Here's a simple fixture that returns a number: import pytest @pytest . fixture () def some_data () \"\"\" Return answer to the ultimate question \"\"\" return 42 def test_some_data ( some_data ): \"\"\" Use fixture return value in a test\"\"\" assert some_data == 42 The @pytest.fixture() decorator is used to tell pytest that a function is a fixture.When you include the fixture name in the parameter list of a test function,pytest knows to run it before running the test. Fixtures can do work, and can also return data to the test function. The test test_some_data() has the name of the fixture, some_data, as a parameter.pytest will see this and look for a fixture with this name. Naming is significant in pytest. pytest will look in the module of the test for a fixture of that name. If the function is defined in the same file as where it's being used pylint will raise an W0621: Redefining name %r from outer scope (line %s) error. To solve it either move the fixture to other file or name the decorated function fixture_<fixturename> and then use @pytest.fixture(name='<fixturename>') . Sharing fixtures through conftest.py \u2691 You can put fixtures into individual test files, but to share fixtures among multiple test files, you need to use a conftest.py file somewhere centrally located for all of the tests. Additionally you can have conftest.py files in subdirectories of the top tests directory. If you do, fixtures defined in these lower level conftest.py files will be available to tests in that directory and subdirectories. Although conftest.py is a Python module, it should not be imported by test files. The file gets read by pytest, and is considered a local plugin . Another option is to save the fixtures in a file by creating a local pytest plugin . File: tests/unit/conftest.py pytest_plugins = [ \"tests.unit.fixtures.some_stuff\" , ] File: tests/unit/fixtures/some_stuff.py import pytest @pytest . fixture def foo (): return 'foobar' Specifying fixture scope \u2691 Fixtures include an optional parameter called scope, which controls how often a fixture gets set up and torn down. The scope parameter to @pytest.fixture() can have the values of function, class, module, or session. Here\u2019s a rundown of each scope value: scope='function' : Run once per test function. The setup portion is run before each test using the fixture. The teardown portion is run after each test using the fixture. This is the default scope used when no scope parameter is specified. scope='class' : Run once per test class, regardless of how many test methods are in the class. scope='module' : Run once per module, regardless of how many test functions or methods or other fixtures in the module use it. scope='session' Run once per session. All test methods and functions using a fixture of session scope share one setup and teardown call. Useful Fixtures \u2691 The tmpdir fixture \u2691 You can use the tmpdir fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory. tmpdir is a py.path.local object which offers os.path methods and more. Here is an example test usage: File: test_tmpdir.py from py._path.local import LocalPath def test_create_file ( tmpdir : LocalPath ): p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) p . write ( \"content\" ) assert p . read () == \"content\" assert len ( tmpdir . listdir ()) == 1 assert 0 The tmpdir fixture has a scope of function so you can't make a session directory. Instead use the tmpdir_factory fixture. from _pytest.tmpdir import TempdirFactory @pytest . fixture ( scope = \"session\" ) def image_file ( tmpdir_factory : TempdirFactory ): img = compute_expensive_image () fn = tmpdir_factory . mktemp ( \"data\" ) . join ( \"img.png\" ) img . save ( str ( fn )) return fn def test_histogram ( image_file ): img = load_image ( image_file ) # compute and test histogram Make a subdirectory \u2691 p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) The caplog fixture \u2691 pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr. You can change the default logging level in the pytest configuration: File: pytest.ini [pytest] log_level = debug Although it may not be a good idea in most cases. It's better to change the log level in the tests that need a lower level. All the logs sent to the logger during the test run are available on the fixture in the form of both the logging.LogRecord instances and the final log text. This is useful for when you want to assert on the contents of a message: from _pytest.logging import LogCaptureFixture def test_baz ( caplog : LogCaptureFixture ): func_under_test () for record in caplog . records : assert record . levelname != \"CRITICAL\" assert \"wally\" not in caplog . text You can also resort to record_tuples if all you want to do is to ensure that certain messages have been logged under a given logger name with a given severity and message: def test_foo ( caplog ): logging . getLogger () . info ( \"boo %s \" , \"arg\" ) assert ( \"root\" , logging . INFO , \"boo arg\" ) in caplog . record_tuples You can call caplog.clear() to reset the captured log records in a test. Change the log level \u2691 Inside tests it's possible to change the log level for the captured log messages. def test_foo ( caplog ): caplog . set_level ( logging . INFO ) pass The capsys fixture \u2691 The capsys builtin fixture provides two bits of functionality: it allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily. Suppose you have a function to print a greeting to stdout: def greeting ( name ): print ( f 'Hi, { name } ' ) You can test the output by using capsys . from _pytest.capture import CaptureFixture def test_greeting ( capsys : CaptureFixture [ Any ]): greeting ( 'Earthling' ) out , err = capsys . readouterr () assert out == 'Hi, Earthling \\n ' assert err == '' The return value is whatever has been captured since the beginning of the function, or from the last time it was called. freezegun \u2691 freezegun lets you freeze time in both the test and fixtures. Install \u2691 pip install pytest-freezegun Usage \u2691 Freeze time by using the freezer fixture: if TYPE_CHECKING : from freezegun.api import FrozenDateTimeFactory def test_frozen_date ( freezer : FrozenDateTimeFactory ): now = datetime . now () time . sleep ( 1 ) later = datetime . now () assert now == later This can then be used to move time: def test_moving_date ( freezer ): now = datetime . now () freezer . move_to ( '2017-05-20' ) later = datetime . now () assert now != later You can also pass arguments to freezegun by using the freeze_time mark: @pytest . mark . freeze_time ( '2017-05-21' ) def test_current_date (): assert date . today () == date ( 2017 , 5 , 21 ) The freezer fixture and freeze_time mark can be used together, and they work with other fixtures: @pytest . fixture def current_date (): return date . today () @pytest . mark . freeze_time def test_changing_date ( current_date , freezer ): freezer . move_to ( '2017-05-20' ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( '2017-05-21' ) assert current_date == date ( 2017 , 5 , 21 ) They can also be used in class-based tests: class TestDate : @pytest . mark . freeze_time def test_changing_date ( self , current_date , freezer ): freezer . move_to ( '2017-05-20' ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( '2017-05-21' ) assert current_date == date ( 2017 , 5 , 21 ) Customize nested fixtures \u2691 Sometimes you need to tweak your fixtures so they can be used in different tests. As usual, there are different solutions to the same problem. !!! note \"TL;DR: For simple cases parametrize your fixtures or use parametrization to override the default valued fixture . As your test suite get's more complex migrate to pytest-case .\" Let's say you're running along merrily with some fixtures that create database objects for you: @pytest . fixture def supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), country = \"US\" , ) db . add ( s ) yield s db . remove ( s ) @pytest . fixture () def product ( db , supplier ): p = Product ( ref = random_ref (), name = random_name (), supplier = supplier , net_price = 9.99 , ) db . add ( p ) yield p db . remove ( p ) And now you're writing a new test and you suddenly realize you need to customize your default \"supplier\" fixture: def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price def test_EU_supplier_has_total_price_including_VAT ( supplier , product ): supplier . country = \"FR\" # oh, this doesn't work assert product . total_price == product . net_price * 1.2 There are different ways to modify your fixtures Add more fixtures \u2691 We can just create more fixtures, and try to do a bit of DRY by extracting out common logic: def _default_supplier (): return Supplier ( ref = random_ref (), name = random_name (), ) @pytest . fixture def us_supplier ( db ): s = _default_supplier () s . country = \"US\" db . add ( s ) yield s db . remove ( s ) @pytest . fixture def eu_supplier ( db ): s = _default_supplier () s . country = \"FR\" db . add ( s ) yield s db . remove ( s ) That's just one way you could do it, maybe you can figure out ways to reduce the duplication of the db.add() stuff as well, but you are going to have a different, named fixture for each customization of Supplier, and eventually you may decide that doesn't scale. Use factory fixtures \u2691 Instead of a fixture returning an object directly, it can return a function that creates an object, and that function can take arguments: @pytest . fixture def make_supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), ) def _make_supplier ( country ): s . country = country db . add ( s ) return s yield _make_supplier db . remove ( s ) The problem with this is that, once you start, you tend to have to go all the way, and make all of your fixture hierarchy into factory functions: def test_EU_supplier_has_total_price_including_VAT ( make_supplier , product ): supplier = make_supplier ( country = \"FR\" ) product . supplier = supplier # OH, now this doesn't work, because it's too late again assert product . total_price == product . net_price * 1.2 And so... @pytest . fixture def make_product ( db ): p = Product ( ref = random_ref (), name = random_name (), ) def _make_product ( supplier ): p . supplier = supplier db . add ( p ) return p yield _make_product db . remove ( p ) def test_EU_supplier_has_total_price_including_VAT ( make_supplier , make_product ): supplier = make_supplier ( country = \"FR\" ) product = make_product ( supplier = supplier ) assert product . total_price == product . net_price * 1.2 That works, but firstly now everything is a factory-fixture, which makes them more convoluted, and secondly, your tests are filling up with extra calls to make_things , and you're having to embed some of the domain knowledge of what-depends-on-what into your tests as well as your fixtures. Ugly! Parametrize your fixtures \u2691 You can also parametrize your fixtures . @pytest . fixture ( params = [ 'US' , 'FR' ]) def supplier ( db , request ): s = Supplier ( ref = random_ref (), name = random_name (), country = request . param ) db . add ( s ) yield s db . remove ( s ) Now any test that depends on supplier, directly or indirectly, will be run twice, once with supplier.country = US and once with FR . That's really cool for checking that a given piece of logic works in a variety of different cases, but it's not really ideal in our case. We have to build a bunch of if logic into our tests: def test_US_supplier_has_no_VAT_but_EU_supplier_has_total_price_including_VAT ( product ): # this test is magically run twice, but: if product . supplier . country == 'US' : assert product . total_price == product . net_price if product . supplier . country == 'FR' : assert product . total_price == product . net_price * 1.2 So that's ugly, and on top of that, now every single test that depends (indirectly) on supplier gets run twice, and some of those extra test runs may be totally irrelevant to what the country is. Use pytest parametrization to override the default valued fixtures \u2691 We introduce an extra fixture that holds a default value for the country field: @pytest . fixture () def country (): return \"US\" @pytest . fixture def supplier ( db , country ): s = Supplier ( ref = random_ref (), name = random_name (), country = country , ) db . add ( s ) yield s db . remove ( s ) And then in the tests that need to change it, we can use parametrize to override the default value of country, even though the country fixture isn't explicitly named in that test: @pytest . mark . parametrize ( 'country' , [ \"US\" ]) def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price @pytest . mark . parametrize ( 'country' , [ \"EU\" ]) def test_EU_supplier_has_total_price_including_VAT ( product ): assert product . total_price == product . net_price * 1.2 The only problem is that you're now likely to build a implicit dependencies where the only way to find out what's actually happening is to spend ages spelunking in conftest.py. Use pytest-case \u2691 pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information. Use a fixture more than once in a function \u2691 One solution is to make your fixture return a factory instead of the resource directly: @pytest . fixture ( name = 'make_user' ) def make_user_ (): created = [] def make_user (): u = models . User () u . commit () created . append ( u ) return u yield make_user for u in created : u . delete () def test_two_users ( make_user ): user1 = make_user () user2 = make_user () # test them # you can even have the normal fixture when you only need a single user @pytest . fixture def user ( make_user ): return make_user () def test_one_user ( user ): # test him/her Marks \u2691 Pytest marks can be used to group tests. It can be useful to: slow Mark the tests that are slow. secondary Mart the tests that use functionality that is being tested in the same file. To mark a test, use the @pytest.mark decorator. For example: @pytest . mark . slow def test_really_slow_test (): pass Pytest requires you to register your marks, do so in the pytest.ini file [pytest] markers = slow: marks tests as slow (deselect with '-m \"not slow\"') secondary: mark tests that use functionality tested in the same file (deselect with '-m \"not secondary\"') Snippets \u2691 Mocking sys.exit \u2691 with pytest . raises ( SystemExit ): # Code to test Testing exceptions with pytest \u2691 def test_value_error_is_raised (): with pytest . raises ( ValueError , match = \"invalid literal for int() with base 10: 'a'\" ): int ( 'a' ) pytest integration with Vim \u2691 Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD. I use Janko-m's Vim-test plugin (which can be installed through Vundle ) with the following configuration. nmap < silent > t :TestNearest < CR > nmap < silent > < leader > t :TestSuite tests/unit < CR > nmap < silent > < leader > i :TestSuite tests/integration < CR > nmap < silent > < leader > T :TestFile < CR > let test#python#runner = 'pytest' let test#strategy = \"neovim\" I often open Vim with a vertical split ( :vs ), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press t when the cursor is inside that test. If you need to make changes in the code, you can press t again while the cursor is at the code you are testing and it will run the last test. Once the unit test has passed, I run the whole unit tests with ;t (as ; is my <leader> ). And finally I use ;i to run the integration tests. Finally, if the test suite is huge, I use ;T to run only the tests of a single file. Reference \u2691 Book Python Testing with pytest by Brian Okken . Docs Vim-test plugin","title":"Pytest"},{"location":"coding/python/pytest/#install","text":"pip install pytest","title":"Install"},{"location":"coding/python/pytest/#usage","text":"Run in the project directory. pytest If you need more information run it with -v . Pytest automatically finds which tests to run in a phase called test discovery . It will get the tests that match one of the following conditions: Test files that are named test_{{ something }}.py or {{ something }}_test.py . Test methods and functions named test_{{ something }} . Test classes named Test{{ Something }} . There are several possible outcomes of a test function: PASSED (.) : The test ran successfully. FAILED (F) : The test did not run usccessfully (or XPASS + strict). SKIPPED (s) : The test was skipped. You can tell pytest to skip a test by using enter the @pytest.mark.skip() or pytest.mark.skipif() decorators. xfail (x) : The test was not supposed to pass, ran, and failed. You can tell pytest that a test is expected to fail by using the @pytest.mark.xfail() decorator. XPASS (X) : The tests was not supposed to pass, ran, and passed. ERROR (E) : An exception happened outside of the test function, in either a fixture or a hook function. Pytest supports several cool flags like: -k EXPRESSION : Used to select a subset of tests to run. For example pytest -k \"asdict or defaults\" will run both test_asdict() and test_defaults() . --lf or --last-failed : Just run the tests that have failed in the previous run. -x , or --exitfirst : Exit on first failed test. -l or --showlocals : Print out the local variables in a test if the test fails. -s Allows any output that normally would be printed to stdout to actually be printed to stdout . It's an alias of --capture=no , so the output is not captured when the tests are run, which is the default behavior. This is useful to debug with print() statements. --durations=N : It reports the slowest N number of tests/setups/teardowns after the test run. If you pass in --durations=0 , it reports everything in order of slowest to fastest.","title":"Usage"},{"location":"coding/python/pytest/#fixtures","text":"Fixtures are functions that are run by pytest before (and sometimes after) the actual test functions. You can use fixtures to get a data set for the tests to work on, or use them to get a system into a known state before running a test. They are also used to get data ready for multiple tests. Here's a simple fixture that returns a number: import pytest @pytest . fixture () def some_data () \"\"\" Return answer to the ultimate question \"\"\" return 42 def test_some_data ( some_data ): \"\"\" Use fixture return value in a test\"\"\" assert some_data == 42 The @pytest.fixture() decorator is used to tell pytest that a function is a fixture.When you include the fixture name in the parameter list of a test function,pytest knows to run it before running the test. Fixtures can do work, and can also return data to the test function. The test test_some_data() has the name of the fixture, some_data, as a parameter.pytest will see this and look for a fixture with this name. Naming is significant in pytest. pytest will look in the module of the test for a fixture of that name. If the function is defined in the same file as where it's being used pylint will raise an W0621: Redefining name %r from outer scope (line %s) error. To solve it either move the fixture to other file or name the decorated function fixture_<fixturename> and then use @pytest.fixture(name='<fixturename>') .","title":"Fixtures"},{"location":"coding/python/pytest/#sharing-fixtures-through-conftestpy","text":"You can put fixtures into individual test files, but to share fixtures among multiple test files, you need to use a conftest.py file somewhere centrally located for all of the tests. Additionally you can have conftest.py files in subdirectories of the top tests directory. If you do, fixtures defined in these lower level conftest.py files will be available to tests in that directory and subdirectories. Although conftest.py is a Python module, it should not be imported by test files. The file gets read by pytest, and is considered a local plugin . Another option is to save the fixtures in a file by creating a local pytest plugin . File: tests/unit/conftest.py pytest_plugins = [ \"tests.unit.fixtures.some_stuff\" , ] File: tests/unit/fixtures/some_stuff.py import pytest @pytest . fixture def foo (): return 'foobar'","title":"Sharing fixtures through conftest.py"},{"location":"coding/python/pytest/#specifying-fixture-scope","text":"Fixtures include an optional parameter called scope, which controls how often a fixture gets set up and torn down. The scope parameter to @pytest.fixture() can have the values of function, class, module, or session. Here\u2019s a rundown of each scope value: scope='function' : Run once per test function. The setup portion is run before each test using the fixture. The teardown portion is run after each test using the fixture. This is the default scope used when no scope parameter is specified. scope='class' : Run once per test class, regardless of how many test methods are in the class. scope='module' : Run once per module, regardless of how many test functions or methods or other fixtures in the module use it. scope='session' Run once per session. All test methods and functions using a fixture of session scope share one setup and teardown call.","title":"Specifying fixture scope"},{"location":"coding/python/pytest/#useful-fixtures","text":"","title":"Useful Fixtures"},{"location":"coding/python/pytest/#the-tmpdir-fixture","text":"You can use the tmpdir fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory. tmpdir is a py.path.local object which offers os.path methods and more. Here is an example test usage: File: test_tmpdir.py from py._path.local import LocalPath def test_create_file ( tmpdir : LocalPath ): p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) p . write ( \"content\" ) assert p . read () == \"content\" assert len ( tmpdir . listdir ()) == 1 assert 0 The tmpdir fixture has a scope of function so you can't make a session directory. Instead use the tmpdir_factory fixture. from _pytest.tmpdir import TempdirFactory @pytest . fixture ( scope = \"session\" ) def image_file ( tmpdir_factory : TempdirFactory ): img = compute_expensive_image () fn = tmpdir_factory . mktemp ( \"data\" ) . join ( \"img.png\" ) img . save ( str ( fn )) return fn def test_histogram ( image_file ): img = load_image ( image_file ) # compute and test histogram","title":"The tmpdir fixture"},{"location":"coding/python/pytest/#make-a-subdirectory","text":"p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" )","title":"Make a subdirectory"},{"location":"coding/python/pytest/#the-caplog-fixture","text":"pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr. You can change the default logging level in the pytest configuration: File: pytest.ini [pytest] log_level = debug Although it may not be a good idea in most cases. It's better to change the log level in the tests that need a lower level. All the logs sent to the logger during the test run are available on the fixture in the form of both the logging.LogRecord instances and the final log text. This is useful for when you want to assert on the contents of a message: from _pytest.logging import LogCaptureFixture def test_baz ( caplog : LogCaptureFixture ): func_under_test () for record in caplog . records : assert record . levelname != \"CRITICAL\" assert \"wally\" not in caplog . text You can also resort to record_tuples if all you want to do is to ensure that certain messages have been logged under a given logger name with a given severity and message: def test_foo ( caplog ): logging . getLogger () . info ( \"boo %s \" , \"arg\" ) assert ( \"root\" , logging . INFO , \"boo arg\" ) in caplog . record_tuples You can call caplog.clear() to reset the captured log records in a test.","title":"The caplog fixture"},{"location":"coding/python/pytest/#change-the-log-level","text":"Inside tests it's possible to change the log level for the captured log messages. def test_foo ( caplog ): caplog . set_level ( logging . INFO ) pass","title":"Change the log level"},{"location":"coding/python/pytest/#the-capsys-fixture","text":"The capsys builtin fixture provides two bits of functionality: it allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily. Suppose you have a function to print a greeting to stdout: def greeting ( name ): print ( f 'Hi, { name } ' ) You can test the output by using capsys . from _pytest.capture import CaptureFixture def test_greeting ( capsys : CaptureFixture [ Any ]): greeting ( 'Earthling' ) out , err = capsys . readouterr () assert out == 'Hi, Earthling \\n ' assert err == '' The return value is whatever has been captured since the beginning of the function, or from the last time it was called.","title":"The capsys fixture"},{"location":"coding/python/pytest/#freezegun","text":"freezegun lets you freeze time in both the test and fixtures.","title":"freezegun"},{"location":"coding/python/pytest/#install_1","text":"pip install pytest-freezegun","title":"Install"},{"location":"coding/python/pytest/#usage_1","text":"Freeze time by using the freezer fixture: if TYPE_CHECKING : from freezegun.api import FrozenDateTimeFactory def test_frozen_date ( freezer : FrozenDateTimeFactory ): now = datetime . now () time . sleep ( 1 ) later = datetime . now () assert now == later This can then be used to move time: def test_moving_date ( freezer ): now = datetime . now () freezer . move_to ( '2017-05-20' ) later = datetime . now () assert now != later You can also pass arguments to freezegun by using the freeze_time mark: @pytest . mark . freeze_time ( '2017-05-21' ) def test_current_date (): assert date . today () == date ( 2017 , 5 , 21 ) The freezer fixture and freeze_time mark can be used together, and they work with other fixtures: @pytest . fixture def current_date (): return date . today () @pytest . mark . freeze_time def test_changing_date ( current_date , freezer ): freezer . move_to ( '2017-05-20' ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( '2017-05-21' ) assert current_date == date ( 2017 , 5 , 21 ) They can also be used in class-based tests: class TestDate : @pytest . mark . freeze_time def test_changing_date ( self , current_date , freezer ): freezer . move_to ( '2017-05-20' ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( '2017-05-21' ) assert current_date == date ( 2017 , 5 , 21 )","title":"Usage"},{"location":"coding/python/pytest/#customize-nested-fixtures","text":"Sometimes you need to tweak your fixtures so they can be used in different tests. As usual, there are different solutions to the same problem. !!! note \"TL;DR: For simple cases parametrize your fixtures or use parametrization to override the default valued fixture . As your test suite get's more complex migrate to pytest-case .\" Let's say you're running along merrily with some fixtures that create database objects for you: @pytest . fixture def supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), country = \"US\" , ) db . add ( s ) yield s db . remove ( s ) @pytest . fixture () def product ( db , supplier ): p = Product ( ref = random_ref (), name = random_name (), supplier = supplier , net_price = 9.99 , ) db . add ( p ) yield p db . remove ( p ) And now you're writing a new test and you suddenly realize you need to customize your default \"supplier\" fixture: def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price def test_EU_supplier_has_total_price_including_VAT ( supplier , product ): supplier . country = \"FR\" # oh, this doesn't work assert product . total_price == product . net_price * 1.2 There are different ways to modify your fixtures","title":"Customize nested fixtures"},{"location":"coding/python/pytest/#add-more-fixtures","text":"We can just create more fixtures, and try to do a bit of DRY by extracting out common logic: def _default_supplier (): return Supplier ( ref = random_ref (), name = random_name (), ) @pytest . fixture def us_supplier ( db ): s = _default_supplier () s . country = \"US\" db . add ( s ) yield s db . remove ( s ) @pytest . fixture def eu_supplier ( db ): s = _default_supplier () s . country = \"FR\" db . add ( s ) yield s db . remove ( s ) That's just one way you could do it, maybe you can figure out ways to reduce the duplication of the db.add() stuff as well, but you are going to have a different, named fixture for each customization of Supplier, and eventually you may decide that doesn't scale.","title":"Add more fixtures"},{"location":"coding/python/pytest/#use-factory-fixtures","text":"Instead of a fixture returning an object directly, it can return a function that creates an object, and that function can take arguments: @pytest . fixture def make_supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), ) def _make_supplier ( country ): s . country = country db . add ( s ) return s yield _make_supplier db . remove ( s ) The problem with this is that, once you start, you tend to have to go all the way, and make all of your fixture hierarchy into factory functions: def test_EU_supplier_has_total_price_including_VAT ( make_supplier , product ): supplier = make_supplier ( country = \"FR\" ) product . supplier = supplier # OH, now this doesn't work, because it's too late again assert product . total_price == product . net_price * 1.2 And so... @pytest . fixture def make_product ( db ): p = Product ( ref = random_ref (), name = random_name (), ) def _make_product ( supplier ): p . supplier = supplier db . add ( p ) return p yield _make_product db . remove ( p ) def test_EU_supplier_has_total_price_including_VAT ( make_supplier , make_product ): supplier = make_supplier ( country = \"FR\" ) product = make_product ( supplier = supplier ) assert product . total_price == product . net_price * 1.2 That works, but firstly now everything is a factory-fixture, which makes them more convoluted, and secondly, your tests are filling up with extra calls to make_things , and you're having to embed some of the domain knowledge of what-depends-on-what into your tests as well as your fixtures. Ugly!","title":"Use factory fixtures"},{"location":"coding/python/pytest/#parametrize-your-fixtures","text":"You can also parametrize your fixtures . @pytest . fixture ( params = [ 'US' , 'FR' ]) def supplier ( db , request ): s = Supplier ( ref = random_ref (), name = random_name (), country = request . param ) db . add ( s ) yield s db . remove ( s ) Now any test that depends on supplier, directly or indirectly, will be run twice, once with supplier.country = US and once with FR . That's really cool for checking that a given piece of logic works in a variety of different cases, but it's not really ideal in our case. We have to build a bunch of if logic into our tests: def test_US_supplier_has_no_VAT_but_EU_supplier_has_total_price_including_VAT ( product ): # this test is magically run twice, but: if product . supplier . country == 'US' : assert product . total_price == product . net_price if product . supplier . country == 'FR' : assert product . total_price == product . net_price * 1.2 So that's ugly, and on top of that, now every single test that depends (indirectly) on supplier gets run twice, and some of those extra test runs may be totally irrelevant to what the country is.","title":"Parametrize your fixtures"},{"location":"coding/python/pytest/#use-pytest-parametrization-to-override-the-default-valued-fixtures","text":"We introduce an extra fixture that holds a default value for the country field: @pytest . fixture () def country (): return \"US\" @pytest . fixture def supplier ( db , country ): s = Supplier ( ref = random_ref (), name = random_name (), country = country , ) db . add ( s ) yield s db . remove ( s ) And then in the tests that need to change it, we can use parametrize to override the default value of country, even though the country fixture isn't explicitly named in that test: @pytest . mark . parametrize ( 'country' , [ \"US\" ]) def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price @pytest . mark . parametrize ( 'country' , [ \"EU\" ]) def test_EU_supplier_has_total_price_including_VAT ( product ): assert product . total_price == product . net_price * 1.2 The only problem is that you're now likely to build a implicit dependencies where the only way to find out what's actually happening is to spend ages spelunking in conftest.py.","title":"Use pytest parametrization to override the default valued fixtures"},{"location":"coding/python/pytest/#use-pytest-case","text":"pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information.","title":"Use pytest-case"},{"location":"coding/python/pytest/#use-a-fixture-more-than-once-in-a-function","text":"One solution is to make your fixture return a factory instead of the resource directly: @pytest . fixture ( name = 'make_user' ) def make_user_ (): created = [] def make_user (): u = models . User () u . commit () created . append ( u ) return u yield make_user for u in created : u . delete () def test_two_users ( make_user ): user1 = make_user () user2 = make_user () # test them # you can even have the normal fixture when you only need a single user @pytest . fixture def user ( make_user ): return make_user () def test_one_user ( user ): # test him/her","title":"Use a fixture more than once in a function"},{"location":"coding/python/pytest/#marks","text":"Pytest marks can be used to group tests. It can be useful to: slow Mark the tests that are slow. secondary Mart the tests that use functionality that is being tested in the same file. To mark a test, use the @pytest.mark decorator. For example: @pytest . mark . slow def test_really_slow_test (): pass Pytest requires you to register your marks, do so in the pytest.ini file [pytest] markers = slow: marks tests as slow (deselect with '-m \"not slow\"') secondary: mark tests that use functionality tested in the same file (deselect with '-m \"not secondary\"')","title":"Marks"},{"location":"coding/python/pytest/#snippets","text":"","title":"Snippets"},{"location":"coding/python/pytest/#mocking-sysexit","text":"with pytest . raises ( SystemExit ): # Code to test","title":"Mocking sys.exit"},{"location":"coding/python/pytest/#testing-exceptions-with-pytest","text":"def test_value_error_is_raised (): with pytest . raises ( ValueError , match = \"invalid literal for int() with base 10: 'a'\" ): int ( 'a' )","title":"Testing exceptions with pytest"},{"location":"coding/python/pytest/#pytest-integration-with-vim","text":"Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD. I use Janko-m's Vim-test plugin (which can be installed through Vundle ) with the following configuration. nmap < silent > t :TestNearest < CR > nmap < silent > < leader > t :TestSuite tests/unit < CR > nmap < silent > < leader > i :TestSuite tests/integration < CR > nmap < silent > < leader > T :TestFile < CR > let test#python#runner = 'pytest' let test#strategy = \"neovim\" I often open Vim with a vertical split ( :vs ), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press t when the cursor is inside that test. If you need to make changes in the code, you can press t again while the cursor is at the code you are testing and it will run the last test. Once the unit test has passed, I run the whole unit tests with ;t (as ; is my <leader> ). And finally I use ;i to run the integration tests. Finally, if the test suite is huge, I use ;T to run only the tests of a single file.","title":"pytest integration with Vim"},{"location":"coding/python/pytest/#reference","text":"Book Python Testing with pytest by Brian Okken . Docs Vim-test plugin","title":"Reference"},{"location":"coding/python/pytest_cases/","text":"pytest-cases is a pytest plugin that allows you to separate your test cases from your test functions . In addition, pytest-cases provides several useful goodies to empower pytest . In particular it improves the fixture mechanism to support \"fixture unions\". This is a major change in the internal pytest engine, unlocking many possibilities such as using fixture references as parameter values in a test function. Installing \u2691 pip install pytest_cases Installing pytest-cases has effects on the order of pytest tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054 . But if you see less desirable ordering please report it . Why pytest-cases ? \u2691 Let's consider the following foo function under test, located in example.py : def foo ( a , b ): return a + 1 , b + 1 If we were using plain pytest to test it with various inputs, we would create a test_foo.py file and use @pytest.mark.parametrize : import pytest from example import foo @pytest . mark . parametrize ( \"a,b\" , [( 1 , 2 ), ( - 1 , - 2 )]) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) This is the fastest and most compact thing to do when you have a few number of test cases, that do not require code to generate each test case. Now imagine that instead of (1, 2) and (-1, -2) each of our test cases: Requires a few lines of code to be generated. Requires documentation to explain the other developers the intent of that precise test case. Requires external resources (data files on the filesystem, databases...), with a variable number of cases depending on what is available on the resource. Requires a readable id , such as 'uniformly_sampled_nonsorted_with_holes' for the above example. Of course we could use pytest.param or ids=<list> but that is \"a pain to maintain\" according to pytest doc. Such a design does not feel right as the id is detached from the case. With standard pytest there is no particular pattern to simplify your life here. Investigating a little bit, people usually end up trying to mix parameters and fixtures and asking this kind of question: so1 , so2 . But by design it is not possible to solve this problem using fixtures, because pytest does not handle \"unions\" of fixtures . There is also an example in pytest doc with a metafunc hook . The issue with such workarounds is that you can do anything . And anything is a bit too much: this does not provide any convention / \"good practice\" on how to organize test cases, which is an open door to developing ad-hoc unreadable or unmaintainable solutions. pytest_cases was created to provide an answer to this precise situation. It proposes a simple framework to separate test cases from test functions. The test cases are typically located in a separate \"companion\" file: test_foo.py is your usual test file containing the test functions (named test_<id> ). test_foo_cases.py contains the test cases , that are also functions. Note: an alternate file naming style cases_foo.py is also available if you prefer it. Basic usage \u2691 Case functions \u2691 Let's create a test_foo_cases.py file. This file will contain test cases generator functions , that we will call case functions for brevity. In these functions, you will typically either parse some test data files, generate some simulated test data, expected results, etc. File: test_foo_cases.py def case_two_positive_ints (): \"\"\" Inputs are two positive integers \"\"\" return 1 , 2 def case_two_negative_ints (): \"\"\" Inputs are two negative integers \"\"\" return - 1 , - 2 Case functions can return anything that is considered useful to run the associated test. You can use all classic pytest mechanism on case functions (id customization, skip/fail marks, parametrization or fixtures injection). Test functions \u2691 As usual we write our pytest test functions starting with test_ , in a test_foo.py file. The only difference is that we now decorate it with @parametrize_with_cases instead of @pytest.mark.parametrize as we were doing previously: File: test_foo.py from example import foo from pytest_cases import parametrize_with_cases @parametrize_with_cases ( \"a,b\" ) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) Executing pytest will now run our test function once for every case function: >>> pytest -s -v ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ two_positive_ints ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ two_negative_ints ] PASSED [ 100 % ] ========================== 2 passed in 0 .24 seconds ========================== Usage \u2691 Cases collection \u2691 Alternate source(s) \u2691 It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use cases='.' or cases=THIS_MODULE to refer to the module in which the test function is located: from pytest_cases import parametrize_with_cases def case_one_positive_int (): return 1 def case_one_negative_int (): return - 1 @parametrize_with_cases ( \"i\" , cases = '.' ) def test_with_this_module ( i ): assert i == int ( i ) Only the case functions defined BEFORE the test function in the module file will be taken into account. @parametrize_with_cases(cases=...) also accepts explicit list of case functions, classes containing case functions, and modules. See API Reference for details. A typical way to organize cases is to use classes for example: from pytest_cases import parametrize_with_cases class Foo : def case_a_positive_int ( self ): return 1 def case_another_positive_int ( self ): return 2 @parametrize_with_cases ( \"a\" , cases = Foo ) def test_foo ( a ): assert a > 0 Note that as for pytest , self is recreated for every test and therefore should not be used to store any useful information. Alternate prefix \u2691 case_ might not be your preferred prefix, especially if you wish to store in the same module or class various kind of case data. @parametrize_with_cases offers a prefix=... argument to select an alternate prefix for your case functions. That way, you can store in the same module or class case functions as diverse as datasets (e.g. data_ ), user descriptions (e.g. user_ ), algorithms or machine learning models (e.g. model_ or algo_ ), etc. from pytest_cases import parametrize_with_cases , parametrize def data_a (): return 'a' @parametrize ( \"hello\" , [ True , False ]) def data_b ( hello ): return \"hello\" if hello else \"world\" def case_c (): return dict ( name = \"hi i'm not used\" ) def user_bob (): return \"bob\" @parametrize_with_cases ( \"data\" , cases = '.' , prefix = \"data_\" ) @parametrize_with_cases ( \"user\" , cases = '.' , prefix = \"user_\" ) def test_with_data ( data , user ): assert data in ( 'a' , \"hello\" , \"world\" ) assert user == 'bob' Yields test_doc_filters_n_tags.py::test_with_data[bob-a] PASSED [ 33%] test_doc_filters_n_tags.py::test_with_data[bob-b-True] PASSED [ 66%] test_doc_filters_n_tags.py::test_with_data[bob-b-False] PASSED [ 100%] Filters and tags \u2691 The easiest way to select only a subset of case functions in a module or a class, is to specify a custom prefix instead of the default one ( 'case_' ). However sometimes more advanced filtering is required. In that case, you can also rely on three additional mechanisms provided in @parametrize_with_cases : The glob argument can contain a glob-like pattern for case ids. This can become handy to separate for example good or bad cases, the latter returning an expected error type and/or message for use with pytest.raises or with our alternative assert_exception . from math import sqrt import pytest from pytest_cases import parametrize_with_cases def case_int_success (): return 1 def case_negative_int_failure (): # note that we decide to return the expected type of failure to check it return - 1 , ValueError , \"math domain error\" @parametrize_with_cases ( \"data\" , cases = '.' , glob = \"*success\" ) def test_good_datasets ( data ): assert sqrt ( data ) > 0 @parametrize_with_cases ( \"data, err_type, err_msg\" , cases = '.' , glob = \"*failure\" ) def test_bad_datasets ( data , err_type , err_msg ): with pytest . raises ( err_type , match = err_msg ): sqrt ( data ) The has_tag argument allows you to filter cases based on tags set on case functions using the @case decorator. See API reference of @case and @parametrize_with_cases . from pytest_cases import parametrize_with_cases , case class FooCases : def case_two_positive_ints ( self ): return 1 , 2 @case ( tags = 'foo' ) def case_one_positive_int ( self ): return 1 @parametrize_with_cases ( \"a\" , cases = FooCases , has_tag = 'foo' ) def test_foo ( a ): assert a > 0 Finally if none of the above matches your expectations, you can provide a callable to filter . This callable will receive each collected case function and should return True in case of success. Note that your function can leverage the _pytestcase attribute available on the case function to read the tags, marks and id found on it. @parametrize_with_cases ( \"data\" , cases = '.' , filter = lambda cf : \"success\" in cf . _pytestcase . id ) def test_good_datasets2 ( data ): assert sqrt ( data ) > 0 Pytest marks ( skip , xfail ...) on cases \u2691 pytest marks such as @pytest.mark.skipif can be applied on case functions the same way as with test functions . import sys import pytest @pytest . mark . skipif ( sys . version_info < ( 3 , 0 ), reason = \"Not useful on python 2\" ) def case_two_positive_ints (): return 1 , 2 Case generators \u2691 In many real-world usage we want to generate one test case per <something> . The most intuitive way would be to use a for loop to create the case functions, and to use the @case decorator to set their names ; however this would not be very readable. Instead, case functions can be parametrized the same way as with test functions : simply add the parameter names as arguments in their signature and decorate with @pytest.mark.parametrize . Even better, you can use the enhanced @parametrize from pytest-cases so as to benefit from its additional usability features: from pytest_cases import parametrize , parametrize_with_cases class CasesFoo : def case_hello ( self ): return \"hello world\" @parametrize ( who = ( 'you' , 'there' )) def case_simple_generator ( self , who ): return \"hello %s \" % who @parametrize_with_cases ( \"msg\" , cases = CasesFoo ) def test_foo ( msg ): assert isinstance ( msg , str ) and msg . startswith ( \"hello\" ) Yields test_generators.py::test_foo[hello] PASSED [ 33%] test_generators.py::test_foo[simple_generator-who=you] PASSED [ 66%] test_generators.py::test_foo[simple_generator-who=there] PASSED [100%] Cases requiring fixtures \u2691 Cases can use fixtures the same way as test functions do : simply add the fixture names as arguments in their signature and make sure the fixture exists either in the same module, or in a conftest.py file in one of the parent packages. See pytest documentation on sharing fixtures . Use @fixture instead of @pytest.fixture If a fixture is used by some of your cases only, then you should use the @fixture decorator from pytest-cases instead of the standard @pytest.fixture . Otherwise you fixture will be setup/teardown for all cases even those not requiring it. See @fixture doc . from pytest_cases import parametrize_with_cases , fixture , parametrize @fixture ( scope = 'session' ) def db (): return { 0 : 'louise' , 1 : 'bob' } def user_bob ( db ): return db [ 1 ] @parametrize ( id = range ( 2 )) def user_from_db ( db , id ): return db [ id ] @parametrize_with_cases ( \"a\" , cases = '.' , prefix = 'user_' ) def test_users ( a , db , request ): print ( \"this is test %r \" % request . node . nodeid ) assert a in db . values () Yields test_fixtures.py::test_users[a_is_bob] test_fixtures.py::test_users[a_is_from_db-id=0] test_fixtures.py::test_users[a_is_from_db-id=1] Parametrize fixtures with cases \u2691 In some scenarios you might wish to parametrize a fixture with the cases, rather than the test function. For example: To inject the same test cases in several test functions without duplicating the @parametrize_with_cases decorator on each of them. To generate the test cases once for the whole session, using a scope='session' fixture or another scope . To modify the test cases, log some message, or perform some other action before injecting them into the test functions, and/or after executing the test function (thanks to yield fixtures ). For this, simply use @fixture from pytest_cases instead of @pytest.fixture to define your fixture. That allows your fixtures to be easily parametrized with @parametrize_with_cases , @parametrize , and even @pytest.mark.parametrize . from pytest_cases import fixture , parametrize_with_cases @fixture @parametrize_with_cases ( \"a,b\" ) def c ( a , b ): return a + b def test_foo ( c ): assert isinstance ( c , int ) Pytest-cases internals \u2691 @fixture \u2691 @fixture is similar to pytest.fixture but without its param and ids arguments. Instead, it is able to pick the parametrization from @pytest.mark.parametrize marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures. Finally it now supports unpacking, see unpacking feature . @fixture deprecation if/when @pytest.fixture supports @pytest.mark.parametrize The ability for pytest fixtures to support the @pytest.mark.parametrize annotation is a feature that clearly belongs to pytest scope, and has been requested already . It is therefore expected that @fixture will be deprecated in favor of @pytest_fixture if/when the pytest team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases. unpack_fixture / unpack_into \u2691 In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With unpack_fixture you can easily do it: import pytest from pytest_cases import unpack_fixture , fixture @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] a , b = unpack_fixture ( \"a,b\" , c ) def test_function ( a , b ): assert a [ 0 ] == b Note that you can also use the unpack_into= argument of @fixture to do the same thing: import pytest from pytest_cases import fixture @fixture ( unpack_into = \"a,b\" ) @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] def test_function ( a , b ): assert a [ 0 ] == b And it is also available in fixture_union : import pytest from pytest_cases import fixture , fixture_union @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] @fixture @pytest . mark . parametrize ( \"o\" , [ 'yeepee' , 'yay' ]) def d ( o ): return o , o [ 0 ] fixture_union ( \"c_or_d\" , [ c , d ], unpack_into = \"a, b\" ) def test_function ( a , b ): assert a [ 0 ] == b param_fixture[s] \u2691 If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples. The two utilities functions param_fixture (for a single parameter name) and param_fixtures (for a tuple of parameter names) handle the difficulty for you: import pytest from pytest_cases import param_fixtures , param_fixture # create a single parameter fixture my_parameter = param_fixture ( \"my_parameter\" , [ 1 , 2 , 3 , 4 ]) @pytest . fixture def fixture_uses_param ( my_parameter ): ... def test_uses_param ( my_parameter , fixture_uses_param ): ... # ----- # create a 2-tuple parameter fixture arg1 , arg2 = param_fixtures ( \"arg1, arg2\" , [( 1 , 2 ), ( 3 , 4 )]) @pytest . fixture def fixture_uses_param2 ( arg2 ): ... def test_uses_param2 ( arg1 , arg2 , fixture_uses_param2 ): ... fixture_union \u2691 As of pytest 5, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that would first take all the possible values of fixture A, then all possible values of fixture B, etc. Indeed all fixture dependencies of each test node are grouped together, and if they have parameters a big \"cross-product\" of the parameters is done by pytest . from pytest_cases import fixture , fixture_union @fixture def first (): return 'hello' @fixture ( params = [ 'a' , 'b' ]) def second ( request ): return request . param # c will first take all the values of 'first', then all of 'second' c = fixture_union ( 'c' , [ first , second ]) def test_basic_union ( c ): print ( c ) yields <...>::test_basic_union[c_is_first] hello PASSED <...>::test_basic_union[c_is_second-a] a PASSED <...>::test_basic_union[c_is_second-b] b PASSED References \u2691 Docs Git","title":"Pytest-cases"},{"location":"coding/python/pytest_cases/#installing","text":"pip install pytest_cases Installing pytest-cases has effects on the order of pytest tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054 . But if you see less desirable ordering please report it .","title":"Installing"},{"location":"coding/python/pytest_cases/#why-pytest-cases","text":"Let's consider the following foo function under test, located in example.py : def foo ( a , b ): return a + 1 , b + 1 If we were using plain pytest to test it with various inputs, we would create a test_foo.py file and use @pytest.mark.parametrize : import pytest from example import foo @pytest . mark . parametrize ( \"a,b\" , [( 1 , 2 ), ( - 1 , - 2 )]) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) This is the fastest and most compact thing to do when you have a few number of test cases, that do not require code to generate each test case. Now imagine that instead of (1, 2) and (-1, -2) each of our test cases: Requires a few lines of code to be generated. Requires documentation to explain the other developers the intent of that precise test case. Requires external resources (data files on the filesystem, databases...), with a variable number of cases depending on what is available on the resource. Requires a readable id , such as 'uniformly_sampled_nonsorted_with_holes' for the above example. Of course we could use pytest.param or ids=<list> but that is \"a pain to maintain\" according to pytest doc. Such a design does not feel right as the id is detached from the case. With standard pytest there is no particular pattern to simplify your life here. Investigating a little bit, people usually end up trying to mix parameters and fixtures and asking this kind of question: so1 , so2 . But by design it is not possible to solve this problem using fixtures, because pytest does not handle \"unions\" of fixtures . There is also an example in pytest doc with a metafunc hook . The issue with such workarounds is that you can do anything . And anything is a bit too much: this does not provide any convention / \"good practice\" on how to organize test cases, which is an open door to developing ad-hoc unreadable or unmaintainable solutions. pytest_cases was created to provide an answer to this precise situation. It proposes a simple framework to separate test cases from test functions. The test cases are typically located in a separate \"companion\" file: test_foo.py is your usual test file containing the test functions (named test_<id> ). test_foo_cases.py contains the test cases , that are also functions. Note: an alternate file naming style cases_foo.py is also available if you prefer it.","title":"Why pytest-cases?"},{"location":"coding/python/pytest_cases/#basic-usage","text":"","title":"Basic usage"},{"location":"coding/python/pytest_cases/#case-functions","text":"Let's create a test_foo_cases.py file. This file will contain test cases generator functions , that we will call case functions for brevity. In these functions, you will typically either parse some test data files, generate some simulated test data, expected results, etc. File: test_foo_cases.py def case_two_positive_ints (): \"\"\" Inputs are two positive integers \"\"\" return 1 , 2 def case_two_negative_ints (): \"\"\" Inputs are two negative integers \"\"\" return - 1 , - 2 Case functions can return anything that is considered useful to run the associated test. You can use all classic pytest mechanism on case functions (id customization, skip/fail marks, parametrization or fixtures injection).","title":"Case functions"},{"location":"coding/python/pytest_cases/#test-functions","text":"As usual we write our pytest test functions starting with test_ , in a test_foo.py file. The only difference is that we now decorate it with @parametrize_with_cases instead of @pytest.mark.parametrize as we were doing previously: File: test_foo.py from example import foo from pytest_cases import parametrize_with_cases @parametrize_with_cases ( \"a,b\" ) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) Executing pytest will now run our test function once for every case function: >>> pytest -s -v ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ two_positive_ints ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ two_negative_ints ] PASSED [ 100 % ] ========================== 2 passed in 0 .24 seconds ==========================","title":"Test functions"},{"location":"coding/python/pytest_cases/#usage","text":"","title":"Usage"},{"location":"coding/python/pytest_cases/#cases-collection","text":"","title":"Cases collection"},{"location":"coding/python/pytest_cases/#alternate-sources","text":"It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use cases='.' or cases=THIS_MODULE to refer to the module in which the test function is located: from pytest_cases import parametrize_with_cases def case_one_positive_int (): return 1 def case_one_negative_int (): return - 1 @parametrize_with_cases ( \"i\" , cases = '.' ) def test_with_this_module ( i ): assert i == int ( i ) Only the case functions defined BEFORE the test function in the module file will be taken into account. @parametrize_with_cases(cases=...) also accepts explicit list of case functions, classes containing case functions, and modules. See API Reference for details. A typical way to organize cases is to use classes for example: from pytest_cases import parametrize_with_cases class Foo : def case_a_positive_int ( self ): return 1 def case_another_positive_int ( self ): return 2 @parametrize_with_cases ( \"a\" , cases = Foo ) def test_foo ( a ): assert a > 0 Note that as for pytest , self is recreated for every test and therefore should not be used to store any useful information.","title":"Alternate source(s)"},{"location":"coding/python/pytest_cases/#alternate-prefix","text":"case_ might not be your preferred prefix, especially if you wish to store in the same module or class various kind of case data. @parametrize_with_cases offers a prefix=... argument to select an alternate prefix for your case functions. That way, you can store in the same module or class case functions as diverse as datasets (e.g. data_ ), user descriptions (e.g. user_ ), algorithms or machine learning models (e.g. model_ or algo_ ), etc. from pytest_cases import parametrize_with_cases , parametrize def data_a (): return 'a' @parametrize ( \"hello\" , [ True , False ]) def data_b ( hello ): return \"hello\" if hello else \"world\" def case_c (): return dict ( name = \"hi i'm not used\" ) def user_bob (): return \"bob\" @parametrize_with_cases ( \"data\" , cases = '.' , prefix = \"data_\" ) @parametrize_with_cases ( \"user\" , cases = '.' , prefix = \"user_\" ) def test_with_data ( data , user ): assert data in ( 'a' , \"hello\" , \"world\" ) assert user == 'bob' Yields test_doc_filters_n_tags.py::test_with_data[bob-a] PASSED [ 33%] test_doc_filters_n_tags.py::test_with_data[bob-b-True] PASSED [ 66%] test_doc_filters_n_tags.py::test_with_data[bob-b-False] PASSED [ 100%]","title":"Alternate prefix"},{"location":"coding/python/pytest_cases/#filters-and-tags","text":"The easiest way to select only a subset of case functions in a module or a class, is to specify a custom prefix instead of the default one ( 'case_' ). However sometimes more advanced filtering is required. In that case, you can also rely on three additional mechanisms provided in @parametrize_with_cases : The glob argument can contain a glob-like pattern for case ids. This can become handy to separate for example good or bad cases, the latter returning an expected error type and/or message for use with pytest.raises or with our alternative assert_exception . from math import sqrt import pytest from pytest_cases import parametrize_with_cases def case_int_success (): return 1 def case_negative_int_failure (): # note that we decide to return the expected type of failure to check it return - 1 , ValueError , \"math domain error\" @parametrize_with_cases ( \"data\" , cases = '.' , glob = \"*success\" ) def test_good_datasets ( data ): assert sqrt ( data ) > 0 @parametrize_with_cases ( \"data, err_type, err_msg\" , cases = '.' , glob = \"*failure\" ) def test_bad_datasets ( data , err_type , err_msg ): with pytest . raises ( err_type , match = err_msg ): sqrt ( data ) The has_tag argument allows you to filter cases based on tags set on case functions using the @case decorator. See API reference of @case and @parametrize_with_cases . from pytest_cases import parametrize_with_cases , case class FooCases : def case_two_positive_ints ( self ): return 1 , 2 @case ( tags = 'foo' ) def case_one_positive_int ( self ): return 1 @parametrize_with_cases ( \"a\" , cases = FooCases , has_tag = 'foo' ) def test_foo ( a ): assert a > 0 Finally if none of the above matches your expectations, you can provide a callable to filter . This callable will receive each collected case function and should return True in case of success. Note that your function can leverage the _pytestcase attribute available on the case function to read the tags, marks and id found on it. @parametrize_with_cases ( \"data\" , cases = '.' , filter = lambda cf : \"success\" in cf . _pytestcase . id ) def test_good_datasets2 ( data ): assert sqrt ( data ) > 0","title":"Filters and tags"},{"location":"coding/python/pytest_cases/#pytest-marks-skip-xfail-on-cases","text":"pytest marks such as @pytest.mark.skipif can be applied on case functions the same way as with test functions . import sys import pytest @pytest . mark . skipif ( sys . version_info < ( 3 , 0 ), reason = \"Not useful on python 2\" ) def case_two_positive_ints (): return 1 , 2","title":"Pytest marks (skip, xfail...) on cases"},{"location":"coding/python/pytest_cases/#case-generators","text":"In many real-world usage we want to generate one test case per <something> . The most intuitive way would be to use a for loop to create the case functions, and to use the @case decorator to set their names ; however this would not be very readable. Instead, case functions can be parametrized the same way as with test functions : simply add the parameter names as arguments in their signature and decorate with @pytest.mark.parametrize . Even better, you can use the enhanced @parametrize from pytest-cases so as to benefit from its additional usability features: from pytest_cases import parametrize , parametrize_with_cases class CasesFoo : def case_hello ( self ): return \"hello world\" @parametrize ( who = ( 'you' , 'there' )) def case_simple_generator ( self , who ): return \"hello %s \" % who @parametrize_with_cases ( \"msg\" , cases = CasesFoo ) def test_foo ( msg ): assert isinstance ( msg , str ) and msg . startswith ( \"hello\" ) Yields test_generators.py::test_foo[hello] PASSED [ 33%] test_generators.py::test_foo[simple_generator-who=you] PASSED [ 66%] test_generators.py::test_foo[simple_generator-who=there] PASSED [100%]","title":"Case generators"},{"location":"coding/python/pytest_cases/#cases-requiring-fixtures","text":"Cases can use fixtures the same way as test functions do : simply add the fixture names as arguments in their signature and make sure the fixture exists either in the same module, or in a conftest.py file in one of the parent packages. See pytest documentation on sharing fixtures . Use @fixture instead of @pytest.fixture If a fixture is used by some of your cases only, then you should use the @fixture decorator from pytest-cases instead of the standard @pytest.fixture . Otherwise you fixture will be setup/teardown for all cases even those not requiring it. See @fixture doc . from pytest_cases import parametrize_with_cases , fixture , parametrize @fixture ( scope = 'session' ) def db (): return { 0 : 'louise' , 1 : 'bob' } def user_bob ( db ): return db [ 1 ] @parametrize ( id = range ( 2 )) def user_from_db ( db , id ): return db [ id ] @parametrize_with_cases ( \"a\" , cases = '.' , prefix = 'user_' ) def test_users ( a , db , request ): print ( \"this is test %r \" % request . node . nodeid ) assert a in db . values () Yields test_fixtures.py::test_users[a_is_bob] test_fixtures.py::test_users[a_is_from_db-id=0] test_fixtures.py::test_users[a_is_from_db-id=1]","title":"Cases requiring fixtures"},{"location":"coding/python/pytest_cases/#parametrize-fixtures-with-cases","text":"In some scenarios you might wish to parametrize a fixture with the cases, rather than the test function. For example: To inject the same test cases in several test functions without duplicating the @parametrize_with_cases decorator on each of them. To generate the test cases once for the whole session, using a scope='session' fixture or another scope . To modify the test cases, log some message, or perform some other action before injecting them into the test functions, and/or after executing the test function (thanks to yield fixtures ). For this, simply use @fixture from pytest_cases instead of @pytest.fixture to define your fixture. That allows your fixtures to be easily parametrized with @parametrize_with_cases , @parametrize , and even @pytest.mark.parametrize . from pytest_cases import fixture , parametrize_with_cases @fixture @parametrize_with_cases ( \"a,b\" ) def c ( a , b ): return a + b def test_foo ( c ): assert isinstance ( c , int )","title":"Parametrize fixtures with cases"},{"location":"coding/python/pytest_cases/#pytest-cases-internals","text":"","title":"Pytest-cases internals"},{"location":"coding/python/pytest_cases/#fixture","text":"@fixture is similar to pytest.fixture but without its param and ids arguments. Instead, it is able to pick the parametrization from @pytest.mark.parametrize marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures. Finally it now supports unpacking, see unpacking feature . @fixture deprecation if/when @pytest.fixture supports @pytest.mark.parametrize The ability for pytest fixtures to support the @pytest.mark.parametrize annotation is a feature that clearly belongs to pytest scope, and has been requested already . It is therefore expected that @fixture will be deprecated in favor of @pytest_fixture if/when the pytest team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases.","title":"@fixture"},{"location":"coding/python/pytest_cases/#unpack_fixture-unpack_into","text":"In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With unpack_fixture you can easily do it: import pytest from pytest_cases import unpack_fixture , fixture @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] a , b = unpack_fixture ( \"a,b\" , c ) def test_function ( a , b ): assert a [ 0 ] == b Note that you can also use the unpack_into= argument of @fixture to do the same thing: import pytest from pytest_cases import fixture @fixture ( unpack_into = \"a,b\" ) @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] def test_function ( a , b ): assert a [ 0 ] == b And it is also available in fixture_union : import pytest from pytest_cases import fixture , fixture_union @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] @fixture @pytest . mark . parametrize ( \"o\" , [ 'yeepee' , 'yay' ]) def d ( o ): return o , o [ 0 ] fixture_union ( \"c_or_d\" , [ c , d ], unpack_into = \"a, b\" ) def test_function ( a , b ): assert a [ 0 ] == b","title":"unpack_fixture / unpack_into"},{"location":"coding/python/pytest_cases/#param_fixtures","text":"If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples. The two utilities functions param_fixture (for a single parameter name) and param_fixtures (for a tuple of parameter names) handle the difficulty for you: import pytest from pytest_cases import param_fixtures , param_fixture # create a single parameter fixture my_parameter = param_fixture ( \"my_parameter\" , [ 1 , 2 , 3 , 4 ]) @pytest . fixture def fixture_uses_param ( my_parameter ): ... def test_uses_param ( my_parameter , fixture_uses_param ): ... # ----- # create a 2-tuple parameter fixture arg1 , arg2 = param_fixtures ( \"arg1, arg2\" , [( 1 , 2 ), ( 3 , 4 )]) @pytest . fixture def fixture_uses_param2 ( arg2 ): ... def test_uses_param2 ( arg1 , arg2 , fixture_uses_param2 ): ...","title":"param_fixture[s]"},{"location":"coding/python/pytest_cases/#fixture_union","text":"As of pytest 5, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that would first take all the possible values of fixture A, then all possible values of fixture B, etc. Indeed all fixture dependencies of each test node are grouped together, and if they have parameters a big \"cross-product\" of the parameters is done by pytest . from pytest_cases import fixture , fixture_union @fixture def first (): return 'hello' @fixture ( params = [ 'a' , 'b' ]) def second ( request ): return request . param # c will first take all the values of 'first', then all of 'second' c = fixture_union ( 'c' , [ first , second ]) def test_basic_union ( c ): print ( c ) yields <...>::test_basic_union[c_is_first] hello PASSED <...>::test_basic_union[c_is_second-a] a PASSED <...>::test_basic_union[c_is_second-b] b PASSED","title":"fixture_union"},{"location":"coding/python/pytest_cases/#references","text":"Docs Git","title":"References"},{"location":"coding/python/pytest_parametrized_testing/","text":"Parametrization is a process of running the same test with varying sets of data. Each combination of a test and data is counted as a new test case. There are multiple ways to parametrize your tests, each differs in complexity and flexibility. Parametrize the test \u2691 The most simple form of parametrization is at test level: @pytest . mark . parametrize ( \"number\" , [ 1 , 2 , 3 , 0 , 42 ]) def test_foo ( number ): assert number > 0 In this case we are getting five tests: for number 1, 2, 3, 0 and 42. Each of those tests can fail independently of one another (if in this example the test with 0 will fail, and four others will pass). Parametrize the fixtures \u2691 Fixtures may have parameters. Those parameters are passed as a list to the argument params of @pytest.fixture() decorator. Those parameters must be iterables, such as lists. Each parameter to a fixture is applied to each function using this fixture. If a few fixtures are used in one test function, pytest generates a Cartesian product of parameters of those fixtures. To use those parameters, a fixture must consume a special fixture named request . It provides the special (built-in) fixture with some information on the function it deals with. request also contains request.param which contains one element from params . The fixture called as many times as the number of elements in the iterable of params argument, and the test function is called with values of fixtures the same number of times. (basically, the fixture is called len(iterable) times with each next element of iterable in the request.param ). @pytest . fixture ( params = [ \"one\" , \"uno\" ]) def fixture1 ( request ): return request . param @pytest . fixture ( params = [ \"two\" , \"duo\" ]) def fixture2 ( request ): return request . paramdef test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) The output is: #OUTPUT 3 collected 4 itemstest_3.py::test_foobar[one-two] PASSED [ 25%] test_3.py::test_foobar[one-duo] PASSED [ 50%] test_3.py::test_foobar[uno-two] PASSED [ 75%] test_3.py::test_foobar[uno-duo] PASSED [100%] Parametrization with pytest_generate_tests \u2691 There is an another way to generate arbitrary parametrization at collection time. It\u2019s a bit more direct and verbose, but it provides introspection of test functions, including the ability to see all other fixture names. At collection time Pytest looks up for and calls (if found) a special function in each module, named pytest_generate_tests . This function is not a fixture, but just a regular function. It receives the argument metafunc , which itself is not a fixture, but a special object. pytest_generate_tests is called for each test function in the module to give a chance to parametrize it. Parametrization may happen only through fixtures that test function requests. There is no way to parametrize a test function like this: def test_simple(): assert 2+2 == 4 You need some variables to be used as parameters, and those variables should be arguments to the test function. Pytest will replace those arguments with values from fixtures, and if there are a few values for a fixture, then this is parametrization at work. metafunc argument to pytest_generate_tests provides some useful information on a test function: Ability to see all fixture names that function requests. Ability to see the name of the function. Ability to see code of the function. Finally, metafunc has a parametrize function, which is the way to provide multiple variants of values for fixtures. The same case as before written with the pytest_generate_tests function is: def pytest_generate_tests ( metafunc ): if \"fixture1\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture1\" , [ \"one\" , \"uno\" ]) if \"fixture2\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture2\" , [ \"two\" , \"duo\" ]) def test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) This solution is a little bit magical, so I'd avoid it in favor of pytest-cases. Use pytest-cases \u2691 pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information. Customizations \u2691 Change the tests name \u2691 Sometimes you want to change how the tests are shown so you can understand better what the test is doing. You can use the ids argument to pytest.mark.parametrize . File tests/unit/test_func.py tasks_to_try = ( Task ( 'sleep' , done = True ), Task ( 'wake' , 'brian' ), Task ( 'wake' , 'brian' ), Task ( 'breathe' , 'BRIAN' , True ), Task ( 'exercise' , 'BrIaN' , False ), ) task_ids = [ f 'Task( { task . summary } , { task . owner } , { task . done } )' for task in tasks_to_try ] @pytest . mark . parametrize ( 'task' , tasks_to_try , ids = task_ids ) def test_add_4 ( task ): task_id = tasks . add ( task ) t_from_db = tasks . get ( task_id ) assert equivalent ( t_from_db , task ) $ pytest -v test_func.py::test_add_4 ===================== test session starts ====================== collected 5 items test_add_variety.py::test_add_4 [ Task ( sleep,None,True )] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 0 ] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 1 ] PASSED test_add_variety.py::test_add_4 [ Task ( breathe,BRIAN,True )] PASSED test_add_variety.py::test_add_4 [ Task ( exercise,BrIaN,False )] PASSED =================== 5 passed in 0 .04 seconds =================== Those identifiers can be used to run that specific test. For example pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\" . parametrize() can be applied to classes as well. If the test id can't be derived from the parameter value, use the id argument for the pytest.param : @pytest . mark . parametrize ( 'task' , [ pytest . param ( Task ( 'create' ), id = 'just summary' ), pytest . param ( Task ( 'inspire' , 'Michelle' ), id = 'summary/owner' ), ]) def test_add_6 ( task ): ... Will yield: $ pytest-v test_add_variety.py::test_add_6 =================== test session starts ==================== collected 2 items test_add_variety.py::test_add_6 [ justsummary ] PASSED test_add_variety.py::test_add_6 [ summary/owner ] PASSED ================= 2 passed in 0 .05 seconds ================= References \u2691 A deep dive into Pytest parametrization by George Shulkin Book Python Testing with pytest by Brian Okken .","title":"Parametrized testing"},{"location":"coding/python/pytest_parametrized_testing/#parametrize-the-test","text":"The most simple form of parametrization is at test level: @pytest . mark . parametrize ( \"number\" , [ 1 , 2 , 3 , 0 , 42 ]) def test_foo ( number ): assert number > 0 In this case we are getting five tests: for number 1, 2, 3, 0 and 42. Each of those tests can fail independently of one another (if in this example the test with 0 will fail, and four others will pass).","title":"Parametrize the test"},{"location":"coding/python/pytest_parametrized_testing/#parametrize-the-fixtures","text":"Fixtures may have parameters. Those parameters are passed as a list to the argument params of @pytest.fixture() decorator. Those parameters must be iterables, such as lists. Each parameter to a fixture is applied to each function using this fixture. If a few fixtures are used in one test function, pytest generates a Cartesian product of parameters of those fixtures. To use those parameters, a fixture must consume a special fixture named request . It provides the special (built-in) fixture with some information on the function it deals with. request also contains request.param which contains one element from params . The fixture called as many times as the number of elements in the iterable of params argument, and the test function is called with values of fixtures the same number of times. (basically, the fixture is called len(iterable) times with each next element of iterable in the request.param ). @pytest . fixture ( params = [ \"one\" , \"uno\" ]) def fixture1 ( request ): return request . param @pytest . fixture ( params = [ \"two\" , \"duo\" ]) def fixture2 ( request ): return request . paramdef test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) The output is: #OUTPUT 3 collected 4 itemstest_3.py::test_foobar[one-two] PASSED [ 25%] test_3.py::test_foobar[one-duo] PASSED [ 50%] test_3.py::test_foobar[uno-two] PASSED [ 75%] test_3.py::test_foobar[uno-duo] PASSED [100%]","title":"Parametrize the fixtures"},{"location":"coding/python/pytest_parametrized_testing/#parametrization-with-pytest_generate_tests","text":"There is an another way to generate arbitrary parametrization at collection time. It\u2019s a bit more direct and verbose, but it provides introspection of test functions, including the ability to see all other fixture names. At collection time Pytest looks up for and calls (if found) a special function in each module, named pytest_generate_tests . This function is not a fixture, but just a regular function. It receives the argument metafunc , which itself is not a fixture, but a special object. pytest_generate_tests is called for each test function in the module to give a chance to parametrize it. Parametrization may happen only through fixtures that test function requests. There is no way to parametrize a test function like this: def test_simple(): assert 2+2 == 4 You need some variables to be used as parameters, and those variables should be arguments to the test function. Pytest will replace those arguments with values from fixtures, and if there are a few values for a fixture, then this is parametrization at work. metafunc argument to pytest_generate_tests provides some useful information on a test function: Ability to see all fixture names that function requests. Ability to see the name of the function. Ability to see code of the function. Finally, metafunc has a parametrize function, which is the way to provide multiple variants of values for fixtures. The same case as before written with the pytest_generate_tests function is: def pytest_generate_tests ( metafunc ): if \"fixture1\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture1\" , [ \"one\" , \"uno\" ]) if \"fixture2\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture2\" , [ \"two\" , \"duo\" ]) def test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) This solution is a little bit magical, so I'd avoid it in favor of pytest-cases.","title":"Parametrization with pytest_generate_tests"},{"location":"coding/python/pytest_parametrized_testing/#use-pytest-cases","text":"pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information.","title":"Use pytest-cases"},{"location":"coding/python/pytest_parametrized_testing/#customizations","text":"","title":"Customizations"},{"location":"coding/python/pytest_parametrized_testing/#change-the-tests-name","text":"Sometimes you want to change how the tests are shown so you can understand better what the test is doing. You can use the ids argument to pytest.mark.parametrize . File tests/unit/test_func.py tasks_to_try = ( Task ( 'sleep' , done = True ), Task ( 'wake' , 'brian' ), Task ( 'wake' , 'brian' ), Task ( 'breathe' , 'BRIAN' , True ), Task ( 'exercise' , 'BrIaN' , False ), ) task_ids = [ f 'Task( { task . summary } , { task . owner } , { task . done } )' for task in tasks_to_try ] @pytest . mark . parametrize ( 'task' , tasks_to_try , ids = task_ids ) def test_add_4 ( task ): task_id = tasks . add ( task ) t_from_db = tasks . get ( task_id ) assert equivalent ( t_from_db , task ) $ pytest -v test_func.py::test_add_4 ===================== test session starts ====================== collected 5 items test_add_variety.py::test_add_4 [ Task ( sleep,None,True )] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 0 ] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 1 ] PASSED test_add_variety.py::test_add_4 [ Task ( breathe,BRIAN,True )] PASSED test_add_variety.py::test_add_4 [ Task ( exercise,BrIaN,False )] PASSED =================== 5 passed in 0 .04 seconds =================== Those identifiers can be used to run that specific test. For example pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\" . parametrize() can be applied to classes as well. If the test id can't be derived from the parameter value, use the id argument for the pytest.param : @pytest . mark . parametrize ( 'task' , [ pytest . param ( Task ( 'create' ), id = 'just summary' ), pytest . param ( Task ( 'inspire' , 'Michelle' ), id = 'summary/owner' ), ]) def test_add_6 ( task ): ... Will yield: $ pytest-v test_add_variety.py::test_add_6 =================== test session starts ==================== collected 2 items test_add_variety.py::test_add_6 [ justsummary ] PASSED test_add_variety.py::test_add_6 [ summary/owner ] PASSED ================= 2 passed in 0 .05 seconds =================","title":"Change the tests name"},{"location":"coding/python/pytest_parametrized_testing/#references","text":"A deep dive into Pytest parametrization by George Shulkin Book Python Testing with pytest by Brian Okken .","title":"References"},{"location":"coding/python/python_anti_patterns/","text":"Mutable default arguments \u2691 What You Wrote \u2691 def append_to ( element , to = []): to . append ( element ) return to What You Might Have Expected to Happen \u2691 my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is: [ 12 ] [ 42 ] What Does Happen [ 12 ] [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well. What You Should Do Instead \u2691 Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice). def append_to ( element , to = None ): if to is None : to = [] to . append ( element ) return to Do not forget, you are passing a list object as the second argument. When the Gotcha Isn\u2019t a Gotcha \u2691 Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.","title":"Anti-Patterns"},{"location":"coding/python/python_anti_patterns/#mutable-default-arguments","text":"","title":"Mutable default arguments"},{"location":"coding/python/python_anti_patterns/#what-you-wrote","text":"def append_to ( element , to = []): to . append ( element ) return to","title":"What You Wrote"},{"location":"coding/python/python_anti_patterns/#what-you-might-have-expected-to-happen","text":"my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is: [ 12 ] [ 42 ] What Does Happen [ 12 ] [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.","title":"What You Might Have Expected to Happen"},{"location":"coding/python/python_anti_patterns/#what-you-should-do-instead","text":"Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice). def append_to ( element , to = None ): if to is None : to = [] to . append ( element ) return to Do not forget, you are passing a list object as the second argument.","title":"What You Should Do Instead"},{"location":"coding/python/python_anti_patterns/#when-the-gotcha-isnt-a-gotcha","text":"Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.","title":"When the Gotcha Isn\u2019t a Gotcha"},{"location":"coding/python/python_code_styling/","text":"Commit message guidelines \u2691 I'm following the Angular commit convention that is backed up by python-semantic-release , with the idea of implementing automatic semantic versioning sometime in the future. Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject: <type>(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <footer> The header is mandatory and the scope of the header is optional. Any line of the commit message cannot be longer 100 characters. The footer should contain a closing reference to an issue if any. Samples: (even more samples) docs(changelog): update changelog to beta.5 fix(release): need to depend on latest rxjs and zone.js The version in our package.json gets copied to the one we publish, and users need the latest of these. docs(router): fix typo 'containa' to 'contains' (#36764) Closes #36763 PR Close #36764 Revert \u2691 If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit. In the body it should say: This reverts commit <hash>. , where the hash is the SHA of the commit to revert. Type \u2691 Must be one of the following: build : Changes that affect the build system or external dependencies. ci : Changes to our CI configuration files and scripts. docs : Documentation changes. feat : A new feature. fix : A bug fix. perf : A code change that improves performance. refactor : A code change that neither fixes a bug nor adds a feature. style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc). test : Adding missing tests or correcting existing tests. Subject \u2691 The subject contains a succinct description of the change: Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". Don't capitalize the first letter. No dot (.) at the end. Body \u2691 Same as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior. Footer \u2691 The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this. Pre-commit \u2691 To ensure that your project follows these guidelines, add the following to your pre-commit configuration : File: .pre-commit-config.yaml - repo : https://github.com/commitizen-tools/commitizen rev : master hooks : - id : commitizen stages : [ commit-msg ] To make your life easier, change your workflow to use commitizen . In Vim, if you're using Vim fugitive change the configuration to: nnoremap <leader>gc :terminal cz c<CR> nnoremap <leader>gr :terminal cz c --retry<CR> \" Open terminal mode in insert mode if has('nvim') autocmd TermOpen term://* startinsert endif autocmd BufLeave term://* stopinsert If some pre-commit hook fails, make the changes and then use <leader>gr to repeat the same commit message. To automatically generate the changelog use cz bump --changelog --no-verify . The --no-verify part is required if you use pre-commit hooks . Whenever you want to release 1.0.0 , use cz bump --changelog --no-verify --increment MAJOR . Black code style \u2691 Black is a style guide enforcement tool. Flake8 \u2691 Flake8 is another style guide enforcement tool. f-strings \u2691 f-strings , also known as formatted string literals , are strings that have an f at the beginning and curly braces containing expressions that will be replaced with their values. Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster. >>> name = \"Eric\" >>> age = 74 >>> f \"Hello, { name } . You are { age } .\" 'Hello, Eric. You are 74.' Arbitrary expressions \u2691 Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within. >>> f \" { name . lower () } is funny.\" 'eric idle is funny.' Multiline f-strings \u2691 >>> name = \"Eric\" >>> profession = \"comedian\" >>> affiliation = \"Monty Python\" >>> message = ( ... f \"Hi { name } . \" ... f \"You are a { profession } . \" ... f \"You were in { affiliation } .\" ... ) >>> message 'Hi Eric. You are a comedian. You were in Monty Python.' Lint error fixes and ignores \u2691 Fix Pylint R0201 error \u2691 The error shows Method could be a function , it is used when there is no reference to the class, suggesting that the method could be used as a static function instead. Attempt using either of the decorators @classmethod or @staticmethod . If you don't need to change or use the class methods, use staticmethod . Example: Class Foo ( object ): ... def bar ( self , baz ): ... return llama Try instead to use: Class Foo ( object ): ... @classmethod def bar ( cls , baz ): ... return llama Or Class Foo ( object ): ... @staticmethod def bar ( baz ): ... return llama W1203 with F-strings \u2691 This rule suggest you to use the % interpolation in the logging methods because it might save some interpolation time when a logging statement is not run. Nevertheless the performance improvement is negligible and the advantages of using f-strings far outweigh them. W0106 in list comprehension \u2691 They just don't support it they suggest to use normal for loops. [SIM105 Use \u2691 'contextlib.suppress(Exception)']( https://docs.python.org/3/library/contextlib.html#contextlib.suppress ) It's better to use from contextlib import suppress with suppress ( FileNotFoundError ): os . remove ( 'somefile.tmp' ) Instead of: try : os . remove ( 'somefile.tmp' ) except FileNotFoundError : pass","title":"Code Styling"},{"location":"coding/python/python_code_styling/#commit-message-guidelines","text":"I'm following the Angular commit convention that is backed up by python-semantic-release , with the idea of implementing automatic semantic versioning sometime in the future. Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject: <type>(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <footer> The header is mandatory and the scope of the header is optional. Any line of the commit message cannot be longer 100 characters. The footer should contain a closing reference to an issue if any. Samples: (even more samples) docs(changelog): update changelog to beta.5 fix(release): need to depend on latest rxjs and zone.js The version in our package.json gets copied to the one we publish, and users need the latest of these. docs(router): fix typo 'containa' to 'contains' (#36764) Closes #36763 PR Close #36764","title":"Commit message guidelines"},{"location":"coding/python/python_code_styling/#revert","text":"If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit. In the body it should say: This reverts commit <hash>. , where the hash is the SHA of the commit to revert.","title":"Revert"},{"location":"coding/python/python_code_styling/#type","text":"Must be one of the following: build : Changes that affect the build system or external dependencies. ci : Changes to our CI configuration files and scripts. docs : Documentation changes. feat : A new feature. fix : A bug fix. perf : A code change that improves performance. refactor : A code change that neither fixes a bug nor adds a feature. style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc). test : Adding missing tests or correcting existing tests.","title":"Type"},{"location":"coding/python/python_code_styling/#subject","text":"The subject contains a succinct description of the change: Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". Don't capitalize the first letter. No dot (.) at the end.","title":"Subject"},{"location":"coding/python/python_code_styling/#body","text":"Same as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior.","title":"Body"},{"location":"coding/python/python_code_styling/#footer","text":"The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this.","title":"Footer"},{"location":"coding/python/python_code_styling/#pre-commit","text":"To ensure that your project follows these guidelines, add the following to your pre-commit configuration : File: .pre-commit-config.yaml - repo : https://github.com/commitizen-tools/commitizen rev : master hooks : - id : commitizen stages : [ commit-msg ] To make your life easier, change your workflow to use commitizen . In Vim, if you're using Vim fugitive change the configuration to: nnoremap <leader>gc :terminal cz c<CR> nnoremap <leader>gr :terminal cz c --retry<CR> \" Open terminal mode in insert mode if has('nvim') autocmd TermOpen term://* startinsert endif autocmd BufLeave term://* stopinsert If some pre-commit hook fails, make the changes and then use <leader>gr to repeat the same commit message. To automatically generate the changelog use cz bump --changelog --no-verify . The --no-verify part is required if you use pre-commit hooks . Whenever you want to release 1.0.0 , use cz bump --changelog --no-verify --increment MAJOR .","title":"Pre-commit"},{"location":"coding/python/python_code_styling/#black-code-style","text":"Black is a style guide enforcement tool.","title":"Black code style"},{"location":"coding/python/python_code_styling/#flake8","text":"Flake8 is another style guide enforcement tool.","title":"Flake8"},{"location":"coding/python/python_code_styling/#f-strings","text":"f-strings , also known as formatted string literals , are strings that have an f at the beginning and curly braces containing expressions that will be replaced with their values. Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster. >>> name = \"Eric\" >>> age = 74 >>> f \"Hello, { name } . You are { age } .\" 'Hello, Eric. You are 74.'","title":"f-strings"},{"location":"coding/python/python_code_styling/#arbitrary-expressions","text":"Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within. >>> f \" { name . lower () } is funny.\" 'eric idle is funny.'","title":"Arbitrary expressions"},{"location":"coding/python/python_code_styling/#multiline-f-strings","text":">>> name = \"Eric\" >>> profession = \"comedian\" >>> affiliation = \"Monty Python\" >>> message = ( ... f \"Hi { name } . \" ... f \"You are a { profession } . \" ... f \"You were in { affiliation } .\" ... ) >>> message 'Hi Eric. You are a comedian. You were in Monty Python.'","title":"Multiline f-strings"},{"location":"coding/python/python_code_styling/#lint-error-fixes-and-ignores","text":"","title":"Lint error fixes and ignores"},{"location":"coding/python/python_code_styling/#fix-pylint-r0201-error","text":"The error shows Method could be a function , it is used when there is no reference to the class, suggesting that the method could be used as a static function instead. Attempt using either of the decorators @classmethod or @staticmethod . If you don't need to change or use the class methods, use staticmethod . Example: Class Foo ( object ): ... def bar ( self , baz ): ... return llama Try instead to use: Class Foo ( object ): ... @classmethod def bar ( cls , baz ): ... return llama Or Class Foo ( object ): ... @staticmethod def bar ( baz ): ... return llama","title":"Fix Pylint R0201 error"},{"location":"coding/python/python_code_styling/#w1203-with-f-strings","text":"This rule suggest you to use the % interpolation in the logging methods because it might save some interpolation time when a logging statement is not run. Nevertheless the performance improvement is negligible and the advantages of using f-strings far outweigh them.","title":"W1203 with F-strings"},{"location":"coding/python/python_code_styling/#w0106-in-list-comprehension","text":"They just don't support it they suggest to use normal for loops.","title":"W0106 in list comprehension"},{"location":"coding/python/python_code_styling/#sim105-use","text":"'contextlib.suppress(Exception)']( https://docs.python.org/3/library/contextlib.html#contextlib.suppress ) It's better to use from contextlib import suppress with suppress ( FileNotFoundError ): os . remove ( 'somefile.tmp' ) Instead of: try : os . remove ( 'somefile.tmp' ) except FileNotFoundError : pass","title":"[SIM105 Use"},{"location":"coding/python/python_config_yaml/","text":"Several programs load the configuration from file. After trying ini, json and yaml I've seen that the last one is the most comfortable. So here are the templates for the tests and class that loads the data from a yaml file and exposes it as a dictionary. In the following sections Jinja templating is used, so substitute everything between {{ }} to their correct values. It's assumed that: The root directory of the project has the same name as the program. A file with a valid config exists in assets/config.yaml . We'll use this file in the documentation, so comment it through. Code \u2691 The class code below is expected to introduced in the file configuration.py . Variables to substitute: program_name File {{ program_name }}/configuration.py \"\"\" Module to define the configuration of the main program. Classes: Config: Class to manipulate the configuration of the program. \"\"\" from collections import UserDict from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError import logging import os import sys log = logging . getLogger ( __name__ ) class Config ( UserDict ): \"\"\" Class to manipulate the configuration of the program. Arguments: config_path (str): Path to the configuration file. Default: ~/.local/share/{{ program_name }}/config.yaml Public methods: get: Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. load: Loads configuration from configuration YAML file. save: Saves configuration in the configuration YAML file. Attributes and properties: config_path (str): Path to the configuration file. data(dict): Program configuration. \"\"\" def __init__ ( self , config_path = '~/.local/share/{{ program_name }}/config.yaml' ): self . config_path = os . path . expanduser ( config_path ) self . load () def get ( self , key ): \"\"\" Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. So if the configuration contents are: self.data = { 'first': { 'second': 'value' }, } self.data.get('first.second') == 'value' Arguments: key(str): Configuration key to fetch \"\"\" keys = key . split ( '.' ) value = self . data . copy () for key in keys : value = value [ key ] return value def load ( self ): \"\"\" Loads configuration from configuration YAML file. \"\"\" try : with open ( os . path . expanduser ( self . config_path ), 'r' ) as f : try : self . data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( self . config_path ) ) sys . exit ( 1 ) def save ( self ): \"\"\" Saves configuration in the configuration YAML file. \"\"\" with open ( os . path . expanduser ( self . config_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( self . data , f ) We use ruamel PyYAML implementation to preserve the file comments. That class is meant to be loaded in the main __init__.py file, below the logging configuration (if there is any). Variables to substitute: program_name config_environmental_variable : The optional environmental variable where the path to the configuration is set. For example PYDO_CONFIG . This will be used in the tests to override the default path. We don't load this configuration from the program argument parser because it's definition often depends on the config file. File {{ program_name}}/ init .py # ... # (optional logging definition) import os from {{ program_name }} . configuration import Config config = Config ( os . getenv ( '{{ config_environmental_variable }}' , '~/.local/share/{{ program_name }}/config.yaml' )) # (Rest of the program) # ... If you want to use the config object in a module of your program, import them from the above file like this: File {{ program_name }}/cli.py from {{ program_name }} import config Tests \u2691 I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the conftest.py file that gets executed by pytest in the tests setup. Variables to substitute: config_environmental_variable : Same as the one defined in the last section. File tests/conftest.py import os os . environ [{{ config_environmental_variable }}] = 'assets/config.yaml' Variables to substitute: program_name As it's really dependent in the config structure, you can improve the test_config_load test to make it more meaningful. File tests/unit/test_configuration.py from {{ program_name }} . configuration import Config from unittest.mock import patch from ruamel.yaml.scanner import ScannerError import os import pytest import shutil import tempfile class TestConfig : \"\"\" Class to test the Config object. Public attributes: config (Config object): Config object to test \"\"\" @pytest . fixture ( autouse = True ) def setup ( self ): self . config_path = 'assets/config.yaml' self . log_patch = patch ( '{{ program_name }}.configuration.log' , autospect = True ) self . log = self . log_patch . start () self . sys_patch = patch ( '{{ program_name }}.configuration.sys' , autospect = True ) self . sys = self . sys_patch . start () self . config = Config ( self . config_path ) yield 'setup' self . log_patch . stop () self . sys_patch . stop () def test_get_can_fetch_nested_items_with_dots ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config . get ( 'first.second' ) == 'value' def test_config_can_fetch_nested_items_with_dictionary_notation ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config [ 'first' ][ 'second' ] == 'value' def test_config_load ( self ): self . config . load () assert len ( self . config . data ) > 0 @patch ( '{{ program_name }}.configuration.YAML' ) def test_load_handles_wrong_file_format ( self , yamlMock ): yamlMock . return_value . load . side_effect = ScannerError ( 'error' , '' , 'problem' , 'mark' , ) self . config . load () self . log . error . assert_called_once_with ( 'Error parsing yaml of configuration file mark: problem' ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.open' ) def test_load_handles_file_not_found ( self , openMock ): openMock . side_effect = FileNotFoundError () self . config . load () self . log . error . assert_called_once_with ( 'Error opening configuration file {} ' . format ( self . config_path ) ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.Config.load' ) def test_init_calls_config_load ( self , loadMock ): Config () loadMock . assert_called_once_with () def test_save_config ( self ): tmp = tempfile . mkdtemp () save_file = os . path . join ( tmp , 'yaml_save_test.yaml' ) self . config = Config ( save_file ) self . config . data = { 'a' : 'b' } self . config . save () with open ( save_file , 'r' ) as f : assert \"a:\" in f . read () shutil . rmtree ( tmp ) Installation \u2691 It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file. Variables to substitute in both files: program_name File setup.py import shutil ... Class PostInstallCommand ( install ): ... def run ( self ): install . run ( self ) try : data_directory = os . path . expanduser ( \"~/.local/share/{{ program_name }}\" ) os . makedirs ( data_directory ) log . info ( \"Data directory created\" ) except FileExistsError : log . info ( \"Data directory already exits\" ) config_path = os . path . join ( data_directory , 'config.yaml' ) if os . path . isfile ( config_path ) and os . access ( config_path , os . R_OK ): log . info ( \"Configuration file already exists, check the documentation \" \"for the new version changes.\" ) else : shutil . copyfile ( 'assets/config.yaml' , config_path ) log . info ( \"Copied default configuration template\" ) README.md ... {{ program_name }} configuration is done through the yaml file located at ~/.local/share/{{ program_name }}/config.yaml . The default template is provided at installation time. ... It's also necessary to add the ruamel.yaml pip package to your setup.py and requirements.txt files.","title":"Load config from YAML"},{"location":"coding/python/python_config_yaml/#code","text":"The class code below is expected to introduced in the file configuration.py . Variables to substitute: program_name File {{ program_name }}/configuration.py \"\"\" Module to define the configuration of the main program. Classes: Config: Class to manipulate the configuration of the program. \"\"\" from collections import UserDict from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError import logging import os import sys log = logging . getLogger ( __name__ ) class Config ( UserDict ): \"\"\" Class to manipulate the configuration of the program. Arguments: config_path (str): Path to the configuration file. Default: ~/.local/share/{{ program_name }}/config.yaml Public methods: get: Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. load: Loads configuration from configuration YAML file. save: Saves configuration in the configuration YAML file. Attributes and properties: config_path (str): Path to the configuration file. data(dict): Program configuration. \"\"\" def __init__ ( self , config_path = '~/.local/share/{{ program_name }}/config.yaml' ): self . config_path = os . path . expanduser ( config_path ) self . load () def get ( self , key ): \"\"\" Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. So if the configuration contents are: self.data = { 'first': { 'second': 'value' }, } self.data.get('first.second') == 'value' Arguments: key(str): Configuration key to fetch \"\"\" keys = key . split ( '.' ) value = self . data . copy () for key in keys : value = value [ key ] return value def load ( self ): \"\"\" Loads configuration from configuration YAML file. \"\"\" try : with open ( os . path . expanduser ( self . config_path ), 'r' ) as f : try : self . data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( self . config_path ) ) sys . exit ( 1 ) def save ( self ): \"\"\" Saves configuration in the configuration YAML file. \"\"\" with open ( os . path . expanduser ( self . config_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( self . data , f ) We use ruamel PyYAML implementation to preserve the file comments. That class is meant to be loaded in the main __init__.py file, below the logging configuration (if there is any). Variables to substitute: program_name config_environmental_variable : The optional environmental variable where the path to the configuration is set. For example PYDO_CONFIG . This will be used in the tests to override the default path. We don't load this configuration from the program argument parser because it's definition often depends on the config file. File {{ program_name}}/ init .py # ... # (optional logging definition) import os from {{ program_name }} . configuration import Config config = Config ( os . getenv ( '{{ config_environmental_variable }}' , '~/.local/share/{{ program_name }}/config.yaml' )) # (Rest of the program) # ... If you want to use the config object in a module of your program, import them from the above file like this: File {{ program_name }}/cli.py from {{ program_name }} import config","title":"Code"},{"location":"coding/python/python_config_yaml/#tests","text":"I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the conftest.py file that gets executed by pytest in the tests setup. Variables to substitute: config_environmental_variable : Same as the one defined in the last section. File tests/conftest.py import os os . environ [{{ config_environmental_variable }}] = 'assets/config.yaml' Variables to substitute: program_name As it's really dependent in the config structure, you can improve the test_config_load test to make it more meaningful. File tests/unit/test_configuration.py from {{ program_name }} . configuration import Config from unittest.mock import patch from ruamel.yaml.scanner import ScannerError import os import pytest import shutil import tempfile class TestConfig : \"\"\" Class to test the Config object. Public attributes: config (Config object): Config object to test \"\"\" @pytest . fixture ( autouse = True ) def setup ( self ): self . config_path = 'assets/config.yaml' self . log_patch = patch ( '{{ program_name }}.configuration.log' , autospect = True ) self . log = self . log_patch . start () self . sys_patch = patch ( '{{ program_name }}.configuration.sys' , autospect = True ) self . sys = self . sys_patch . start () self . config = Config ( self . config_path ) yield 'setup' self . log_patch . stop () self . sys_patch . stop () def test_get_can_fetch_nested_items_with_dots ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config . get ( 'first.second' ) == 'value' def test_config_can_fetch_nested_items_with_dictionary_notation ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config [ 'first' ][ 'second' ] == 'value' def test_config_load ( self ): self . config . load () assert len ( self . config . data ) > 0 @patch ( '{{ program_name }}.configuration.YAML' ) def test_load_handles_wrong_file_format ( self , yamlMock ): yamlMock . return_value . load . side_effect = ScannerError ( 'error' , '' , 'problem' , 'mark' , ) self . config . load () self . log . error . assert_called_once_with ( 'Error parsing yaml of configuration file mark: problem' ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.open' ) def test_load_handles_file_not_found ( self , openMock ): openMock . side_effect = FileNotFoundError () self . config . load () self . log . error . assert_called_once_with ( 'Error opening configuration file {} ' . format ( self . config_path ) ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.Config.load' ) def test_init_calls_config_load ( self , loadMock ): Config () loadMock . assert_called_once_with () def test_save_config ( self ): tmp = tempfile . mkdtemp () save_file = os . path . join ( tmp , 'yaml_save_test.yaml' ) self . config = Config ( save_file ) self . config . data = { 'a' : 'b' } self . config . save () with open ( save_file , 'r' ) as f : assert \"a:\" in f . read () shutil . rmtree ( tmp )","title":"Tests"},{"location":"coding/python/python_config_yaml/#installation","text":"It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file. Variables to substitute in both files: program_name File setup.py import shutil ... Class PostInstallCommand ( install ): ... def run ( self ): install . run ( self ) try : data_directory = os . path . expanduser ( \"~/.local/share/{{ program_name }}\" ) os . makedirs ( data_directory ) log . info ( \"Data directory created\" ) except FileExistsError : log . info ( \"Data directory already exits\" ) config_path = os . path . join ( data_directory , 'config.yaml' ) if os . path . isfile ( config_path ) and os . access ( config_path , os . R_OK ): log . info ( \"Configuration file already exists, check the documentation \" \"for the new version changes.\" ) else : shutil . copyfile ( 'assets/config.yaml' , config_path ) log . info ( \"Copied default configuration template\" ) README.md ... {{ program_name }} configuration is done through the yaml file located at ~/.local/share/{{ program_name }}/config.yaml . The default template is provided at installation time. ... It's also necessary to add the ruamel.yaml pip package to your setup.py and requirements.txt files.","title":"Installation"},{"location":"coding/python/python_project_template/","text":"It's hard to correctly define the directory structure to make python programs work as expected. Even more if testing, documentation or databases are involved. I've automated the creation of the python project skeleton following most of these section guidelines with this cookiecutter template . cruft https://github.com/lyz-code/cookiecutter-python-project If you don't know cruft, take a look here . Basic Python project \u2691 Create virtualenv mkdir {{ project_directory }} mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository workon {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration' Project structure \u2691 After using different project structures, I've ended up using the following: . \u251c\u2500\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 mypy.ini \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements-dev.in \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 package_name \u2502 \u251c\u2500\u2500 adapters \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 entrypoints \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 py.typed \u2502 \u251c\u2500\u2500 services.py \u2502 \u2514\u2500\u2500 version.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 conftest.py \u251c\u2500\u2500 e2e \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 integration \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 unit \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_services.py Heavily inspired by ionel packaging python library post and the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory. Project types \u2691 Depending on the type of project you want to build there are different layouts: Command-line program . A single Flask web application . Multiple interconnected Flask microservices . Additional configurations \u2691 Once the basic project structure is defined, there are several common enhancements to be applied: Manage dependencies with pip-tools Create the documentation repository Continuous integration pipelines Configure SQLAlchemy to use the MariaDB/Mysql backend Configure Docker and Docker compose to host the application Load config from YAML Configure a Flask project Code tests \u2691 Unit, integration, end-to-end, edge-to-edge tests define the behaviour of the application. Trigger hooks: Github Actions: To run the tests each time a push or pull request is created in Github, create the .github/workflows/test.yml file with the following Jinja template. Make sure to check: The correct Python versions are configured. The steps make sense to your case scenario. Variables to substitute: program_name : your program name --- name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 , 3.8 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov {{ program_name }} tests If you want to add a badge stating the last build status in your readme, use the following template. Variables to substitute: repository_url : Github repository url, like https://github.com/lyz-code/pydo . [![Actions Status]({{ repository_url }}/workflows/Python%20package/badge.svg)]({{ repository_url }}/actions) References \u2691 ionel packaging a python library post and he's cookiecutter template","title":"Project Template"},{"location":"coding/python/python_project_template/#basic-python-project","text":"Create virtualenv mkdir {{ project_directory }} mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository workon {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration'","title":"Basic Python project"},{"location":"coding/python/python_project_template/#project-structure","text":"After using different project structures, I've ended up using the following: . \u251c\u2500\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 mypy.ini \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements-dev.in \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 package_name \u2502 \u251c\u2500\u2500 adapters \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 entrypoints \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 py.typed \u2502 \u251c\u2500\u2500 services.py \u2502 \u2514\u2500\u2500 version.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 conftest.py \u251c\u2500\u2500 e2e \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 integration \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 unit \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_services.py Heavily inspired by ionel packaging python library post and the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"Project structure"},{"location":"coding/python/python_project_template/#project-types","text":"Depending on the type of project you want to build there are different layouts: Command-line program . A single Flask web application . Multiple interconnected Flask microservices .","title":"Project types"},{"location":"coding/python/python_project_template/#additional-configurations","text":"Once the basic project structure is defined, there are several common enhancements to be applied: Manage dependencies with pip-tools Create the documentation repository Continuous integration pipelines Configure SQLAlchemy to use the MariaDB/Mysql backend Configure Docker and Docker compose to host the application Load config from YAML Configure a Flask project","title":"Additional configurations"},{"location":"coding/python/python_project_template/#code-tests","text":"Unit, integration, end-to-end, edge-to-edge tests define the behaviour of the application. Trigger hooks: Github Actions: To run the tests each time a push or pull request is created in Github, create the .github/workflows/test.yml file with the following Jinja template. Make sure to check: The correct Python versions are configured. The steps make sense to your case scenario. Variables to substitute: program_name : your program name --- name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 , 3.8 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov {{ program_name }} tests If you want to add a badge stating the last build status in your readme, use the following template. Variables to substitute: repository_url : Github repository url, like https://github.com/lyz-code/pydo . [![Actions Status]({{ repository_url }}/workflows/Python%20package/badge.svg)]({{ repository_url }}/actions)","title":"Code tests"},{"location":"coding/python/python_project_template/#references","text":"ionel packaging a python library post and he's cookiecutter template","title":"References"},{"location":"coding/python/python_snippets/","text":"Iterate over an instance object's data attributes in Python \u2691 @dataclass ( frozen = True ) class Search : center : str distance : str se = Search ( 'a' , 'b' ) for key , value in se . __dict__ . items (): print ( key , value ) Generate ssh key \u2691 pip install cryptography from os import chmod from cryptography.hazmat.primitives import serialization from cryptography.hazmat.primitives.asymmetric import rsa from cryptography.hazmat.backends import default_backend as crypto_default_backend private_key = rsa . generate_private_key ( backend = crypto_default_backend (), public_exponent = 65537 , key_size = 4096 ) pem = private_key . private_bytes ( encoding = serialization . Encoding . PEM , format = serialization . PrivateFormat . TraditionalOpenSSL , encryption_algorithm = serialization . NoEncryption () ) with open ( \"/tmp/private.key\" , 'wb' ) as content_file : chmod ( \"/tmp/private.key\" , 0600 ) content_file . write ( pem ) public_key = ( private_key . public_key () . public_bytes ( encoding = serialization . Encoding . OpenSSH , format = serialization . PublicFormat . OpenSSH , ) + b ' user@email.org' ) with open ( \"/tmp/public.key\" , 'wb' ) as content_file : content_file . write ( public_key ) Make multiline code look clean \u2691 If you need variables that contain multiline strings inside functions or methods you need to remove the indentation def test (): # end first line with \\ to avoid the empty line! s = ''' \\ hello world ''' Which is inconvenient as it breaks some editor source code folding and it's ugly for the eye. The solution is to use textwrap.dedent() import textwrap def test (): # end first line with \\ to avoid the empty line! s = ''' \\ hello world ''' print ( repr ( s )) # prints ' hello\\n world\\n ' print ( repr ( textwrap . dedent ( s ))) # prints 'hello\\n world\\n' If you forget to add the trailing \\ character of s = '''\\ or use s = '''hello , you're going to have a bad time with black . Play a sound \u2691 pip install playsound from playsound import playsound playsound ( 'path/to/file.wav' ) [Deep copy \u2691 a dictionary]( https://stackoverflow.com/questions/5105517/deep-copy-of-a-dict-in-python ) import copy d = { ... } d2 = copy . deepcopy ( d ) Find the root directory of a package \u2691 pyprojroot finds the root working directory for your project as a pathlib object. You can now use the here function to pass in a relative path from the project root directory (no matter what working directory you are in the project), and you will get a full path to the specified file. Installation \u2691 pip install pyprojroot Usage \u2691 from pyprojroot import here here () Check if an object has an attribute \u2691 if hasattr ( a , 'property' ): a . property","title":"Python Snippets"},{"location":"coding/python/python_snippets/#iterate-over-an-instance-objects-data-attributes-in-python","text":"@dataclass ( frozen = True ) class Search : center : str distance : str se = Search ( 'a' , 'b' ) for key , value in se . __dict__ . items (): print ( key , value )","title":"Iterate over an instance object's data attributes in Python"},{"location":"coding/python/python_snippets/#generate-ssh-key","text":"pip install cryptography from os import chmod from cryptography.hazmat.primitives import serialization from cryptography.hazmat.primitives.asymmetric import rsa from cryptography.hazmat.backends import default_backend as crypto_default_backend private_key = rsa . generate_private_key ( backend = crypto_default_backend (), public_exponent = 65537 , key_size = 4096 ) pem = private_key . private_bytes ( encoding = serialization . Encoding . PEM , format = serialization . PrivateFormat . TraditionalOpenSSL , encryption_algorithm = serialization . NoEncryption () ) with open ( \"/tmp/private.key\" , 'wb' ) as content_file : chmod ( \"/tmp/private.key\" , 0600 ) content_file . write ( pem ) public_key = ( private_key . public_key () . public_bytes ( encoding = serialization . Encoding . OpenSSH , format = serialization . PublicFormat . OpenSSH , ) + b ' user@email.org' ) with open ( \"/tmp/public.key\" , 'wb' ) as content_file : content_file . write ( public_key )","title":"Generate ssh key"},{"location":"coding/python/python_snippets/#make-multiline-code-look-clean","text":"If you need variables that contain multiline strings inside functions or methods you need to remove the indentation def test (): # end first line with \\ to avoid the empty line! s = ''' \\ hello world ''' Which is inconvenient as it breaks some editor source code folding and it's ugly for the eye. The solution is to use textwrap.dedent() import textwrap def test (): # end first line with \\ to avoid the empty line! s = ''' \\ hello world ''' print ( repr ( s )) # prints ' hello\\n world\\n ' print ( repr ( textwrap . dedent ( s ))) # prints 'hello\\n world\\n' If you forget to add the trailing \\ character of s = '''\\ or use s = '''hello , you're going to have a bad time with black .","title":"Make multiline code look clean"},{"location":"coding/python/python_snippets/#play-a-sound","text":"pip install playsound from playsound import playsound playsound ( 'path/to/file.wav' )","title":"Play a sound"},{"location":"coding/python/python_snippets/#deep-copy","text":"a dictionary]( https://stackoverflow.com/questions/5105517/deep-copy-of-a-dict-in-python ) import copy d = { ... } d2 = copy . deepcopy ( d )","title":"[Deep copy"},{"location":"coding/python/python_snippets/#find-the-root-directory-of-a-package","text":"pyprojroot finds the root working directory for your project as a pathlib object. You can now use the here function to pass in a relative path from the project root directory (no matter what working directory you are in the project), and you will get a full path to the specified file.","title":"Find the root directory of a package"},{"location":"coding/python/python_snippets/#installation","text":"pip install pyprojroot","title":"Installation"},{"location":"coding/python/python_snippets/#usage","text":"from pyprojroot import here here ()","title":"Usage"},{"location":"coding/python/python_snippets/#check-if-an-object-has-an-attribute","text":"if hasattr ( a , 'property' ): a . property","title":"Check if an object has an attribute"},{"location":"coding/python/redis-py/","text":"Redis-py is The Python interface to the Redis key-value store. The library encapsulates an actual TCP connection to a Redis server and sends raw commands, as bytes serialized using the REdis Serialization Protocol (RESP) , to the server. It then takes the raw reply and parses it back into a Python object such as bytes, int, or even datetime.datetime. Installation \u2691 pip install redis Usage \u2691 import redis r = redis . Redis ( host = 'localhost' , port = 6379 , db = 0 , password = None , socket_timeout = None , ) The arguments specified above are the default ones, so it's the same as calling r = redis.Redis() . The db parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default. Common pitfalls \u2691 Redis returned objects are bytes type, so you may need to convert it to string with r.get(\"Bahamas\").decode(\"utf-8\") . References \u2691 Real Python introduction to Redis-py Git Docs : Very technical and small.","title":"Redis-py"},{"location":"coding/python/redis-py/#installation","text":"pip install redis","title":"Installation"},{"location":"coding/python/redis-py/#usage","text":"import redis r = redis . Redis ( host = 'localhost' , port = 6379 , db = 0 , password = None , socket_timeout = None , ) The arguments specified above are the default ones, so it's the same as calling r = redis.Redis() . The db parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default.","title":"Usage"},{"location":"coding/python/redis-py/#common-pitfalls","text":"Redis returned objects are bytes type, so you may need to convert it to string with r.get(\"Bahamas\").decode(\"utf-8\") .","title":"Common pitfalls"},{"location":"coding/python/redis-py/#references","text":"Real Python introduction to Redis-py Git Docs : Very technical and small.","title":"References"},{"location":"coding/python/requests_mock/","text":"The requests-mock library is a requests transport adapter that can be preloaded with responses that are returned if certain URIs are requested. This is particularly useful in unit tests where you want to return known responses from HTTP requests without making actual calls. Installation \u2691 pip install requests-mock Usage \u2691 Object initialization \u2691 Select one of the following ways to initialize the mock. As a pytest fixture \u2691 The ease of use with pytest it is awesome. requests-mock provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition. >>> import pytest >>> import requests >>> def test_url ( requests_mock ): ... requests_mock . get ( 'http://test.com' , text = 'data' ) ... assert 'data' == requests . get ( 'http://test.com' ) . text ... As a function decorator \u2691 >>> @requests_mock . Mocker () ... def test_function ( m ): ... m . get ( 'http://test.com' , text = 'resp' ) ... return requests . get ( 'http://test.com' ) . text ... >>> test_function () 'resp' As a context manager \u2691 >>> import requests >>> import requests_mock >>> with requests_mock . Mocker () as m : ... m . get ( 'http://test.com' , text = 'resp' ) ... requests . get ( 'http://test.com' ) . text ... 'resp' Mocking responses \u2691 Return a json \u2691 requests_mock . get ( ' {} /api/repos/owner/repository/builds' . format ( self . url ), json = { \"id\" : 882 , \"number\" : 209 , \"finished\" : 1591197904 , }, ) Multiple responses \u2691 Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list. requests_mock . get ( 'mock://test.com/4' , [ { 'text' : 'resp1' , 'status_code' : 300 }, { 'text' : 'resp2' , 'status_code' : 200 } ] ) Get requests history \u2691 Called \u2691 The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property. >>> import requests >>> import requests_mock >>> with requests_mock . mock () as m : ... m . get ( 'http://test.com, text=' resp ') ... resp = requests . get ( 'http://test.com' ) ... >>> m . called True >>> m . call_count 1 Requests history \u2691 The history of objects that passed through the mocker/adapter can also be retrieved. >>> history = m . request_history >>> len ( history ) 1 >>> history [ 0 ] . method 'GET' >>> history [ 0 ] . url 'http://test.com/' References \u2691 Docs Git","title":"Requests-mock"},{"location":"coding/python/requests_mock/#installation","text":"pip install requests-mock","title":"Installation"},{"location":"coding/python/requests_mock/#usage","text":"","title":"Usage"},{"location":"coding/python/requests_mock/#object-initialization","text":"Select one of the following ways to initialize the mock.","title":"Object initialization"},{"location":"coding/python/requests_mock/#as-a-pytest-fixture","text":"The ease of use with pytest it is awesome. requests-mock provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition. >>> import pytest >>> import requests >>> def test_url ( requests_mock ): ... requests_mock . get ( 'http://test.com' , text = 'data' ) ... assert 'data' == requests . get ( 'http://test.com' ) . text ...","title":"As a pytest fixture"},{"location":"coding/python/requests_mock/#as-a-function-decorator","text":">>> @requests_mock . Mocker () ... def test_function ( m ): ... m . get ( 'http://test.com' , text = 'resp' ) ... return requests . get ( 'http://test.com' ) . text ... >>> test_function () 'resp'","title":"As a function decorator"},{"location":"coding/python/requests_mock/#as-a-context-manager","text":">>> import requests >>> import requests_mock >>> with requests_mock . Mocker () as m : ... m . get ( 'http://test.com' , text = 'resp' ) ... requests . get ( 'http://test.com' ) . text ... 'resp'","title":"As a context manager"},{"location":"coding/python/requests_mock/#mocking-responses","text":"","title":"Mocking responses"},{"location":"coding/python/requests_mock/#return-a-json","text":"requests_mock . get ( ' {} /api/repos/owner/repository/builds' . format ( self . url ), json = { \"id\" : 882 , \"number\" : 209 , \"finished\" : 1591197904 , }, )","title":"Return a json"},{"location":"coding/python/requests_mock/#multiple-responses","text":"Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list. requests_mock . get ( 'mock://test.com/4' , [ { 'text' : 'resp1' , 'status_code' : 300 }, { 'text' : 'resp2' , 'status_code' : 200 } ] )","title":"Multiple responses"},{"location":"coding/python/requests_mock/#get-requests-history","text":"","title":"Get requests history"},{"location":"coding/python/requests_mock/#called","text":"The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property. >>> import requests >>> import requests_mock >>> with requests_mock . mock () as m : ... m . get ( 'http://test.com, text=' resp ') ... resp = requests . get ( 'http://test.com' ) ... >>> m . called True >>> m . call_count 1","title":"Called"},{"location":"coding/python/requests_mock/#requests-history","text":"The history of objects that passed through the mocker/adapter can also be retrieved. >>> history = m . request_history >>> len ( history ) 1 >>> history [ 0 ] . method 'GET' >>> history [ 0 ] . url 'http://test.com/'","title":"Requests history"},{"location":"coding/python/requests_mock/#references","text":"Docs Git","title":"References"},{"location":"coding/python/rq/","text":"RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry. Getting started \u2691 Assuming that a Redis server is running, define the function you want to run: import requests def count_words_at_url ( url ): resp = requests . get ( url ) return len ( resp . text . split ()) The, create a RQ queue: from redis import Redis from rq import Queue q = Queue ( connection = Redis ()) And enqueue the function call: from my_module import count_words_at_url result = q . enqueue ( count_words_at_url , 'http://nvie.com' ) To start executing enqueued function calls in the background, start a worker from your project\u2019s directory: $ rq worker *** Listening for work on default Got count_words_at_url ( 'http://nvie.com' ) from default Job result = 818 *** Listening for work on default Install \u2691 pip install rq Reference \u2691 Homepage Git Docs","title":"Rq"},{"location":"coding/python/rq/#getting-started","text":"Assuming that a Redis server is running, define the function you want to run: import requests def count_words_at_url ( url ): resp = requests . get ( url ) return len ( resp . text . split ()) The, create a RQ queue: from redis import Redis from rq import Queue q = Queue ( connection = Redis ()) And enqueue the function call: from my_module import count_words_at_url result = q . enqueue ( count_words_at_url , 'http://nvie.com' ) To start executing enqueued function calls in the background, start a worker from your project\u2019s directory: $ rq worker *** Listening for work on default Got count_words_at_url ( 'http://nvie.com' ) from default Job result = 818 *** Listening for work on default","title":"Getting started"},{"location":"coding/python/rq/#install","text":"pip install rq","title":"Install"},{"location":"coding/python/rq/#reference","text":"Homepage Git Docs","title":"Reference"},{"location":"coding/python/ruamel_yaml/","text":"ruamel.yaml is a YAML 1.2 loader/dumper package for Python. It is a derivative of Kirill Simonov\u2019s PyYAML 3.11. It has the following enhancements: Comments. Block style and key ordering are kept, so you can diff the round-tripped source. Flow style sequences ( \u2018a: b, c, d\u2019). Anchor names that are hand-crafted (i.e. not of the form idNNN ). Merges in dictionaries are preserved. Installation \u2691 pip install ruamel.yaml Usage \u2691 Very similar to PyYAML. If invoked with YAML(typ='safe') either the load or the write of the data, the comments of the yaml will be lost. Load from file \u2691 from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError try : with open ( os . path . expanduser ( file_path ), 'r' ) as f : try : data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( file_path ) ) sys . exit ( 1 ) Save to file \u2691 with open ( os . path . expanduser ( file_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( data , f ) Save to a string \u2691 For some unknown reason, they don't want to output the result to a string, you need to mess up with streams. # Configure YAML formatter yaml = YAML () yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) yaml . allow_duplicate_keys = True yaml . explicit_start = False # Return the output to a string string_stream = StringIO () yaml . dump ({ 'products' : [ 'item 1' , 'item 2' ]}, string_stream ) source_code = string_stream . getvalue () string_stream . close () I've opened an issue in the ruyaml fork to solve it. References \u2691 Docs Code","title":"Ruamel YAML"},{"location":"coding/python/ruamel_yaml/#installation","text":"pip install ruamel.yaml","title":"Installation"},{"location":"coding/python/ruamel_yaml/#usage","text":"Very similar to PyYAML. If invoked with YAML(typ='safe') either the load or the write of the data, the comments of the yaml will be lost.","title":"Usage"},{"location":"coding/python/ruamel_yaml/#load-from-file","text":"from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError try : with open ( os . path . expanduser ( file_path ), 'r' ) as f : try : data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( file_path ) ) sys . exit ( 1 )","title":"Load from file"},{"location":"coding/python/ruamel_yaml/#save-to-file","text":"with open ( os . path . expanduser ( file_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( data , f )","title":"Save to file"},{"location":"coding/python/ruamel_yaml/#save-to-a-string","text":"For some unknown reason, they don't want to output the result to a string, you need to mess up with streams. # Configure YAML formatter yaml = YAML () yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) yaml . allow_duplicate_keys = True yaml . explicit_start = False # Return the output to a string string_stream = StringIO () yaml . dump ({ 'products' : [ 'item 1' , 'item 2' ]}, string_stream ) source_code = string_stream . getvalue () string_stream . close () I've opened an issue in the ruyaml fork to solve it.","title":"Save to a string"},{"location":"coding/python/ruamel_yaml/#references","text":"Docs Code","title":"References"},{"location":"coding/python/sh/","text":"sh is a full-fledged subprocess replacement so beautiful that makes you want to cry. It allows you to call any program as if it were a function: from sh import ifconfig print ( ifconfig ( \"wlan0\" )) Output: wlan0 Link encap : Ethernet HWaddr 00 : 00 : 00 : 00 : 00 : 00 inet addr : 192.168 . 1.100 Bcast : 192.168 . 1.255 Mask : 255.255 . 255.0 inet6 addr : ffff :: ffff : ffff : ffff : fff / 64 Scope : Link UP BROADCAST RUNNING MULTICAST MTU : 1500 Metric : 1 RX packets : 0 errors : 0 dropped : 0 overruns : 0 frame : 0 TX packets : 0 errors : 0 dropped : 0 overruns : 0 carrier : 0 collisions : 0 txqueuelen : 1000 RX bytes : 0 ( 0 GB ) TX bytes : 0 ( 0 GB ) Note that these aren't Python functions, these are running the binary commands on your system by dynamically resolving your $PATH, much like Bash does, and then wrapping the binary in a function. In this way, all the programs on your system are available to you from within Python. Installation \u2691 pip install sh Usage \u2691 Passing arguments \u2691 sh . ls ( \"-l\" , \"/tmp\" , color = \"never\" ) If the command gives you a syntax error (like pass ), you can use bash. sh . bash ( \"-c\" , \"pass\" ) Handling exceptions \u2691 Normal processes exit with exit code 0. You can access the program return code with RunningCommand.exit_code : output = ls ( \"/\" ) print ( output . exit_code ) # should be 0 If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class ErrorReturnCode : try : print ( ls ( \"/some/non-existant/folder\" )) except sh . ErrorReturnCode_2 : print ( \"folder doesn't exist!\" ) create_the_folder () except sh . ErrorReturnCode : print ( \"unknown error\" ) The exception object is an sh command object, which has, between other , the stderr and stdout bytes attributes with the errors. To show them use: except sh . ErrorReturnCode as error : print ( str ( error . stderr , 'utf8' )) Redirecting output \u2691 sh . ifconfig ( _out = \"/tmp/interfaces\" ) Interacting with programs that ask input from the user \u2691 sh allows you to interact with programs that asks for user input. The documentation is not clear on how to do it, but between the function callbacks documentation, and the example on how to enter an SSH password we can deduce how to do it. Imagine we've got a python script that asks the user to enter a username so it can save it in a file. File: /tmp/script.py answer = input ( \"Enter username: \" ) with open ( \"/tmp/user.txt\" , \"w+\" ) as f : f . write ( answer ) When we run it in the terminal we get prompted and answer with lyz : $: /tmp/script.py Enter username: lyz $: cat /tmp/user.txt lyz To achieve the same goal automatically with sh we'll need to use the function callbacks. They are functions we pass to the sh command through the _out argument. import sys import re aggregated = \"\" def interact ( char , stdin ): global aggregated sys . stdout . write ( char . encode ()) sys . stdout . flush () aggregated += char if re . search ( r \"Enter username: \" , aggregated , re . MULTILINE ): stdin . put ( \"lyz \\n \" ) sh . bash ( \"-c\" , \"/tmp/script.py\" , _out = interact , _out_bufsize = 0 ) In the example above we've created an interact function that will get called on each character of the stdout of the command. It will be called on each character because we passed the argument _out_bufsize=0 . Check the ssh password example to see why we need that. As it's run on each character, and we need to input the username once the program is expecting us to enter the input and not before, we need to keep track of all the printed characters through the global aggregated variable. Once the regular expression matches what we want, sh will inject the desired value. Remember to add the \\n at the end of the string you want to inject. If the output never matches the regular expression, you'll enter an endless loop, so you need to know before hand all the possible user input prompts. References \u2691 Docs Git","title":"sh"},{"location":"coding/python/sh/#installation","text":"pip install sh","title":"Installation"},{"location":"coding/python/sh/#usage","text":"","title":"Usage"},{"location":"coding/python/sh/#passing-arguments","text":"sh . ls ( \"-l\" , \"/tmp\" , color = \"never\" ) If the command gives you a syntax error (like pass ), you can use bash. sh . bash ( \"-c\" , \"pass\" )","title":"Passing arguments"},{"location":"coding/python/sh/#handling-exceptions","text":"Normal processes exit with exit code 0. You can access the program return code with RunningCommand.exit_code : output = ls ( \"/\" ) print ( output . exit_code ) # should be 0 If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class ErrorReturnCode : try : print ( ls ( \"/some/non-existant/folder\" )) except sh . ErrorReturnCode_2 : print ( \"folder doesn't exist!\" ) create_the_folder () except sh . ErrorReturnCode : print ( \"unknown error\" ) The exception object is an sh command object, which has, between other , the stderr and stdout bytes attributes with the errors. To show them use: except sh . ErrorReturnCode as error : print ( str ( error . stderr , 'utf8' ))","title":"Handling exceptions"},{"location":"coding/python/sh/#redirecting-output","text":"sh . ifconfig ( _out = \"/tmp/interfaces\" )","title":"Redirecting output"},{"location":"coding/python/sh/#interacting-with-programs-that-ask-input-from-the-user","text":"sh allows you to interact with programs that asks for user input. The documentation is not clear on how to do it, but between the function callbacks documentation, and the example on how to enter an SSH password we can deduce how to do it. Imagine we've got a python script that asks the user to enter a username so it can save it in a file. File: /tmp/script.py answer = input ( \"Enter username: \" ) with open ( \"/tmp/user.txt\" , \"w+\" ) as f : f . write ( answer ) When we run it in the terminal we get prompted and answer with lyz : $: /tmp/script.py Enter username: lyz $: cat /tmp/user.txt lyz To achieve the same goal automatically with sh we'll need to use the function callbacks. They are functions we pass to the sh command through the _out argument. import sys import re aggregated = \"\" def interact ( char , stdin ): global aggregated sys . stdout . write ( char . encode ()) sys . stdout . flush () aggregated += char if re . search ( r \"Enter username: \" , aggregated , re . MULTILINE ): stdin . put ( \"lyz \\n \" ) sh . bash ( \"-c\" , \"/tmp/script.py\" , _out = interact , _out_bufsize = 0 ) In the example above we've created an interact function that will get called on each character of the stdout of the command. It will be called on each character because we passed the argument _out_bufsize=0 . Check the ssh password example to see why we need that. As it's run on each character, and we need to input the username once the program is expecting us to enter the input and not before, we need to keep track of all the printed characters through the global aggregated variable. Once the regular expression matches what we want, sh will inject the desired value. Remember to add the \\n at the end of the string you want to inject. If the output never matches the regular expression, you'll enter an endless loop, so you need to know before hand all the possible user input prompts.","title":"Interacting with programs that ask input from the user"},{"location":"coding/python/sh/#references","text":"Docs Git","title":"References"},{"location":"coding/python/sqlalchemy/","text":"SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Creating an SQL Schema \u2691 First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward. Mapping styles \u2691 Modern SQLAlchemy features two distinct styles of mapper configuration. The \u201cClassical\u201d style is SQLAlchemy\u2019s original mapping API, whereas \u201cDeclarative\u201d is the richer and more succinct system that builds on top of \u201cClassical\u201d. The Classical, on the other hand, doesn't lock your models to the ORM, something that we avoid when using the repository pattern . If you aren't going to use the repository pattern, use the declarative way, otherwise use the classical one Creating Tables \u2691 If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object. Declarative type \u2691 class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . Text: long_text = Column(Text) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example String(16) . For reference this are the common lengths: url: 2083 name: 64 (it occupies the same 2 and 255). email: 64 (it occupies the same 2 and 255). username: 64 (it occupies the same 2 and 255). Classical type \u2691 File: model.py class User (): def __init__ ( self , id , name = None ): self . id = id self . name = name File: orm.py from models import User from sqlalchemy import ( Column , MetaData , String , Table , Text , ) metadata = MetaData () user = Table ( \"user\" , metadata , Column ( \"id\" , String ( 64 ), primary_key = True ), Column ( \"name\" , String ( 64 )), ) def start_mappers (): mapper ( User , user ) Creating relationships \u2691 Joined table inheritance \u2691 In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class. Declarative \u2691 class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , } Classical \u2691 File: model.py class Employee : def __init__ ( self , name ): self . name = name class Manager ( Employee ): def __init__ ( self , name , manager_data ): super () . __init__ ( name ) self . manager_data = manager_data class Engineer ( Employee ): def __init__ ( self , name , engineer_info ): super () . __init__ ( name ) self . engineer_info = engineer_info File: orm.py metadata = MetaData () employee = Table ( 'employee' , metadata , Column ( 'id' , Integer , primary_key = True ), Column ( 'name' , String ( 50 )), Column ( 'type' , String ( 20 )), Column ( 'manager_data' , String ( 50 )), Column ( 'engineer_info' , String ( 50 )) ) mapper ( Employee , employee , polymorphic_on = employee . c . type , polymorphic_identity = 'employee' , exclude_properties = { 'engineer_info' , 'manager_data' }) mapper ( Manager , inherits = Employee , polymorphic_identity = 'manager' , exclude_properties = { 'engineer_info' }) mapper ( Engineer , inherits = Employee , polymorphic_identity = 'engineer' , exclude_properties = { 'manager_data' }) One to many \u2691 from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'user' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey . Self referenced one to many \u2691 class Task ( Base ): __tablename__ = 'task' id = Column ( String , primary_key = True , doc = 'fulid of creation' ) parent_id = Column ( String , ForeignKey ( 'task.id' )) parent = relationship ( 'Task' , remote_side = [ id ], backref = 'children' ) Many to many \u2691 # Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , ) Self referenced many to many \u2691 Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side. Testing SQLAlchemy Code \u2691 The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest . Exporting database to json \u2691 import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result ) Cloning an SQLAlchemy object \u2691 The following function: Copies all the non-primary-key columns from the input model to a new model instance. Allows definition of specific arguments. Leaves the original model object unmodified. def clone_model ( model , ** kwargs ): \"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\" table = model . __table__ non_primary_key_columns = [ column_name for column_name in table . __mapper__ . attrs .. keys () if column_name not in table . primary_key ] data = { column_name : getattr ( model , column_name ) for column_name in non_pk_columns } data . update ( kwargs ) return model . __class__ ( ** data ) References \u2691 Home Docs","title":"SQLAlchemy"},{"location":"coding/python/sqlalchemy/#creating-an-sql-schema","text":"First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward.","title":"Creating an SQL Schema"},{"location":"coding/python/sqlalchemy/#mapping-styles","text":"Modern SQLAlchemy features two distinct styles of mapper configuration. The \u201cClassical\u201d style is SQLAlchemy\u2019s original mapping API, whereas \u201cDeclarative\u201d is the richer and more succinct system that builds on top of \u201cClassical\u201d. The Classical, on the other hand, doesn't lock your models to the ORM, something that we avoid when using the repository pattern . If you aren't going to use the repository pattern, use the declarative way, otherwise use the classical one","title":"Mapping styles"},{"location":"coding/python/sqlalchemy/#creating-tables","text":"If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object.","title":"Creating Tables"},{"location":"coding/python/sqlalchemy/#declarative-type","text":"class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . Text: long_text = Column(Text) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example String(16) . For reference this are the common lengths: url: 2083 name: 64 (it occupies the same 2 and 255). email: 64 (it occupies the same 2 and 255). username: 64 (it occupies the same 2 and 255).","title":"Declarative type"},{"location":"coding/python/sqlalchemy/#classical-type","text":"File: model.py class User (): def __init__ ( self , id , name = None ): self . id = id self . name = name File: orm.py from models import User from sqlalchemy import ( Column , MetaData , String , Table , Text , ) metadata = MetaData () user = Table ( \"user\" , metadata , Column ( \"id\" , String ( 64 ), primary_key = True ), Column ( \"name\" , String ( 64 )), ) def start_mappers (): mapper ( User , user )","title":"Classical type"},{"location":"coding/python/sqlalchemy/#creating-relationships","text":"","title":"Creating relationships"},{"location":"coding/python/sqlalchemy/#joined-table-inheritance","text":"In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class.","title":"Joined table inheritance"},{"location":"coding/python/sqlalchemy/#declarative","text":"class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , }","title":"Declarative"},{"location":"coding/python/sqlalchemy/#classical","text":"File: model.py class Employee : def __init__ ( self , name ): self . name = name class Manager ( Employee ): def __init__ ( self , name , manager_data ): super () . __init__ ( name ) self . manager_data = manager_data class Engineer ( Employee ): def __init__ ( self , name , engineer_info ): super () . __init__ ( name ) self . engineer_info = engineer_info File: orm.py metadata = MetaData () employee = Table ( 'employee' , metadata , Column ( 'id' , Integer , primary_key = True ), Column ( 'name' , String ( 50 )), Column ( 'type' , String ( 20 )), Column ( 'manager_data' , String ( 50 )), Column ( 'engineer_info' , String ( 50 )) ) mapper ( Employee , employee , polymorphic_on = employee . c . type , polymorphic_identity = 'employee' , exclude_properties = { 'engineer_info' , 'manager_data' }) mapper ( Manager , inherits = Employee , polymorphic_identity = 'manager' , exclude_properties = { 'engineer_info' }) mapper ( Engineer , inherits = Employee , polymorphic_identity = 'engineer' , exclude_properties = { 'manager_data' })","title":"Classical"},{"location":"coding/python/sqlalchemy/#one-to-many","text":"from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'user' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey .","title":"One to many"},{"location":"coding/python/sqlalchemy/#self-referenced-one-to-many","text":"class Task ( Base ): __tablename__ = 'task' id = Column ( String , primary_key = True , doc = 'fulid of creation' ) parent_id = Column ( String , ForeignKey ( 'task.id' )) parent = relationship ( 'Task' , remote_side = [ id ], backref = 'children' )","title":"Self referenced one to many"},{"location":"coding/python/sqlalchemy/#many-to-many","text":"# Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , )","title":"Many to many"},{"location":"coding/python/sqlalchemy/#self-referenced-many-to-many","text":"Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side.","title":"Self referenced many to many"},{"location":"coding/python/sqlalchemy/#testing-sqlalchemy-code","text":"The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest .","title":"Testing SQLAlchemy Code"},{"location":"coding/python/sqlalchemy/#exporting-database-to-json","text":"import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result )","title":"Exporting database to json"},{"location":"coding/python/sqlalchemy/#cloning-an-sqlalchemy-object","text":"The following function: Copies all the non-primary-key columns from the input model to a new model instance. Allows definition of specific arguments. Leaves the original model object unmodified. def clone_model ( model , ** kwargs ): \"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\" table = model . __table__ non_primary_key_columns = [ column_name for column_name in table . __mapper__ . attrs .. keys () if column_name not in table . primary_key ] data = { column_name : getattr ( model , column_name ) for column_name in non_pk_columns } data . update ( kwargs ) return model . __class__ ( ** data )","title":"Cloning an SQLAlchemy object"},{"location":"coding/python/sqlalchemy/#references","text":"Home Docs","title":"References"},{"location":"coding/python/tinydb/","text":"Tinydb is a document oriented database that stores data in a json file. It's the closest solution to a NoSQL SQLite solution that I've found. The advantages are that you can use a NoSQL database without installing a server. Tinydb is small, simple to use, well tested, optimized and extensible . On the other hand, if you are searching for advanced database features like more than one connection or high performance, you should consider using databases like SQLite or MongoDB. I think it's the perfect solution for initial versions of a program, when the database schema is variable and there is no need of high performance. Once the program is stabilized and the performance drops, you can change the storage provider to a production ready one. To make this change doable, I recommend implementing the repository pattern to decouple the storage layer from your application logic. Install \u2691 pip install tinydb Basic usage \u2691 TL;DR: Operation Cheatsheet Inserting data : db.insert(...) . Getting data : db.all() : Get all documents. iter(db) : Iterate over all the documents. db.search(query) : Get a list of documents matching the query. Updating : db.update(fields, query) : Update all documents matching the query to contain fields. Removing : db.remove(query) : Remove all documents matching the query. db.truncate() : Remove all documents. Querying : Query() : Create a new query object. Query().field == 2 : Match any document that has a key field with value == 2 (also possible: != , > , >= , < , <= ). First you need to setup the database: from tinydb import TinyDB , Query db = TinyDB ( 'db.json' ) TinyDB expects the data to be Python dictionaries: db . insert ({ 'type' : 'apple' , 'count' : 7 }) db . insert ({ 'type' : 'peach' , 'count' : 3 }) You can also iterate over stored documents: >>> for item in db : >>> print ( item ) { 'count' : 7 , 'type' : 'apple' } { 'count' : 3 , 'type' : 'peach' } You can search for specific documents: >>> Fruit = Query () >>> db . search ( Fruit . type == 'peach' ) [{ 'count' : 3 , 'type' : 'peach' }] >>> db . search ( Fruit . count > 5 ) [{ 'count' : 7 , 'type' : 'apple' }] You can update fields: >>> db . update ({ 'count' : 10 }, Fruit . type == 'apple' ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }, { 'count' : 3 , 'type' : 'peach' }] And remove documents: >>> db . remove ( Fruit . count < 5 ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }] Query construction \u2691 Match any document where a field called field exists: Query().field.exists() . Match any document with the whole field matching the regular expression: Query().field.matches(regex) . Match any document with a substring of the field matching the regular expression: Query().field.search(regex) . Match any document for which the function returns True : Query().field.test(func, *args) . If given a query, match all documents where all documents in the list field match the query. If given a list, matches all documents where all documents in the list field are a member of the given list: Query().field.all(query | list) . If given a query, match all documents where at least one document in the list field match the query. If given a list, matches all documents where at least one documents in the list field are a member of the given list: Query().field.any(query | list) . Match if the field is contained in the list: Query().field.one_of(list) . Logical operations on queries Match documents that don't match the query: ~ (query) . Match documents that match both queries: (query1) & (query2) . Match documents that match at least one of the queries: (query1) | (query2) . To retrieve the data from the database, you need to use Query objects in a similar way as you do with ORMs. from tinydb import Query User = Query () db . search ( User . name == 'John' ) db . search ( User . birthday . year == 1990 ) If the field is not a valid Python identifier use the following syntax: db . search ( User [ 'country-code' ] == 'foo' ) Advanced queries \u2691 TinyDB supports other ways to search in your data: Testing the existence of a field: db . search ( User . name . exists ()) Testing values against regular expressions: # Full item has to match the regex: db . search ( User . name . matches ( '[aZ]*' )) # Any part of the item has to match the regex: db . search ( User . name . search ( 'b+' )) Testing using custom tests: # Custom test: test_func = lambda s : s == 'John' db . search ( User . name . test ( test_func )) # Custom test with parameters: def test_func ( val , m , n ): return m <= val <= n db . search ( User . age . test ( test_func , 0 , 21 )) db . search ( User . age . test ( test_func , 21 , 99 )) Testing fields that contain lists with the any and all methods: Assuming we have a user object with a groups list like this: db . insert ({ 'name' : 'user1' , 'groups' : [ 'user' ]}) db . insert ({ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}) db . insert ({ 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}) You can use the following queries: # User's groups include at least one value from ['admin', 'sudo'] >>> db . search ( User . groups . any ([ 'admin' , 'sudo' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}, { 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}] # User's groups include all values from ['admin', 'user'] >>> db . search ( User . groups . all ([ 'admin' , 'user' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}] Testing nested queries: Assuming we have the following table: Group = Query () Permission = Query () groups = db . table ( 'groups' ) groups . insert ({ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}) groups . insert ({ 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}) groups . insert ({ 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}) You can search this table using nested any / all queries: # Group has a permission with type 'read' >>> groups . search ( Group . permissions . any ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}, { 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}, { 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}] # Group has ONLY permission 'read' >>> groups . search ( Group . permissions . all ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}] any tests if there is at least one document matching the query while all ensures all documents match the query. The opposite operation, checking if a list contains a single item, is also possible using one_of : >>> db . search ( User . name . one_of ([ 'jane' , 'john' ])) Query modifiers \u2691 TinyDB allows you to use logical operations to change and combine queries Negate a query: db.search(~ (User.name == 'John')) . Logical AND : db.search((User.name == 'John') & (User.age <= 30)) . Logical OR : db.search((User.name == 'John') | (User.name == 'Bob')) . Inserting more than one document \u2691 In case you want to insert more than one document, you can use db.insert_multiple(...) : >>> db . insert_multiple ([ { 'name' : 'John' , 'age' : 22 }, { 'name' : 'John' , 'age' : 37 }]) >>> db . insert_multiple ({ 'int' : 1 , 'value' : i } for i in range ( 2 )) Updating data \u2691 To update all the documents of the database, leave out the query argument: db . update ({ 'foo' : 'bar' }) When you pass a dictionary to db.update(fields, query) , you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data: delete(key) : Delete a key from the document. increment(key) : Increment the value of a key. decrement(key) : Decrement the value of a key. add(key, value) : Add value to the value of a key (also works for strings). subtract(key, value) : Subtract value from the value of a key. set(key, value) : Set key to value. >>> from tinydb.operations import delete >>> db . update ( delete ( 'key1' ), User . name == 'John' ) This will remove the key key1 from all matching documents. You also can write your own operations: >>> def your_operation ( your_arguments ): ... def transform ( doc ): ... # do something with the document ... # ... ... return transform ... >>> db . update ( your_operation ( arguments ), query ) Retrieving data \u2691 If you want to get one element use db.get(...) . Be warned, if more than one document match the query, a random will be returned. >>> db . get ( User . name == 'John' ) { 'name' : 'John' , 'age' : 22 } If you want to know if the database stores a document, use db.contains(...) . >>> db . contains ( User . name == 'John' ) True If you want to know the number of documents that match a query use db.count(...) . >>> db . count ( User . name == 'John' ) 2 Tables \u2691 TinyDB supports working with more than one table. To create and use a table, use db.table(name) . They behave as the TinyDB class. >>> table = db . table ( 'table_name' ) >>> table . insert ({ 'value' : True }) >>> table . all () [{ 'value' : True }] >>> for row in table : >>> print ( row ) { 'value' : True } To remove a table from a database, use: db . drop_table ( 'table_name' ) To remove all tables, use: db . drop_tables () To get a list with the names of all tables in your database: >>> db . tables () { '_default' , 'table_name' } Query caching \u2691 TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the table(...) function: table = db . table ( 'table_name' , cache_size = 30 ) You can set cache_size to None to make the cache unlimited in size. Also, you can set cache_size to 0 to disable it. Storage types \u2691 TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it: from tinydb import TinyDB , where db = TinyDB ( 'path/to/db.json' ) To use the in-memory storage, use: from tinydb.storages import MemoryStorage db = TinyDB ( storage = MemoryStorage ) All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s json.dump(\u2026) method. For example, you can set it to create prettified JSON files like this: >>> db = TinyDB ( 'db.json' , sort_keys = True , indent = 4 , separators = ( ',' , ': ' )) References \u2691 Docs Git Issues","title":"TinyDB"},{"location":"coding/python/tinydb/#install","text":"pip install tinydb","title":"Install"},{"location":"coding/python/tinydb/#basic-usage","text":"TL;DR: Operation Cheatsheet Inserting data : db.insert(...) . Getting data : db.all() : Get all documents. iter(db) : Iterate over all the documents. db.search(query) : Get a list of documents matching the query. Updating : db.update(fields, query) : Update all documents matching the query to contain fields. Removing : db.remove(query) : Remove all documents matching the query. db.truncate() : Remove all documents. Querying : Query() : Create a new query object. Query().field == 2 : Match any document that has a key field with value == 2 (also possible: != , > , >= , < , <= ). First you need to setup the database: from tinydb import TinyDB , Query db = TinyDB ( 'db.json' ) TinyDB expects the data to be Python dictionaries: db . insert ({ 'type' : 'apple' , 'count' : 7 }) db . insert ({ 'type' : 'peach' , 'count' : 3 }) You can also iterate over stored documents: >>> for item in db : >>> print ( item ) { 'count' : 7 , 'type' : 'apple' } { 'count' : 3 , 'type' : 'peach' } You can search for specific documents: >>> Fruit = Query () >>> db . search ( Fruit . type == 'peach' ) [{ 'count' : 3 , 'type' : 'peach' }] >>> db . search ( Fruit . count > 5 ) [{ 'count' : 7 , 'type' : 'apple' }] You can update fields: >>> db . update ({ 'count' : 10 }, Fruit . type == 'apple' ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }, { 'count' : 3 , 'type' : 'peach' }] And remove documents: >>> db . remove ( Fruit . count < 5 ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }]","title":"Basic usage"},{"location":"coding/python/tinydb/#query-construction","text":"Match any document where a field called field exists: Query().field.exists() . Match any document with the whole field matching the regular expression: Query().field.matches(regex) . Match any document with a substring of the field matching the regular expression: Query().field.search(regex) . Match any document for which the function returns True : Query().field.test(func, *args) . If given a query, match all documents where all documents in the list field match the query. If given a list, matches all documents where all documents in the list field are a member of the given list: Query().field.all(query | list) . If given a query, match all documents where at least one document in the list field match the query. If given a list, matches all documents where at least one documents in the list field are a member of the given list: Query().field.any(query | list) . Match if the field is contained in the list: Query().field.one_of(list) . Logical operations on queries Match documents that don't match the query: ~ (query) . Match documents that match both queries: (query1) & (query2) . Match documents that match at least one of the queries: (query1) | (query2) . To retrieve the data from the database, you need to use Query objects in a similar way as you do with ORMs. from tinydb import Query User = Query () db . search ( User . name == 'John' ) db . search ( User . birthday . year == 1990 ) If the field is not a valid Python identifier use the following syntax: db . search ( User [ 'country-code' ] == 'foo' )","title":"Query construction"},{"location":"coding/python/tinydb/#advanced-queries","text":"TinyDB supports other ways to search in your data: Testing the existence of a field: db . search ( User . name . exists ()) Testing values against regular expressions: # Full item has to match the regex: db . search ( User . name . matches ( '[aZ]*' )) # Any part of the item has to match the regex: db . search ( User . name . search ( 'b+' )) Testing using custom tests: # Custom test: test_func = lambda s : s == 'John' db . search ( User . name . test ( test_func )) # Custom test with parameters: def test_func ( val , m , n ): return m <= val <= n db . search ( User . age . test ( test_func , 0 , 21 )) db . search ( User . age . test ( test_func , 21 , 99 )) Testing fields that contain lists with the any and all methods: Assuming we have a user object with a groups list like this: db . insert ({ 'name' : 'user1' , 'groups' : [ 'user' ]}) db . insert ({ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}) db . insert ({ 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}) You can use the following queries: # User's groups include at least one value from ['admin', 'sudo'] >>> db . search ( User . groups . any ([ 'admin' , 'sudo' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}, { 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}] # User's groups include all values from ['admin', 'user'] >>> db . search ( User . groups . all ([ 'admin' , 'user' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}] Testing nested queries: Assuming we have the following table: Group = Query () Permission = Query () groups = db . table ( 'groups' ) groups . insert ({ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}) groups . insert ({ 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}) groups . insert ({ 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}) You can search this table using nested any / all queries: # Group has a permission with type 'read' >>> groups . search ( Group . permissions . any ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}, { 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}, { 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}] # Group has ONLY permission 'read' >>> groups . search ( Group . permissions . all ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}] any tests if there is at least one document matching the query while all ensures all documents match the query. The opposite operation, checking if a list contains a single item, is also possible using one_of : >>> db . search ( User . name . one_of ([ 'jane' , 'john' ]))","title":"Advanced queries"},{"location":"coding/python/tinydb/#query-modifiers","text":"TinyDB allows you to use logical operations to change and combine queries Negate a query: db.search(~ (User.name == 'John')) . Logical AND : db.search((User.name == 'John') & (User.age <= 30)) . Logical OR : db.search((User.name == 'John') | (User.name == 'Bob')) .","title":"Query modifiers"},{"location":"coding/python/tinydb/#inserting-more-than-one-document","text":"In case you want to insert more than one document, you can use db.insert_multiple(...) : >>> db . insert_multiple ([ { 'name' : 'John' , 'age' : 22 }, { 'name' : 'John' , 'age' : 37 }]) >>> db . insert_multiple ({ 'int' : 1 , 'value' : i } for i in range ( 2 ))","title":"Inserting more than one document"},{"location":"coding/python/tinydb/#updating-data","text":"To update all the documents of the database, leave out the query argument: db . update ({ 'foo' : 'bar' }) When you pass a dictionary to db.update(fields, query) , you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data: delete(key) : Delete a key from the document. increment(key) : Increment the value of a key. decrement(key) : Decrement the value of a key. add(key, value) : Add value to the value of a key (also works for strings). subtract(key, value) : Subtract value from the value of a key. set(key, value) : Set key to value. >>> from tinydb.operations import delete >>> db . update ( delete ( 'key1' ), User . name == 'John' ) This will remove the key key1 from all matching documents. You also can write your own operations: >>> def your_operation ( your_arguments ): ... def transform ( doc ): ... # do something with the document ... # ... ... return transform ... >>> db . update ( your_operation ( arguments ), query )","title":"Updating data"},{"location":"coding/python/tinydb/#retrieving-data","text":"If you want to get one element use db.get(...) . Be warned, if more than one document match the query, a random will be returned. >>> db . get ( User . name == 'John' ) { 'name' : 'John' , 'age' : 22 } If you want to know if the database stores a document, use db.contains(...) . >>> db . contains ( User . name == 'John' ) True If you want to know the number of documents that match a query use db.count(...) . >>> db . count ( User . name == 'John' ) 2","title":"Retrieving data"},{"location":"coding/python/tinydb/#tables","text":"TinyDB supports working with more than one table. To create and use a table, use db.table(name) . They behave as the TinyDB class. >>> table = db . table ( 'table_name' ) >>> table . insert ({ 'value' : True }) >>> table . all () [{ 'value' : True }] >>> for row in table : >>> print ( row ) { 'value' : True } To remove a table from a database, use: db . drop_table ( 'table_name' ) To remove all tables, use: db . drop_tables () To get a list with the names of all tables in your database: >>> db . tables () { '_default' , 'table_name' }","title":"Tables"},{"location":"coding/python/tinydb/#query-caching","text":"TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the table(...) function: table = db . table ( 'table_name' , cache_size = 30 ) You can set cache_size to None to make the cache unlimited in size. Also, you can set cache_size to 0 to disable it.","title":"Query caching"},{"location":"coding/python/tinydb/#storage-types","text":"TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it: from tinydb import TinyDB , where db = TinyDB ( 'path/to/db.json' ) To use the in-memory storage, use: from tinydb.storages import MemoryStorage db = TinyDB ( storage = MemoryStorage ) All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s json.dump(\u2026) method. For example, you can set it to create prettified JSON files like this: >>> db = TinyDB ( 'db.json' , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ))","title":"Storage types"},{"location":"coding/python/tinydb/#references","text":"Docs Git Issues","title":"References"},{"location":"coding/python/type_hints/","text":"Type hints are the Python native way to define the type of the objects in a program. Traditionally, the Python interpreter handles types in a flexible but implicit way. Recent versions of Python allow you to specify explicit type hints that different tools can use to help you develop your code more efficiently. TL;DR Use Type hints whenever unit tests are worth writing def headline ( text : str , align : bool = True ) -> str : if align : return f \" { text . title () } \\n { '-' * len ( text ) } \" else : return f \" { text . title () } \" . center ( 50 , \"o\" ) Type hints are not enforced on their own by python. So you won't catch an error if you try to run headline(\"use mypy\", align=\"center\") unless you use a static type checker like Mypy . Advantages and disadvantages \u2691 Advantages: Help catch certain errors if used with a static type checker. Help check your code . It's not trivial to use docstrings to do automatic checks. Help to reason about code: Knowing the parameters type makes it a lot easier to understand and maintain a code base. It can speed up the time required to catch up with a code snippet. Always remember that you read code a lot more often than you write it, so you should optimize for ease of reading. Help you build and maintain a cleaner architecture . The act of writing type hints force you to think about the types in your program. Helps refactoring: Type hints make it trivial to find where a given class is used when you're trying to refactor your code base. Improve IDEs and linters. Cons: Type hints take developer time and effort to add . Even though it probably pays off in spending less time debugging, you will spend more time entering code. Introduce a slight penalty in start-up time . If you need to use the typing module, the import time may be significant, even more in short scripts. Work best in modern Pythons. Follow these guidelines when deciding if you want to add types to your project: In libraries that will be used by others, they add a lot of value. In complex projects, type hints help you understand how types flow through your code and are highly recommended. If you are beginning to learn Python, don't use them yet. If you are writing throw-away scripts, don't use them. So, Use Type hints whenever unit tests are worth writing . Usage \u2691 Function annotations \u2691 def func ( arg : arg_type , optarg : arg_type = default ) -> return_type : ... For arguments the syntax is argument: annotation , while the return type is annotated using -> annotation . Note that the annotation must be a valid Python expression. When running the code, the special .__annotations__ attribute on the function stores the typing information. Variable annotations \u2691 Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar: pi : float = 3.142 def circumference ( radius : float ) -> float : return 2 * pi * radius Composite types \u2691 If you need to hint other types than str , float and bool , you'll need to import the typing module. For example to define the hint types of list, dictionaries and tuples: >>> from typing import Dict , List , Tuple >>> names : List [ str ] = [ \"Guido\" , \"Jukka\" , \"Ivan\" ] >>> version : Tuple [ int , int , int ] = ( 3 , 7 , 1 ) >>> options : Dict [ str , bool ] = { \"centered\" : False , \"capitalize\" : True } If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the typing.Sequence object. Dictionaries with different value types per key . \u2691 TypedDict declares a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. TypedDict started life as an experimental Mypy feature to wrangle typing onto the heterogeneous, structure-oriented use of dictionaries. As of Python 3.8, it was adopted into the standard library. try : from typing import TypedDict # >=3.8 except ImportError : from mypy_extensions import TypedDict # <=3.7 Movie = TypedDict ( 'Movie' , { 'name' : str , 'year' : int }) A class-based type constructor is also available: class Movie ( TypedDict ): name : str year : int By default, all keys must be present in a TypedDict . It is possible to override this by specifying totality. Usage: class point2D ( TypedDict , total = False ): x : int y : int This means that a point2D TypedDict can have any of the keys omitted. A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. Functions without return values \u2691 Some functions aren't meant to return anything. Use the -> None hint in these cases. def play ( player_name : str ) -> None : print ( f \" { player_name } plays\" ) ret_val = play ( \"Filip\" ) The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value. If your function doesn't return any object, use the NoReturn type. Note This is the first iteration of the synoptical reading of the full Real python article on type checking . Optional arguments \u2691 A common pattern is to use None as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior. This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be None . We use the Optional type to address this case. from typing import Optional def player ( name : str , start : Optional [ str ] = None ) -> str : ... A similar way would be to use Union[None, str] . Allow any subclass \u2691 It's not yet supported (unless inheriting from an abstract class), so the expected format class A : pass class B ( A ): pass def process_any_subclass_type_of_A ( cls : A ): pass process_any_subclass_type_of_A ( B ) Will fail with error: Argument 1 to \"process_any_subclass_type_of_A\" has incompatible type \"Type[B]\"; expected \"A\" . The solution is to use the Union operator: class A : pass class B ( A ): pass class C ( A ): pass def process_any_subclass_type_of_A ( cls : Union [ B , C ]): pass The following works: class A ( abc . ABC ): @abc . abstractmethod def add ( self ): raise NotImplementedError class B ( A ): def add ( self ): pass class C ( A ): def add ( self ): pass def process_any_subclass_type_of_A ( cls : A ): pass Type aliases \u2691 Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead. from typing import List , Tuple Card = Tuple [ str , str ] Deck = List [ Card ] def deal_hands ( deck : Deck ) -> Tuple [ Deck , Deck , Deck , Deck ]: \"\"\"Deal the cards in the deck into four hands\"\"\" return ( deck [ 0 :: 4 ], deck [ 1 :: 4 ], deck [ 2 :: 4 ], deck [ 3 :: 4 ]) Generic types \u2691 This can be useful when you need lists of subclasses or optional list of subclasses. The expected behavior doesn't work. Entity = TypeVar ( 'Entity' , model . Project , model . Tag , model . Task ) Entities = List [ Entity ] If you just want to specify any children of a parent class , use: from .model import Entity as EntityModel Entity = TypeVar ( 'Entity' , bound = EntityModel ) Try to use TypeVar instead of Union of the different types, as it's able to deduce better the type of the return value of a function. def do_something ( entity : Entity ) -> Entity : return Entity If you use TypeVar , if you call the function with a type Card , it will know that the result is of type Card , if you use Union , even if you call it with Card the return value will be Union[Card,Deck] . More generally, Generics can be parameterized by using a new factory available in typing called TypeVar . Example: from typing import Sequence , TypeVar T = TypeVar ( 'T' ) # Declare type variable def first ( l : Sequence [ T ]) -> T : # Generic function return l [ 0 ] In this case the contract is that the returned value is consistent with the elements held by the collection. A TypeVar() expression must always directly be assigned to a variable (it should not be used as part of a larger expression). The argument to TypeVar() must be a string equal to the variable name to which it is assigned. Type variables must not be redefined. TypeVar supports constraining parametric types to a fixed set of possible types (note: those types cannot be parameterized by type variables). For example, we can define a type variable that ranges over just str and bytes. By default, a type variable ranges over all possible types. Example of constraining a type variable: from typing import TypeVar , Text AnyStr = TypeVar ( 'AnyStr' , Text , bytes ) def concat ( x : AnyStr , y : AnyStr ) -> AnyStr : return x + y Specify the type of the class in it's method and attributes \u2691 If you are using Python 3.10 or later, it just works. Python 3.7 introduces PEP 563: postponed evaluation of annotations. A module that uses the future statement from __future__ import annotations to store annotations as strings automatically: from __future__ import annotations class Position : def __add__ ( self , other : Position ) -> Position : ... But pyflakes will still complain, so I've used strings. from __future__ import annotations class Position : def __add__ ( self , other : 'Position' ) -> 'Position' : ... Using mypy with an existing codebase \u2691 These steps will get you started with mypy on an existing codebase: Start small : Pick a subset of your codebase to run mypy on, without any annotations. You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding # type: ignore comments to silence errors you don\u2019t want to fix now. Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors : Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this: python3 - m pip install mypy == 0.600 # Pinned version avoids surprises scripts / mypy # Runs with the correct options * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions: Developers should add annotations for any new code. It\u2019s also encouraged to write annotations when you change existing code. Reveal the type of an expression \u2691 You can use reveal_type(expr) to ask mypy to display the inferred static type of an expression. This can be useful when you don't quite understand how mypy handles a particular piece of code. Example: reveal_type (( 1 , 'hello' )) # Revealed type is 'Tuple[builtins.int, builtins.str]' You can also use reveal_locals() at any line in a file to see the types of all local variables at once. Example: a = 1 b = 'one' reveal_locals () # Revealed local types are: # a: builtins.int # b: builtins.str reveal_type and reveal_locals are only understood by mypy and don't exist in Python. If you try to run your program, you\u2019ll have to remove any reveal_type and reveal_locals calls before you can run your code. Both are always available and you don't need to import them. Solve cyclic imports due to typing \u2691 You can use a conditional import that is only active in \"type hinting mode\", but doesn't interfere at run time. The typing.TYPE_CHECKING constant makes this easily possible. For example: # thing.py from typing import TYPE_CHECKING if TYPE_CHECKING : from connection import ApiConnection class Thing : def __init__ ( self , connection : 'ApiConnection' ): self . _conn = connection The code will now execute properly as there is no circular import issue anymore. Type hinting tools on the other hand should still be able to resolve the ApiConnection type hint in Thing.__init__ . Make your library compatible with mypy \u2691 PEP 561 notes three main ways to distribute type information. The first is a package that has only inline type annotations in the code itself. The second is a package that ships stub files with type information alongside the runtime code. The third method, also known as a \u201cstub only package\u201d is a package that ships type information for a package separately as stub files. If you would like to publish a library package to a package repository (e.g. PyPI) for either internal or external use in type checking, packages that supply type information via type comments or annotations in the code should put a py.typed file in their package directory. For example, with a directory structure as follows setup.py package_a/ __init__.py lib.py py.typed the setup.py might look like: from distutils.core import setup setup ( name = \"SuperPackageA\" , author = \"Me\" , version = \"0.1\" , package_data = { \"package_a\" : [ \"py.typed\" ]}, packages = [ \"package_a\" ] ) If you use setuptools, you must pass the option zip_safe=False to setup() , or mypy will not be able to find the installed package. Reference \u2691 Bernat gabor article on the state of type hints in python Real python article on type checking","title":"Type Hints"},{"location":"coding/python/type_hints/#advantages-and-disadvantages","text":"Advantages: Help catch certain errors if used with a static type checker. Help check your code . It's not trivial to use docstrings to do automatic checks. Help to reason about code: Knowing the parameters type makes it a lot easier to understand and maintain a code base. It can speed up the time required to catch up with a code snippet. Always remember that you read code a lot more often than you write it, so you should optimize for ease of reading. Help you build and maintain a cleaner architecture . The act of writing type hints force you to think about the types in your program. Helps refactoring: Type hints make it trivial to find where a given class is used when you're trying to refactor your code base. Improve IDEs and linters. Cons: Type hints take developer time and effort to add . Even though it probably pays off in spending less time debugging, you will spend more time entering code. Introduce a slight penalty in start-up time . If you need to use the typing module, the import time may be significant, even more in short scripts. Work best in modern Pythons. Follow these guidelines when deciding if you want to add types to your project: In libraries that will be used by others, they add a lot of value. In complex projects, type hints help you understand how types flow through your code and are highly recommended. If you are beginning to learn Python, don't use them yet. If you are writing throw-away scripts, don't use them. So, Use Type hints whenever unit tests are worth writing .","title":"Advantages and disadvantages"},{"location":"coding/python/type_hints/#usage","text":"","title":"Usage"},{"location":"coding/python/type_hints/#function-annotations","text":"def func ( arg : arg_type , optarg : arg_type = default ) -> return_type : ... For arguments the syntax is argument: annotation , while the return type is annotated using -> annotation . Note that the annotation must be a valid Python expression. When running the code, the special .__annotations__ attribute on the function stores the typing information.","title":"Function annotations"},{"location":"coding/python/type_hints/#variable-annotations","text":"Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar: pi : float = 3.142 def circumference ( radius : float ) -> float : return 2 * pi * radius","title":"Variable annotations"},{"location":"coding/python/type_hints/#composite-types","text":"If you need to hint other types than str , float and bool , you'll need to import the typing module. For example to define the hint types of list, dictionaries and tuples: >>> from typing import Dict , List , Tuple >>> names : List [ str ] = [ \"Guido\" , \"Jukka\" , \"Ivan\" ] >>> version : Tuple [ int , int , int ] = ( 3 , 7 , 1 ) >>> options : Dict [ str , bool ] = { \"centered\" : False , \"capitalize\" : True } If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the typing.Sequence object.","title":"Composite types"},{"location":"coding/python/type_hints/#dictionaries-with-different-value-types-per-key","text":"TypedDict declares a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. TypedDict started life as an experimental Mypy feature to wrangle typing onto the heterogeneous, structure-oriented use of dictionaries. As of Python 3.8, it was adopted into the standard library. try : from typing import TypedDict # >=3.8 except ImportError : from mypy_extensions import TypedDict # <=3.7 Movie = TypedDict ( 'Movie' , { 'name' : str , 'year' : int }) A class-based type constructor is also available: class Movie ( TypedDict ): name : str year : int By default, all keys must be present in a TypedDict . It is possible to override this by specifying totality. Usage: class point2D ( TypedDict , total = False ): x : int y : int This means that a point2D TypedDict can have any of the keys omitted. A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required.","title":"Dictionaries with different value types per key."},{"location":"coding/python/type_hints/#functions-without-return-values","text":"Some functions aren't meant to return anything. Use the -> None hint in these cases. def play ( player_name : str ) -> None : print ( f \" { player_name } plays\" ) ret_val = play ( \"Filip\" ) The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value. If your function doesn't return any object, use the NoReturn type. Note This is the first iteration of the synoptical reading of the full Real python article on type checking .","title":"Functions without return values"},{"location":"coding/python/type_hints/#optional-arguments","text":"A common pattern is to use None as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior. This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be None . We use the Optional type to address this case. from typing import Optional def player ( name : str , start : Optional [ str ] = None ) -> str : ... A similar way would be to use Union[None, str] .","title":"Optional arguments"},{"location":"coding/python/type_hints/#allow-any-subclass","text":"It's not yet supported (unless inheriting from an abstract class), so the expected format class A : pass class B ( A ): pass def process_any_subclass_type_of_A ( cls : A ): pass process_any_subclass_type_of_A ( B ) Will fail with error: Argument 1 to \"process_any_subclass_type_of_A\" has incompatible type \"Type[B]\"; expected \"A\" . The solution is to use the Union operator: class A : pass class B ( A ): pass class C ( A ): pass def process_any_subclass_type_of_A ( cls : Union [ B , C ]): pass The following works: class A ( abc . ABC ): @abc . abstractmethod def add ( self ): raise NotImplementedError class B ( A ): def add ( self ): pass class C ( A ): def add ( self ): pass def process_any_subclass_type_of_A ( cls : A ): pass","title":"Allow any subclass"},{"location":"coding/python/type_hints/#type-aliases","text":"Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead. from typing import List , Tuple Card = Tuple [ str , str ] Deck = List [ Card ] def deal_hands ( deck : Deck ) -> Tuple [ Deck , Deck , Deck , Deck ]: \"\"\"Deal the cards in the deck into four hands\"\"\" return ( deck [ 0 :: 4 ], deck [ 1 :: 4 ], deck [ 2 :: 4 ], deck [ 3 :: 4 ])","title":"Type aliases"},{"location":"coding/python/type_hints/#generic-types","text":"This can be useful when you need lists of subclasses or optional list of subclasses. The expected behavior doesn't work. Entity = TypeVar ( 'Entity' , model . Project , model . Tag , model . Task ) Entities = List [ Entity ] If you just want to specify any children of a parent class , use: from .model import Entity as EntityModel Entity = TypeVar ( 'Entity' , bound = EntityModel ) Try to use TypeVar instead of Union of the different types, as it's able to deduce better the type of the return value of a function. def do_something ( entity : Entity ) -> Entity : return Entity If you use TypeVar , if you call the function with a type Card , it will know that the result is of type Card , if you use Union , even if you call it with Card the return value will be Union[Card,Deck] . More generally, Generics can be parameterized by using a new factory available in typing called TypeVar . Example: from typing import Sequence , TypeVar T = TypeVar ( 'T' ) # Declare type variable def first ( l : Sequence [ T ]) -> T : # Generic function return l [ 0 ] In this case the contract is that the returned value is consistent with the elements held by the collection. A TypeVar() expression must always directly be assigned to a variable (it should not be used as part of a larger expression). The argument to TypeVar() must be a string equal to the variable name to which it is assigned. Type variables must not be redefined. TypeVar supports constraining parametric types to a fixed set of possible types (note: those types cannot be parameterized by type variables). For example, we can define a type variable that ranges over just str and bytes. By default, a type variable ranges over all possible types. Example of constraining a type variable: from typing import TypeVar , Text AnyStr = TypeVar ( 'AnyStr' , Text , bytes ) def concat ( x : AnyStr , y : AnyStr ) -> AnyStr : return x + y","title":"Generic types"},{"location":"coding/python/type_hints/#specify-the-type-of-the-class-in-its-method-and-attributes","text":"If you are using Python 3.10 or later, it just works. Python 3.7 introduces PEP 563: postponed evaluation of annotations. A module that uses the future statement from __future__ import annotations to store annotations as strings automatically: from __future__ import annotations class Position : def __add__ ( self , other : Position ) -> Position : ... But pyflakes will still complain, so I've used strings. from __future__ import annotations class Position : def __add__ ( self , other : 'Position' ) -> 'Position' : ...","title":"Specify the type of the class in it's method and attributes"},{"location":"coding/python/type_hints/#using-mypy-with-an-existing-codebase","text":"These steps will get you started with mypy on an existing codebase: Start small : Pick a subset of your codebase to run mypy on, without any annotations. You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding # type: ignore comments to silence errors you don\u2019t want to fix now. Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors : Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this: python3 - m pip install mypy == 0.600 # Pinned version avoids surprises scripts / mypy # Runs with the correct options * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions: Developers should add annotations for any new code. It\u2019s also encouraged to write annotations when you change existing code.","title":"Using mypy with an existing codebase"},{"location":"coding/python/type_hints/#reveal-the-type-of-an-expression","text":"You can use reveal_type(expr) to ask mypy to display the inferred static type of an expression. This can be useful when you don't quite understand how mypy handles a particular piece of code. Example: reveal_type (( 1 , 'hello' )) # Revealed type is 'Tuple[builtins.int, builtins.str]' You can also use reveal_locals() at any line in a file to see the types of all local variables at once. Example: a = 1 b = 'one' reveal_locals () # Revealed local types are: # a: builtins.int # b: builtins.str reveal_type and reveal_locals are only understood by mypy and don't exist in Python. If you try to run your program, you\u2019ll have to remove any reveal_type and reveal_locals calls before you can run your code. Both are always available and you don't need to import them.","title":"Reveal the type of an expression"},{"location":"coding/python/type_hints/#solve-cyclic-imports-due-to-typing","text":"You can use a conditional import that is only active in \"type hinting mode\", but doesn't interfere at run time. The typing.TYPE_CHECKING constant makes this easily possible. For example: # thing.py from typing import TYPE_CHECKING if TYPE_CHECKING : from connection import ApiConnection class Thing : def __init__ ( self , connection : 'ApiConnection' ): self . _conn = connection The code will now execute properly as there is no circular import issue anymore. Type hinting tools on the other hand should still be able to resolve the ApiConnection type hint in Thing.__init__ .","title":"Solve cyclic imports due to typing"},{"location":"coding/python/type_hints/#make-your-library-compatible-with-mypy","text":"PEP 561 notes three main ways to distribute type information. The first is a package that has only inline type annotations in the code itself. The second is a package that ships stub files with type information alongside the runtime code. The third method, also known as a \u201cstub only package\u201d is a package that ships type information for a package separately as stub files. If you would like to publish a library package to a package repository (e.g. PyPI) for either internal or external use in type checking, packages that supply type information via type comments or annotations in the code should put a py.typed file in their package directory. For example, with a directory structure as follows setup.py package_a/ __init__.py lib.py py.typed the setup.py might look like: from distutils.core import setup setup ( name = \"SuperPackageA\" , author = \"Me\" , version = \"0.1\" , package_data = { \"package_a\" : [ \"py.typed\" ]}, packages = [ \"package_a\" ] ) If you use setuptools, you must pass the option zip_safe=False to setup() , or mypy will not be able to find the installed package.","title":"Make your library compatible with mypy"},{"location":"coding/python/type_hints/#reference","text":"Bernat gabor article on the state of type hints in python Real python article on type checking","title":"Reference"},{"location":"coding/python/yoyo/","text":"Yoyo is a database schema migration tool. Migrations are written as SQL files or Python scripts that define a list of migration steps. Installation \u2691 pip install yoyo-migrations Usage \u2691 Command line \u2691 Start a new migration: yoyo new ./migrations -m \"Add column to foo\" Apply migrations from directory migrations to a PostgreSQL database: yoyo apply --database postgresql://scott:tiger@localhost/db ./migrations Rollback migrations previously applied to a MySQL database: yoyo rollback --database mysql://scott:tiger@localhost/database ./migrations Reapply (ie rollback then apply again) migrations to a SQLite database at location /home/sheila/important.db: yoyo reapply --database sqlite:////home/sheila/important.db ./migrations List available migrations: yoyo list --database sqlite:////home/sheila/important.db ./migrations By default, yoyo-migrations starts in an interactive mode, prompting you for each migration file before applying it, making it easy to preview which migrations to apply and rollback. Connecting to the database \u2691 Database connections are specified using a URL. Examples: # SQLite: use 4 slashes for an absolute database path on unix like platforms database = sqlite : //// home / user / mydb . sqlite # SQLite: use 3 slashes for a relative path database = sqlite : /// mydb . sqlite # SQLite: absolute path on Windows. database = sqlite : /// c : \\ home \\ user \\ mydb . sqlite # MySQL: Network database connection database = mysql : // scott : tiger @localhost / mydatabase # MySQL: unix socket connection database = mysql : // scott : tiger @/ mydatabase ? unix_socket =/ tmp / mysql . sock # MySQL with the MySQLdb driver (instead of pymysql) database = mysql + mysqldb : // scott : tiger @localhost / mydatabase # MySQL with SSL/TLS enabled database = mysql + mysqldb : // scott : tiger @localhost / mydatabase ? ssl = yes & sslca =/ path / to / cert # PostgreSQL: database connection database = postgresql : // scott : tiger @localhost / mydatabase # PostgreSQL: unix socket connection database = postgresql : // scott : tiger @/ mydatabase # PostgreSQL: changing the schema (via set search_path) database = postgresql : // scott : tiger @/ mydatabase ? schema = some_schema You can specify your database username and password either as part of the database connection string on the command line (exposing your database password in the process list) or in a configuration file where other users may be able to read it. The -p or --prompt-password flag causes yoyo to prompt for a password, helping prevent your credentials from being leaked. Migration files \u2691 The migrations directory contains a series of migration scripts. Each migration script is a Python (.py) or SQL file (.sql). The name of each file without the extension is used as the migration\u2019s unique identifier. Migrations scripts are run in dependency then filename order. Each migration file is run in a single transaction where this is supported by the database. Yoyo creates tables in your target database to track which migrations have been applied. Migrations as Python scripts \u2691 A migration script written in Python has the following structure: # # file: migrations/0001_create_foo.py # from yoyo import step __depends__ = { \"0000.initial-schema\" } steps = [ step ( \"CREATE TABLE foo (id INT, bar VARCHAR(20), PRIMARY KEY (id))\" , \"DROP TABLE foo\" , ), step ( \"ALTER TABLE foo ADD COLUMN baz INT NOT NULL\" ) ] The step function may take up to 3 arguments: apply : an SQL query (or Python function, see below) to apply the migration step. rollback : (optional) an SQL query (or Python function) to rollback the migration step. ignore_errors : (optional, one of apply , rollback or all ) causes yoyo to ignore database errors in either the apply stage, rollback stage or both. Migrations may declare dependencies on other migrations via the __depends__ attribute: If you use the yoyo new command the __depends__ attribute will be auto populated for you. Migration steps as Python functions \u2691 If SQL is not flexible enough, you may supply a Python function as either or both of the apply or rollback arguments of step. Each function should take a database connection as its only argument: # # file: migrations/0001_create_foo.py # from yoyo import step def apply_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to perform the migration ) def rollback_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to undo the above ) steps = [ step ( apply_step , rollback_step ) ] Post-apply hook \u2691 It can be useful to have a script that is run after every successful migration. For example you could use this to update database permissions or re-create views. To do this, create a special migration file called post-apply.py . Configuration file \u2691 Yoyo looks for a configuration file named yoyo.ini in the current working directory or any ancestor directory. The configuration file may contain the following options: [DEFAULT] # List of migration source directories. \"%(here)s\" is expanded to the # full path of the directory containing this ini file. sources = %(here)s/migrations %(here)s/lib/module/migrations # Target database database = postgresql://scott:tiger@localhost/mydb # Verbosity level. Goes from 0 (least verbose) to 3 (most verbose) verbosity = 3 # Disable interactive features batch_mode = on # Editor to use when starting new migrations # \"{}\" is expanded to the filename of the new migration editor = /usr/local/bin/vim -f {} # An arbitrary command to run after a migration has been created # \"{}\" is expanded to the filename of the new migration post_create_command = hg add {} # A prefix to use for generated migration filenames prefix = myproject_ Calling Yoyo from Python code \u2691 The following example shows how to apply migrations from inside python code: from yoyo import read_migrations from yoyo import get_backend backend = get_backend ( 'postgres://myuser@localhost/mydatabase' ) migrations = read_migrations ( 'path/to/migrations' ) with backend . lock (): # Apply any outstanding migrations backend . apply_migrations ( backend . to_apply ( migrations )) # Rollback all migrations backend . rollback_migrations ( backend . to_rollback ( migrations )) References \u2691 Docs Source Issue Tracker Mailing list","title":"Yoyo"},{"location":"coding/python/yoyo/#installation","text":"pip install yoyo-migrations","title":"Installation"},{"location":"coding/python/yoyo/#usage","text":"","title":"Usage"},{"location":"coding/python/yoyo/#command-line","text":"Start a new migration: yoyo new ./migrations -m \"Add column to foo\" Apply migrations from directory migrations to a PostgreSQL database: yoyo apply --database postgresql://scott:tiger@localhost/db ./migrations Rollback migrations previously applied to a MySQL database: yoyo rollback --database mysql://scott:tiger@localhost/database ./migrations Reapply (ie rollback then apply again) migrations to a SQLite database at location /home/sheila/important.db: yoyo reapply --database sqlite:////home/sheila/important.db ./migrations List available migrations: yoyo list --database sqlite:////home/sheila/important.db ./migrations By default, yoyo-migrations starts in an interactive mode, prompting you for each migration file before applying it, making it easy to preview which migrations to apply and rollback.","title":"Command line"},{"location":"coding/python/yoyo/#connecting-to-the-database","text":"Database connections are specified using a URL. Examples: # SQLite: use 4 slashes for an absolute database path on unix like platforms database = sqlite : //// home / user / mydb . sqlite # SQLite: use 3 slashes for a relative path database = sqlite : /// mydb . sqlite # SQLite: absolute path on Windows. database = sqlite : /// c : \\ home \\ user \\ mydb . sqlite # MySQL: Network database connection database = mysql : // scott : tiger @localhost / mydatabase # MySQL: unix socket connection database = mysql : // scott : tiger @/ mydatabase ? unix_socket =/ tmp / mysql . sock # MySQL with the MySQLdb driver (instead of pymysql) database = mysql + mysqldb : // scott : tiger @localhost / mydatabase # MySQL with SSL/TLS enabled database = mysql + mysqldb : // scott : tiger @localhost / mydatabase ? ssl = yes & sslca =/ path / to / cert # PostgreSQL: database connection database = postgresql : // scott : tiger @localhost / mydatabase # PostgreSQL: unix socket connection database = postgresql : // scott : tiger @/ mydatabase # PostgreSQL: changing the schema (via set search_path) database = postgresql : // scott : tiger @/ mydatabase ? schema = some_schema You can specify your database username and password either as part of the database connection string on the command line (exposing your database password in the process list) or in a configuration file where other users may be able to read it. The -p or --prompt-password flag causes yoyo to prompt for a password, helping prevent your credentials from being leaked.","title":"Connecting to the database"},{"location":"coding/python/yoyo/#migration-files","text":"The migrations directory contains a series of migration scripts. Each migration script is a Python (.py) or SQL file (.sql). The name of each file without the extension is used as the migration\u2019s unique identifier. Migrations scripts are run in dependency then filename order. Each migration file is run in a single transaction where this is supported by the database. Yoyo creates tables in your target database to track which migrations have been applied.","title":"Migration files"},{"location":"coding/python/yoyo/#migrations-as-python-scripts","text":"A migration script written in Python has the following structure: # # file: migrations/0001_create_foo.py # from yoyo import step __depends__ = { \"0000.initial-schema\" } steps = [ step ( \"CREATE TABLE foo (id INT, bar VARCHAR(20), PRIMARY KEY (id))\" , \"DROP TABLE foo\" , ), step ( \"ALTER TABLE foo ADD COLUMN baz INT NOT NULL\" ) ] The step function may take up to 3 arguments: apply : an SQL query (or Python function, see below) to apply the migration step. rollback : (optional) an SQL query (or Python function) to rollback the migration step. ignore_errors : (optional, one of apply , rollback or all ) causes yoyo to ignore database errors in either the apply stage, rollback stage or both. Migrations may declare dependencies on other migrations via the __depends__ attribute: If you use the yoyo new command the __depends__ attribute will be auto populated for you.","title":"Migrations as Python scripts"},{"location":"coding/python/yoyo/#migration-steps-as-python-functions","text":"If SQL is not flexible enough, you may supply a Python function as either or both of the apply or rollback arguments of step. Each function should take a database connection as its only argument: # # file: migrations/0001_create_foo.py # from yoyo import step def apply_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to perform the migration ) def rollback_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to undo the above ) steps = [ step ( apply_step , rollback_step ) ]","title":"Migration steps as Python functions"},{"location":"coding/python/yoyo/#post-apply-hook","text":"It can be useful to have a script that is run after every successful migration. For example you could use this to update database permissions or re-create views. To do this, create a special migration file called post-apply.py .","title":"Post-apply hook"},{"location":"coding/python/yoyo/#configuration-file","text":"Yoyo looks for a configuration file named yoyo.ini in the current working directory or any ancestor directory. The configuration file may contain the following options: [DEFAULT] # List of migration source directories. \"%(here)s\" is expanded to the # full path of the directory containing this ini file. sources = %(here)s/migrations %(here)s/lib/module/migrations # Target database database = postgresql://scott:tiger@localhost/mydb # Verbosity level. Goes from 0 (least verbose) to 3 (most verbose) verbosity = 3 # Disable interactive features batch_mode = on # Editor to use when starting new migrations # \"{}\" is expanded to the filename of the new migration editor = /usr/local/bin/vim -f {} # An arbitrary command to run after a migration has been created # \"{}\" is expanded to the filename of the new migration post_create_command = hg add {} # A prefix to use for generated migration filenames prefix = myproject_","title":"Configuration file"},{"location":"coding/python/yoyo/#calling-yoyo-from-python-code","text":"The following example shows how to apply migrations from inside python code: from yoyo import read_migrations from yoyo import get_backend backend = get_backend ( 'postgres://myuser@localhost/mydatabase' ) migrations = read_migrations ( 'path/to/migrations' ) with backend . lock (): # Apply any outstanding migrations backend . apply_migrations ( backend . to_apply ( migrations )) # Rollback all migrations backend . rollback_migrations ( backend . to_rollback ( migrations ))","title":"Calling Yoyo from Python code"},{"location":"coding/python/yoyo/#references","text":"Docs Source Issue Tracker Mailing list","title":"References"},{"location":"coding/python/python_project_template/python_cli_template/","text":"Create the tests directories mkdir -p tests/unit touch tests/__init__.py touch tests/unit/__init__.py Create the program module structure mkdir {{ program_name }} Create the program setup.py file from setuptools import find_packages , setup # Get the version from drode/version.py without importing the package exec ( compile ( open ( '{{ program_name }}/version.py' ) . read (), '{{ program_name }}/version.py' , 'exec' )) setup ( name = '{{ program_name }}' , version = __version__ , # noqa: F821 description = '{{ program_description }}' , author = '{{ author }}' , author_email = '{{ author_email }}' , license = 'GPLv3' , long_description = open ( 'README.md' ) . read (), packages = find_packages ( exclude = ( 'tests' ,)), package_data = { '{{ program_name }}' : [ 'migrations/*' , 'migrations/versions/*' , ]}, entry_points = { 'console_scripts' : [ '{{ program_name }} = {{ program_name }}:main' ]}, install_requires = [ ] ) Remember to fill up the install_requirements with the dependencies that need to be installed at installation time. Create the {{ program_name }}/version.py file with the following contents: __version__ = '1.0.0' This ugly way of loading the __version__ was stolen from youtube-dl, it loads and executes the version.py without loading the whole module. Solutions like from {{ program_name }}.version import __version__ will fail as it tries to load the whole module. Defining it in the setup.py file doesn't work either if you need to load the version in your program code. from setup.py import __version__ will also fail. The only problem with this approach is that as the __version__ is not defined in the code it will raise a Flake8 error, therefore the # noqa: F821 in the setup.py code. Create the requirements.txt file. It should contain the install_requirements in addition to the testing requirements such as: pytest pytest-cov Configure SQLAlchemy for projects without flask","title":"Command-line Project Template"},{"location":"coding/python/python_project_template/python_docker/","text":"Docker is a popular way to distribute applications. Assuming that you've set all required dependencies in the setup.py , we're going to create an image with these properties: Run by an unprivileged user : Create an unprivileged user with permissions to run our program. Robust to vulnerabilities: Don't use Alpine as it's known to react slow to new vulnerabilities. Use a base of Debian instead. Smallest possible: Use Docker multi build step. Create a builder Docker that will run pip install and copies the required executables to the final image. FROM python:3.8-slim-buster as base FROM base as builder RUN python -m venv /opt/venv # Make sure we use the virtualenv: ENV PATH = \"/opt/venv/bin: $PATH \" COPY . /app WORKDIR /app RUN pip install . FROM base COPY --from = builder /opt/venv /opt/venv RUN useradd -m myapp WORKDIR /home/myapp # Copy the required directories for your program to work. COPY --from = builder /root/.local/share/myapp /home/myapp/.local/share/myapp COPY --from = builder /app/myapp /home/myapp/myapp RUN chown -R myapp:myapp /home/myapp/.local USER myapp ENV PATH = \"/opt/venv/bin: $PATH \" ENTRYPOINT [ \"/opt/venv/bin/myapp\" ] If we need to use it with MariaDB or with Redis, the easiest way is to use docker-compose . version : '3.8' services : myapp : image : myapp:latest restart : always links : - db depends_on : - db environment : - AIRSS_DATABASE_URL=mysql+pymysql://myapp:supersecurepassword@db/myapp db : image : mariadb:latest restart : always environment : - MYSQL_USER=myapp - MYSQL_PASSWORD=supersecurepassword - MYSQL_DATABASE=myapp - MYSQL_ALLOW_EMPTY_PASSWORD=yes ports : - 3306:3306 command : - '--character-set-server=utf8mb4' - '--collation-server=utf8mb4_unicode_ci' volumes : - /data/myapp/mariadb:/var/lib/mysql The depends_on flag is not enough to ensure that the database is up when our application tries to connect. So we need to use external programs like wait-for-it . To use it, change the earlier Dockerfile to match these lines: ... FROM base RUN apt-get update && apt-get install -y \\ wait-for-it \\ && rm -rf /var/lib/apt/lists/* ... ENTRYPOINT [ \"/home/myapp/entrypoint.sh\" ] Where entrypoint.sh is something like: #!/bin/bash # Wait for the database to be up if [[ -n $DATABASE_URL ]] ; then wait-for-it db:3306 fi # Execute database migrations /opt/venv/bin/myapp install # Enter in daemon mode /opt/venv/bin/myapp daemon Remember to add the permissions to run the script: chmod +x entrypoint.sh","title":"Configure Docker to host the application"},{"location":"coding/python/python_project_template/python_docs/","text":"It's important to create the documentation at the same time as you code your project, otherwise you won't ever do it or you'll die trying. Right now I use mkdocs with Github Pages for the documentation. Follow the steps under Installation to configure it. I've automated the creation of the mkdocs site in this cookiecutter template .","title":"Create the documentation repository"},{"location":"coding/python/python_project_template/python_flask_template/","text":"Flask is very flexible when it comes to define the project layout, as a result, there are several different approaches, which can be confusing if you're building your first application. Follow this template if you want an application that meets these requirements: Use SQLAlchemy as ORM. Use pytest as testing framework (instead of unittest). Sets a robust foundation for application growth. Set a clear defined project structure that can be used for frontend applications as well as backend APIs. Microservice friendly. I've crafted this template after studying the following projects: Miguel's Flask mega tutorial ( code ). Greb Obinna Flask-RESTPlus tutorial ( code ). Abhinav Suri Flask tutorial ( code ). Patrick's software blog project layout and pytest definition ( code ). Jaime Buelta Hands On Docker for Microservices with Python ( code ). Each has it's strengths and weaknesses: Project Alembic Pytest Complex app Friendly layout Strong points Miguel True False True False Has a book explaining the code Greb False False False False flask-restplus Abhinav True False True True flask-base Patrick False True False True pytest Jaime False True True False Microservices, CI, Kubernetes, logging, metrics I'm going to start with Abhinav base layout as it's the most clear and complete. Furthermore, it's based in flask-base , a simple Flask boilerplate app with SQLAlchemy, Redis, User Authentication, and more. Which can be used directly to start a frontend flask project. I won't use it for a backend API though. With that base layout, I'm going to take Patrick's pytest layout to configure the tests using pytest-flask , Greb flask-restplus code to create the API and Miguel's book to glue everything together. Finally, I'll follow Jaime's book to merge the different microservices into an integrated project. As well as defining the deployment process, CI definition, logging, metrics and integration with Kubernetes.","title":"Flask Project Template"},{"location":"coding/python/python_project_template/python_microservices_template/","text":"Follow this template if you want to build a project that meets these requirements: Based on Python Flask microservices. Easily expandable. Tested by unit, functional and integration tests through continuous integration pipelines. Deployed through uWSGI and Nginx dockers. Orchestrated through docker-compose or Kubernetes. Defining the project layout of a flask application is not easy , even less one with several","title":"Microservices Project Template"},{"location":"coding/python/python_project_template/python_sqlalchemy_mariadb/","text":"I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . To use Mysql you'll need to first install (or add to your requirements) pymysql : pip install pymysql The url to connect to the database will be: 'mysql+pymysql:// {} : {} @ {} : {} / {} ' . format ( DB_USER , DB_PASS , DB_HOST , DB_PORT , DATABASE ) It's probable that you'll need to use UTF8 with multi byte , otherwise the addition of some strings into the database will fail. I've tried adding it to the database url without success. So I've modified the MariaDB Docker-compose section to use that character and collation set: services : db : image : mariadb:latest restart : always environment : - MYSQL_USER=xxxx - MYSQL_PASSWORD=xxxx - MYSQL_DATABASE=xxxx - MYSQL_ALLOW_EMPTY_PASSWORD=yes ports : - 3306:3306 command : - '--character-set-server=utf8mb4' - '--collation-server=utf8mb4_unicode_ci'","title":"Configure SQLAlchemy to use the MariaDB/Mysql backend"},{"location":"coding/python/python_project_template/python_sqlalchemy_without_flask/","text":"I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Install Alembic : pip install alembic It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations Create the basic models.py file under the project code. \"\"\" Module to store the models. Classes: Class_name: Class description. ... \"\"\" import os from sqlalchemy import \\ create_engine , \\ Column , \\ Integer from sqlalchemy.ext.declarative import declarative_base db_path = os . path . expanduser ( '{{ path_to_sqlite_file }}' ) engine = create_engine ( os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or 'sqlite:///' + db_path ) Base = declarative_base ( bind = engine ) class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) Create the migrations/env.py file as specified in the alembic article . Create the first alembic revision. alembic \\ -c {{ program_name }} /migrations/alembic.ini \\ revision \\ --autogenerate \\ -m \"Initial schema\" Set up the testing environment for SQLAlchemy","title":"Configure SQLAlchemy for projects without flask"},{"location":"coding/react/react/","text":"React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called \u201ccomponents\u201d. Set up a new project \u2691 Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start . Start a React + Flask project \u2691 Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run The basics \u2691 Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components. Pass data between components \u2691 Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... } Use of the state \u2691 React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); } Share the state between parent and children \u2691 To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components. Handling data change \u2691 There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering. Function components \u2691 Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); } Miscellaneous \u2691 List rendering \u2691 When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key . Links \u2691 React tutorial Awesome React Awesome React components Responsive React \u2691 Responsive react Responsive websites without css react-responsive library With Flask \u2691 How to create a react + flask project How to deploy a react + flask project","title":"React"},{"location":"coding/react/react/#set-up-a-new-project","text":"Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start .","title":"Set up a new project"},{"location":"coding/react/react/#start-a-react-flask-project","text":"Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run","title":"Start a React + Flask project"},{"location":"coding/react/react/#the-basics","text":"Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components.","title":"The basics"},{"location":"coding/react/react/#pass-data-between-components","text":"Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... }","title":"Pass data between components"},{"location":"coding/react/react/#use-of-the-state","text":"React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); }","title":"Use of the state"},{"location":"coding/react/react/#share-the-state-between-parent-and-children","text":"To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components.","title":"Share the state between parent and children"},{"location":"coding/react/react/#handling-data-change","text":"There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering.","title":"Handling data change"},{"location":"coding/react/react/#function-components","text":"Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); }","title":"Function components"},{"location":"coding/react/react/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"coding/react/react/#list-rendering","text":"When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key .","title":"List rendering"},{"location":"coding/react/react/#links","text":"React tutorial Awesome React Awesome React components","title":"Links"},{"location":"coding/react/react/#responsive-react","text":"Responsive react Responsive websites without css react-responsive library","title":"Responsive React"},{"location":"coding/react/react/#with-flask","text":"How to create a react + flask project How to deploy a react + flask project","title":"With Flask"},{"location":"coding/sql/sql/","text":"SQL Data Types \u2691 String data types: VARCHAR(size) : A variable length string (can contain letters, numbers, and special characters). The size parameter specifies the maximum column length in characters - can be from 0 to 65535. TEXT(size) : Holds a string with a maximum length of 65,535 bytes. MEDIUMTEXT : Holds a string with a maximum length of 16,777,215 characters. LONGTEXT : Holds a string with a maximum length of 4,294,967,295 characters. ENUM(val1, val2, val3, ...) : A string object that can have only one value, chosen from a list of possible values. You can list up to 65535 values in an ENUM list. If a value is inserted that is not in the list, a blank value will be inserted. The values are sorted in the order you enter them. SET(val1, val2, val3, ...) : A string object that can have 0 or more values, chosen from a list of possible values. You can list up to 64 values in a SET list. Numeric data types: BOOL or BOOLEAN : Zero is considered as false, nonzero values are considered as true. TINYINT(size) : A very small integer. Signed range is from -128 to 127. Unsigned range is from 0 to 255. The size parameter specifies the maximum display width (which is 255). SMALLINT(size) : A small integer. Signed range is from -32768 to 32767. Unsigned range is from 0 to 65535. The size parameter specifies the maximum display width (which is 255). INT(size) : A medium integer. Signed range is from -2147483648 to 2147483647. Unsigned range is from 0 to 4294967295. The size parameter specifies the maximum display width (which is 255). FLOAT(p) : A floating point number. MySQL uses the p value to determine whether to use FLOAT or DOUBLE for the resulting data type. If p is from 0 to 24, the data type becomes FLOAT() . If p is from 25 to 53, the data type becomes DOUBLE() . Date and time data types: DATE : A date. Format: YYYY-MM-DD. The supported range is from '1000-01-01' to '9999-12-31'. DATETIME(fsp) : A date and time combination. Format: YYYY-MM-DD hh:mm:ss . The supported range is from 1000-01-01 00:00:00 to 9999-12-31 23:59:59 . Adding DEFAULT and ON UPDATE in the column definition to get automatic initialization and updating to the current date and time. Table relationships \u2691 One to One \u2691 A one-to-one relationship between two entities exists when a particular entity instance exists in one table, and it can have only one associated entity instance in another table. For example, a user can have only one address, and an address belongs to only one user. This sort of relationship is implemented by setting the PRIMARY KEY of the users table ( id ) as both the FOREIGN KEY and PRIMARY KEY of the addresses table. CREATE TABLE addresses ( user_id int , -- Both a primary and foreign key street varchar ( 30 ) NOT NULL , city varchar ( 30 ) NOT NULL , state varchar ( 30 ) NOT NULL , PRIMARY KEY ( user_id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE ); The ON DELETE CASCADE clause of the FOREIGN_KEY definition instructs the database to delete the referencing row if the referenced row is deleted. There are alternatives to CASCADE such as SET NULL or SET DEFAULT which instead of deleting the referencing row will set a new value in the appropriate column for that row. One to many \u2691 A one-to-many relationship exists between two entities if an entity instance in one of the tables can be associated with multiple records (entity instances) in the other table. The opposite relationship does not exist; that is, each entity instance in the second table can only be associated with one entity instance in the first table. For example, a review belongs to only one book while a book has many reviews. CREATE TABLE books ( id serial , title varchar ( 100 ) NOT NULL , author varchar ( 100 ) NOT NULL , published_date timestamp NOT NULL , isbn char ( 12 ), PRIMARY KEY ( id ), UNIQUE ( isbn ) ); /* one to many: Book has many reviews */ CREATE TABLE reviews ( id serial , book_id integer NOT NULL , reviewer_name varchar ( 255 ), content varchar ( 255 ), rating integer , published_date timestamp DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( id ), FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE ); Unlike our addresses table, the PRIMARY KEY and FOREIGN KEY reference different columns, id and book_id respectively. This means that the FOREIGN KEY column, book_id is not bound by the UNIQUE constraint of our PRIMARY KEY and so the same value from the id column of the books table can appear in this column more than once. In other words a book can have many reviews. Many to many \u2691 A many-to-many relationship exists between two entities if for one entity instance there may be multiple records in the other table, and vice versa. For example, a user can check out many books, and a book can be checked out by many users. In order to implement this sort of relationship we need to introduce a third, cross-reference, table. This table holds the relationship between the two entities, by having two FOREIGN KEY s, each of which references the PRIMARY KEY of one of the tables for which we want to create this relationship. We already have our books and users tables, so we just need to create the cross-reference table: checkouts . CREATE TABLE checkouts ( id serial , user_id int NOT NULL , book_id int NOT NULL , checkout_date timestamp , return_date timestamp , PRIMARY KEY ( id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE , FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE );","title":"SQL"},{"location":"coding/sql/sql/#sql-data-types","text":"String data types: VARCHAR(size) : A variable length string (can contain letters, numbers, and special characters). The size parameter specifies the maximum column length in characters - can be from 0 to 65535. TEXT(size) : Holds a string with a maximum length of 65,535 bytes. MEDIUMTEXT : Holds a string with a maximum length of 16,777,215 characters. LONGTEXT : Holds a string with a maximum length of 4,294,967,295 characters. ENUM(val1, val2, val3, ...) : A string object that can have only one value, chosen from a list of possible values. You can list up to 65535 values in an ENUM list. If a value is inserted that is not in the list, a blank value will be inserted. The values are sorted in the order you enter them. SET(val1, val2, val3, ...) : A string object that can have 0 or more values, chosen from a list of possible values. You can list up to 64 values in a SET list. Numeric data types: BOOL or BOOLEAN : Zero is considered as false, nonzero values are considered as true. TINYINT(size) : A very small integer. Signed range is from -128 to 127. Unsigned range is from 0 to 255. The size parameter specifies the maximum display width (which is 255). SMALLINT(size) : A small integer. Signed range is from -32768 to 32767. Unsigned range is from 0 to 65535. The size parameter specifies the maximum display width (which is 255). INT(size) : A medium integer. Signed range is from -2147483648 to 2147483647. Unsigned range is from 0 to 4294967295. The size parameter specifies the maximum display width (which is 255). FLOAT(p) : A floating point number. MySQL uses the p value to determine whether to use FLOAT or DOUBLE for the resulting data type. If p is from 0 to 24, the data type becomes FLOAT() . If p is from 25 to 53, the data type becomes DOUBLE() . Date and time data types: DATE : A date. Format: YYYY-MM-DD. The supported range is from '1000-01-01' to '9999-12-31'. DATETIME(fsp) : A date and time combination. Format: YYYY-MM-DD hh:mm:ss . The supported range is from 1000-01-01 00:00:00 to 9999-12-31 23:59:59 . Adding DEFAULT and ON UPDATE in the column definition to get automatic initialization and updating to the current date and time.","title":"SQL Data Types"},{"location":"coding/sql/sql/#table-relationships","text":"","title":"Table relationships"},{"location":"coding/sql/sql/#one-to-one","text":"A one-to-one relationship between two entities exists when a particular entity instance exists in one table, and it can have only one associated entity instance in another table. For example, a user can have only one address, and an address belongs to only one user. This sort of relationship is implemented by setting the PRIMARY KEY of the users table ( id ) as both the FOREIGN KEY and PRIMARY KEY of the addresses table. CREATE TABLE addresses ( user_id int , -- Both a primary and foreign key street varchar ( 30 ) NOT NULL , city varchar ( 30 ) NOT NULL , state varchar ( 30 ) NOT NULL , PRIMARY KEY ( user_id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE ); The ON DELETE CASCADE clause of the FOREIGN_KEY definition instructs the database to delete the referencing row if the referenced row is deleted. There are alternatives to CASCADE such as SET NULL or SET DEFAULT which instead of deleting the referencing row will set a new value in the appropriate column for that row.","title":"One to One"},{"location":"coding/sql/sql/#one-to-many","text":"A one-to-many relationship exists between two entities if an entity instance in one of the tables can be associated with multiple records (entity instances) in the other table. The opposite relationship does not exist; that is, each entity instance in the second table can only be associated with one entity instance in the first table. For example, a review belongs to only one book while a book has many reviews. CREATE TABLE books ( id serial , title varchar ( 100 ) NOT NULL , author varchar ( 100 ) NOT NULL , published_date timestamp NOT NULL , isbn char ( 12 ), PRIMARY KEY ( id ), UNIQUE ( isbn ) ); /* one to many: Book has many reviews */ CREATE TABLE reviews ( id serial , book_id integer NOT NULL , reviewer_name varchar ( 255 ), content varchar ( 255 ), rating integer , published_date timestamp DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( id ), FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE ); Unlike our addresses table, the PRIMARY KEY and FOREIGN KEY reference different columns, id and book_id respectively. This means that the FOREIGN KEY column, book_id is not bound by the UNIQUE constraint of our PRIMARY KEY and so the same value from the id column of the books table can appear in this column more than once. In other words a book can have many reviews.","title":"One to many"},{"location":"coding/sql/sql/#many-to-many","text":"A many-to-many relationship exists between two entities if for one entity instance there may be multiple records in the other table, and vice versa. For example, a user can check out many books, and a book can be checked out by many users. In order to implement this sort of relationship we need to introduce a third, cross-reference, table. This table holds the relationship between the two entities, by having two FOREIGN KEY s, each of which references the PRIMARY KEY of one of the tables for which we want to create this relationship. We already have our books and users tables, so we just need to create the cross-reference table: checkouts . CREATE TABLE checkouts ( id serial , user_id int NOT NULL , book_id int NOT NULL , checkout_date timestamp , return_date timestamp , PRIMARY KEY ( id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE , FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE );","title":"Many to many"},{"location":"coding/yaml/yaml/","text":"YAML (a recursive acronym for \"YAML Ain't Markup Language\") is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted. YAML targets many of the same communications applications as Extensible Markup Language (XML) but has a minimal syntax which intentionally differs from SGML. It uses both Python-style indentation to indicate nesting, and a more compact format that uses [...] for lists and {...} for maps making YAML 1.2 a superset of JSON. Break long lines \u2691 Use > most of the time: interior line breaks are stripped out, although you get one at the end: key : > Your long string here. Use | if you want those line breaks to be preserved as \\n (for instance, embedded markdown with paragraphs): key : | ### Heading * Bullet * Points Use >- or |- instead if you don't want a line break appended at the end. Use \"...\" if you need to split lines in the middle of words or want to literally type line breaks as \\n : key : \"Antidisestab\\ lishmentarianism.\\n\\nGet on it.\" YAML is crazy.","title":"YAML"},{"location":"coding/yaml/yaml/#break-long-lines","text":"Use > most of the time: interior line breaks are stripped out, although you get one at the end: key : > Your long string here. Use | if you want those line breaks to be preserved as \\n (for instance, embedded markdown with paragraphs): key : | ### Heading * Bullet * Points Use >- or |- instead if you don't want a line break appended at the end. Use \"...\" if you need to split lines in the middle of words or want to literally type line breaks as \\n : key : \"Antidisestab\\ lishmentarianism.\\n\\nGet on it.\" YAML is crazy.","title":"Break long lines"},{"location":"dancing/cutting_shapes_basics/","text":"Basic crossing \u2691 8-and : Left legs straight under our body, with the weight in the toes and the foot a little turned with the heel pointing to our right (7 o'clock), right leg lifted up straight to our 3 o'clock with toes pointing down. with knee up front. Weight lies on the left foot. 1 : Right foot crosses before the left one, toes touch the floor first in our longitudinal axes. The left foot pivots over the toes to end up pointing to our 11. Weight is evenly shared between the feet toes. 1-and : Mirrors 8-and . 2 : Mirrors 1 . 2-and : Equal to 8-and . Sources: 1 Wiggle feet \u2691 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot touches the floor behind the hips pointing to 10.5, left foot pivots over the toes to point to 1.5. Weight is even between feet. 1-and : Right foot pivots on the heel till 1.5. Left foot pivots on the toes till 10.5. 2 : Equal 1 . 2-and : Mirrors 1-and . 3 : Equal 1 . 3-and : Equal 1-and . 4 : Equal 1 . Sources: 1 References \u2691 Where to discover more tutorials \u2691 Elements tutorial playlist Anderson Jovani playlist","title":"Cutting Shapes"},{"location":"dancing/cutting_shapes_basics/#basic-crossing","text":"8-and : Left legs straight under our body, with the weight in the toes and the foot a little turned with the heel pointing to our right (7 o'clock), right leg lifted up straight to our 3 o'clock with toes pointing down. with knee up front. Weight lies on the left foot. 1 : Right foot crosses before the left one, toes touch the floor first in our longitudinal axes. The left foot pivots over the toes to end up pointing to our 11. Weight is evenly shared between the feet toes. 1-and : Mirrors 8-and . 2 : Mirrors 1 . 2-and : Equal to 8-and . Sources: 1","title":"Basic crossing"},{"location":"dancing/cutting_shapes_basics/#wiggle-feet","text":"8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot touches the floor behind the hips pointing to 10.5, left foot pivots over the toes to point to 1.5. Weight is even between feet. 1-and : Right foot pivots on the heel till 1.5. Left foot pivots on the toes till 10.5. 2 : Equal 1 . 2-and : Mirrors 1-and . 3 : Equal 1 . 3-and : Equal 1-and . 4 : Equal 1 . Sources: 1","title":"Wiggle feet"},{"location":"dancing/cutting_shapes_basics/#references","text":"","title":"References"},{"location":"dancing/cutting_shapes_basics/#where-to-discover-more-tutorials","text":"Elements tutorial playlist Anderson Jovani playlist","title":"Where to discover more tutorials"},{"location":"dancing/rave_dances/","text":"Shuffle \u2691 Shuffle is a rave dance that developed in the 1980s. Typically performed to electronic music, the dance originated in the Melbourne rave scene, and was popular in the 1980s and 1990s. The dance moves involve a fast heel-and-toe movement or T-step, combined with a variation of the running man coupled with a matching arm action. It's an improvised dance and involves \"shuffling your feet inwards, then outwards, while thrusting your arms up and down, or side to side, in time with the beat\". It seems to be a minority underground dance without official competitions or schools. Given the music, it's origins and the energy it requires, most of the videos are uploaded by young people that reproduce the nasty gender roles that the society embeds in us. So expect to see over sexualized girls and over testosteroned guys, which is sad. Styles \u2691 Melbourne Shuffle \u2691 It's the original shuffle, which was the one that caught my eye several years back with Francis' video: It's \"simple and straightforward\", the base is the running man, with a few kicks and spins sprinkled in from time to time, so there is not much room for variety. It has a low skill floor and a moderate skill ceiling. Usually the dancers wear trousers with a wide end that makes a nice flying visual effect and a hoodie. The percent of read women Melbourne shuffle videos is low. Here is a list of other Melbourne shuffle videos: 1 , 2 , 3 , 4 The malasian and russian styles are, to my untrained eye, similar to the Melbourne, although they've got different T-step. Although in the source post they say that these styles are usually danced between 150 and 200 bpms, I've found that the videos range from 130 to 160 bpms with an average of 150 bpms. Cutting shapes \u2691 It's the most popular right now (at least for the number of videos I've found). The base steps are the Charleston or the heel variants of the running man, which looks less like running. The basic step is used less than in the other styles, filling most of the moves with heel-toe movements, criss-crossings, and spins, so there is little \"shuffling\" in cutting shapes. It has a low skill floor, but an absurdly high skill ceiling. There is a lot of room for variety and you can even mix it up by adding other dance styles to it, as it's very flexible. Usually the dancers wear street clothes and some use shoes with led lights. The percent of read women Cutting shapes shuffle videos is even with read males. Here is a list of other cutting shapes shuffle videos: 1 , 2 Cutting shapes songs are mostly around the 130 bpms. Jumpstyle \u2691 Jumpstyle is an electronic dance style and music genre popular in Western Europe, with existent scenes in Eastern Europe, Australia, and the Americas. It's music is an offspring of tech-trance, hardstyle, gabber and m\u00e1kina. Its tempo is usually between 140 and 150 BPM. Here are another Jumpstyle videos: 1 , 2 .","title":"Rave Dances"},{"location":"dancing/rave_dances/#shuffle","text":"Shuffle is a rave dance that developed in the 1980s. Typically performed to electronic music, the dance originated in the Melbourne rave scene, and was popular in the 1980s and 1990s. The dance moves involve a fast heel-and-toe movement or T-step, combined with a variation of the running man coupled with a matching arm action. It's an improvised dance and involves \"shuffling your feet inwards, then outwards, while thrusting your arms up and down, or side to side, in time with the beat\". It seems to be a minority underground dance without official competitions or schools. Given the music, it's origins and the energy it requires, most of the videos are uploaded by young people that reproduce the nasty gender roles that the society embeds in us. So expect to see over sexualized girls and over testosteroned guys, which is sad.","title":"Shuffle"},{"location":"dancing/rave_dances/#styles","text":"","title":"Styles"},{"location":"dancing/rave_dances/#melbourne-shuffle","text":"It's the original shuffle, which was the one that caught my eye several years back with Francis' video: It's \"simple and straightforward\", the base is the running man, with a few kicks and spins sprinkled in from time to time, so there is not much room for variety. It has a low skill floor and a moderate skill ceiling. Usually the dancers wear trousers with a wide end that makes a nice flying visual effect and a hoodie. The percent of read women Melbourne shuffle videos is low. Here is a list of other Melbourne shuffle videos: 1 , 2 , 3 , 4 The malasian and russian styles are, to my untrained eye, similar to the Melbourne, although they've got different T-step. Although in the source post they say that these styles are usually danced between 150 and 200 bpms, I've found that the videos range from 130 to 160 bpms with an average of 150 bpms.","title":"Melbourne Shuffle"},{"location":"dancing/rave_dances/#cutting-shapes","text":"It's the most popular right now (at least for the number of videos I've found). The base steps are the Charleston or the heel variants of the running man, which looks less like running. The basic step is used less than in the other styles, filling most of the moves with heel-toe movements, criss-crossings, and spins, so there is little \"shuffling\" in cutting shapes. It has a low skill floor, but an absurdly high skill ceiling. There is a lot of room for variety and you can even mix it up by adding other dance styles to it, as it's very flexible. Usually the dancers wear street clothes and some use shoes with led lights. The percent of read women Cutting shapes shuffle videos is even with read males. Here is a list of other cutting shapes shuffle videos: 1 , 2 Cutting shapes songs are mostly around the 130 bpms.","title":"Cutting shapes"},{"location":"dancing/rave_dances/#jumpstyle","text":"Jumpstyle is an electronic dance style and music genre popular in Western Europe, with existent scenes in Eastern Europe, Australia, and the Americas. It's music is an offspring of tech-trance, hardstyle, gabber and m\u00e1kina. Its tempo is usually between 140 and 150 BPM. Here are another Jumpstyle videos: 1 , 2 .","title":"Jumpstyle"},{"location":"dancing/shuffle_basics/","text":"Shuffle steps are usually split each tempo into two phases, songs usually follow a 4/4 structure. To differentiate the steps between tempos I'm going to use the tempo number they come from followed by an -and so the structure will be: 8-and , 1 , 1-and , 2 ... Where the 1 is the phrase starter. Most of the steps don't have names so I've put one that felt it fits. To specify the relative position of body parts, I'm using an imaginary clock, so 12 means upfront, 3 our right, 6 our back and 9 our left. The steps below can be danced with \"slow\" or quick tempos. It's best to learn with the slow ones as they build the muscle memory to be able to skip parts of it as you move to higher tempos. Dance routine \u2691 After several iterations I've come to this routine: Start dancing a bit to low tempo songs, I've got a playlist with increasing tempo songs, the aim is to start warming up and remember what I've learnt the last time. Once you're warmed up dance some songs to your current tempo level. Then put some songs a little above your tempo level and try to speed up your running man and T-Step . Improve this notes with the new discoveries. Analyze a video to extract some steps. Add one or two songs to the shuffle playlist. Prepend the song name with the bpms (it's better to measure them manually as automatic software tends to fail badly). Running man \u2691 The running man is the basic step of Melbourne shuffle, so make sure to master it. 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot kicks forward, left foot slides backward, in a way that each travel the same distance. Weight is evenly shared between the feet. 1-and : Right foot slides backwards, right foot goes up, knee to the front. Weight lies on the right foot. 2 : Mirrors 1 . 2-and : Mirrors 1-and and it's equal to 8-and . Once you feel comfortable with it, start going around the room doing running mans, so you get a grasp of doing it without being stationary. Sources: 1 , 2 The foot going forward can land in the floor flat, heel first or toes first. Quick tempo Running man \u2691 As the tempo speeds up, even if it seems there is no time to stop in the 1-and or 2-and they do manage to do so. To reach there, I'd put a song a little bit quicker than you are comfortable and: Get familiarity with the movement of the foot in the ground: Start with your basic running man stance left foot in front. When the 1 comes, try to reach the 1-and focusing to lift your right knee and stomp in the 2 . Rest a pair of beats and then try the other foot. Keep on doing it till you are comfortable doing this movement. Then try to do 2 complete running mans. Then dance :). For me the most difficult part is to bring back the non dominant leg backwards, to get used to the movement I slided backwards with that leg several times, then do three running mans starting with that leg, rest, and then do another three starting with the other one. T-Step \u2691 The T-Step is other basic step of Melbourne shuffle, used to do lateral movement. Learning the T-Step \u2691 The following instructions are a nice way to get the grasp of the movement, but we'll change them slightly to be able to link it with the other moves. 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot, which looks at 1.5 although the rest of you look at 12. 1 : Right foot kicks sideways towards 3, try to leave the foot at 90 degrees with your leg and that it doesn't wobble, so it's a clean kick. The weight is on the left leg's toes and it pivotes over them till it points to 10.5. Waist ends up turned. 1-and : Right foot is retrieved till the left leg's knee, keeping the base parallel to the ground. Weight is on the left leg's heel and pivotes over it to reach the 1.5 position. Waist is straight now. 2 : Equal to 1 . 2-and : Equal to 1-and . 3 : Right foot goes front, left foot goes back to the running man stance. To move to the other side is the mirror. Keep the leg that holds you a bit bent so you don't harm your knee when twisting. The usual basic pattern of Melbourne shuffle is doing some running mans pointing at 10.5, then doing a side step (like the T-step) to the right, then another running man pointing to 1.5, finishing with another side step to the left. To join the T-Step with the running man, when you are in the 1-and make it the 8-and of the T-Step. Sources: 1 2 Francis T-Step \u2691 I've seen that Francis when finishes his T-Step with the other leg. So instead of moving the right leg in front and slide back the left (if we are going to the right), he: 1-and : Equal to normal T-Step. 2 : When you put your right foot down, shift your weight till it's evenly spread between feet. 2-and : Switch your weight to the right foot, lift the left while you keep on shifting your body weight to your 3. 3 : Left foot touches ground at your 1.5, slide the right back to the running man stance. What I like of this variation is that you conserve the energy, it makes total sense. What it doesn't is that he is dancing with sandals (\u00ac\u00ba-\u00b0)\u00ac . Try to move from side to side with 2 T-Steps in between using this variation. Quick tempo T-Step \u2691 To improve the speed of your T-Steps, I'd put a song a little bit quicker than you are comfortable and: Get familiarity with the movement of the foot in the ground: Start with your right knee up in position 8-and of the T-Step. First focus only to move the feet that you've got in the floor, so when the 1 comes, start pivoting the foot a pair of tempos. Put your right foot down and lift the left knee. Wait 3 or 4 tempos and when the 1 comes, start going to the other side. Once you are comfortable with the movement, repeat the same exercise but doing the kicks. Then instead of finishing with both feet under your body, finish in a running man stance. Then instead of starting with your knee up, start from the running man stance. Future steps \u2691 Sacco kicks at 14:50, 15:49, 17:08 Rocky kicks at 13:26 or 27:58. Rocky version at 1:19, 2:34, 7:29 Recce kicks at 15:56 (explanation) or 27:43 or 28:05. Reverse kick spin at 18:01 or 27:65. Rocky version at 1:18, 2:16, 2:55, 3:26, 3:59, 4:58, 6:24, 8:17, 8:35, 15:00, 15:55 Forward kick spin at 19:33. Rocky version at 2:20, 3:16, 3:47, 8:40, 8:49 Rocky side to side at 15:04, 16:08 Beats per minute progression \u2691 2020-09-07: First time to be able to dance a song at 118bpm to C2C - Happy . References \u2691 More videos to study \u2691 Rocky Shuffle compilation Francis Shuffle compilation Siera Shuffle compilation Bulldog Shuffle compilation","title":"Basics"},{"location":"dancing/shuffle_basics/#dance-routine","text":"After several iterations I've come to this routine: Start dancing a bit to low tempo songs, I've got a playlist with increasing tempo songs, the aim is to start warming up and remember what I've learnt the last time. Once you're warmed up dance some songs to your current tempo level. Then put some songs a little above your tempo level and try to speed up your running man and T-Step . Improve this notes with the new discoveries. Analyze a video to extract some steps. Add one or two songs to the shuffle playlist. Prepend the song name with the bpms (it's better to measure them manually as automatic software tends to fail badly).","title":"Dance routine"},{"location":"dancing/shuffle_basics/#running-man","text":"The running man is the basic step of Melbourne shuffle, so make sure to master it. 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot kicks forward, left foot slides backward, in a way that each travel the same distance. Weight is evenly shared between the feet. 1-and : Right foot slides backwards, right foot goes up, knee to the front. Weight lies on the right foot. 2 : Mirrors 1 . 2-and : Mirrors 1-and and it's equal to 8-and . Once you feel comfortable with it, start going around the room doing running mans, so you get a grasp of doing it without being stationary. Sources: 1 , 2 The foot going forward can land in the floor flat, heel first or toes first.","title":"Running man"},{"location":"dancing/shuffle_basics/#quick-tempo-running-man","text":"As the tempo speeds up, even if it seems there is no time to stop in the 1-and or 2-and they do manage to do so. To reach there, I'd put a song a little bit quicker than you are comfortable and: Get familiarity with the movement of the foot in the ground: Start with your basic running man stance left foot in front. When the 1 comes, try to reach the 1-and focusing to lift your right knee and stomp in the 2 . Rest a pair of beats and then try the other foot. Keep on doing it till you are comfortable doing this movement. Then try to do 2 complete running mans. Then dance :). For me the most difficult part is to bring back the non dominant leg backwards, to get used to the movement I slided backwards with that leg several times, then do three running mans starting with that leg, rest, and then do another three starting with the other one.","title":"Quick tempo Running man"},{"location":"dancing/shuffle_basics/#t-step","text":"The T-Step is other basic step of Melbourne shuffle, used to do lateral movement.","title":"T-Step"},{"location":"dancing/shuffle_basics/#learning-the-t-step","text":"The following instructions are a nice way to get the grasp of the movement, but we'll change them slightly to be able to link it with the other moves. 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot, which looks at 1.5 although the rest of you look at 12. 1 : Right foot kicks sideways towards 3, try to leave the foot at 90 degrees with your leg and that it doesn't wobble, so it's a clean kick. The weight is on the left leg's toes and it pivotes over them till it points to 10.5. Waist ends up turned. 1-and : Right foot is retrieved till the left leg's knee, keeping the base parallel to the ground. Weight is on the left leg's heel and pivotes over it to reach the 1.5 position. Waist is straight now. 2 : Equal to 1 . 2-and : Equal to 1-and . 3 : Right foot goes front, left foot goes back to the running man stance. To move to the other side is the mirror. Keep the leg that holds you a bit bent so you don't harm your knee when twisting. The usual basic pattern of Melbourne shuffle is doing some running mans pointing at 10.5, then doing a side step (like the T-step) to the right, then another running man pointing to 1.5, finishing with another side step to the left. To join the T-Step with the running man, when you are in the 1-and make it the 8-and of the T-Step. Sources: 1 2","title":"Learning the T-Step"},{"location":"dancing/shuffle_basics/#francis-t-step","text":"I've seen that Francis when finishes his T-Step with the other leg. So instead of moving the right leg in front and slide back the left (if we are going to the right), he: 1-and : Equal to normal T-Step. 2 : When you put your right foot down, shift your weight till it's evenly spread between feet. 2-and : Switch your weight to the right foot, lift the left while you keep on shifting your body weight to your 3. 3 : Left foot touches ground at your 1.5, slide the right back to the running man stance. What I like of this variation is that you conserve the energy, it makes total sense. What it doesn't is that he is dancing with sandals (\u00ac\u00ba-\u00b0)\u00ac . Try to move from side to side with 2 T-Steps in between using this variation.","title":"Francis T-Step"},{"location":"dancing/shuffle_basics/#quick-tempo-t-step","text":"To improve the speed of your T-Steps, I'd put a song a little bit quicker than you are comfortable and: Get familiarity with the movement of the foot in the ground: Start with your right knee up in position 8-and of the T-Step. First focus only to move the feet that you've got in the floor, so when the 1 comes, start pivoting the foot a pair of tempos. Put your right foot down and lift the left knee. Wait 3 or 4 tempos and when the 1 comes, start going to the other side. Once you are comfortable with the movement, repeat the same exercise but doing the kicks. Then instead of finishing with both feet under your body, finish in a running man stance. Then instead of starting with your knee up, start from the running man stance.","title":"Quick tempo T-Step"},{"location":"dancing/shuffle_basics/#future-steps","text":"Sacco kicks at 14:50, 15:49, 17:08 Rocky kicks at 13:26 or 27:58. Rocky version at 1:19, 2:34, 7:29 Recce kicks at 15:56 (explanation) or 27:43 or 28:05. Reverse kick spin at 18:01 or 27:65. Rocky version at 1:18, 2:16, 2:55, 3:26, 3:59, 4:58, 6:24, 8:17, 8:35, 15:00, 15:55 Forward kick spin at 19:33. Rocky version at 2:20, 3:16, 3:47, 8:40, 8:49 Rocky side to side at 15:04, 16:08","title":"Future steps"},{"location":"dancing/shuffle_basics/#beats-per-minute-progression","text":"2020-09-07: First time to be able to dance a song at 118bpm to C2C - Happy .","title":"Beats per minute progression"},{"location":"dancing/shuffle_basics/#references","text":"","title":"References"},{"location":"dancing/shuffle_basics/#more-videos-to-study","text":"Rocky Shuffle compilation Francis Shuffle compilation Siera Shuffle compilation Bulldog Shuffle compilation","title":"More videos to study"},{"location":"dancing/shuffle_kicks/","text":"Kicking-Running man \u2691 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot kicks forward without touching the ground, it's important that the kick is clean, so swift till the end and stop suddenly. At the same time the left foot has slided back a little bit. 1-and : Right foot is retrieved, knee up and foot parallel to the ground, left foot keeps on sliding back. 2 : Right foot hits the ground, left foot keeps on sliding, weight is evenly shared between feet like the 1 of the running man. Sources: 1 Sacco kicks \u2691 Sacco kicks looks similar to the T-Step adding some side kicks. There are two variations, either kick first in and then out or the other way around. I'm more comfortable with the first and I think it gives an advantage when combining it with the running man. 8-and : Equal to the T-Step. 1 : The foot of the floor equal to the T-Step but the right foot kicks to your 10.5. 1-and : Equal to the T-Step. 2 : The foot of the floor equal to the T-Step but the right foot kicks to your 3. The advantage of the Sacco kicks is that as you are going to kick to your 10.5 in 1 , you can slide the left foot quite a lot to your back (from 8 to 1 ), therefore gliding further than with a simple T-Step (you only have half a tempo before you need to put all your weight in your left leg because you need to twist). Although the guy in the video slides the foot backwards when going to his left, try to do the twist of the T-Step. (The guy is crazy, shuffling barefoot over gravel soil (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b ). Source: 1","title":"Kicks"},{"location":"dancing/shuffle_kicks/#kicking-running-man","text":"8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot kicks forward without touching the ground, it's important that the kick is clean, so swift till the end and stop suddenly. At the same time the left foot has slided back a little bit. 1-and : Right foot is retrieved, knee up and foot parallel to the ground, left foot keeps on sliding back. 2 : Right foot hits the ground, left foot keeps on sliding, weight is evenly shared between feet like the 1 of the running man. Sources: 1","title":"Kicking-Running man"},{"location":"dancing/shuffle_kicks/#sacco-kicks","text":"Sacco kicks looks similar to the T-Step adding some side kicks. There are two variations, either kick first in and then out or the other way around. I'm more comfortable with the first and I think it gives an advantage when combining it with the running man. 8-and : Equal to the T-Step. 1 : The foot of the floor equal to the T-Step but the right foot kicks to your 10.5. 1-and : Equal to the T-Step. 2 : The foot of the floor equal to the T-Step but the right foot kicks to your 3. The advantage of the Sacco kicks is that as you are going to kick to your 10.5 in 1 , you can slide the left foot quite a lot to your back (from 8 to 1 ), therefore gliding further than with a simple T-Step (you only have half a tempo before you need to put all your weight in your left leg because you need to twist). Although the guy in the video slides the foot backwards when going to his left, try to do the twist of the T-Step. (The guy is crazy, shuffling barefoot over gravel soil (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b ). Source: 1","title":"Sacco kicks"},{"location":"dancing/shuffle_spins/","text":"Tap spin \u2691 8 : Feet at the end of a running man, we generate a slight upper body twist clockwise to load the twist. 8-and : Left foot goes back to the body axes and the right foot goes to the body with knee up like a running man, with the difference that we release the twist energy and we start turning counterclockwise. Weight is on the left foot toes and it's knee is a little bit bent. 1 : Right foot kicks sideways towards 3 touching the ground but not putting any weight into it while we unbend the left knee taking the chance to keep the spin going. 1-and : Right foot goes back to the body and left knee is again a little bit bent waiting for the next kick. 2 : Equal to 1 2-and : Equal to 1-and 3 : Becomes the 1 of the running man. You can spin as long as you want, I've explained a two times spin, but if you want to go further repeat 1 and 1-and as long as you want. Sources: 1 Francis Spin \u2691 Awesome to start dancing. 8 : Both feet under your body, weight evenly spread between them. Give a small jump to start rotating clockwise (for example). 8-and : After 90 degrees of turn, your feet touch the ground, take the chance to give the big push to rotate the following 270. Bring your exterior arm to your shoulder level. 1 : Arrive to the running man stance with the left foot in front, but don't stop the rotation inertia. 1-and : Retrieve the left foot like any running man, but the foot points to your 5, right knee up. 2 : End your running man to your 5 with your right foot down. Add the final touch by lowering your external arm down. To get there : First practice the last part, the running man with a spin. The steps 1 to 2 described above until you are comfortable with it in both spinning directions. It's good to change the spin direction not to get dizzy. Do the full move until you are comfortable. Get it in time. Count in the song to start the movement in the 8 of the last time before a big drop. Source: 1","title":"Spins"},{"location":"dancing/shuffle_spins/#tap-spin","text":"8 : Feet at the end of a running man, we generate a slight upper body twist clockwise to load the twist. 8-and : Left foot goes back to the body axes and the right foot goes to the body with knee up like a running man, with the difference that we release the twist energy and we start turning counterclockwise. Weight is on the left foot toes and it's knee is a little bit bent. 1 : Right foot kicks sideways towards 3 touching the ground but not putting any weight into it while we unbend the left knee taking the chance to keep the spin going. 1-and : Right foot goes back to the body and left knee is again a little bit bent waiting for the next kick. 2 : Equal to 1 2-and : Equal to 1-and 3 : Becomes the 1 of the running man. You can spin as long as you want, I've explained a two times spin, but if you want to go further repeat 1 and 1-and as long as you want. Sources: 1","title":"Tap spin"},{"location":"dancing/shuffle_spins/#francis-spin","text":"Awesome to start dancing. 8 : Both feet under your body, weight evenly spread between them. Give a small jump to start rotating clockwise (for example). 8-and : After 90 degrees of turn, your feet touch the ground, take the chance to give the big push to rotate the following 270. Bring your exterior arm to your shoulder level. 1 : Arrive to the running man stance with the left foot in front, but don't stop the rotation inertia. 1-and : Retrieve the left foot like any running man, but the foot points to your 5, right knee up. 2 : End your running man to your 5 with your right foot down. Add the final touch by lowering your external arm down. To get there : First practice the last part, the running man with a spin. The steps 1 to 2 described above until you are comfortable with it in both spinning directions. It's good to change the spin direction not to get dizzy. Do the full move until you are comfortable. Get it in time. Count in the song to start the movement in the 8 of the last time before a big drop. Source: 1","title":"Francis Spin"},{"location":"data_analysis/recommender_systems/recommender_systems/","text":"A recommender system, or a recommendation system , is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. The entity to which the recommendation is provided is referred to as the user, and the product being recommended is also referred to as an item. Therefore, recommendation analysis is often based on the previous interaction between users and items, because past interests and proclivities are often good indicators of future choices. These relations can be learned in a data-driven manner from the ratings matrix, and the resulting model is used to make predictions for target users.The larger the number of rated items that are available for a user, the easier it is to make robust predictions about the future behavior of the user. The problem may be formulated in two ways: Prediction version of problem: This approach aims to predict the rating value for a user-item combination. It is assumed that training data is available, indicating user preferences for items. For m users and n items, this corresponds to an incomplete m x n matrix, hwere the specified (or observed) values are used for training. The missing (or unobserved) values are predicted using this training model. This problem is also referred to as the matrix completion problem . Ranking version of problem: This approach aims to recommend the top- k items for a particular user, or determine the top- k users to target for a particular item. Being the first one more common. The problem is also referred to as the top-k recommendation problem . Goals of recommender systems \u2691 The common operational and technical goals of recommender systems are: Relevance : Recommend items that are relevant to the user at hand. Although it's the primary operational goal, it is not sufficient in isolation. Novelty : Recommend items that the user has not seen in the past. Serendipity : Recommend items that are outside the user's bubble, rather than simply something they did not know about before. For example, if a new Indian restaurant opens in a neighborhood, then the recommendation of that restaurant to a user who normally eats Indian food is novel but not necessarily serendipitous. On the other hand, when the same user is recommended Ethiopian food, and it was unknown to the user that such food might appeal to her, then the recommendation is serendipitous. Serendipity has the beneficial side effect of beginning new areas of interest. Increase recommendation diversity: Recommend items that aren't similar to increase the chances that the user likes at least one of these items. Basic Models of recommender systems \u2691 There are four types of basic models: Collaborative filtering : Use collaborative power of the ratings provided by multiple users to make recommendations. Content-based : Use the descriptive attributes of the user rated items to create a user-specific model that predicts the rating of unobserved items. Knowledge-based : Use the similarities between customer requirements and item descriptions. Hybrid systems : Combine the above to benefit from the mix of their strengths to perform more robustly. Collaborative Filtering Models \u2691 These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items. There are two common types of methods. Memory-based methods : Also referred to as neighborhood-based collaborative filtering algorithms , predict ratings on the basis of their neighborhoods. These can be defined through two ways: User-based collaborative filtering : Ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the goal is to determine users who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing the averages of the ratings of this peer group. Item-based collaborative filtering : In order to make the rating predictions for target item B by user A, the first step is to determine a set S of items that are most similar to target item B. The ratings in the item set S, which are defined by A, are used to predict whether the user A will like item B. These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices. Model-based methods : Machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterized, the parameters of this model are learned within the context of an optimization framework. Some examples of such methods include decision trees, rule-based models, Bayesian methods and latent factor models. Many of these methods have a high level of coverage even for sparse ratings matrices. Types of ratings \u2691 The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings: interval-based : A discrete set of ordered numbers are used to quantify like or dislike. ordinal : A discrete set of ordered categorical values, such as agree or strongly agree, are used to achieve the same goal. binary : Only the like or dislike for the item can be specified. unary : Only liking of an item can be specified. Another categorization of rating systems is based in the way the feedback is retrieved: explicit ratings : Users actively give information on their preferences. implicit ratings : Users preferences are derived from their behavior. Such as visiting a link. Therefore, implicit ratings are usually unary. Content-Based Recommender Systems \u2691 In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items. These systems have the following advantages: Works well for new items, when sufficient data is not available. If the user has rated items with similar attributes. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. It requires large number of rating user data to produce robust predictions without overfitting. Knowledge-based Recommender Systems \u2691 The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process. Knowledge-based systems can be classified by the type of user interface: Constraint-based recommender systems : Users specify requirements on the item attributes or give information about their attributes. Then domain specific rules are used to select the items to recommend. Case-based recommender systems : Specific cases are selected by the user as targets or anchor points. Similarity metrics are defined in a domain specific way on the item attributes to retrieve similar items to these cases. These user interfaces can interact with the users through several ways: Conversational systems : User preferences are determined iteratively in the context of a feedback loop. It's useful if the item domain is complex. Search-based systems : User preferences are elicited by using a preset of questions. Navigation-based recommendation : Users specify a number of attribute changes to the item being recommended. Also known as critiquing recommender systems . The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests. These systems have the following advantages: Works well for items with varied properties and/or few ratings. Such as in cold start scenarios, if it's difficult to capture the user interests with historical data or if the item is not often consumed. Allows the users to explicitly specify what they want, thus giving them a greater control over the recommendation process. Allows the user to iteratively change their specified requirements to reach the desired items. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. Domain-Specific recommender systems \u2691 Demographic recommender systems \u2691 In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings. Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems. Pitfalls to avoid \u2691 Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior. Interesting resources \u2691 Content indexers \u2691 Open Library : Open, editable library catalog, building towards a web page for every book ever published. Data can be retrieved through their API or bulk downloaded . Rating Datasets \u2691 Books \u2691 Book-Crossing : 278,858 users providing 1,149,780 ratings (explicit / implicit) about 271,379 books. Movies \u2691 MovieLens : 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. HetRec 2011 Movielens + IMDB/rotten Tomatoes : 86,000 ratings from 2113 users. Netflix prize dataset : 480,000 users doing 100 million ratings on 17,000 movies. Music \u2691 HetRec 2011 Last.FM : 92,800 artist listening records from 1892 users. Web \u2691 HetRec 2011 Delicious : 105,000 bookmarks from 1867 users. Miscelaneous \u2691 Wikilens : generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items. Past Projects \u2691 GroupLens: Pioneer recommender system to recommend Usenet news. BookLens : Books implementation of Grouplens. MovieLens : Movies implementation of Grouplens. References \u2691 Books \u2691 Recommender Systems by Chary C.Aggarwal . Recommender systems, an introduction by Dietmar Jannach, Markus Zanker, Alexander Felfernig and Gerhard Friedrich. Practical Recommender Systems by Kim Falk. Hands On recommendation systems in Python by Rounak Banik. Awesome recommender systems \u2691 Grahamjenson","title":"Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#goals-of-recommender-systems","text":"The common operational and technical goals of recommender systems are: Relevance : Recommend items that are relevant to the user at hand. Although it's the primary operational goal, it is not sufficient in isolation. Novelty : Recommend items that the user has not seen in the past. Serendipity : Recommend items that are outside the user's bubble, rather than simply something they did not know about before. For example, if a new Indian restaurant opens in a neighborhood, then the recommendation of that restaurant to a user who normally eats Indian food is novel but not necessarily serendipitous. On the other hand, when the same user is recommended Ethiopian food, and it was unknown to the user that such food might appeal to her, then the recommendation is serendipitous. Serendipity has the beneficial side effect of beginning new areas of interest. Increase recommendation diversity: Recommend items that aren't similar to increase the chances that the user likes at least one of these items.","title":"Goals of recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#basic-models-of-recommender-systems","text":"There are four types of basic models: Collaborative filtering : Use collaborative power of the ratings provided by multiple users to make recommendations. Content-based : Use the descriptive attributes of the user rated items to create a user-specific model that predicts the rating of unobserved items. Knowledge-based : Use the similarities between customer requirements and item descriptions. Hybrid systems : Combine the above to benefit from the mix of their strengths to perform more robustly.","title":"Basic Models of recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#collaborative-filtering-models","text":"These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items. There are two common types of methods. Memory-based methods : Also referred to as neighborhood-based collaborative filtering algorithms , predict ratings on the basis of their neighborhoods. These can be defined through two ways: User-based collaborative filtering : Ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the goal is to determine users who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing the averages of the ratings of this peer group. Item-based collaborative filtering : In order to make the rating predictions for target item B by user A, the first step is to determine a set S of items that are most similar to target item B. The ratings in the item set S, which are defined by A, are used to predict whether the user A will like item B. These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices. Model-based methods : Machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterized, the parameters of this model are learned within the context of an optimization framework. Some examples of such methods include decision trees, rule-based models, Bayesian methods and latent factor models. Many of these methods have a high level of coverage even for sparse ratings matrices.","title":"Collaborative Filtering Models"},{"location":"data_analysis/recommender_systems/recommender_systems/#types-of-ratings","text":"The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings: interval-based : A discrete set of ordered numbers are used to quantify like or dislike. ordinal : A discrete set of ordered categorical values, such as agree or strongly agree, are used to achieve the same goal. binary : Only the like or dislike for the item can be specified. unary : Only liking of an item can be specified. Another categorization of rating systems is based in the way the feedback is retrieved: explicit ratings : Users actively give information on their preferences. implicit ratings : Users preferences are derived from their behavior. Such as visiting a link. Therefore, implicit ratings are usually unary.","title":"Types of ratings"},{"location":"data_analysis/recommender_systems/recommender_systems/#content-based-recommender-systems","text":"In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items. These systems have the following advantages: Works well for new items, when sufficient data is not available. If the user has rated items with similar attributes. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. It requires large number of rating user data to produce robust predictions without overfitting.","title":"Content-Based Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#knowledge-based-recommender-systems","text":"The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process. Knowledge-based systems can be classified by the type of user interface: Constraint-based recommender systems : Users specify requirements on the item attributes or give information about their attributes. Then domain specific rules are used to select the items to recommend. Case-based recommender systems : Specific cases are selected by the user as targets or anchor points. Similarity metrics are defined in a domain specific way on the item attributes to retrieve similar items to these cases. These user interfaces can interact with the users through several ways: Conversational systems : User preferences are determined iteratively in the context of a feedback loop. It's useful if the item domain is complex. Search-based systems : User preferences are elicited by using a preset of questions. Navigation-based recommendation : Users specify a number of attribute changes to the item being recommended. Also known as critiquing recommender systems . The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests. These systems have the following advantages: Works well for items with varied properties and/or few ratings. Such as in cold start scenarios, if it's difficult to capture the user interests with historical data or if the item is not often consumed. Allows the users to explicitly specify what they want, thus giving them a greater control over the recommendation process. Allows the user to iteratively change their specified requirements to reach the desired items. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items.","title":"Knowledge-based Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#domain-specific-recommender-systems","text":"","title":"Domain-Specific recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#demographic-recommender-systems","text":"In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings. Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems.","title":"Demographic recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#pitfalls-to-avoid","text":"Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior.","title":"Pitfalls to avoid"},{"location":"data_analysis/recommender_systems/recommender_systems/#interesting-resources","text":"","title":"Interesting resources"},{"location":"data_analysis/recommender_systems/recommender_systems/#content-indexers","text":"Open Library : Open, editable library catalog, building towards a web page for every book ever published. Data can be retrieved through their API or bulk downloaded .","title":"Content indexers"},{"location":"data_analysis/recommender_systems/recommender_systems/#rating-datasets","text":"","title":"Rating Datasets"},{"location":"data_analysis/recommender_systems/recommender_systems/#books","text":"Book-Crossing : 278,858 users providing 1,149,780 ratings (explicit / implicit) about 271,379 books.","title":"Books"},{"location":"data_analysis/recommender_systems/recommender_systems/#movies","text":"MovieLens : 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. HetRec 2011 Movielens + IMDB/rotten Tomatoes : 86,000 ratings from 2113 users. Netflix prize dataset : 480,000 users doing 100 million ratings on 17,000 movies.","title":"Movies"},{"location":"data_analysis/recommender_systems/recommender_systems/#music","text":"HetRec 2011 Last.FM : 92,800 artist listening records from 1892 users.","title":"Music"},{"location":"data_analysis/recommender_systems/recommender_systems/#web","text":"HetRec 2011 Delicious : 105,000 bookmarks from 1867 users.","title":"Web"},{"location":"data_analysis/recommender_systems/recommender_systems/#miscelaneous","text":"Wikilens : generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items.","title":"Miscelaneous"},{"location":"data_analysis/recommender_systems/recommender_systems/#past-projects","text":"GroupLens: Pioneer recommender system to recommend Usenet news. BookLens : Books implementation of Grouplens. MovieLens : Movies implementation of Grouplens.","title":"Past Projects"},{"location":"data_analysis/recommender_systems/recommender_systems/#references","text":"","title":"References"},{"location":"data_analysis/recommender_systems/recommender_systems/#books_1","text":"Recommender Systems by Chary C.Aggarwal . Recommender systems, an introduction by Dietmar Jannach, Markus Zanker, Alexander Felfernig and Gerhard Friedrich. Practical Recommender Systems by Kim Falk. Hands On recommendation systems in Python by Rounak Banik.","title":"Books"},{"location":"data_analysis/recommender_systems/recommender_systems/#awesome-recommender-systems","text":"Grahamjenson","title":"Awesome recommender systems"},{"location":"devops/alex/","text":"Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. For example, when We\u2019ve confirmed his identity is given , alex will warn you and suggest using their instead of his . Give alex a spin on the Online demo . Installation \u2691 npm install alex --global You can use it with Vim through the ALE plugin . As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter. References \u2691 Git","title":"Alex"},{"location":"devops/alex/#installation","text":"npm install alex --global You can use it with Vim through the ALE plugin . As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.","title":"Installation"},{"location":"devops/alex/#references","text":"Git","title":"References"},{"location":"devops/api_management/","text":"API management is the process of creating and publishing web application programming interfaces (APIs) under a service that: Enforces the usage of policies. Controls access. Collects and analyzes usage statistics. Reports on performance. Components \u2691 While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","title":"API Management"},{"location":"devops/api_management/#components","text":"While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","title":"Components"},{"location":"devops/bandit/","text":"Bandit finds common security issues in Python code. To do this, Bandit processes each file, builds an AST from it, and runs appropriate plugins against the AST nodes. Once Bandit has finished scanning all the files, it generates a report. You can use this cookiecutter template to create a python project with bandit already configured. Installation \u2691 pip install bandit Usage \u2691 Ignore an error. \u2691 Add the # nosec comment in the line. Configuration \u2691 You can run bandit through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-bandit rev : v1.0.4 hooks : - id : python-bandit-vulnerability-check bandit takes a lot of time to run, so it slows down too much the commiting, therefore it should be run only in the CI. Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : bandit : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install bandit - name : Execute bandit run : bandit -r project References \u2691 Docs","title":"Bandit"},{"location":"devops/bandit/#installation","text":"pip install bandit","title":"Installation"},{"location":"devops/bandit/#usage","text":"","title":"Usage"},{"location":"devops/bandit/#ignore-an-error","text":"Add the # nosec comment in the line.","title":"Ignore an error."},{"location":"devops/bandit/#configuration","text":"You can run bandit through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-bandit rev : v1.0.4 hooks : - id : python-bandit-vulnerability-check bandit takes a lot of time to run, so it slows down too much the commiting, therefore it should be run only in the CI. Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : bandit : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install bandit - name : Execute bandit run : bandit -r project","title":"Configuration"},{"location":"devops/bandit/#references","text":"Docs","title":"References"},{"location":"devops/black/","text":"Black is a style guide enforcement tool. You can use this cookiecutter template to create a python project with black already configured. Installation \u2691 pip install black Configuration \u2691 Its configuration is stored in pyproject.toml . File: pyproject.toml # Example configuration for Black. # NOTE: you have to use single-quoted strings in TOML for regular expressions. # It's the equivalent of r-strings in Python. Multiline strings are treated as # verbose regular expressions by Black. Use [ ] to denote a significant space # character. [tool.black] line-length = 88 target-version = ['py36', 'py37', 'py38'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist # The following are specific to Black, you probably don't want those. | blib2to3 | tests/data | profiling )/ ''' You can use it both with: The Vim plugin Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/ambv/black rev : stable hooks : - id : black language_version : python3.7 Github Actions: File: .github/workflows/lint.yml --- name : Lint on : [ push , pull_request ] jobs : Black : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Black uses : psf/black@stable Split long lines \u2691 If you want to split long lines, you need to use the --experimental-string-processing flag. I haven't found how to set that option in the config file. Disable the formatting of some lines \u2691 You can use the comments # fmt: off and # fmt: on References \u2691 Docs Git","title":"Black"},{"location":"devops/black/#installation","text":"pip install black","title":"Installation"},{"location":"devops/black/#configuration","text":"Its configuration is stored in pyproject.toml . File: pyproject.toml # Example configuration for Black. # NOTE: you have to use single-quoted strings in TOML for regular expressions. # It's the equivalent of r-strings in Python. Multiline strings are treated as # verbose regular expressions by Black. Use [ ] to denote a significant space # character. [tool.black] line-length = 88 target-version = ['py36', 'py37', 'py38'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist # The following are specific to Black, you probably don't want those. | blib2to3 | tests/data | profiling )/ ''' You can use it both with: The Vim plugin Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/ambv/black rev : stable hooks : - id : black language_version : python3.7 Github Actions: File: .github/workflows/lint.yml --- name : Lint on : [ push , pull_request ] jobs : Black : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Black uses : psf/black@stable","title":"Configuration"},{"location":"devops/black/#split-long-lines","text":"If you want to split long lines, you need to use the --experimental-string-processing flag. I haven't found how to set that option in the config file.","title":"Split long lines"},{"location":"devops/black/#disable-the-formatting-of-some-lines","text":"You can use the comments # fmt: off and # fmt: on","title":"Disable the formatting of some lines"},{"location":"devops/black/#references","text":"Docs Git","title":"References"},{"location":"devops/ci/","text":"Continuous Integration (CI) allows to automatically run processes on the code each time a commit is pushed. For example it can be used to run the tests, build the documentation, build a package or maintain dependencies updated. I've automated the configuration of CI/CD pipelines for python projects in this cookiecutter template . There are three non exclusive ways to run the tests: Integrate them in your editor, so it's executed each time you save the file. Through a pre-commit hook to make it easy for the collaborator to submit correctly formatted code. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. Through a CI server (like Drone or Github Actions) to ensure that the commited code meets the quality standards. Developers can bypass the pre-commit filter, so we need to set up the quality gate in an agnostic environment. Depending on the time the test takes to run and their different implementations, we'll choose from one to three of the choices above. Configuring pre-commit \u2691 To adopt pre-commit to our system we have to: Install pre-commit: pip3 install pre-commit and add it to the development requirements.txt . Define .pre-commit-config.yaml with the hooks you want to include (they don't plan to support pyproject.toml ). Execute pre-commit install to install git hooks in your .git/ directory. Execute pre-commit run --all-files to tests all the files. Usually pre-commit will only run on the changed files during git hooks. Static analysis checkers \u2691 Static analysis is the analysis of computer software that is performed without actually executing programs. Formatters \u2691 Formatters are tools that change your files to meet a linter requirements. Black : A python style guide formatter tool. Linters \u2691 Lint, or a linter , is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. Flake8 : A python style guide checker tool. markdownlint : A linter for Markdown files. proselint : Is another linter for prose. Yamllint : A linter for YAML files. write-good is a naive linter for English prose. Type checkers \u2691 Type checkers are programs that the code is compliant with a defined type system which is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. The main purpose of a type system is to reduce possibilities for bugs by defining interfaces between different parts of the program, and then checking that the parts have been connected in a consistent way. Mypy : A static type checker for Python. Security vulnerability checkers \u2691 Tools that check potential vulnerabilities in the code. Bandit : Finds common security issues in Python code. Safety : Checks your installed dependencies for known security vulnerabilities. Other pre-commit tests \u2691 Pre-commit comes with several tests by default. These are the ones I've chosen. File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.1.0 hooks : - id : trailing-whitespace - id : check-added-large-files - id : check-docstring-first - id : check-merge-conflict - id : end-of-file-fixer - id : detect-private-key Update package dependencies \u2691 Tools to automatically keep your dependencies updated. pip-tools Coverage reports \u2691 Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test. Save the secret in the repository configuration and add this step to your tests job. - name : Coveralls uses : coverallsapp/github-action@master with : github-token : ${{ secrets.COVERALLS_TOKEN }} Add the following badge to your README.md. Variables to substitute: repository_path : Github repository path, like lyz-code/pydo . [![Coverage Status](https://coveralls.io/repos/github/{{ repository_path }}/badge.svg?branch=master)](https://coveralls.io/github/{{ repository_path }}?branch=master)","title":"Continuous Integration pipelines"},{"location":"devops/ci/#configuring-pre-commit","text":"To adopt pre-commit to our system we have to: Install pre-commit: pip3 install pre-commit and add it to the development requirements.txt . Define .pre-commit-config.yaml with the hooks you want to include (they don't plan to support pyproject.toml ). Execute pre-commit install to install git hooks in your .git/ directory. Execute pre-commit run --all-files to tests all the files. Usually pre-commit will only run on the changed files during git hooks.","title":"Configuring pre-commit"},{"location":"devops/ci/#static-analysis-checkers","text":"Static analysis is the analysis of computer software that is performed without actually executing programs.","title":"Static analysis checkers"},{"location":"devops/ci/#formatters","text":"Formatters are tools that change your files to meet a linter requirements. Black : A python style guide formatter tool.","title":"Formatters"},{"location":"devops/ci/#linters","text":"Lint, or a linter , is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. Flake8 : A python style guide checker tool. markdownlint : A linter for Markdown files. proselint : Is another linter for prose. Yamllint : A linter for YAML files. write-good is a naive linter for English prose.","title":"Linters"},{"location":"devops/ci/#type-checkers","text":"Type checkers are programs that the code is compliant with a defined type system which is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. The main purpose of a type system is to reduce possibilities for bugs by defining interfaces between different parts of the program, and then checking that the parts have been connected in a consistent way. Mypy : A static type checker for Python.","title":"Type checkers"},{"location":"devops/ci/#security-vulnerability-checkers","text":"Tools that check potential vulnerabilities in the code. Bandit : Finds common security issues in Python code. Safety : Checks your installed dependencies for known security vulnerabilities.","title":"Security vulnerability checkers"},{"location":"devops/ci/#other-pre-commit-tests","text":"Pre-commit comes with several tests by default. These are the ones I've chosen. File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.1.0 hooks : - id : trailing-whitespace - id : check-added-large-files - id : check-docstring-first - id : check-merge-conflict - id : end-of-file-fixer - id : detect-private-key","title":"Other pre-commit tests"},{"location":"devops/ci/#update-package-dependencies","text":"Tools to automatically keep your dependencies updated. pip-tools","title":"Update package dependencies"},{"location":"devops/ci/#coverage-reports","text":"Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test. Save the secret in the repository configuration and add this step to your tests job. - name : Coveralls uses : coverallsapp/github-action@master with : github-token : ${{ secrets.COVERALLS_TOKEN }} Add the following badge to your README.md. Variables to substitute: repository_path : Github repository path, like lyz-code/pydo . [![Coverage Status](https://coveralls.io/repos/github/{{ repository_path }}/badge.svg?branch=master)](https://coveralls.io/github/{{ repository_path }}?branch=master)","title":"Coverage reports"},{"location":"devops/devops/","text":"DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops) which aims to shorten the systems development life cycle and provide continuous delivery with high software quality. Learn path \u2691 DevOps is has become a juicy work, if you want to introduce yourself into this world I suggest you to follow these steps: Learn basic Linux administration, otherwise you'll be lost. Learn how to use Git. If you can host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions .","title":"DevOps"},{"location":"devops/devops/#learn-path","text":"DevOps is has become a juicy work, if you want to introduce yourself into this world I suggest you to follow these steps: Learn basic Linux administration, otherwise you'll be lost. Learn how to use Git. If you can host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions .","title":"Learn path"},{"location":"devops/flake8/","text":"DEPRECATION: Use Flakehell instead Flake8 doesn't support pyproject.toml , which is becoming the standard, so I suggest using Flakehell instead. Flake8 is a style guide enforcement tool. Its configuration is stored in setup.cfg , tox.ini or .flake8 . File: .flake8 [flake8] # ignore = E203, E266, E501, W503, F403, F401 max-line-length = 88 # max-complexity = 18 # select = B,C,E,F,W,T4,B9 You can use it both with: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://gitlab.com/pycqa/flake8 rev : master hooks : - id : flake8 Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Flake8 : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Flake8 uses : cclauss/GitHub-Action-for-Flake8@v0.5.0 References \u2691 Docs","title":"Flake8"},{"location":"devops/flake8/#references","text":"Docs","title":"References"},{"location":"devops/flakehell/","text":"Flakehell is a Flake8 wrapper to make it cool. Some of it's features are: Lint md, rst, ipynb, and more . Shareable and remote configs . Legacy-friendly : ability to get report only about new errors. Caching for much better performance. Use only specified plugins , not everything installed. Make output beautiful . pyproject.toml support. Check that all required plugins are installed . Syntax highlighting in messages and code snippets . PyLint integration. Remove unused noqa . Powerful GitLab support . Codes management: Manage codes per plugin. Enable and disable plugins and codes by wildcard. Show codes for installed plugins . Show all messages and codes for a plugin . Allow codes intersection for different plugins. You can use this cookiecutter template to create a python project with flakehell already configured. Installation \u2691 pip install flakehell Configuration \u2691 FlakeHell can be configured in pyproject.toml . You can specify any Flake8 options and FlakeHell-specific parameters. Plugins \u2691 In pyproject.toml you can specify [tool.flakehell.plugins] table. It's a list of flake8 plugins and associated to them rules. Key can be exact plugin name or wildcard template. For example \"flake8-commas\" or \"flake8-*\" . FlakeHell will choose the longest match for every plugin if possible. In the previous example, flake8-commas will match to the first pattern, flake8-bandit and flake8-bugbear to the second, and pycodestyle will not match to any pattern. Value is a list of templates for error codes for this plugin. First symbol in every template must be + (include) or - (exclude). The latest matched pattern wins. For example, [\"+*\", \"-F*\", \"-E30?\", \"-E401\"] means \"Include everything except all checks that starts with F , check from E301 to E310 , and E401 \". Example: pyproject.toml [tool.flakehell] # optionally inherit from remote config (or local if you want) base = \"https://raw.githubusercontent.com/life4/flakehell/master/pyproject.toml\" # specify any flake8 options. For example, exclude \"example.py\": exclude = [\"example.py\"] # make output nice format = \"grouped\" # don't limit yourself max_line_length = 120 # show line of source code in output show_source = true # list of plugins and rules for them [tool.flakehell.plugins] # include everything in pyflakes except F401 pyflakes = [\"+*\", \"-F401\"] # enable only codes from S100 to S199 flake8-bandit = [\"-*\", \"+S1??\"] # enable everything that starts from `flake8-` \"flake8-*\" = [\"+*\"] # explicitly disable plugin flake8-docstrings = [\"-*\"] # disable some checks for tests [tool.flakehell.exceptions.\"tests/\"] pycodestyle = [\"-F401\"] # disable a check pyflakes = [\"-*\"] # disable a plugin # do not disable `pyflakes` for one file in tests [tool.flakehell.exceptions.\"tests/test_example.py\"] pyflakes = [\"+*\"] # enable a plugin Check a complete list of flake8 extensions. flake8-bugbear : Finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle. flake8-fixme : Check for FIXME, TODO and other temporary developer notes. flake8-debugger : Check for pdb or idbp imports and set traces. flake8-mutable : Checks for mutable default arguments anti-pattern. flake8-pytest : Check for uses of Django-style assert-statements in tests. So no more self.assertEqual(a, b) , but instead assert a == b . flake8-pytest-style : Checks common style issues or inconsistencies with pytest-based tests. flake8-simplify : Helps you to simplify code. flake8-variables-names : Helps to make more readable variables names. pep8-naming : Check your code against PEP 8 naming conventions. flake8-expression-complexity : Check expression complexity. flake8-use-fstring : Checks you're using f-strings. flake8-docstrings : adds an extension for the fantastic pydocstyle tool to Flake8 . flake8-markdown : lints GitHub-style Python code blocks in Markdown files using flake8. pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. dlint : Encourage best coding practices and helping ensure Python code is secure. flake8-aaa : Checks Python tests follow the Arrange-Act-Assert pattern . flake8-annotations-complexity : Report on too complex type annotations. flake8-annotations : Detects the absence of PEP 3107-style function annotations and PEP 484-style type comments. flake8-typing-imports : Checks that typing imports are properly guarded. flake8-comprehensions : Help you write better list/set/dict comprehensions. flake8-eradicate : find commented out (or so called \"dead\") code. Usage \u2691 When using FlakeHell, I frequently use the following commands: flakehell lint Runs the linter, similar to the flake8 command. flakehell plugins Lists all the plugins used, and their configuration status. flakehell missed Shows any plugins that are in the configuration but not installed properly. flakehell code S322 (or any other code) Shows the explanation for that specific warning code. flakehell yesqa Removes unused codes from # noqa and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice. Integrations \u2691 Flakehell checks can be run in: In Vim though the ALE plugin . Through a pre-commit: - repo : https://github.com/life4/flakehell/ rev : master hooks : - name : Run flakehell static analysis tool id : flakehell In the CI: - name : Test linters run : make lint Assuming you're using a Makefile like the one in my cookiecutter-python-project . References \u2691 Git Docs Using Flake8 and pyproject.toml with FlakeHell article by Jonathan Bowman","title":"Flakehell"},{"location":"devops/flakehell/#installation","text":"pip install flakehell","title":"Installation"},{"location":"devops/flakehell/#configuration","text":"FlakeHell can be configured in pyproject.toml . You can specify any Flake8 options and FlakeHell-specific parameters.","title":"Configuration"},{"location":"devops/flakehell/#plugins","text":"In pyproject.toml you can specify [tool.flakehell.plugins] table. It's a list of flake8 plugins and associated to them rules. Key can be exact plugin name or wildcard template. For example \"flake8-commas\" or \"flake8-*\" . FlakeHell will choose the longest match for every plugin if possible. In the previous example, flake8-commas will match to the first pattern, flake8-bandit and flake8-bugbear to the second, and pycodestyle will not match to any pattern. Value is a list of templates for error codes for this plugin. First symbol in every template must be + (include) or - (exclude). The latest matched pattern wins. For example, [\"+*\", \"-F*\", \"-E30?\", \"-E401\"] means \"Include everything except all checks that starts with F , check from E301 to E310 , and E401 \". Example: pyproject.toml [tool.flakehell] # optionally inherit from remote config (or local if you want) base = \"https://raw.githubusercontent.com/life4/flakehell/master/pyproject.toml\" # specify any flake8 options. For example, exclude \"example.py\": exclude = [\"example.py\"] # make output nice format = \"grouped\" # don't limit yourself max_line_length = 120 # show line of source code in output show_source = true # list of plugins and rules for them [tool.flakehell.plugins] # include everything in pyflakes except F401 pyflakes = [\"+*\", \"-F401\"] # enable only codes from S100 to S199 flake8-bandit = [\"-*\", \"+S1??\"] # enable everything that starts from `flake8-` \"flake8-*\" = [\"+*\"] # explicitly disable plugin flake8-docstrings = [\"-*\"] # disable some checks for tests [tool.flakehell.exceptions.\"tests/\"] pycodestyle = [\"-F401\"] # disable a check pyflakes = [\"-*\"] # disable a plugin # do not disable `pyflakes` for one file in tests [tool.flakehell.exceptions.\"tests/test_example.py\"] pyflakes = [\"+*\"] # enable a plugin Check a complete list of flake8 extensions. flake8-bugbear : Finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle. flake8-fixme : Check for FIXME, TODO and other temporary developer notes. flake8-debugger : Check for pdb or idbp imports and set traces. flake8-mutable : Checks for mutable default arguments anti-pattern. flake8-pytest : Check for uses of Django-style assert-statements in tests. So no more self.assertEqual(a, b) , but instead assert a == b . flake8-pytest-style : Checks common style issues or inconsistencies with pytest-based tests. flake8-simplify : Helps you to simplify code. flake8-variables-names : Helps to make more readable variables names. pep8-naming : Check your code against PEP 8 naming conventions. flake8-expression-complexity : Check expression complexity. flake8-use-fstring : Checks you're using f-strings. flake8-docstrings : adds an extension for the fantastic pydocstyle tool to Flake8 . flake8-markdown : lints GitHub-style Python code blocks in Markdown files using flake8. pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. dlint : Encourage best coding practices and helping ensure Python code is secure. flake8-aaa : Checks Python tests follow the Arrange-Act-Assert pattern . flake8-annotations-complexity : Report on too complex type annotations. flake8-annotations : Detects the absence of PEP 3107-style function annotations and PEP 484-style type comments. flake8-typing-imports : Checks that typing imports are properly guarded. flake8-comprehensions : Help you write better list/set/dict comprehensions. flake8-eradicate : find commented out (or so called \"dead\") code.","title":"Plugins"},{"location":"devops/flakehell/#usage","text":"When using FlakeHell, I frequently use the following commands: flakehell lint Runs the linter, similar to the flake8 command. flakehell plugins Lists all the plugins used, and their configuration status. flakehell missed Shows any plugins that are in the configuration but not installed properly. flakehell code S322 (or any other code) Shows the explanation for that specific warning code. flakehell yesqa Removes unused codes from # noqa and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice.","title":"Usage"},{"location":"devops/flakehell/#integrations","text":"Flakehell checks can be run in: In Vim though the ALE plugin . Through a pre-commit: - repo : https://github.com/life4/flakehell/ rev : master hooks : - name : Run flakehell static analysis tool id : flakehell In the CI: - name : Test linters run : make lint Assuming you're using a Makefile like the one in my cookiecutter-python-project .","title":"Integrations"},{"location":"devops/flakehell/#references","text":"Git Docs Using Flake8 and pyproject.toml with FlakeHell article by Jonathan Bowman","title":"References"},{"location":"devops/helmfile/","text":"Helmfile is a declarative spec for deploying Helm charts. It lets you: Keep a directory of chart value files and maintain changes in version control. Apply CI/CD to configuration changes. Environmental chart promotion. Periodically sync to avoid skew in environments. To avoid upgrades for each iteration of helm, the helmfile executable delegates to helm - as a result, helm must be installed. All information is saved in the helmfile.yaml file. In case we need custom yamls, we'll use kustomize . Installation \u2691 Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile Usage \u2691 How to deploy a new chart \u2691 When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are updated, if they are not, run helmfile apply . Configure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Be careful becase some charts specify the docker image version in the name. Comment out that line because upgrading the chart version without upgrading the image tag can break the service. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes. Keep charts updated \u2691 To have your charts updated, this would be my suggested workflow, although the developers haven't confirmed it yet : A periodic CI job would run helmfile deps , once a change is detected in the lock file, the job will run helmfile --environment=staging apply . Developers are notified that the new version is deployed and are prompted to test it. Once it's validated, the developers will manually introduce the new version in the lockfile and run helmfile --environment=production apply . Delegate to the developers the manual introduction of the version in the lockfile isn't the ideal solution, but it's the one I can come up to avoid race conditions on chart releases. To be able to see the differences of long diff files, you can filter it with egrep . helmfile diff | egrep -A20 -B20 \"^.{5}(\\-|\\+)\" It will show you all the changed lines with the 20 previous and next ones. Uninstall charts \u2691 Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands. Force the reinstallation of everything \u2691 If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases. Debugging helmfile \u2691 Error: \"release-name\" has no deployed releases \u2691 This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again. Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help) \u2691 I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false. Links \u2691 Git","title":"Helmfile"},{"location":"devops/helmfile/#installation","text":"Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile","title":"Installation"},{"location":"devops/helmfile/#usage","text":"","title":"Usage"},{"location":"devops/helmfile/#how-to-deploy-a-new-chart","text":"When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are updated, if they are not, run helmfile apply . Configure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Be careful becase some charts specify the docker image version in the name. Comment out that line because upgrading the chart version without upgrading the image tag can break the service. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes.","title":"How to deploy a new chart"},{"location":"devops/helmfile/#keep-charts-updated","text":"To have your charts updated, this would be my suggested workflow, although the developers haven't confirmed it yet : A periodic CI job would run helmfile deps , once a change is detected in the lock file, the job will run helmfile --environment=staging apply . Developers are notified that the new version is deployed and are prompted to test it. Once it's validated, the developers will manually introduce the new version in the lockfile and run helmfile --environment=production apply . Delegate to the developers the manual introduction of the version in the lockfile isn't the ideal solution, but it's the one I can come up to avoid race conditions on chart releases. To be able to see the differences of long diff files, you can filter it with egrep . helmfile diff | egrep -A20 -B20 \"^.{5}(\\-|\\+)\" It will show you all the changed lines with the 20 previous and next ones.","title":"Keep charts updated"},{"location":"devops/helmfile/#uninstall-charts","text":"Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands.","title":"Uninstall charts"},{"location":"devops/helmfile/#force-the-reinstallation-of-everything","text":"If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases.","title":"Force the reinstallation of everything"},{"location":"devops/helmfile/#debugging-helmfile","text":"","title":"Debugging helmfile"},{"location":"devops/helmfile/#error-release-name-has-no-deployed-releases","text":"This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again.","title":"Error: \"release-name\" has no deployed releases"},{"location":"devops/helmfile/#error-failed-to-download-stablemetrics-server-hint-running-helm-repo-update-may-help","text":"I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false.","title":"Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help)"},{"location":"devops/helmfile/#links","text":"Git","title":"Links"},{"location":"devops/markdownlint/","text":"markdownlint-cli is a command line interface for the markdownlint Node.js style checker and lint tool for Markdown/CommonMark files. I've evaluated these other projects ( 1 , 2 , but their configuration is less user friendly and are less maintained. You can use this cookiecutter template to create a python project with markdownlint already configured. Installation \u2691 npm install -g markdownlint-cli Configuration \u2691 To configure your project , add a .markdownlint.json in your project root directory, or in any parent. I've opened an issue to see if they are going to support pyproject.toml to save the configuration. Check the styles examples . Go to the rules document if you ever need to check more information on a specific rule. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/igorshubovych/markdownlint-cli rev : v0.23.2 hooks : - id : markdownlint Troubleshooting \u2691 Until the #2926 PR is merged you need to change the let l:pattern=.* file to make the linting work to: File: ~/.vim/bundle/ale/autoload/ale/handlers let l :pattern = ': \\?\\(\\d*\\):\\? \\(MD\\d\\{3}\\)\\(\\/\\)\\([A-Za-z0-9-\\/]\\+\\)\\(.*\\)$' References \u2691 Git","title":"Markdownlint"},{"location":"devops/markdownlint/#installation","text":"npm install -g markdownlint-cli","title":"Installation"},{"location":"devops/markdownlint/#configuration","text":"To configure your project , add a .markdownlint.json in your project root directory, or in any parent. I've opened an issue to see if they are going to support pyproject.toml to save the configuration. Check the styles examples . Go to the rules document if you ever need to check more information on a specific rule. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/igorshubovych/markdownlint-cli rev : v0.23.2 hooks : - id : markdownlint","title":"Configuration"},{"location":"devops/markdownlint/#troubleshooting","text":"Until the #2926 PR is merged you need to change the let l:pattern=.* file to make the linting work to: File: ~/.vim/bundle/ale/autoload/ale/handlers let l :pattern = ': \\?\\(\\d*\\):\\? \\(MD\\d\\{3}\\)\\(\\/\\)\\([A-Za-z0-9-\\/]\\+\\)\\(.*\\)$'","title":"Troubleshooting"},{"location":"devops/markdownlint/#references","text":"Git","title":"References"},{"location":"devops/mypy/","text":"Mypy is an optional static type checker for Python that aims to combine the benefits of dynamic (or \"duck\") typing and static typing. Mypy combines the expressive power and convenience of Python with a powerful type system and compile-time type checking. You can use this cookiecutter template to create a python project with mypy already configured. Installation \u2691 pip install mypy Configuration \u2691 Mypy configuration is saved in the mypy.ini file, and they don't yet support pyproject.toml . File: mypy.ini [mypy] show_error_codes = True follow_imports = silent strict_optional = True warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True check_untyped_defs = True no_implicit_reexport = True warn_unused_configs = True disallow_subclassing_any = True disallow_incomplete_defs = True disallow_untyped_decorators = True disallow_untyped_calls = True # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = True You can use it both with: Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/mirrors-mypy rev : v0.782 hooks : - name : Run mypy static analysis tool id : mypy Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Mypy : runs-on : ubuntu-latest name : Mypy steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install Dependencies run : pip install mypy - name : mypy run : mypy Ignore one line \u2691 Add # type: ignore to the line you want to skip. Troubleshooting \u2691 Module X has no attribute Y \u2691 If you're importing objects from your own module, you need to tell mypy that those objects are available. To do so in the __init__.py of your module, list them under the __all__ variable . File: init .py from .model import Entity __all__ = [ \"Entity\" , ] [W0707: Consider explicitly re-raising using the 'from' \u2691 keyword]( https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with ) The error can be raised by two cases. An exception was raised, we were handling it, and something went wrong in the process of handling it. An exception was raised, and we decided to replace it with a different exception that will make more sense to whoever called this code. try : self . connection , _ = self . sock . accept () except socket . timeout as error : raise IPCException ( 'The socket timed out' ) from error The error bit at the end tells Python: The IPCException that we\u2019re raising is just a friendlier version of the socket.timeout that we just caught. When we run that code and reach that exception, the traceback is going to look like this: Traceback (most recent call last): File \"foo.py\", line 19, in self.connection, _ = self.sock.accept() File \"foo.py\", line 7, in accept raise socket.timeout socket.timeout The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"foo.py\", line 21, in raise IPCException('The socket timed out') from e IPCException: The socket timed out The The above exception was the direct cause of the following exception: part tells us that we are in the second case. If you were dealing with the first one, the message between the two tracebacks would be: During handling of the above exception, another exception occurred: References \u2691 Docs Git Homepage","title":"Mypy"},{"location":"devops/mypy/#installation","text":"pip install mypy","title":"Installation"},{"location":"devops/mypy/#configuration","text":"Mypy configuration is saved in the mypy.ini file, and they don't yet support pyproject.toml . File: mypy.ini [mypy] show_error_codes = True follow_imports = silent strict_optional = True warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True check_untyped_defs = True no_implicit_reexport = True warn_unused_configs = True disallow_subclassing_any = True disallow_incomplete_defs = True disallow_untyped_decorators = True disallow_untyped_calls = True # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = True You can use it both with: Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/mirrors-mypy rev : v0.782 hooks : - name : Run mypy static analysis tool id : mypy Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Mypy : runs-on : ubuntu-latest name : Mypy steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install Dependencies run : pip install mypy - name : mypy run : mypy","title":"Configuration"},{"location":"devops/mypy/#ignore-one-line","text":"Add # type: ignore to the line you want to skip.","title":"Ignore one line"},{"location":"devops/mypy/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"devops/mypy/#module-x-has-no-attribute-y","text":"If you're importing objects from your own module, you need to tell mypy that those objects are available. To do so in the __init__.py of your module, list them under the __all__ variable . File: init .py from .model import Entity __all__ = [ \"Entity\" , ]","title":"Module X has no attribute Y"},{"location":"devops/mypy/#w0707-consider-explicitly-re-raising-using-the-from","text":"keyword]( https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with ) The error can be raised by two cases. An exception was raised, we were handling it, and something went wrong in the process of handling it. An exception was raised, and we decided to replace it with a different exception that will make more sense to whoever called this code. try : self . connection , _ = self . sock . accept () except socket . timeout as error : raise IPCException ( 'The socket timed out' ) from error The error bit at the end tells Python: The IPCException that we\u2019re raising is just a friendlier version of the socket.timeout that we just caught. When we run that code and reach that exception, the traceback is going to look like this: Traceback (most recent call last): File \"foo.py\", line 19, in self.connection, _ = self.sock.accept() File \"foo.py\", line 7, in accept raise socket.timeout socket.timeout The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"foo.py\", line 21, in raise IPCException('The socket timed out') from e IPCException: The socket timed out The The above exception was the direct cause of the following exception: part tells us that we are in the second case. If you were dealing with the first one, the message between the two tracebacks would be: During handling of the above exception, another exception occurred:","title":"[W0707: Consider explicitly re-raising using the 'from'"},{"location":"devops/mypy/#references","text":"Docs Git Homepage","title":"References"},{"location":"devops/pip_tools/","text":"Pip-tools is a set of command line tools to help you keep your pip-based packages fresh, even when you've pinned them. For stability reasons it's a good idea to hardcode the dependencies versions. Furthermore, safety needs them to work properly. You can use this cookiecutter template to create a python project with pip-tools already configured. We've got three places where the dependencies are defined: setup.py should declare the loosest possible dependency versions that are still workable. Its job is to say what a particular package can work with. requirements.txt is a deployment manifest that defines an entire installation job, and shouldn't be thought of as tied to any one package. Its job is to declare an exhaustive list of all the necessary packages to make a deployment work. requirements-dev.txt Adds the dependencies required for the development of the program. Content of examples may be outdated An updated version of setup.py and requirements-dev.in can be found in the cookiecutter template . With pip-tools, the dependency management is trivial. Install the tool: pip install pip-tools Set the general dependencies in the setup.py install_requires . Generate the requirements.txt file: pip-compile -U --allow-unsafe ` The -U flag will try to upgrade the dependencies, and --allow-unsafe will let you manage the setuptools and pip dependencies. Add the additional testing dependencies in the requirements-dev.in file. File: requirements-dev.in -r requirements.txt pip-tools factory_boy pytest pytest-cov Compile the development requirements requirements-dev.txt with pip-compile dev-requirements.in . If you have another requirements.txt for the mkdocs documentation, run pip-compile docs/requirements.txt . To uninstall all pip packages use pip freeze | xargs pip uninstall -y Trigger hooks: Pre-commit: File: .pre-commit-config.yaml - repo : https://github.com/jazzband/pip-tools rev : 5.0.0 hooks : - name : Build requirements.txt id : pip-compile - name : Build dev-requirements.txt id : pip-compile args : [ 'dev-requirements.in' ] - name : Build mkdocs requirements.txt id : pip-compile args : [ 'docs/requirements.txt' ] pip-tools generates different results in the CI than in the development environment breaking the CI without an easy way to fix it. Therefore it should be run by the developers periodically. References \u2691 Git","title":"Pip-tools"},{"location":"devops/pip_tools/#references","text":"Git","title":"References"},{"location":"devops/proselint/","text":"Proselint is another linter for prose. Installation \u2691 pip install proselint Configuration \u2691 It can be configured through the ~/.config/proselint/config file, such as: { \"checks\" : { \"typography.diacritical_marks\" : false } } The Vim through the ALE plugin . Pre-commit : - repo : https://github.com/amperser/proselint/ rev : 0.10.2 hooks : - id : proselint exclude : LICENSE|requirements files : \\.(md|mdown|markdown)$ References \u2691 Git","title":"Proselint"},{"location":"devops/proselint/#installation","text":"pip install proselint","title":"Installation"},{"location":"devops/proselint/#configuration","text":"It can be configured through the ~/.config/proselint/config file, such as: { \"checks\" : { \"typography.diacritical_marks\" : false } } The Vim through the ALE plugin . Pre-commit : - repo : https://github.com/amperser/proselint/ rev : 0.10.2 hooks : - id : proselint exclude : LICENSE|requirements files : \\.(md|mdown|markdown)$","title":"Configuration"},{"location":"devops/proselint/#references","text":"Git","title":"References"},{"location":"devops/safety/","text":"Safety checks your installed dependencies for known security vulnerabilities. You can use this cookiecutter template to create a python project with safety already configured. Installation \u2691 pip install safety Configuration \u2691 Safety can be used through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-safety rev : v1.1.3 hooks : - id : python-safety-dependencies-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : Safety : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install safety - name : Execute safety run : safety check References \u2691 Git","title":"Safety"},{"location":"devops/safety/#installation","text":"pip install safety","title":"Installation"},{"location":"devops/safety/#configuration","text":"Safety can be used through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-safety rev : v1.1.3 hooks : - id : python-safety-dependencies-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : Safety : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install safety - name : Execute safety run : safety check","title":"Configuration"},{"location":"devops/safety/#references","text":"Git","title":"References"},{"location":"devops/write_good/","text":"write-good is a naive linter for English prose. Installation \u2691 npm install -g write-good There is no way to configure it through a configuration file, but it accepts command line arguments. The ALE vim implementation supports the specification of such flags with the ale_writegood_options variable: let g :ale_writegood_options = \"--no-passive\" Use write-good --help to see the available flags. As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter. References \u2691 Git","title":"Write Good"},{"location":"devops/write_good/#installation","text":"npm install -g write-good There is no way to configure it through a configuration file, but it accepts command line arguments. The ALE vim implementation supports the specification of such flags with the ale_writegood_options variable: let g :ale_writegood_options = \"--no-passive\" Use write-good --help to see the available flags. As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.","title":"Installation"},{"location":"devops/write_good/#references","text":"Git","title":"References"},{"location":"devops/yamllint/","text":"Yamllint is a linter for YAML files. yamllint does not only check for syntax validity, but for weirdnesses like key repetition and cosmetic problems such as lines length, trailing spaces or indentation. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/adrienverge/yamllint rev : v1.21.0 hooks : - id : yamllint References \u2691 Git Docs","title":"Yamllint"},{"location":"devops/yamllint/#references","text":"Git Docs","title":"References"},{"location":"devops/aws/aws/","text":"Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. In aggregate, these cloud computing web services provide a set of primitive abstract technical infrastructure and distributed computing building blocks and tools. Learn path \u2691 TBD","title":"AWS"},{"location":"devops/aws/aws/#learn-path","text":"TBD","title":"Learn path"},{"location":"devops/aws/eks/","text":"Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane. Upgrade an EKS cluster \u2691 New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes. To upgrade a cluster follow these steps: Upgrade all your charts to the latest version with helmfile . helmfile deps helmfile apply Check your current version and compare it with the one you want to upgrade. kubectl version --short kubectl get nodes Check the docs to see if the version you want to upgrade requires some special steps. If your worker nodes aren't at the same version as the cluster control plane upgrade them to the control plane version (never higher). Edit the cluster_version attribute of the eks terraform module and apply the changes (reviewing them first). terraform apply This is a long step (approximately 40 minutes) * Upgrade your charts again. References \u2691 Docs","title":"EKS"},{"location":"devops/aws/eks/#upgrade-an-eks-cluster","text":"New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes. To upgrade a cluster follow these steps: Upgrade all your charts to the latest version with helmfile . helmfile deps helmfile apply Check your current version and compare it with the one you want to upgrade. kubectl version --short kubectl get nodes Check the docs to see if the version you want to upgrade requires some special steps. If your worker nodes aren't at the same version as the cluster control plane upgrade them to the control plane version (never higher). Edit the cluster_version attribute of the eks terraform module and apply the changes (reviewing them first). terraform apply This is a long step (approximately 40 minutes) * Upgrade your charts again.","title":"Upgrade an EKS cluster"},{"location":"devops/aws/eks/#references","text":"Docs","title":"References"},{"location":"devops/aws/s3/","text":"S3 is the secure, durable, and scalable object storage infrastructure of AWS. Often used for serving static website content or holding backups or data. Commands \u2691 Bucket management \u2691 List buckets \u2691 aws s3 ls Create bucket \u2691 aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1 Enable versioning \u2691 aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled Enable encryption \u2691 aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' Download bucket \u2691 aws s3 cp --recursive s3:// {{ bucket_name }} . Audit the S3 bucket policy \u2691 IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done Add cache header to all items in a bucket \u2691 Log in to AWS Management Console. Go into S3 bucket. Select all files by route. Choose \"More\" from the menu. Select \"Change metadata\". In the \"Key\" field, select \"Cache-Control\" from the drop down menu max-age=604800Enter (7 days in seconds) for Value. Press \"Save\" button. Object management \u2691 Remove an object \u2691 aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }} Upload \u2691 Upload a local file with the cli \u2691 aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }} Upload a file unauthenticated \u2691 curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/ Restore an object \u2691 First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }} Copy objects between buckets \u2691 aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME Troubleshooting \u2691 get_environ_proxies() missing 1 required positional argument: 'no_proxy' \u2691 sudo pip3 install --upgrade boto3 Links \u2691 User guide","title":"S3"},{"location":"devops/aws/s3/#commands","text":"","title":"Commands"},{"location":"devops/aws/s3/#bucket-management","text":"","title":"Bucket management"},{"location":"devops/aws/s3/#list-buckets","text":"aws s3 ls","title":"List buckets"},{"location":"devops/aws/s3/#create-bucket","text":"aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1","title":"Create bucket"},{"location":"devops/aws/s3/#enable-versioning","text":"aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled","title":"Enable versioning"},{"location":"devops/aws/s3/#enable-encryption","text":"aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }'","title":"Enable encryption"},{"location":"devops/aws/s3/#download-bucket","text":"aws s3 cp --recursive s3:// {{ bucket_name }} .","title":"Download bucket"},{"location":"devops/aws/s3/#audit-the-s3-bucket-policy","text":"IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done","title":"Audit the S3 bucket policy"},{"location":"devops/aws/s3/#add-cache-header-to-all-items-in-a-bucket","text":"Log in to AWS Management Console. Go into S3 bucket. Select all files by route. Choose \"More\" from the menu. Select \"Change metadata\". In the \"Key\" field, select \"Cache-Control\" from the drop down menu max-age=604800Enter (7 days in seconds) for Value. Press \"Save\" button.","title":"Add cache header to all items in a bucket"},{"location":"devops/aws/s3/#object-management","text":"","title":"Object management"},{"location":"devops/aws/s3/#remove-an-object","text":"aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }}","title":"Remove an object"},{"location":"devops/aws/s3/#upload","text":"","title":"Upload"},{"location":"devops/aws/s3/#upload-a-local-file-with-the-cli","text":"aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }}","title":"Upload a local file with the cli"},{"location":"devops/aws/s3/#upload-a-file-unauthenticated","text":"curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/","title":"Upload a file unauthenticated"},{"location":"devops/aws/s3/#restore-an-object","text":"First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }}","title":"Restore an object"},{"location":"devops/aws/s3/#copy-objects-between-buckets","text":"aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME","title":"Copy objects between buckets"},{"location":"devops/aws/s3/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"devops/aws/s3/#get_environ_proxies-missing-1-required-positional-argument-no_proxy","text":"sudo pip3 install --upgrade boto3","title":"get_environ_proxies() missing 1 required positional argument: 'no_proxy'"},{"location":"devops/aws/s3/#links","text":"User guide","title":"Links"},{"location":"devops/aws/security_groups/","text":"Security groups are the AWS way of defining firewall rules between the resources. If not handled properly they can soon become hard to read, which can lead to an insecure infrastructure. It has helped me to use four types of security groups: Default security groups: Security groups created by AWS per VPC and region, they can't be deleted. Naming security groups: Used to identify an aws resource. They are usually referenced in other security groups. Ingress security groups: Used to define the rules of ingress traffic to the resource. Egress security groups: Used to define the rules of egress traffic to the resource. But what helped most has been using clinv while refactoring all the security groups. With clinv unused I got rid of all the security groups that weren't used by any AWS resource (beware of #16 , 17 , #18 and #19 ), then used the clinv unassigned security_groups to methodically decide if they were correct and add them to my inventory or if I needed to refactor them. Best practices \u2691 Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them. Naming convention \u2691 A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources Default security groups \u2691 There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws . {{ region_id }} } Where the provider must be configured in the `terraform_config . tf` file , for example: ``` terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" } Naming security groups \u2691 For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$ { 3:vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2 . id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file. Ingress security groups \u2691 For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Egress security groups \u2691 For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Instance security group definition \u2691 When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data . terraform_remote_state . security_groups . outputs . gitea_instance_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_monitoring_at_privatedmz_id , data . terraform_remote_state . security_groups . outputs . ingress_administration_from_bastion_at_connectiondmz_id , data . terraform_remote_state . security_groups . outputs . egress_ldap_to_ldap_instance_from_publicdmz_id , data . terraform_remote_state . security_groups . outputs . egress_https_to_debian_repositories_from_publicdmz_id , ]","title":"Security groups workflow"},{"location":"devops/aws/security_groups/#best-practices","text":"Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them.","title":"Best practices"},{"location":"devops/aws/security_groups/#naming-convention","text":"A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources","title":"Naming convention"},{"location":"devops/aws/security_groups/#default-security-groups","text":"There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws . {{ region_id }} } Where the provider must be configured in the `terraform_config . tf` file , for example: ``` terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" }","title":"Default security groups"},{"location":"devops/aws/security_groups/#naming-security-groups","text":"For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$ { 3:vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2 . id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Naming security groups"},{"location":"devops/aws/security_groups/#ingress-security-groups","text":"For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group . ingress_$ 1 _from_$ 2 _at_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Ingress security groups"},{"location":"devops/aws/security_groups/#egress-security-groups","text":"For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data . terraform_remote_state . vpc . outputs . vpc_$3_id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = ${ 4:port } to_port = ${ 5:$4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id source_security_group_id = aws_security_group . $ 7 . id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group . egress_$ 1 _to_$ 2 _from_$ 3 . id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Egress security groups"},{"location":"devops/aws/security_groups/#instance-security-group-definition","text":"When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data . terraform_remote_state . security_groups . outputs . gitea_instance_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data . terraform_remote_state . security_groups . outputs . ingress_http_from_monitoring_at_privatedmz_id , data . terraform_remote_state . security_groups . outputs . ingress_administration_from_bastion_at_connectiondmz_id , data . terraform_remote_state . security_groups . outputs . egress_ldap_to_ldap_instance_from_publicdmz_id , data . terraform_remote_state . security_groups . outputs . egress_https_to_debian_repositories_from_publicdmz_id , ]","title":"Instance security group definition"},{"location":"devops/aws/iam/iam/","text":"AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization). Configurable AWS access controls: Grant access to AWS Management console, APIs Create individual users Manage permissions with groups Configure a strong password policy Enable Multi-Factor Authentication for privileged users Use IAM roles for EC2 instances Use IAM roles to share access Rotate security credentials regularly Restrict privileged access further with conditions Use your corporate directory system or a third party authentication Links \u2691 Docs","title":"IAM"},{"location":"devops/aws/iam/iam/#links","text":"Docs","title":"Links"},{"location":"devops/aws/iam/iam_commands/","text":"Information gathering \u2691 List roles \u2691 aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table List policies \u2691 aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table List attached policies \u2691 aws iam list-attached-role-policies --role-name {{ role_name }} Get role configuration \u2691 aws iam get-role --role-name {{ role_name }} Get role policies \u2691 aws iam list-role-policies --role-name {{ role_name }}","title":"IAM Commands"},{"location":"devops/aws/iam/iam_commands/#information-gathering","text":"","title":"Information gathering"},{"location":"devops/aws/iam/iam_commands/#list-roles","text":"aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table","title":"List roles"},{"location":"devops/aws/iam/iam_commands/#list-policies","text":"aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table","title":"List policies"},{"location":"devops/aws/iam/iam_commands/#list-attached-policies","text":"aws iam list-attached-role-policies --role-name {{ role_name }}","title":"List attached policies"},{"location":"devops/aws/iam/iam_commands/#get-role-configuration","text":"aws iam get-role --role-name {{ role_name }}","title":"Get role configuration"},{"location":"devops/aws/iam/iam_commands/#get-role-policies","text":"aws iam list-role-policies --role-name {{ role_name }}","title":"Get role policies"},{"location":"devops/aws/iam/iam_debug/","text":"MFADevice entity at the same path and name already exists \u2691 It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"IAM Debugging"},{"location":"devops/aws/iam/iam_debug/#mfadevice-entity-at-the-same-path-and-name-already-exists","text":"It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"MFADevice entity at the same path and name already exists"},{"location":"devops/helm/helm/","text":"Helm is the package manager for Kubernetes. Through charts it helps you define, install and upgrade even the most complex Kubernetes applications. The advantages of using helm over kubectl apply are the easiness of: Repeatable application installation. CI integration. Versioning and sharing. Charts are a group of Go templates of kubernetes yaml resource manifests, they are easy to create, version, share, and publish. Helm alone lacks some features, that are satisfied through some external programs: Helmfile is used to declaratively configure your charts, so they can be versioned through git. Helm-secrets is used to remove hardcoded credentials from values.yaml files. Helm has an open issue to integrate it into it's codebase. Helm-git is used to install helm charts directly from Git repositories. Links \u2691 Homepage Docs Git Chart hub Git charts repositories","title":"Helm"},{"location":"devops/helm/helm/#links","text":"Homepage Docs Git Chart hub Git charts repositories","title":"Links"},{"location":"devops/helm/helm_commands/","text":"Small cheatsheet on how to use the helm command. List charts \u2691 helm ls Get information of chart \u2691 helm inspect {{ package_name }} List all the available versions of a chart \u2691 helm search -l {{ package_name }} Download a chart \u2691 helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }} Search charts \u2691 helm search {{ package_name }} Operations you should do with helmfile \u2691 The following operations can be done with helm, but consider using helmfile instead. Install chart \u2691 Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }} Give it a name \u2691 helm install --name {{ release_name }} {{ package_name }} Give it a namespace \u2691 helm install --namespace {{ namespace }} {{ package_name }} Customize the chart before installing \u2691 helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }} Upgrade a release \u2691 If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }} Rollback an upgrade \u2691 First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }} Delete a release \u2691 helm delete --purge {{ release_name }} Working with repositories \u2691 List repositories \u2691 helm repo list Add repository \u2691 helm repo add {{ repo_name }} {{ repo_url }} Update repositories \u2691 helm repo update","title":"Helm Commands"},{"location":"devops/helm/helm_commands/#list-charts","text":"helm ls","title":"List charts"},{"location":"devops/helm/helm_commands/#get-information-of-chart","text":"helm inspect {{ package_name }}","title":"Get information of chart"},{"location":"devops/helm/helm_commands/#list-all-the-available-versions-of-a-chart","text":"helm search -l {{ package_name }}","title":"List all the available versions of a chart"},{"location":"devops/helm/helm_commands/#download-a-chart","text":"helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }}","title":"Download a chart"},{"location":"devops/helm/helm_commands/#search-charts","text":"helm search {{ package_name }}","title":"Search charts"},{"location":"devops/helm/helm_commands/#operations-you-should-do-with-helmfile","text":"The following operations can be done with helm, but consider using helmfile instead.","title":"Operations you should do with helmfile"},{"location":"devops/helm/helm_commands/#install-chart","text":"Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }}","title":"Install chart"},{"location":"devops/helm/helm_commands/#give-it-a-name","text":"helm install --name {{ release_name }} {{ package_name }}","title":"Give it a name"},{"location":"devops/helm/helm_commands/#give-it-a-namespace","text":"helm install --namespace {{ namespace }} {{ package_name }}","title":"Give it a namespace"},{"location":"devops/helm/helm_commands/#customize-the-chart-before-installing","text":"helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }}","title":"Customize the chart before installing"},{"location":"devops/helm/helm_commands/#upgrade-a-release","text":"If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }}","title":"Upgrade a release"},{"location":"devops/helm/helm_commands/#rollback-an-upgrade","text":"First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }}","title":"Rollback an upgrade"},{"location":"devops/helm/helm_commands/#delete-a-release","text":"helm delete --purge {{ release_name }}","title":"Delete a release"},{"location":"devops/helm/helm_commands/#working-with-repositories","text":"","title":"Working with repositories"},{"location":"devops/helm/helm_commands/#list-repositories","text":"helm repo list","title":"List repositories"},{"location":"devops/helm/helm_commands/#add-repository","text":"helm repo add {{ repo_name }} {{ repo_url }}","title":"Add repository"},{"location":"devops/helm/helm_commands/#update-repositories","text":"helm repo update","title":"Update repositories"},{"location":"devops/helm/helm_installation/","text":"There are two usable versions of Helm, v2 and v3, the latter is quite new so some of the things we need to install as of 2020-01-27 are not yet supported (Prometheus operator), so we are going to stick to the version 2. Helm has a client-server architecture, the server is installed in the Kubernetes cluster and the client is a Go executable installed in the user computer. Helm client \u2691 You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm Installation"},{"location":"devops/helm/helm_installation/#helm-client","text":"You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm client"},{"location":"devops/helm/helm_secrets/","text":"Helm-secrets is a helm plugin that manages secrets with Git workflow and stores them anywhere. It delegates the cryptographic operations to Mozilla's Sops tool, which supports PGP, AWS KMS and GCP KMS. The configuration is stored in .sops.yaml files. You can find in Mozilla's documentation a detailed configuration guide. For my use case, I'm only going to use a list of PGP keys, so the following contents should be in the .sops.yaml file at the project root directory. creation_rules : - pgp : >- {{ gpg_key_1 }}, {{ gpg_key_2}} Prevent committing decrypted files to git \u2691 From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit Usage \u2691 Encrypt secret files \u2691 Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials. Edit secret files \u2691 helm secrets edit secrets.yaml Decrypt secret files \u2691 helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git . Clean all the decrypted files \u2691 helm secrets clean . Add or remove keys \u2691 Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options. Links \u2691 Git","title":"Helm Secrets"},{"location":"devops/helm/helm_secrets/#prevent-committing-decrypted-files-to-git","text":"From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit","title":"Prevent committing decrypted files to git"},{"location":"devops/helm/helm_secrets/#usage","text":"","title":"Usage"},{"location":"devops/helm/helm_secrets/#encrypt-secret-files","text":"Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials.","title":"Encrypt secret files"},{"location":"devops/helm/helm_secrets/#edit-secret-files","text":"helm secrets edit secrets.yaml","title":"Edit secret files"},{"location":"devops/helm/helm_secrets/#decrypt-secret-files","text":"helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git .","title":"Decrypt secret files"},{"location":"devops/helm/helm_secrets/#clean-all-the-decrypted-files","text":"helm secrets clean .","title":"Clean all the decrypted files"},{"location":"devops/helm/helm_secrets/#add-or-remove-keys","text":"Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options.","title":"Add or remove keys"},{"location":"devops/helm/helm_secrets/#links","text":"Git","title":"Links"},{"location":"devops/kong/kong/","text":"Kong is a lua application API platform running in Nginx. Installation \u2691 Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use. Links \u2691 Homepage Docs","title":"Kong"},{"location":"devops/kong/kong/#installation","text":"Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use.","title":"Installation"},{"location":"devops/kong/kong/#links","text":"Homepage Docs","title":"Links"},{"location":"devops/kubectl/kubectl/","text":"Kubectl Definition Kubectl is a command line tool for controlling Kubernetes clusters. kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag. Resource types and it's aliases \u2691 Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources Links \u2691 Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.","title":"Kubectl"},{"location":"devops/kubectl/kubectl/#resource-types-and-its-aliases","text":"Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources","title":"Resource types and it's aliases"},{"location":"devops/kubectl/kubectl/#links","text":"Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.","title":"Links"},{"location":"devops/kubectl/kubectl_commands/","text":"Configuration and context \u2691 Add a new cluster to your kubeconf that supports basic auth \u2691 kubectl config set-credentials {{ username }} / {{ cluster_dns }} --username ={{ username }} --password ={{ password }} Create new context \u2691 kubectl config set-context {{ context_name }} --user ={{ username }} --namespace ={{ namespace }} Get current context \u2691 kubectl config current-context List contexts \u2691 kubectl config get-contexts Switch context \u2691 kubectl config use-context {{ context_name }} Creating objects \u2691 Create Resource \u2691 kubectl create -f {{ resource.definition.yaml | dir.where.yamls.live | url.to.yaml }} --record Deleting resources \u2691 Delete the pod using the type and name specified in a file \u2691 kubectl delete -f {{ path_to_file }} Delete pods and services by name \u2691 kubectl delete pod,service {{ pod_names }} {{ service_names }} Delete pods and services by label \u2691 kubectl delete pod,services -l {{ label_name }}={{ label_value }} Delete all pods and services in namespace \u2691 kubectl -n {{ namespace_name }} delete po,svc --all Delete all evicted pods \u2691 while read i ; do kubectl delete pod \" $i \" ; done < < ( kubectl get pods | grep -i evicted | sed 's/ .*//g' ) Editing resources \u2691 Edit a service \u2691 kubectl edit svc/ {{ service_name }} Information gathering \u2691 Get credentials \u2691 Get credentials kubectl config view --minify Deployments \u2691 View status of deployments \u2691 kubectl get deployments Describe Deployments \u2691 kubectl describe deployment {{ deployment_name }} Get images of deployment \u2691 kubectl get pods --selector = app ={{ deployment_name }} -o json | \\ jq '.items[] | .metadata.name + \": \" + .spec.containers[0].image' Nodes \u2691 List all nodes \u2691 kubectl get nodes Check which nodes are ready \u2691 JSONPATH = '{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath = $JSONPATH | grep \"Ready=True\" External IPs of all nodes \u2691 kubectl get nodes -o jsonpath = '{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' Get exposed ports of node \u2691 export NODE_PORT = $( kubectl get services/kubernetes-bootcamp -o go-template = '{{(index .spec.ports 0).nodePort}}' ) Pods \u2691 List all pods in the current namespace \u2691 kubectl get pods List all pods in all namespaces \u2691 kubectl get pods --all-namespaces List all pods of a selected namespace \u2691 kubectl get pods -n {{ namespace }} List with more detail \u2691 kubectl get pods -o wide Get pods of a selected deployment \u2691 kubectl get pods --selector = \"name={{ name }}\" Get pods of a given label \u2691 kubectl get pods -l {{ label_name }} Get pods by IP \u2691 kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE alpine-3835730047-ggn2v 1 /1 Running 0 5d 10 .22.19.69 ip-10-35-80-221.ec2.internal Sort pods by restart count \u2691 kubectl get pods --sort-by = '.status.containerStatuses[0].restartCount' Describe Pods \u2691 kubectl describe pods {{ pod_name }} Get name of pod \u2691 pod = $( kubectl get pod --selector ={{ selector_label }}={{ selector_value }} -o jsonpath ={ .items..metadata.name } ) List pods that belong to a particular RC \u2691 sel = ${ $( kubectl get rc my-rc --output = json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"' ) %? } echo $( kubectl get pods --selector = $sel --output = jsonpath ={ .items..metadata.name } ) Services \u2691 List services in namespace \u2691 kubectl get services List services sorted by name kubectl get services --sort-by = .metadata.name Describe Services \u2691 kubectl describe services {{ service_name }} kubectl describe svc {{ service_name }} Replication controller \u2691 List all replication controller \u2691 kubectl get rc Secrets \u2691 View status of secrets \u2691 kubectl get secrets Namespaces \u2691 View namespaces \u2691 kubectl get namespaces Limits \u2691 kubectl get limitrange kubectl describe limitrange limits Jobs and cronjobs \u2691 Get cronjobs of a namespace \u2691 kubectl get cronjobs -n {{ namespace }} Get jobs of a namespace \u2691 kubectl get jobs -n {{ namespace }} You can then describe a specific job to get the pod it created. kubectl describe job -n {{ namespace }} {{ job_name }} And now you can see the evolution of the job with: kubectl logs -n {{ namespace }} {{ pod_name }} Interacting with nodes and cluster \u2691 Mark node as unschedulable \u2691 kubectl cordon {{ node_name }} Mark node as schedulable \u2691 kubectl uncordon {{ node_name }} Drain node in preparation for maintenance \u2691 kubectl drain {{ node_name }} Show metrics of all node \u2691 kubectl top node Show metrics of a node \u2691 kubectl top node {{ node_name }} Display addresses of the master and servies \u2691 kubectl cluster-info Dump current cluster state to stdout \u2691 kubectl cluster-info dump Dump current cluster state to directory \u2691 kubectl cluster-info dump --output-directory ={{ path_to_directory }} Interacting with pods \u2691 Dump logs of pod \u2691 kubectl logs {{ pod_name }} Dump logs of pod and specified container \u2691 kubectl logs {{ pod_name }} -c {{ container_name }} Stream logs of pod \u2691 kubectl logs -f {{ pod_name }} kubectl logs -f {{ pod_name }} -c {{ container_name }} Another option is to use the kubetail program. Attach to running container \u2691 kubectl attach {{ pod_name }} -i Get a shell of a running container \u2691 kubectl exec {{ pod_name }} -it bash Get a debian container inside kubernetes \u2691 kubectl run --generator = run-pod/v1 -i --tty debian --image = debian -- bash Get a root shell of a running container \u2691 Get the Node where the pod is and the docker ID kubectl describe pod {{ pod_name }} SSH into the node ssh {{ node }} Get into docker docker exec -it -u root {{ docker_id }} bash Forward port of pod to your local machine \u2691 kubectl port-forward {{ pod_name }} {{ pod_port }} : {{ local_port }} Expose port \u2691 kubectl expose {{ deployment_name }} --type = \"{{ expose_type }}\" --port {{ port_number }} Where expose_type is one of: ['NodePort', 'ClusterIP', 'LoadBalancer', 'externalName'] Run command on existing pod \u2691 kubectl exec {{ pod_name }} -- ls / kubectl exec {{ pod_name }} -c {{ container_name }} -- ls / Show metrics for a given pod and it's containers \u2691 kubectl top pod {{ pod_name }} --containers Extract file from pod \u2691 kubectl cp {{ container_id }} : {{ path_to_file }} {{ path_to_local_file }} Scaling resources \u2691 Scale a deployment with a specified size \u2691 kubectl scale deployment {{ deployment_name }} --replicas {{ replicas_number }} Scale a replicaset \u2691 kubectl scale --replicas ={{ replicas_number }} rs/ {{ replicaset_name }} Scale a resource specified in a file \u2691 kubectl scale --replicas ={{ replicas_number }} -f {{ path_to_yaml }} Updating resources \u2691 Namespaces \u2691 Temporary set the namespace for a request \u2691 kubectl -n {{ namespace_name }} {{ command_to_execute }} kubectl --namespace ={{ namespace_name }} {{ command_to_execute }} Permanently set the namespace for a request \u2691 kubectl config set-context $( kubectl config current-context ) --namespace ={{ namespace_name }} Deployment \u2691 Modify the image of a deployment \u2691 kubectl set image {{ deployment_name }} {{ label }} : {{ label_value }} for example kubectl set image deployment/nginx-deployment nginx = nginx:1.9.1 Or edit it by hand kubectl edit {{ deployment_name }} Get the status of the rolling update \u2691 kubectl rollout status {{ deployment_name }} Get the history of the deployment \u2691 kubectl rollout history deployment {{ deployment_name }} To get more details of a selected revision: kubectl rollout history deployment {{ deployment_name }} --revision ={{ revision_number }} Get back to a specified revision \u2691 To get to the last version kubectl rollout undo deployment {{ deployment_name }} To go to a specific version kubectl rollout undo {{ deployment_name }} --to-revision ={{ revision_number }} Pods \u2691 Rolling update of pods \u2691 Is prefered to use the deployment rollout kubectl rolling-update {{ pod_name }} -f {{ new_pod_definition_yaml }} Change the name of the resource and update the image \u2691 kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --image = image: {{ new_pod_version }} Abort existing rollout in progress \u2691 kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --rollback Force replace, delete and then re-create the resource \u2691 ** Will cause a service outage ** kubectl replace --force -f {{ new_pod_yaml }} Add a label \u2691 kubectl label pods {{ pod_name }} new-label ={{ new_label }} Autoscale a deployment \u2691 kubectl autoscale deployment {{ deployment_name }} --min ={{ min_instances }} --max ={{ max_instances }} [ --cpu-percent ={{ cpu_percent }}] Copy resources between namespaces \u2691 kubectl get rs,secrets -o json --namespace old | jq '.items[].metadata.namespace = \"new\"' | kubectl create-f - Formatting output \u2691 Print a table using a comma separated list of custom columns \u2691 -o = custom-columns = <spec> Print a table using the custom columns template in the file \u2691 -o = custom-columns-file = <filename> Output a JSON formatted API object \u2691 -o = json Print the fields defined in a jsonpath expression \u2691 -o = jsonpath = <template> Print the fields defined by the jsonpath expression in the file \u2691 -o = jsonpath-file = <filename> Print only the resource name and nothing else \u2691 -o = name Output in the plain-text format with any additional information, and for pods, the node name is included \u2691 -o = wide Output a YAML formatted API object \u2691 -o = yaml","title":"Kubectl Commands"},{"location":"devops/kubectl/kubectl_commands/#configuration-and-context","text":"","title":"Configuration and context"},{"location":"devops/kubectl/kubectl_commands/#add-a-new-cluster-to-your-kubeconf-that-supports-basic-auth","text":"kubectl config set-credentials {{ username }} / {{ cluster_dns }} --username ={{ username }} --password ={{ password }}","title":"Add a new cluster to your kubeconf that supports basic auth"},{"location":"devops/kubectl/kubectl_commands/#create-new-context","text":"kubectl config set-context {{ context_name }} --user ={{ username }} --namespace ={{ namespace }}","title":"Create new context"},{"location":"devops/kubectl/kubectl_commands/#get-current-context","text":"kubectl config current-context","title":"Get current context"},{"location":"devops/kubectl/kubectl_commands/#list-contexts","text":"kubectl config get-contexts","title":"List contexts"},{"location":"devops/kubectl/kubectl_commands/#switch-context","text":"kubectl config use-context {{ context_name }}","title":"Switch context"},{"location":"devops/kubectl/kubectl_commands/#creating-objects","text":"","title":"Creating objects"},{"location":"devops/kubectl/kubectl_commands/#create-resource","text":"kubectl create -f {{ resource.definition.yaml | dir.where.yamls.live | url.to.yaml }} --record","title":"Create Resource"},{"location":"devops/kubectl/kubectl_commands/#deleting-resources","text":"","title":"Deleting resources"},{"location":"devops/kubectl/kubectl_commands/#delete-the-pod-using-the-type-and-name-specified-in-a-file","text":"kubectl delete -f {{ path_to_file }}","title":"Delete the pod using the type and name specified in a file"},{"location":"devops/kubectl/kubectl_commands/#delete-pods-and-services-by-name","text":"kubectl delete pod,service {{ pod_names }} {{ service_names }}","title":"Delete pods and services by name"},{"location":"devops/kubectl/kubectl_commands/#delete-pods-and-services-by-label","text":"kubectl delete pod,services -l {{ label_name }}={{ label_value }}","title":"Delete pods and services by label"},{"location":"devops/kubectl/kubectl_commands/#delete-all-pods-and-services-in-namespace","text":"kubectl -n {{ namespace_name }} delete po,svc --all","title":"Delete all pods and services in namespace"},{"location":"devops/kubectl/kubectl_commands/#delete-all-evicted-pods","text":"while read i ; do kubectl delete pod \" $i \" ; done < < ( kubectl get pods | grep -i evicted | sed 's/ .*//g' )","title":"Delete all evicted pods"},{"location":"devops/kubectl/kubectl_commands/#editing-resources","text":"","title":"Editing resources"},{"location":"devops/kubectl/kubectl_commands/#edit-a-service","text":"kubectl edit svc/ {{ service_name }}","title":"Edit a service"},{"location":"devops/kubectl/kubectl_commands/#information-gathering","text":"","title":"Information gathering"},{"location":"devops/kubectl/kubectl_commands/#get-credentials","text":"Get credentials kubectl config view --minify","title":"Get credentials"},{"location":"devops/kubectl/kubectl_commands/#deployments","text":"","title":"Deployments"},{"location":"devops/kubectl/kubectl_commands/#view-status-of-deployments","text":"kubectl get deployments","title":"View status of deployments"},{"location":"devops/kubectl/kubectl_commands/#describe-deployments","text":"kubectl describe deployment {{ deployment_name }}","title":"Describe Deployments"},{"location":"devops/kubectl/kubectl_commands/#get-images-of-deployment","text":"kubectl get pods --selector = app ={{ deployment_name }} -o json | \\ jq '.items[] | .metadata.name + \": \" + .spec.containers[0].image'","title":"Get images of deployment"},{"location":"devops/kubectl/kubectl_commands/#nodes","text":"","title":"Nodes"},{"location":"devops/kubectl/kubectl_commands/#list-all-nodes","text":"kubectl get nodes","title":"List all nodes"},{"location":"devops/kubectl/kubectl_commands/#check-which-nodes-are-ready","text":"JSONPATH = '{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath = $JSONPATH | grep \"Ready=True\"","title":"Check which nodes are ready"},{"location":"devops/kubectl/kubectl_commands/#external-ips-of-all-nodes","text":"kubectl get nodes -o jsonpath = '{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}'","title":"External IPs of all nodes"},{"location":"devops/kubectl/kubectl_commands/#get-exposed-ports-of-node","text":"export NODE_PORT = $( kubectl get services/kubernetes-bootcamp -o go-template = '{{(index .spec.ports 0).nodePort}}' )","title":"Get exposed ports of node"},{"location":"devops/kubectl/kubectl_commands/#pods","text":"","title":"Pods"},{"location":"devops/kubectl/kubectl_commands/#list-all-pods-in-the-current-namespace","text":"kubectl get pods","title":"List all pods in the current namespace"},{"location":"devops/kubectl/kubectl_commands/#list-all-pods-in-all-namespaces","text":"kubectl get pods --all-namespaces","title":"List all pods in all namespaces"},{"location":"devops/kubectl/kubectl_commands/#list-all-pods-of-a-selected-namespace","text":"kubectl get pods -n {{ namespace }}","title":"List all pods of a selected namespace"},{"location":"devops/kubectl/kubectl_commands/#list-with-more-detail","text":"kubectl get pods -o wide","title":"List with more detail"},{"location":"devops/kubectl/kubectl_commands/#get-pods-of-a-selected-deployment","text":"kubectl get pods --selector = \"name={{ name }}\"","title":"Get pods of a selected deployment"},{"location":"devops/kubectl/kubectl_commands/#get-pods-of-a-given-label","text":"kubectl get pods -l {{ label_name }}","title":"Get pods of a given label"},{"location":"devops/kubectl/kubectl_commands/#get-pods-by-ip","text":"kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE alpine-3835730047-ggn2v 1 /1 Running 0 5d 10 .22.19.69 ip-10-35-80-221.ec2.internal","title":"Get pods by IP"},{"location":"devops/kubectl/kubectl_commands/#sort-pods-by-restart-count","text":"kubectl get pods --sort-by = '.status.containerStatuses[0].restartCount'","title":"Sort pods by restart count"},{"location":"devops/kubectl/kubectl_commands/#describe-pods","text":"kubectl describe pods {{ pod_name }}","title":"Describe Pods"},{"location":"devops/kubectl/kubectl_commands/#get-name-of-pod","text":"pod = $( kubectl get pod --selector ={{ selector_label }}={{ selector_value }} -o jsonpath ={ .items..metadata.name } )","title":"Get name of pod"},{"location":"devops/kubectl/kubectl_commands/#list-pods-that-belong-to-a-particular-rc","text":"sel = ${ $( kubectl get rc my-rc --output = json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"' ) %? } echo $( kubectl get pods --selector = $sel --output = jsonpath ={ .items..metadata.name } )","title":"List pods that belong to a particular RC"},{"location":"devops/kubectl/kubectl_commands/#services","text":"","title":"Services"},{"location":"devops/kubectl/kubectl_commands/#list-services-in-namespace","text":"kubectl get services List services sorted by name kubectl get services --sort-by = .metadata.name","title":"List services in namespace"},{"location":"devops/kubectl/kubectl_commands/#describe-services","text":"kubectl describe services {{ service_name }} kubectl describe svc {{ service_name }}","title":"Describe Services"},{"location":"devops/kubectl/kubectl_commands/#replication-controller","text":"","title":"Replication controller"},{"location":"devops/kubectl/kubectl_commands/#list-all-replication-controller","text":"kubectl get rc","title":"List all replication controller"},{"location":"devops/kubectl/kubectl_commands/#secrets","text":"","title":"Secrets"},{"location":"devops/kubectl/kubectl_commands/#view-status-of-secrets","text":"kubectl get secrets","title":"View status of secrets"},{"location":"devops/kubectl/kubectl_commands/#namespaces","text":"","title":"Namespaces"},{"location":"devops/kubectl/kubectl_commands/#view-namespaces","text":"kubectl get namespaces","title":"View namespaces"},{"location":"devops/kubectl/kubectl_commands/#limits","text":"kubectl get limitrange kubectl describe limitrange limits","title":"Limits"},{"location":"devops/kubectl/kubectl_commands/#jobs-and-cronjobs","text":"","title":"Jobs and cronjobs"},{"location":"devops/kubectl/kubectl_commands/#get-cronjobs-of-a-namespace","text":"kubectl get cronjobs -n {{ namespace }}","title":"Get cronjobs of a namespace"},{"location":"devops/kubectl/kubectl_commands/#get-jobs-of-a-namespace","text":"kubectl get jobs -n {{ namespace }} You can then describe a specific job to get the pod it created. kubectl describe job -n {{ namespace }} {{ job_name }} And now you can see the evolution of the job with: kubectl logs -n {{ namespace }} {{ pod_name }}","title":"Get jobs of a namespace"},{"location":"devops/kubectl/kubectl_commands/#interacting-with-nodes-and-cluster","text":"","title":"Interacting with nodes and cluster"},{"location":"devops/kubectl/kubectl_commands/#mark-node-as-unschedulable","text":"kubectl cordon {{ node_name }}","title":"Mark node as unschedulable"},{"location":"devops/kubectl/kubectl_commands/#mark-node-as-schedulable","text":"kubectl uncordon {{ node_name }}","title":"Mark node as schedulable"},{"location":"devops/kubectl/kubectl_commands/#drain-node-in-preparation-for-maintenance","text":"kubectl drain {{ node_name }}","title":"Drain node in preparation for maintenance"},{"location":"devops/kubectl/kubectl_commands/#show-metrics-of-all-node","text":"kubectl top node","title":"Show metrics of all node"},{"location":"devops/kubectl/kubectl_commands/#show-metrics-of-a-node","text":"kubectl top node {{ node_name }}","title":"Show metrics of a node"},{"location":"devops/kubectl/kubectl_commands/#display-addresses-of-the-master-and-servies","text":"kubectl cluster-info","title":"Display addresses of the master and servies"},{"location":"devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-stdout","text":"kubectl cluster-info dump","title":"Dump current cluster state to stdout"},{"location":"devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-directory","text":"kubectl cluster-info dump --output-directory ={{ path_to_directory }}","title":"Dump current cluster state to directory"},{"location":"devops/kubectl/kubectl_commands/#interacting-with-pods","text":"","title":"Interacting with pods"},{"location":"devops/kubectl/kubectl_commands/#dump-logs-of-pod","text":"kubectl logs {{ pod_name }}","title":"Dump logs of pod"},{"location":"devops/kubectl/kubectl_commands/#dump-logs-of-pod-and-specified-container","text":"kubectl logs {{ pod_name }} -c {{ container_name }}","title":"Dump logs of pod and specified container"},{"location":"devops/kubectl/kubectl_commands/#stream-logs-of-pod","text":"kubectl logs -f {{ pod_name }} kubectl logs -f {{ pod_name }} -c {{ container_name }} Another option is to use the kubetail program.","title":"Stream logs of pod"},{"location":"devops/kubectl/kubectl_commands/#attach-to-running-container","text":"kubectl attach {{ pod_name }} -i","title":"Attach to running container"},{"location":"devops/kubectl/kubectl_commands/#get-a-shell-of-a-running-container","text":"kubectl exec {{ pod_name }} -it bash","title":"Get a shell of a running container"},{"location":"devops/kubectl/kubectl_commands/#get-a-debian-container-inside-kubernetes","text":"kubectl run --generator = run-pod/v1 -i --tty debian --image = debian -- bash","title":"Get a debian container inside kubernetes"},{"location":"devops/kubectl/kubectl_commands/#get-a-root-shell-of-a-running-container","text":"Get the Node where the pod is and the docker ID kubectl describe pod {{ pod_name }} SSH into the node ssh {{ node }} Get into docker docker exec -it -u root {{ docker_id }} bash","title":"Get a root shell of a running container"},{"location":"devops/kubectl/kubectl_commands/#forward-port-of-pod-to-your-local-machine","text":"kubectl port-forward {{ pod_name }} {{ pod_port }} : {{ local_port }}","title":"Forward port of pod to your local machine"},{"location":"devops/kubectl/kubectl_commands/#expose-port","text":"kubectl expose {{ deployment_name }} --type = \"{{ expose_type }}\" --port {{ port_number }} Where expose_type is one of: ['NodePort', 'ClusterIP', 'LoadBalancer', 'externalName']","title":"Expose port"},{"location":"devops/kubectl/kubectl_commands/#run-command-on-existing-pod","text":"kubectl exec {{ pod_name }} -- ls / kubectl exec {{ pod_name }} -c {{ container_name }} -- ls /","title":"Run command on existing pod"},{"location":"devops/kubectl/kubectl_commands/#show-metrics-for-a-given-pod-and-its-containers","text":"kubectl top pod {{ pod_name }} --containers","title":"Show metrics for a given pod and it's containers"},{"location":"devops/kubectl/kubectl_commands/#extract-file-from-pod","text":"kubectl cp {{ container_id }} : {{ path_to_file }} {{ path_to_local_file }}","title":"Extract file from pod"},{"location":"devops/kubectl/kubectl_commands/#scaling-resources","text":"","title":"Scaling resources"},{"location":"devops/kubectl/kubectl_commands/#scale-a-deployment-with-a-specified-size","text":"kubectl scale deployment {{ deployment_name }} --replicas {{ replicas_number }}","title":"Scale a deployment with a specified size"},{"location":"devops/kubectl/kubectl_commands/#scale-a-replicaset","text":"kubectl scale --replicas ={{ replicas_number }} rs/ {{ replicaset_name }}","title":"Scale a replicaset"},{"location":"devops/kubectl/kubectl_commands/#scale-a-resource-specified-in-a-file","text":"kubectl scale --replicas ={{ replicas_number }} -f {{ path_to_yaml }}","title":"Scale a resource specified in a file"},{"location":"devops/kubectl/kubectl_commands/#updating-resources","text":"","title":"Updating resources"},{"location":"devops/kubectl/kubectl_commands/#namespaces_1","text":"","title":"Namespaces"},{"location":"devops/kubectl/kubectl_commands/#temporary-set-the-namespace-for-a-request","text":"kubectl -n {{ namespace_name }} {{ command_to_execute }} kubectl --namespace ={{ namespace_name }} {{ command_to_execute }}","title":"Temporary set the namespace for a request"},{"location":"devops/kubectl/kubectl_commands/#permanently-set-the-namespace-for-a-request","text":"kubectl config set-context $( kubectl config current-context ) --namespace ={{ namespace_name }}","title":"Permanently set the namespace for a request"},{"location":"devops/kubectl/kubectl_commands/#deployment","text":"","title":"Deployment"},{"location":"devops/kubectl/kubectl_commands/#modify-the-image-of-a-deployment","text":"kubectl set image {{ deployment_name }} {{ label }} : {{ label_value }} for example kubectl set image deployment/nginx-deployment nginx = nginx:1.9.1 Or edit it by hand kubectl edit {{ deployment_name }}","title":"Modify the image of a deployment"},{"location":"devops/kubectl/kubectl_commands/#get-the-status-of-the-rolling-update","text":"kubectl rollout status {{ deployment_name }}","title":"Get the status of the rolling update"},{"location":"devops/kubectl/kubectl_commands/#get-the-history-of-the-deployment","text":"kubectl rollout history deployment {{ deployment_name }} To get more details of a selected revision: kubectl rollout history deployment {{ deployment_name }} --revision ={{ revision_number }}","title":"Get the history of the deployment"},{"location":"devops/kubectl/kubectl_commands/#get-back-to-a-specified-revision","text":"To get to the last version kubectl rollout undo deployment {{ deployment_name }} To go to a specific version kubectl rollout undo {{ deployment_name }} --to-revision ={{ revision_number }}","title":"Get back to a specified revision"},{"location":"devops/kubectl/kubectl_commands/#pods_1","text":"","title":"Pods"},{"location":"devops/kubectl/kubectl_commands/#rolling-update-of-pods","text":"Is prefered to use the deployment rollout kubectl rolling-update {{ pod_name }} -f {{ new_pod_definition_yaml }}","title":"Rolling update of pods"},{"location":"devops/kubectl/kubectl_commands/#change-the-name-of-the-resource-and-update-the-image","text":"kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --image = image: {{ new_pod_version }}","title":"Change the name of the resource and update the image"},{"location":"devops/kubectl/kubectl_commands/#abort-existing-rollout-in-progress","text":"kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --rollback","title":"Abort existing rollout in progress"},{"location":"devops/kubectl/kubectl_commands/#force-replace-delete-and-then-re-create-the-resource","text":"** Will cause a service outage ** kubectl replace --force -f {{ new_pod_yaml }}","title":"Force replace, delete and then re-create the resource"},{"location":"devops/kubectl/kubectl_commands/#add-a-label","text":"kubectl label pods {{ pod_name }} new-label ={{ new_label }}","title":"Add a label"},{"location":"devops/kubectl/kubectl_commands/#autoscale-a-deployment","text":"kubectl autoscale deployment {{ deployment_name }} --min ={{ min_instances }} --max ={{ max_instances }} [ --cpu-percent ={{ cpu_percent }}]","title":"Autoscale a deployment"},{"location":"devops/kubectl/kubectl_commands/#copy-resources-between-namespaces","text":"kubectl get rs,secrets -o json --namespace old | jq '.items[].metadata.namespace = \"new\"' | kubectl create-f -","title":"Copy resources between namespaces"},{"location":"devops/kubectl/kubectl_commands/#formatting-output","text":"","title":"Formatting output"},{"location":"devops/kubectl/kubectl_commands/#print-a-table-using-a-comma-separated-list-of-custom-columns","text":"-o = custom-columns = <spec>","title":"Print a table using a comma separated list of custom columns"},{"location":"devops/kubectl/kubectl_commands/#print-a-table-using-the-custom-columns-template-in-the-file","text":"-o = custom-columns-file = <filename>","title":"Print a table using the custom columns template in the  file"},{"location":"devops/kubectl/kubectl_commands/#output-a-json-formatted-api-object","text":"-o = json","title":"Output a JSON formatted API object"},{"location":"devops/kubectl/kubectl_commands/#print-the-fields-defined-in-a-jsonpath-expression","text":"-o = jsonpath = <template>","title":"Print the fields defined in a jsonpath expression"},{"location":"devops/kubectl/kubectl_commands/#print-the-fields-defined-by-the-jsonpath-expression-in-the-file","text":"-o = jsonpath-file = <filename>","title":"Print the fields defined by the jsonpath expression in the  file"},{"location":"devops/kubectl/kubectl_commands/#print-only-the-resource-name-and-nothing-else","text":"-o = name","title":"Print only the resource name and nothing else"},{"location":"devops/kubectl/kubectl_commands/#output-in-the-plain-text-format-with-any-additional-information-and-for-pods-the-node-name-is-included","text":"-o = wide","title":"Output in the plain-text format with any additional information, and for pods, the node name is included"},{"location":"devops/kubectl/kubectl_commands/#output-a-yaml-formatted-api-object","text":"-o = yaml","title":"Output a YAML formatted API object"},{"location":"devops/kubectl/kubectl_installation/","text":"Kubectl is not yet in the distribution package managers, so we'll need to install it manually. curl -LO \"https://storage.googleapis.com/kubernetes-release/release/ $( \\ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt \\ ) /bin/linux/amd64/kubectl\" chmod +x kubectl mv kubectl ~/.local/bin/kubectl Configure kubectl \u2691 Set editor \u2691 # File ~/.bashrc KUBE_EDITOR = \"vim\" Set auto completion \u2691 # File ~/.bashrc source < ( kubectl completion bash ) Configure EKS cluster \u2691 To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}","title":"Kubectl Installation"},{"location":"devops/kubectl/kubectl_installation/#configure-kubectl","text":"","title":"Configure kubectl"},{"location":"devops/kubectl/kubectl_installation/#set-editor","text":"# File ~/.bashrc KUBE_EDITOR = \"vim\"","title":"Set editor"},{"location":"devops/kubectl/kubectl_installation/#set-auto-completion","text":"# File ~/.bashrc source < ( kubectl completion bash )","title":"Set auto completion"},{"location":"devops/kubectl/kubectl_installation/#configure-eks-cluster","text":"To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}","title":"Configure EKS cluster"},{"location":"devops/kubernetes/kubernetes/","text":"Kubernetes (commonly stylized as k8s) is an open-source container-orchestration system for automating application deployment, scaling, and management. Developed by Google in Go under the Apache 2.0 license, it was first released on June 7, 2014 reaching 1.0 by July 21, 2015. It works with a range of container tools, including Docker. Many cloud services offer a Kubernetes-based platform or infrastructure as a service ( PaaS or IaaS ) on which Kubernetes can be deployed as a platform-providing service. Many vendors also provide their own branded Kubernetes distributions. It has become the standard infrastructure to manage containers in production environments. Docker Swarm would be an alternative but it falls short in features compared with Kubernetes. These are some of the advantages of using Kubernetes: Widely used in production and actively developed. Ensure high availability of your services with autohealing and autoscaling. Easy, quickly and predictable deployment and promotion of applications. Seamless roll out of features. Optimize hardware use while guaranteeing resource isolation. Easiest way to build multi-cloud and baremetal environments. Several companies have used Kubernetes to release their own PaaS : OpenShift by Red Hat. Tectonic by CoreOS. Rancher labs by Rancher. Learn roadmap \u2691 K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af . Links \u2691 Docs Awesome K8s Katacoda playground Comic Diving deeper \u2691 Architecture Resources Kubectl Additional Components Networking Helm Tools Reference \u2691 References API conventions","title":"Kubernetes"},{"location":"devops/kubernetes/kubernetes/#learn-roadmap","text":"K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af .","title":"Learn roadmap"},{"location":"devops/kubernetes/kubernetes/#links","text":"Docs Awesome K8s Katacoda playground Comic","title":"Links"},{"location":"devops/kubernetes/kubernetes/#diving-deeper","text":"Architecture Resources Kubectl Additional Components Networking Helm Tools","title":"Diving deeper"},{"location":"devops/kubernetes/kubernetes/#reference","text":"References API conventions","title":"Reference"},{"location":"devops/kubernetes/kubernetes_annotations/","text":"Annotations are non-identifying metadata key/value pairs attached to objects, such as pods. Annotations are intended to give meaningful and relevant information to libraries and tools. Annotations, like labels, are key/value maps: \"annotations\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" } Here are some examples of information that could be recorded in annotations: Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto generated fields and fields set by auto sizing or auto scaling systems. Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address. Pointers to logging, monitoring, analytics, or audit repositories. Client library or tool information that can be used for debugging purposes, for example, name, version, and build information. User or tool/system provenance information, such as URLs of related objects from other ecosystem components. Lightweight rollout tool metadata: for example, config or checkpoints.","title":"Annotations"},{"location":"devops/kubernetes/kubernetes_architecture/","text":"Kubernetes is a combination of components distributed between two kind of nodes, Masters and Workers . Master Nodes \u2691 Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes. Worker Nodes \u2691 Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself. kube-proxy operation modes \u2691 kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode. Kubectl \u2691 The kubectl is the command line client used to communicate with the Masters. Links \u2691 Kubernetes components overview","title":"Architecture"},{"location":"devops/kubernetes/kubernetes_architecture/#master-nodes","text":"Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.","title":"Master Nodes"},{"location":"devops/kubernetes/kubernetes_architecture/#worker-nodes","text":"Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself.","title":"Worker Nodes"},{"location":"devops/kubernetes/kubernetes_architecture/#kube-proxy-operation-modes","text":"kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode.","title":"kube-proxy operation modes"},{"location":"devops/kubernetes/kubernetes_architecture/#kubectl","text":"The kubectl is the command line client used to communicate with the Masters.","title":"Kubectl"},{"location":"devops/kubernetes/kubernetes_architecture/#links","text":"Kubernetes components overview","title":"Links"},{"location":"devops/kubernetes/kubernetes_cluster_autoscaler/","text":"While Horizontal pod autoscaling allows a deployment to scale given the resources needed, they are limited to the kubernetes existing working nodes. To autoscale the number of working nodes we need the cluster autoscaler . For AWS, there are the Amazon guidelines to enable it . But I'd use the cluster-autoscaler helm chart.","title":"Cluster Autoscaler"},{"location":"devops/kubernetes/kubernetes_dashboard/","text":"Dashboard definition Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. Deployment \u2691 The best way to install it is with the stable/kubernetes-dashboard chart with helmfile . Links \u2691 Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide","title":"Dashboard"},{"location":"devops/kubernetes/kubernetes_dashboard/#deployment","text":"The best way to install it is with the stable/kubernetes-dashboard chart with helmfile .","title":"Deployment"},{"location":"devops/kubernetes/kubernetes_dashboard/#links","text":"Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide","title":"Links"},{"location":"devops/kubernetes/kubernetes_deployments/","text":"The different types of deployments configure a ReplicaSet and a PodSchema for your application. Depending on the type of application we'll use one of the following types. Deployments \u2691 Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80 StatefulSets \u2691 StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator . DaemonSet \u2691 DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd Job \u2691 Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.","title":"Deployments"},{"location":"devops/kubernetes/kubernetes_deployments/#deployments","text":"Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80","title":"Deployments"},{"location":"devops/kubernetes/kubernetes_deployments/#statefulsets","text":"StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator .","title":"StatefulSets"},{"location":"devops/kubernetes/kubernetes_deployments/#daemonset","text":"DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd","title":"DaemonSet"},{"location":"devops/kubernetes/kubernetes_deployments/#job","text":"Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.","title":"Job"},{"location":"devops/kubernetes/kubernetes_external_dns/","text":"The external-dns resource allows the creation of DNS records from within kubernetes inside the definition of service and ingress resources. It currently supports the following providers: Provider Status Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta AzureDNS Beta CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha Akamai FastDNS Alpha There are two reasons to enable it: If there is any change in the ingress or service load balancer endpoint, due to a deployment, the dns records are automatically changed. It's easier for developers to connect their applications. Deployment in AWS \u2691 To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .","title":"External DNS"},{"location":"devops/kubernetes/kubernetes_external_dns/#deployment-in-aws","text":"To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .","title":"Deployment in AWS"},{"location":"devops/kubernetes/kubernetes_hpa/","text":"With Horizontal pod autoscaling , Kubernetes automatically scales the number of pods in a deployment or replication controller based on observed CPU utilization or on some other application provided metrics. The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user. To make it work, the definition of pod resource consumption needs to be specified.","title":"Horizontal Pod Autoscaling"},{"location":"devops/kubernetes/kubernetes_ingress/","text":"An Ingress is An API object that manages external access to the services in a cluster, typically HTTP. Ingress provide a centralized way to: Load balancing. SSL termination. Dynamic service discovery. Traffic routing. Authentication. Traffic distribution: canary deployments, A/B testing, mirroring/shadowing. Graphical user interface. JWT validation. WAF and DDOS protection. Requests tracing. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer .","title":"Ingress"},{"location":"devops/kubernetes/kubernetes_ingress_controller/","text":"Ingress controllers monitor the cluster events for the creation or modification of Ingress resources, modifying accordingly the underlying load balancers. They are not part of the master kube-controller-manager , so you'll need to install them manually. There are different Ingress controllers, such as AWS ALB, Nginx, HAProxy or Traefik, using one or other depends on your needs. Almost all controllers are open sourced and support dynamic service discovery, SSL termination or WebSockets. But they differ in: Supported protocols : HTTP, HTTPS, gRPC, HTTP/2.0, TCP (with SNI) or UDP. Underlying software : NGINX, Traefik, HAProxy or Envoy. Traffic routing : host and path, regular expression support. Namespace limitations : supported or not. Upstream probes : active checks, passive checks, retries, circuit breakers, custom health checks... Load balancing algorithms : round-robin, sticky sessions, rdp-cookie... Authentication : Basic, digest, Oauth, external auth, SSL certificate... Traffic distribution : canary deployments, A/B testing, mirroring/shadowing. Paid subscription : extended functionality or technical support. Graphical user interface : JWT validation : Customization of configuration : Basic DDOS protection mechanisms : rate limit, traffic filtering. WAF : Requests tracing : monitor, trace and debug requests via OpenTracing or other options. Both ITNext and Flant provide good ingress controller comparisons, a synoptical resume of both articles follows. Kubernetes Ingress controller \u2691 The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s. Traefik \u2691 Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik. HAProxy \u2691 HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1. Istio Ingress \u2691 Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio. ALB Ingress controller \u2691 The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service. ALB ingress deployment \u2691 This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so. Links \u2691 ITNext ingress controller comparison Flant ingress controller comparison","title":"Ingress Controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#kubernetes-ingress-controller","text":"The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s.","title":"Kubernetes Ingress controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#traefik","text":"Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik.","title":"Traefik"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#haproxy","text":"HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1.","title":"HAProxy"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#istio-ingress","text":"Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio.","title":"Istio Ingress"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-controller","text":"The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service.","title":"ALB Ingress controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-deployment","text":"This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so.","title":"ALB ingress deployment"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#links","text":"ITNext ingress controller comparison Flant ingress controller comparison","title":"Links"},{"location":"devops/kubernetes/kubernetes_jobs/","text":"Kubernetes jobs creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Cronjobs creates Jobs on a repeating schedule. This example CronJob manifest prints the current time and a hello message every minute: apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure To deploy cronjobs you can use the bambash helm chart . Check the kubectl commands to interact with jobs . Debugging job logs \u2691 To obtain the logs of a completed or failed job, you need to: Locate the cronjob you want to debug: kubectl get cronjobs -n cronjobs . Locate the associated job: kubectl get jobs -n cronjobs . Locate the associated pod: kubectl get pods -n cronjobs . If the pod still exists, you can execute kubectl logs -n cronjobs {{ pod_name }} . If the pod doesn't exist anymore, you need to search the pod in your log centralizer solution. Monitorization of cronjobs \u2691 Alerting of traditional Unix cronjobs meant sending an email if the job failed. Most job scheduling systems that have followed have provided the same experience, Kubernetes does not. One approach to alerting jobs is to use the Prometheus push gateway, allowing us to push richer metrics than the success/failure status. This approach has it\u2019s downsides; we have to update the code for our jobs, we also have to explicitly configure a push gateway location and update it if it changes (a burden alleviated by the pull based metrics for long lived workloads). You can use tools such as Sentry, but it will also require changes to the jobs. Jobs are powerful things allowing us to implement several different workflows, the combination of options can be overwhelming compared to a traditional Unix cron job. This variety makes it difficult to establish one simple rule for alerting failed jobs. Things get easier if we restrict ourselves to a subset of possible options. We will focus on non-concurrent jobs. The relationship between cronjobs and jobs makes the task ahead difficult. To make our life easier we will put one requirement on the jobs we create, they will have to include a label that associates them with the original cronjob. Below we present an example of our ideal cronjob (which matches what the helm chart deploys): apiVersion : batch/v1beta1 kind : CronJob metadata : name : our-task spec : schedule : \"*/5 * * * *\" successfulJobsHistoryLimit : 3 concurrencyPolicy : Forbid jobTemplate : metadata : labels : cron : our-task # <-- match created jobs with the cronjob spec : backoffLimit : 3 template : metadata : labels : cronjob : our-task spec : containers : - name : our-task command : - /user/bin/false image : alpine restartPolicy : Never Building our alert \u2691 We are also going to need some metrics to get us started. K8s does not provide us any by default, but fortunately kube-state-metrics is installed with the Prometheus operator chart, so we have the following metrics: kube_cronjob_labels{ cronjob=\"our-task\", namespace=\"default\"} 1 kube_job_created{ job=\"our-task-1520165700\", namespace=\"default\"} 1.520165707e+09 kube_job_failed{ condition=\"false\", job=\"our-task-1520165700\", namespace=\"default\"} 0 kube_job_failed{ condition=\"true\", job=\"our-task-1520165700\", namespace=\"default\"} 1 kube_job_labels{ job=\"our-task-1520165700\", label_cron=\"our-task\", namespace=\"default\"} 1 This shows the primary set of metrics we will be using to construct our alert. What is not shown above is the status of the cronjob. The big challenge with K8s cronjob alerting is that cronjobs themselves do not possess any status information, beyond the last time the cronjob created a job. The status information only exists on the job that the cronjob creates. In order to determine if our cronjob is failing, our first order of business is to find which jobs we should be looking at. A K8s cronjob creates new job objects and keeps a number of them around to help us debug the runs of our jobs. We have to be determine which job corresponds to the last run of our cronjob. If we have added the cron label to the jobs as above, we can find the last run time of the jobs for a given cronjob as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) This query demonstrates an important technique when working with kube-state-metrics . For each API object it exported data on, it exports a time series including all the labels for that object. These time series have a value of 1. As such we can join the set of labels for an object onto the metrics about that object by multiplication. Depending on how your Prometheus instance is configured, the value of the job label on your metrics will likely be kube-state-metrics . kube-state-metrics adds a job label itself with the name of the job object. Prometheus resolves this collision of label names by including the raw metric\u2019s label as an job_name label. Since we are querying the start time of jobs, and there should only ever be one job with a given name. You may wonder why we need the max aggregation. Manually plugging the query into Prometheus may convince you that it is unnecessary. Consider though that you may have multiple instances of kube-state-metrics running for redundancy. Using max ensures our query is valid even if we have multiple instances of kube-state-metrics running. Issues of duplicate metrics are common when constructing production recording rules and alerts. We can find the start time of the most recent job for a given cronjob by finding the maximum of all job start times as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) The only difference between this and the previous query is in the labels used for the aggregation. Now that we have the start time of each job, and the start time of the most recent job, we can do a simple equality match to find the start time of the most recent job for a given cronjob. We will create a metric for this: - record : job_cronjob : kube_job_status_start_time : max expr : | sum without ( label_cron , job_name ) ( label_replace ( label_replace ( max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) == ON ( label_cron ) GROUP_LEFT () max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) We have also taken the opportunity to adjust the labels to be a little more aesthetically pleasing. By copying job_name to job , label_cron to cronjob and removing job_name and label_cron . Now that we have the most recently started job for a given cronjob, we can find which, if any, have failed attempts: - record : job_cronjob : kube_job_status_failed : sum expr : | sum without ( label_cron , job_name ) ( clamp_max ( job_cronjob : kube_job_status_start_time : max , 1 ) * ON ( job ) GROUP_LEFT () label_replace ( label_replace ( ( kube_job_status_failed != 0 ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) The initial clamp_max clause is used to transform our start times metric into a set of time series to perform label matching to filter another set of metrics. Multiplication by 1 (or addition of 0), is a useful means of filter and merging time series and it is well worth taking the time to understand the technique. We adjust the labels on the raw kube_job_status_failed to match our start time metric so ensure the labels have the same meaning as those on our job_cronjob:kube_job_status_start_time:max metric. The label matching on the multiplication will then perform our filtering. We now have a metric containing the set of most recently failed jobs, labeled by their parent cronjob, so we can now construct the alert: - alert : CronJobStatusFailed expr : job_cronjob : kube_job_status_failed : sum > 0 for : 1m annotations : description : ' {{ $labels.cronjob }} last run has failed {{ $value }} times. ' We use the kube_cronjob_labels here to merge in labels from the original cronjob.","title":"Jobs"},{"location":"devops/kubernetes/kubernetes_jobs/#debugging-job-logs","text":"To obtain the logs of a completed or failed job, you need to: Locate the cronjob you want to debug: kubectl get cronjobs -n cronjobs . Locate the associated job: kubectl get jobs -n cronjobs . Locate the associated pod: kubectl get pods -n cronjobs . If the pod still exists, you can execute kubectl logs -n cronjobs {{ pod_name }} . If the pod doesn't exist anymore, you need to search the pod in your log centralizer solution.","title":"Debugging job logs"},{"location":"devops/kubernetes/kubernetes_jobs/#monitorization-of-cronjobs","text":"Alerting of traditional Unix cronjobs meant sending an email if the job failed. Most job scheduling systems that have followed have provided the same experience, Kubernetes does not. One approach to alerting jobs is to use the Prometheus push gateway, allowing us to push richer metrics than the success/failure status. This approach has it\u2019s downsides; we have to update the code for our jobs, we also have to explicitly configure a push gateway location and update it if it changes (a burden alleviated by the pull based metrics for long lived workloads). You can use tools such as Sentry, but it will also require changes to the jobs. Jobs are powerful things allowing us to implement several different workflows, the combination of options can be overwhelming compared to a traditional Unix cron job. This variety makes it difficult to establish one simple rule for alerting failed jobs. Things get easier if we restrict ourselves to a subset of possible options. We will focus on non-concurrent jobs. The relationship between cronjobs and jobs makes the task ahead difficult. To make our life easier we will put one requirement on the jobs we create, they will have to include a label that associates them with the original cronjob. Below we present an example of our ideal cronjob (which matches what the helm chart deploys): apiVersion : batch/v1beta1 kind : CronJob metadata : name : our-task spec : schedule : \"*/5 * * * *\" successfulJobsHistoryLimit : 3 concurrencyPolicy : Forbid jobTemplate : metadata : labels : cron : our-task # <-- match created jobs with the cronjob spec : backoffLimit : 3 template : metadata : labels : cronjob : our-task spec : containers : - name : our-task command : - /user/bin/false image : alpine restartPolicy : Never","title":"Monitorization of cronjobs"},{"location":"devops/kubernetes/kubernetes_jobs/#building-our-alert","text":"We are also going to need some metrics to get us started. K8s does not provide us any by default, but fortunately kube-state-metrics is installed with the Prometheus operator chart, so we have the following metrics: kube_cronjob_labels{ cronjob=\"our-task\", namespace=\"default\"} 1 kube_job_created{ job=\"our-task-1520165700\", namespace=\"default\"} 1.520165707e+09 kube_job_failed{ condition=\"false\", job=\"our-task-1520165700\", namespace=\"default\"} 0 kube_job_failed{ condition=\"true\", job=\"our-task-1520165700\", namespace=\"default\"} 1 kube_job_labels{ job=\"our-task-1520165700\", label_cron=\"our-task\", namespace=\"default\"} 1 This shows the primary set of metrics we will be using to construct our alert. What is not shown above is the status of the cronjob. The big challenge with K8s cronjob alerting is that cronjobs themselves do not possess any status information, beyond the last time the cronjob created a job. The status information only exists on the job that the cronjob creates. In order to determine if our cronjob is failing, our first order of business is to find which jobs we should be looking at. A K8s cronjob creates new job objects and keeps a number of them around to help us debug the runs of our jobs. We have to be determine which job corresponds to the last run of our cronjob. If we have added the cron label to the jobs as above, we can find the last run time of the jobs for a given cronjob as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) This query demonstrates an important technique when working with kube-state-metrics . For each API object it exported data on, it exports a time series including all the labels for that object. These time series have a value of 1. As such we can join the set of labels for an object onto the metrics about that object by multiplication. Depending on how your Prometheus instance is configured, the value of the job label on your metrics will likely be kube-state-metrics . kube-state-metrics adds a job label itself with the name of the job object. Prometheus resolves this collision of label names by including the raw metric\u2019s label as an job_name label. Since we are querying the start time of jobs, and there should only ever be one job with a given name. You may wonder why we need the max aggregation. Manually plugging the query into Prometheus may convince you that it is unnecessary. Consider though that you may have multiple instances of kube-state-metrics running for redundancy. Using max ensures our query is valid even if we have multiple instances of kube-state-metrics running. Issues of duplicate metrics are common when constructing production recording rules and alerts. We can find the start time of the most recent job for a given cronjob by finding the maximum of all job start times as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) The only difference between this and the previous query is in the labels used for the aggregation. Now that we have the start time of each job, and the start time of the most recent job, we can do a simple equality match to find the start time of the most recent job for a given cronjob. We will create a metric for this: - record : job_cronjob : kube_job_status_start_time : max expr : | sum without ( label_cron , job_name ) ( label_replace ( label_replace ( max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) == ON ( label_cron ) GROUP_LEFT () max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) We have also taken the opportunity to adjust the labels to be a little more aesthetically pleasing. By copying job_name to job , label_cron to cronjob and removing job_name and label_cron . Now that we have the most recently started job for a given cronjob, we can find which, if any, have failed attempts: - record : job_cronjob : kube_job_status_failed : sum expr : | sum without ( label_cron , job_name ) ( clamp_max ( job_cronjob : kube_job_status_start_time : max , 1 ) * ON ( job ) GROUP_LEFT () label_replace ( label_replace ( ( kube_job_status_failed != 0 ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) The initial clamp_max clause is used to transform our start times metric into a set of time series to perform label matching to filter another set of metrics. Multiplication by 1 (or addition of 0), is a useful means of filter and merging time series and it is well worth taking the time to understand the technique. We adjust the labels on the raw kube_job_status_failed to match our start time metric so ensure the labels have the same meaning as those on our job_cronjob:kube_job_status_start_time:max metric. The label matching on the multiplication will then perform our filtering. We now have a metric containing the set of most recently failed jobs, labeled by their parent cronjob, so we can now construct the alert: - alert : CronJobStatusFailed expr : job_cronjob : kube_job_status_failed : sum > 0 for : 1m annotations : description : ' {{ $labels.cronjob }} last run has failed {{ $value }} times. ' We use the kube_cronjob_labels here to merge in labels from the original cronjob.","title":"Building our alert"},{"location":"devops/kubernetes/kubernetes_labels/","text":"Labels are identifying metadata key/value pairs attached to objects, such as pods. Labels are intended to give meaningful and relevant information to users, but which do not directly imply semantics to the core system. \"labels\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" }","title":"Labels"},{"location":"devops/kubernetes/kubernetes_metric_server/","text":"The metrics server monitors the resource consumption inside the cluster. It populates the information in kubectl top nodes to get the node status and gives the information to automatically autoscale deployments with Horizontal pod autoscaling . To install it, you can use the metrics-server helm chart. To test that the horizontal pod autoscaling is working, follow the AWS EKS guide .","title":"Metrics Server"},{"location":"devops/kubernetes/kubernetes_namespaces/","text":"Namespaces are virtual clusters backed by the same physical cluster. It's the first level of isolation between applications. When to Use Multiple Namespaces \u2691 Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","title":"Namespaces"},{"location":"devops/kubernetes/kubernetes_namespaces/#when-to-use-multiple-namespaces","text":"Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","title":"When to Use Multiple Namespaces"},{"location":"devops/kubernetes/kubernetes_networking/","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. If you want to get a quickly grasp on how k8s networking works, I suggest you to read StackRox's Kubernetes networking demystified article . CNI comparison \u2691 Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis . TL;DR \u2691 When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast. Flannel \u2691 Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work. Calico \u2691 Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide . Canal \u2691 Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition. Weave Net \u2691 Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting. AWS CNI \u2691 AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS. Advantages of the AWS CNI \u2691 Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive. Disadvantages of the AWS CNI \u2691 On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet. VPC CNI Pod Density Limitations \u2691 First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached. Cloud Portability \u2691 Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea. Links \u2691 StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Networking"},{"location":"devops/kubernetes/kubernetes_networking/#cni-comparison","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis .","title":"CNI comparison"},{"location":"devops/kubernetes/kubernetes_networking/#tldr","text":"When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast.","title":"TL;DR"},{"location":"devops/kubernetes/kubernetes_networking/#flannel","text":"Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work.","title":"Flannel"},{"location":"devops/kubernetes/kubernetes_networking/#calico","text":"Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide .","title":"Calico"},{"location":"devops/kubernetes/kubernetes_networking/#canal","text":"Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition.","title":"Canal"},{"location":"devops/kubernetes/kubernetes_networking/#weave-net","text":"Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting.","title":"Weave Net"},{"location":"devops/kubernetes/kubernetes_networking/#aws-cni","text":"AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS.","title":"AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#advantages-of-the-aws-cni","text":"Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive.","title":"Advantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#disadvantages-of-the-aws-cni","text":"On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet.","title":"Disadvantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#vpc-cni-pod-density-limitations","text":"First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached.","title":"VPC CNI Pod Density Limitations"},{"location":"devops/kubernetes/kubernetes_networking/#cloud-portability","text":"Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea.","title":"Cloud Portability"},{"location":"devops/kubernetes/kubernetes_networking/#links","text":"StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Links"},{"location":"devops/kubernetes/kubernetes_operators/","text":"Operators are Kubernetes specific applications (pods) that configure, manage and optimize other Kubernetes deployments automatically. A Kubernetes Operator might be able to: Install and provide sane initial configuration and sizing for your deployment, according to the specs of your Kubernetes cluster. Perform live reloading of deployments and pods to accommodate for any user requested parameter modification (hot config reloading). Safe coordination of application upgrades. Automatically scale up or down according to performance metrics. Service discovery via native Kubernetes APIs Application TLS certificate configuration Disaster recovery. Perform backups to offsite storage, integrity checks or any other maintenance task. How do they work? \u2691 An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1. Links \u2691 CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Operators"},{"location":"devops/kubernetes/kubernetes_operators/#how-do-they-work","text":"An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1.","title":"How do they work?"},{"location":"devops/kubernetes/kubernetes_operators/#links","text":"CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Links"},{"location":"devops/kubernetes/kubernetes_pods/","text":"Pods are the basic building block of Kubernetes, the smallest and simplest unit in the object model that you create or deploy. A Pod represents a running process on your cluster. A Pod represents a unit of deployment. It encapsulates: An application container (or, in some cases, multiple tightly coupled containers). Storage resources. A unique network IP. Options that govern how the container(s) should run.","title":"Pods"},{"location":"devops/kubernetes/kubernetes_replicasets/","text":"ReplicaSet maintains a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. You'll probably never manually use these resources, as they are defined inside the deployments . The older version of this resource are the Replication controllers .","title":"ReplicaSets"},{"location":"devops/kubernetes/kubernetes_services/","text":"A Service defines a policy to access a logical set of Pods using a reliable endpoint. Users and other programs can access pods running on your cluster seamlessly. Therefore allowing a loose coupling between dependent Pods. When a request arrives the endpoint, the kube-proxy pod of the node forwards the request to the Pods that match the service LabelSelector. Services can be exposed in different ways by specifying a type in the ServiceSpec: ClusterIP (default): Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster. NodePort : Exposes the Service on the same port of each selected Node in the cluster using NAT to the outside. LoadBalancer : Creates an external load balancer in the current cloud and assigns a fixed, external IP to the Service. To create an internal ELB of AWs add to the annotations: annotations : service.beta.kubernetes.io/aws-load-balancer-internal : 0.0.0.0/0 ExternalName : Exposes the Service using an arbitrary name by returning a CNAME record with the name. No proxy is used. If no RBAC or NetworkPolicies are applied, you can call a service of another namespace with the following nomenclature. curl {{ service_name }} . {{ service_namespace }} .svc.cluster.local","title":"Services"},{"location":"devops/kubernetes/kubernetes_storage_driver/","text":"Storage drivers are pods that through the Container Storage Interface or CSI provide an interface to use external storage services from within Kubernetes. Amazon EBS CSI storage driver \u2691 Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .","title":"Storage Driver"},{"location":"devops/kubernetes/kubernetes_storage_driver/#amazon-ebs-csi-storage-driver","text":"Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .","title":"Amazon EBS CSI storage driver"},{"location":"devops/kubernetes/kubernetes_tools/","text":"There are several tools built to enhance the operation, installation and use of Kubernetes. Tried \u2691 K3s : Recommended small kubernetes, like hyperkube. To try \u2691 crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.","title":"Tools"},{"location":"devops/kubernetes/kubernetes_tools/#tried","text":"K3s : Recommended small kubernetes, like hyperkube.","title":"Tried"},{"location":"devops/kubernetes/kubernetes_tools/#to-try","text":"crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.","title":"To try"},{"location":"devops/kubernetes/kubernetes_vertical_pod_autoscaler/","text":"Kubernetes knows the amount of resources a pod needs to operate through some metadata specified in the deployment. Generally this values change and manually maintaining all the resources requested and limits is a nightmare. The Vertical pod autoscaler does data analysis on the pod metrics to automatically adjust these values. Nevertheless it's still not suggested to use it in conjunction with the horizontal pod autoscaler , so we'll need to watch out for future improvements.","title":"Vertical Pod Autoscaler"},{"location":"devops/kubernetes/kubernetes_volumes/","text":"On disk files in a Container are ephemeral by default, which presents the following issues: When a Container crashes, kubelet will restart it, but the files will be lost. When running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems with several types . configMap \u2691 The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod. emptyDir \u2691 An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever. hostPath \u2691 A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys . secret \u2691 A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage. awsElasticBlockStore \u2691 An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume. nfs \u2691 An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously. local \u2691 A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk. Others \u2691 glusterfs cephfs","title":"Volumes"},{"location":"devops/kubernetes/kubernetes_volumes/#configmap","text":"The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod.","title":"configMap"},{"location":"devops/kubernetes/kubernetes_volumes/#emptydir","text":"An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever.","title":"emptyDir"},{"location":"devops/kubernetes/kubernetes_volumes/#hostpath","text":"A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys .","title":"hostPath"},{"location":"devops/kubernetes/kubernetes_volumes/#secret","text":"A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.","title":"secret"},{"location":"devops/kubernetes/kubernetes_volumes/#awselasticblockstore","text":"An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume.","title":"awsElasticBlockStore"},{"location":"devops/kubernetes/kubernetes_volumes/#nfs","text":"An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously.","title":"nfs"},{"location":"devops/kubernetes/kubernetes_volumes/#local","text":"A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.","title":"local"},{"location":"devops/kubernetes/kubernetes_volumes/#others","text":"glusterfs cephfs","title":"Others"},{"location":"devops/prometheus/alertmanager/","text":"The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. It is configured through the alertmanager.config key of the values.yaml of the helm chart. As stated in the configuration file , it has four main keys (as templates is handled in alertmanager.config.templateFiles ): global : SMTP and API main configuration, it will be inherited by the other elements. route : Route tree definition. receivers : Notification integrations configuration. inhibit_rules : Alert inhibition configuration. Route \u2691 A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node. Receivers \u2691 Notification receivers are the named configurations of one or more notification integrations. Email notifications \u2691 To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts. Rocketchat Notifications \u2691 Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }} Inhibit rules \u2691 Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch Alert rules \u2691 Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules Links \u2691 Awesome prometheus alert rules","title":"AlertManager"},{"location":"devops/prometheus/alertmanager/#route","text":"A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node.","title":"Route"},{"location":"devops/prometheus/alertmanager/#receivers","text":"Notification receivers are the named configurations of one or more notification integrations.","title":"Receivers"},{"location":"devops/prometheus/alertmanager/#email-notifications","text":"To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts.","title":"Email notifications"},{"location":"devops/prometheus/alertmanager/#rocketchat-notifications","text":"Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }}","title":"Rocketchat Notifications"},{"location":"devops/prometheus/alertmanager/#inhibit-rules","text":"Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch","title":"Inhibit rules"},{"location":"devops/prometheus/alertmanager/#alert-rules","text":"Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules","title":"Alert rules"},{"location":"devops/prometheus/alertmanager/#links","text":"Awesome prometheus alert rules","title":"Links"},{"location":"devops/prometheus/blackbox_exporter/","text":"The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP. It can be used to test: Website accessibility . Both for availability and security purposes. Website loading time . DNS response times to diagnose network latency issues. SSL certificates expiration . ICMP requests to gather network health information . Security protections such as if and endpoint stops being protected by VPN, WAF or SSL client certificate. Unauthorized read or write S3 buckets . When running, the Blackbox exporter is going to expose a HTTP endpoint that can be used in order to monitor targets over the network. By default, the Blackbox exporter exposes the /probe endpoint that is used to retrieve those metrics. The blackbox exporter is configured with a YAML configuration file made of modules . Installation \u2691 To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply Blackbox exporter probes \u2691 Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx HTTP endpoint working correctly \u2691 http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint working correctly \u2691 https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint behind client SSL certificate \u2691 https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation. HTTPS endpoint with an specific error \u2691 If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*' HTTP endpoint returning an error \u2691 http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] no_follow_redirects : false HTTPS endpoint through an HTTP proxy \u2691 https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\" HTTPS endpoint with basic auth \u2691 https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }} HTTPs endpoint with API key \u2691 https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }} HTTPS Put file \u2691 Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" Check open port \u2691 tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect Ping to the resource \u2691 Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\" Blackbox exporter alerts \u2691 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests Availability alerts \u2691 The most basic probes, test if the service is up and returning. Blackbox probe failed \u2691 Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" message : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0 Blackbox probe HTTP failure \u2691 HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" message : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Performance alerts \u2691 Blackbox slow probe \u2691 Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" message : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow HTTP \u2691 HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" message : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow ping \u2691 Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" message : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" SSL certificate alerts \u2691 Blackbox SSL certificate will expire in a month \u2691 SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Blackbox SSL certificate will expire in a few days \u2691 SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\" Blackbox SSL certificate expired \u2691 SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" message : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Security alerts \u2691 To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx Test endpoints protected with network policies \u2691 Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with SSL client certificate \u2691 Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with credentials. \u2691 Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with WAF. \u2691 Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized read of S3 buckets \u2691 Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized write of S3 buckets \u2691 Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Monitoring external access to internal services \u2691 There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic. Troubleshooting \u2691 To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true . Service monitors are not being created \u2691 When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again. probe_success == 0 when using an http proxy \u2691 Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. Links \u2691 Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .","title":"Blackbox Exporter"},{"location":"devops/prometheus/blackbox_exporter/#installation","text":"To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply","title":"Installation"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-exporter-probes","text":"Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx","title":"Blackbox exporter probes"},{"location":"devops/prometheus/blackbox_exporter/#http-endpoint-working-correctly","text":"http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTP endpoint working correctly"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-working-correctly","text":"https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTPS endpoint working correctly"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-behind-client-ssl-certificate","text":"https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation.","title":"HTTPS endpoint behind client SSL certificate"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-an-specific-error","text":"If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*'","title":"HTTPS endpoint with an specific error"},{"location":"devops/prometheus/blackbox_exporter/#http-endpoint-returning-an-error","text":"http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] no_follow_redirects : false","title":"HTTP endpoint returning an error"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-through-an-http-proxy","text":"https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\"","title":"HTTPS endpoint through an HTTP proxy"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-basic-auth","text":"https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }}","title":"HTTPS endpoint with basic auth"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-api-key","text":"https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }}","title":"HTTPs endpoint with API key"},{"location":"devops/prometheus/blackbox_exporter/#https-put-file","text":"Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTPS Put file"},{"location":"devops/prometheus/blackbox_exporter/#check-open-port","text":"tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect","title":"Check open port"},{"location":"devops/prometheus/blackbox_exporter/#ping-to-the-resource","text":"Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\"","title":"Ping to the resource"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-exporter-alerts","text":"Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests","title":"Blackbox exporter alerts"},{"location":"devops/prometheus/blackbox_exporter/#availability-alerts","text":"The most basic probes, test if the service is up and returning.","title":"Availability alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-failed","text":"Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" message : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0","title":"Blackbox probe failed"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-http-failure","text":"HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" message : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"","title":"Blackbox probe HTTP failure"},{"location":"devops/prometheus/blackbox_exporter/#performance-alerts","text":"","title":"Performance alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-slow-probe","text":"Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" message : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1","title":"Blackbox slow probe"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-slow-http","text":"HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" message : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1","title":"Blackbox probe slow HTTP"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-slow-ping","text":"Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" message : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\"","title":"Blackbox probe slow ping"},{"location":"devops/prometheus/blackbox_exporter/#ssl-certificate-alerts","text":"","title":"SSL certificate alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-month","text":"SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"","title":"Blackbox SSL certificate will expire in a month"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-few-days","text":"SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\"","title":"Blackbox SSL certificate will expire in a few days"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-expired","text":"SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" message : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Blackbox SSL certificate expired"},{"location":"devops/prometheus/blackbox_exporter/#security-alerts","text":"To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx","title":"Security alerts"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-network-policies","text":"Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with network policies"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-ssl-client-certificate","text":"Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with SSL client certificate"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-credentials","text":"Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with credentials."},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-waf","text":"Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with WAF."},{"location":"devops/prometheus/blackbox_exporter/#unauthorized-read-of-s3-buckets","text":"Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Unauthorized read of S3 buckets"},{"location":"devops/prometheus/blackbox_exporter/#unauthorized-write-of-s3-buckets","text":"Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Unauthorized write of S3 buckets"},{"location":"devops/prometheus/blackbox_exporter/#monitoring-external-access-to-internal-services","text":"There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic.","title":"Monitoring external access to internal services"},{"location":"devops/prometheus/blackbox_exporter/#troubleshooting","text":"To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true .","title":"Troubleshooting"},{"location":"devops/prometheus/blackbox_exporter/#service-monitors-are-not-being-created","text":"When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again.","title":"Service monitors are not being created"},{"location":"devops/prometheus/blackbox_exporter/#probe_success-0-when-using-an-http-proxy","text":"Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist.","title":"probe_success == 0 when using an http proxy"},{"location":"devops/prometheus/blackbox_exporter/#links","text":"Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .","title":"Links"},{"location":"devops/prometheus/instance_sizing_analysis/","text":"Once we gather the instance metrics with the Node exporter , we can do statistical analysis on the evolution of time to detect the instances that are undersized or oversized. RAM analysis \u2691 Instance RAM percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_memory_MemAvailable_percent expr : (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes ) * 100 The average , standard deviation and the standard score of the last two weeks would be: - record : instance_path:node_memory_MemAvailable_percent:avg_over_time_2w expr : avg_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w expr : stddev_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:z_score expr : > ( instance_path:node_memory_MemAvailable_percent - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w ) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w With that data we can define that an instance is oversized if the average plus the standard deviation is less than 60% and undersized if its greater than 90%. With the average we take into account the nominal RAM consumption, and with the standard deviation we take into account the spikes. Tweak this rule to your use case The criteria of undersized and oversized is just a first approximation I'm going to use. You can use it as a base criteria, but don't go through with it blindly. See the disclaimer below for more information. # RAM - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w > 90 labels : type : EC2 metric : RAM problem : undersized Where avg_plus_stddev_over_time_2w is: - record : instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w expr : > instance_path:node_memory_MemAvailable_percent:avg_over_time_2w + instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w CPU analysis \u2691 Instance CPU percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 The node_cpu_seconds_total doesn't give us the percent of usage, that is why we need to do the average of the rate of the last minute. The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. CPU usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # CPU - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 - record : instance_path:node_cpu_percent:rate1m:avg_over_time_2w expr : avg_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:stddev_over_time_2w expr : stddev_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_cpu_percent:rate1m:avg_over_time_2w + instance_path:node_cpu_percent:rate1m:stddev_over_time_2w - record : instance_path:node_cpu_percent:rate1m:z_score expr : > ( instance_path:node_cpu_percent:rate1m - instance_path:node_cpu_percent:rate1m:avg_over_time_2w ) / instance_path:node_cpu_percent:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # CPU - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : CPU problem : oversized - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w > 80 labels : type : EC2 metric : CPU problem : undersized Network analysis \u2691 We can deduce the network usage from the node_network_receive_bytes_total and node_network_transmit_bytes_total metrics. For example for the transmit, the Gigabits per second transmitted can be calculated with the following Prometheus rule : - record : instance_path:node_network_transmit_gigabits_per_second:rate5m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 Where we: Filter the traffic only to the external network interfaces node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"} . Those are the ones used by AWS , but you'll need to tweak that for your case. Convert the increase of Kilobytes per minute [1m] to Gigabits per second by multiplying it by 7.450580596923828 * 10^-9 / 60 . The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. Network usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # NetworkReceive - record : instance_path:node_network_receive_gigabits_per_second:rate1m expr : > increase( node_network_receive_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_receive_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_receive_gigabits_per_second:rate1m - instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w # NetworkTransmit - record : instance_path:node_network_transmit_gigabits_per_second:rate1m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_transmit_gigabits_per_second:rate1m - instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # NetworkReceive - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkReceive problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkReceive problem : undersized # NetworkTransmit - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkTransmit problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkTransmit problem : undersized The difference with network is that we don't have a percent of the total instance bandwidth, In my case, my instances support from 0.5 to 5 Gbps which is more than I need, so most of my instances are marked as oversized with the < 0.5 rule. I will manually study the ones that go over 3 Gbps. The correct way to do it, is to tag the baseline, burst or/and maximum network performance by instance type. In the AWS case, the data can be extracted using the AWS docs or external benchmarks . Once you know the network performance per instance type, you can use relabeling in the Node exporter service monitor to add a label like max_network_performance and use it later in the rules. If you do follow this path, please contact me or do a pull request so I can test your solution. Overall analysis \u2691 Now that we have all the analysis under the metric instance_path:wrong_resource_size with labels, we can aggregate them to see the number of rules each instance is breaking with the following rule: # Mark the number of oversize rules matched by each instance - record : instance_path:wrong_instance_size expr : count by (instance) (sum by (metric, instance) (instance_path:wrong_resource_size)) By executing sort_desc(instance_path:wrong_instance_size) in the Prometheus web application, we'll be able to see such instances. instance_path:wrong_instance_size{instance=\"frontend-production:192.168.1.2\"} 4 instance_path:wrong_instance_size{instance=\"backend-production-instance:172.30.0.195\"} 2 ... To see the detail of what rules is our instance breaking we can use something like instance_path:wrong_resource_size{instance =~'frontend.*'} instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"RAM\",problem=\"oversized\",type=\"EC2\"} 5.126602454544287 instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",metric=\"CPU\",problem=\"oversized\",type=\"EC2\"} 0.815639209497615 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkReceive\",problem=\"oversized\",type=\"EC2\"} 0.02973250128744766 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkTransmit\",problem=\"oversized\",type=\"EC2\"} 0.01586461503849804 Here we see that the frontend-production is a c4.2xlarge instance that is consuming an average plus standard deviation of CPU of 0.81%, RAM 5.12%, NetworkTransmit 0.015Gbps and NetworkReceive 0.029Gbps, which results in an oversized alert on all four metrics. If you want to see the evolution over the time, instead of Console click on Graph under the text box where you have entered the query. With this information, we can decide which is the correct instance for each application. Once all instances are migrated to their ideal size, we can add alerts on these metrics so we can have a continuous analysis of our instances. Once I've done it, I'll add the alerts here. Disclaimer \u2691 We haven't tested this rules yet in production to resize our infrastructure (will do soon), so use all the information in this document cautiously. What I can expect to fail is that the assumption of average plus a standard deviation criteria can not be enough, maybe I need to increase the resolution of the standard deviation so it can be more sensible to the spikes, or we need to use a safety factor of 2 or 3. We'll see :) Read throughly the Gitlab post on anomaly detection using Prometheus , it's awesome and it may give you insights on why this approach is not working with you, as well as other algorithms that for example take into account the seasonality of the metrics. In particular it's interesting to analyze your resources z-score evolution over time, if all values fall in the +4 to -4 range, you can statistically assert that your metric similarly follows the normal distribution, and can assume that any value of z_score above 3 is an anomaly. If your results return with a range of +20 to -20 , the tail is too long and your results will be skewed. To test it you can use the following queries to test the RAM behaviour, adapt them for the rest of the resources: # Minimum z_score value sort_desc ( abs (( min_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) # Maximum z_score value sort_desc ( abs (( max_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) For a less exhaustive but more graphical analysis, execute instance_path:node_memory_MemAvailable_percent:z_score in Graph mode. In my case the RAM is in the +-5 interval, with some peaks of 20, but after reviewing instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w in those periods, I feel it's still safe to use the assumption. Same criteria applies to instance_path:node_cpu_percent:rate1m:z_score , instance_path:node_network_receive_gigabits_per_second:rate1m:z_score , and instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score , metrics. References \u2691 Gitlab post on anomaly detection using Prometheus .","title":"Instance sizing analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#ram-analysis","text":"Instance RAM percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_memory_MemAvailable_percent expr : (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes ) * 100 The average , standard deviation and the standard score of the last two weeks would be: - record : instance_path:node_memory_MemAvailable_percent:avg_over_time_2w expr : avg_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w expr : stddev_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:z_score expr : > ( instance_path:node_memory_MemAvailable_percent - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w ) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w With that data we can define that an instance is oversized if the average plus the standard deviation is less than 60% and undersized if its greater than 90%. With the average we take into account the nominal RAM consumption, and with the standard deviation we take into account the spikes. Tweak this rule to your use case The criteria of undersized and oversized is just a first approximation I'm going to use. You can use it as a base criteria, but don't go through with it blindly. See the disclaimer below for more information. # RAM - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w > 90 labels : type : EC2 metric : RAM problem : undersized Where avg_plus_stddev_over_time_2w is: - record : instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w expr : > instance_path:node_memory_MemAvailable_percent:avg_over_time_2w + instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w","title":"RAM analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#cpu-analysis","text":"Instance CPU percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 The node_cpu_seconds_total doesn't give us the percent of usage, that is why we need to do the average of the rate of the last minute. The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. CPU usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # CPU - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 - record : instance_path:node_cpu_percent:rate1m:avg_over_time_2w expr : avg_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:stddev_over_time_2w expr : stddev_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_cpu_percent:rate1m:avg_over_time_2w + instance_path:node_cpu_percent:rate1m:stddev_over_time_2w - record : instance_path:node_cpu_percent:rate1m:z_score expr : > ( instance_path:node_cpu_percent:rate1m - instance_path:node_cpu_percent:rate1m:avg_over_time_2w ) / instance_path:node_cpu_percent:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # CPU - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : CPU problem : oversized - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w > 80 labels : type : EC2 metric : CPU problem : undersized","title":"CPU analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#network-analysis","text":"We can deduce the network usage from the node_network_receive_bytes_total and node_network_transmit_bytes_total metrics. For example for the transmit, the Gigabits per second transmitted can be calculated with the following Prometheus rule : - record : instance_path:node_network_transmit_gigabits_per_second:rate5m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 Where we: Filter the traffic only to the external network interfaces node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"} . Those are the ones used by AWS , but you'll need to tweak that for your case. Convert the increase of Kilobytes per minute [1m] to Gigabits per second by multiplying it by 7.450580596923828 * 10^-9 / 60 . The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. Network usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # NetworkReceive - record : instance_path:node_network_receive_gigabits_per_second:rate1m expr : > increase( node_network_receive_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_receive_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_receive_gigabits_per_second:rate1m - instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w # NetworkTransmit - record : instance_path:node_network_transmit_gigabits_per_second:rate1m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_transmit_gigabits_per_second:rate1m - instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # NetworkReceive - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkReceive problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkReceive problem : undersized # NetworkTransmit - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkTransmit problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkTransmit problem : undersized The difference with network is that we don't have a percent of the total instance bandwidth, In my case, my instances support from 0.5 to 5 Gbps which is more than I need, so most of my instances are marked as oversized with the < 0.5 rule. I will manually study the ones that go over 3 Gbps. The correct way to do it, is to tag the baseline, burst or/and maximum network performance by instance type. In the AWS case, the data can be extracted using the AWS docs or external benchmarks . Once you know the network performance per instance type, you can use relabeling in the Node exporter service monitor to add a label like max_network_performance and use it later in the rules. If you do follow this path, please contact me or do a pull request so I can test your solution.","title":"Network analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#overall-analysis","text":"Now that we have all the analysis under the metric instance_path:wrong_resource_size with labels, we can aggregate them to see the number of rules each instance is breaking with the following rule: # Mark the number of oversize rules matched by each instance - record : instance_path:wrong_instance_size expr : count by (instance) (sum by (metric, instance) (instance_path:wrong_resource_size)) By executing sort_desc(instance_path:wrong_instance_size) in the Prometheus web application, we'll be able to see such instances. instance_path:wrong_instance_size{instance=\"frontend-production:192.168.1.2\"} 4 instance_path:wrong_instance_size{instance=\"backend-production-instance:172.30.0.195\"} 2 ... To see the detail of what rules is our instance breaking we can use something like instance_path:wrong_resource_size{instance =~'frontend.*'} instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"RAM\",problem=\"oversized\",type=\"EC2\"} 5.126602454544287 instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",metric=\"CPU\",problem=\"oversized\",type=\"EC2\"} 0.815639209497615 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkReceive\",problem=\"oversized\",type=\"EC2\"} 0.02973250128744766 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkTransmit\",problem=\"oversized\",type=\"EC2\"} 0.01586461503849804 Here we see that the frontend-production is a c4.2xlarge instance that is consuming an average plus standard deviation of CPU of 0.81%, RAM 5.12%, NetworkTransmit 0.015Gbps and NetworkReceive 0.029Gbps, which results in an oversized alert on all four metrics. If you want to see the evolution over the time, instead of Console click on Graph under the text box where you have entered the query. With this information, we can decide which is the correct instance for each application. Once all instances are migrated to their ideal size, we can add alerts on these metrics so we can have a continuous analysis of our instances. Once I've done it, I'll add the alerts here.","title":"Overall analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#disclaimer","text":"We haven't tested this rules yet in production to resize our infrastructure (will do soon), so use all the information in this document cautiously. What I can expect to fail is that the assumption of average plus a standard deviation criteria can not be enough, maybe I need to increase the resolution of the standard deviation so it can be more sensible to the spikes, or we need to use a safety factor of 2 or 3. We'll see :) Read throughly the Gitlab post on anomaly detection using Prometheus , it's awesome and it may give you insights on why this approach is not working with you, as well as other algorithms that for example take into account the seasonality of the metrics. In particular it's interesting to analyze your resources z-score evolution over time, if all values fall in the +4 to -4 range, you can statistically assert that your metric similarly follows the normal distribution, and can assume that any value of z_score above 3 is an anomaly. If your results return with a range of +20 to -20 , the tail is too long and your results will be skewed. To test it you can use the following queries to test the RAM behaviour, adapt them for the rest of the resources: # Minimum z_score value sort_desc ( abs (( min_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) # Maximum z_score value sort_desc ( abs (( max_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) For a less exhaustive but more graphical analysis, execute instance_path:node_memory_MemAvailable_percent:z_score in Graph mode. In my case the RAM is in the +-5 interval, with some peaks of 20, but after reviewing instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w in those periods, I feel it's still safe to use the assumption. Same criteria applies to instance_path:node_cpu_percent:rate1m:z_score , instance_path:node_network_receive_gigabits_per_second:rate1m:z_score , and instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score , metrics.","title":"Disclaimer"},{"location":"devops/prometheus/instance_sizing_analysis/#references","text":"Gitlab post on anomaly detection using Prometheus .","title":"References"},{"location":"devops/prometheus/node_exporter/","text":"Node Exporter is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors. Install \u2691 To install in kubernetes nodes, use this chart . Elsewhere use this ansible role . If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them. To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key prometheus.prometheusSpec.additionalScrapeConfigs - job_name : node_exporter ec2_sd_configs : - region : us-east-1 port : 9100 refresh_interval : 1m relabel_configs : - source_labels : [ '__meta_ec2_tag_Name' , '__meta_ec2_private_ip' ] separator : ':' target_label : instance - source_labels : - __meta_ec2_instance_type target_label : instance_type The relabel_configs part will substitute the instance label of each target from {{ instance_ip }}:9100 to {{ instance_name }}:{{ instance_ip }} . If the worker nodes already have an IAM role with the ec2:DescribeInstances permission there is no need to specify the role_arn or access_keys and secret_key . If you have stopped instances, the node exporter will raise an alert because it won't be able to scrape the metrics from them. To only fetch data from running instances add a filter: ec2_sd_configs : - region : us-east-1 filters : - name : instance-state-name values : - running To monitor only the instances of a list of VPCs use this filter: ec2_sd_configs : - region : us-east-1 filters : - name : vpc-id values : - vpc-xxxxxxxxxxxxxxxxx - vpc-yyyyyyyyyyyyyyyyy By default, prometheus will try to scrape the private instance ip. To use the public one you need to relabel it with the following snippet: ec2_sd_configs : - region : us-east-1 relabel_configs : - source_labels : [ '__meta_ec2_public_ip' ] regex : ^(.*)$ target_label : __address__ replacement : ${1}:9100 I'm using the 11074 grafana dashboards for the blackbox exporter, which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : node_exporter : # Ref: https://grafana.com/dashboards/11074 gnetId : 11074 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply Node exporter size analysis \u2691 Once the instance metrics are being ingested, we can do a periodic analysis to deduce which instances are undersized or oversized. Node exporter alerts \u2691 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. Host out of memory \u2691 Node memory is filling up ( < 10% left). - alert : HostOutOfMemory expr : node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of memory (instance {{ $labels.instance }})\" message : \"Node memory is filling up (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host memory under memory pressure \u2691 The node is under heavy memory pressure. High rate of major page faults. - alert : HostMemoryUnderMemoryPressure expr : rate(node_vmstat_pgmajfault[1m]) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host memory under memory pressure (instance {{ $labels.instance }})\" message : \"The node is under heavy memory pressure. High rate of major page faults.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual network throughput in \u2691 Host network interfaces are probably receiving too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputIn expr : sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput in (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably receiving too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual network throughput out \u2691 Host network interfaces are probably sending too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputOut expr : sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput out (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably sending too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk read rate \u2691 Disk is probably reading too much data (> 50 MB/s) - alert : HostUnusualDiskReadRate expr : sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read rate (instance {{ $labels.instance }})\" message : \"Disk is probably reading too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk write rate \u2691 Disk is probably writing too much data (> 50 MB/s) - alert : HostUnusualDiskWriteRate expr : sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write rate (instance {{ $labels.instance }})\" message : \"Disk is probably writing too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host out of disk space \u2691 Disk is worryingly almost full ( < 10% left ). - alert : HostOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 10 for : 5m labels : severity : critical annotations : summary : \"Host out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Disk is almost full ( < 20% left ) - alert : HostReachingOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 20 for : 5m labels : severity : warning annotations : summary : \"Host reaching out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 20% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host disk will fill in 4 hours \u2691 Disk will fill in 4 hours at current write rate - alert : HostDiskWillFillIn4Hours expr : predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) < 0 for : 5m labels : severity : critical annotations : summary : \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\" message : \"Disk will fill in 4 hours at current write rate\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host out of inodes \u2691 Disk is almost running out of available inodes ( < 10% left ). - alert : HostOutOfInodes expr : node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of inodes (instance {{ $labels.instance }})\" message : \"Disk is almost running out of available inodes (< 10% left)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk read latency \u2691 Disk latency is growing (read operations > 100ms). - alert : HostUnusualDiskReadLatency expr : rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (read operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk write latency \u2691 Disk latency is growing (write operations > 100ms) - alert : HostUnusualDiskWriteLatency expr : rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (write operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host high CPU load \u2691 CPU load is > 80% - alert : HostHighCpuLoad expr : 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80 for : 5m labels : severity : warning annotations : summary : \"Host high CPU load (instance {{ $labels.instance }})\" message : \"CPU load is > 80%\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host context switching \u2691 Context switching is growing on node (> 1000 / s) # 1000 context switches is an arbitrary number. # Alert threshold depends on nature of application. # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58 - alert : HostContextSwitching expr : (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host context switching (instance {{ $labels.instance }})\" message : \"Context switching is growing on node (> 1000 / s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host swap is filling up \u2691 Swap is filling up (>80%) - alert : HostSwapIsFillingUp expr : (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80 for : 5m labels : severity : warning annotations : summary : \"Host swap is filling up (instance {{ $labels.instance }})\" message : \"Swap is filling up (>80%)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host SystemD service crashed \u2691 SystemD service crashed - alert : HostSystemdServiceCrashed expr : node_systemd_unit_state{state=\"failed\"} == 1 for : 5m labels : severity : warning annotations : summary : \"Host SystemD service crashed (instance {{ $labels.instance }})\" message : \"SystemD service crashed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host physical component too hot \u2691 Physical hardware component too hot - alert : HostPhysicalComponentTooHot expr : node_hwmon_temp_celsius > 75 for : 5m labels : severity : warning annotations : summary : \"Host physical component too hot (instance {{ $labels.instance }})\" message : \"Physical hardware component too hot\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host node overtemperature alarm \u2691 Physical node temperature alarm triggered - alert : HostNodeOvertemperatureAlarm expr : node_hwmon_temp_alarm == 1 for : 5m labels : severity : critical annotations : summary : \"Host node overtemperature alarm (instance {{ $labels.instance }})\" message : \"Physical node temperature alarm triggered\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host RAID array got inactive \u2691 RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. - alert : HostRaidArrayGotInactive expr : node_md_state{state=\"inactive\"} > 0 for : 5m labels : severity : critical annotations : summary : \"Host RAID array got inactive (instance {{ $labels.instance }})\" message : \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host RAID disk failure \u2691 At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap. - alert : HostRaidDiskFailure expr : node_md_disks{state=\"fail\"} > 0 for : 5m labels : severity : warning annotations : summary : \"Host RAID disk failure (instance {{ $labels.instance }})\" message : \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host kernel version deviations \u2691 Different kernel versions are running. - alert : HostKernelVersionDeviations expr : count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) > 1 for : 5m labels : severity : warning annotations : summary : \"Host kernel version deviations (instance {{ $labels.instance }})\" message : \"Different kernel versions are running\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host OOM kill detected \u2691 OOM kill detected - alert : HostOomKillDetected expr : increase(node_vmstat_oom_kill[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host OOM kill detected (instance {{ $labels.instance }})\" message : \"OOM kill detected\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host Network Receive Errors \u2691 {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes. - alert : HostNetworkReceiveErrors expr : increase(node_network_receive_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Receive Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host Network Transmit Errors \u2691 {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes. - alert : HostNetworkTransmitErrors expr : increase(node_network_transmit_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Transmit Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" References \u2691 Git Prometheus node exporter guide Node exporter alerts","title":"Node Exporter"},{"location":"devops/prometheus/node_exporter/#install","text":"To install in kubernetes nodes, use this chart . Elsewhere use this ansible role . If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them. To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key prometheus.prometheusSpec.additionalScrapeConfigs - job_name : node_exporter ec2_sd_configs : - region : us-east-1 port : 9100 refresh_interval : 1m relabel_configs : - source_labels : [ '__meta_ec2_tag_Name' , '__meta_ec2_private_ip' ] separator : ':' target_label : instance - source_labels : - __meta_ec2_instance_type target_label : instance_type The relabel_configs part will substitute the instance label of each target from {{ instance_ip }}:9100 to {{ instance_name }}:{{ instance_ip }} . If the worker nodes already have an IAM role with the ec2:DescribeInstances permission there is no need to specify the role_arn or access_keys and secret_key . If you have stopped instances, the node exporter will raise an alert because it won't be able to scrape the metrics from them. To only fetch data from running instances add a filter: ec2_sd_configs : - region : us-east-1 filters : - name : instance-state-name values : - running To monitor only the instances of a list of VPCs use this filter: ec2_sd_configs : - region : us-east-1 filters : - name : vpc-id values : - vpc-xxxxxxxxxxxxxxxxx - vpc-yyyyyyyyyyyyyyyyy By default, prometheus will try to scrape the private instance ip. To use the public one you need to relabel it with the following snippet: ec2_sd_configs : - region : us-east-1 relabel_configs : - source_labels : [ '__meta_ec2_public_ip' ] regex : ^(.*)$ target_label : __address__ replacement : ${1}:9100 I'm using the 11074 grafana dashboards for the blackbox exporter, which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : node_exporter : # Ref: https://grafana.com/dashboards/11074 gnetId : 11074 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply","title":"Install"},{"location":"devops/prometheus/node_exporter/#node-exporter-size-analysis","text":"Once the instance metrics are being ingested, we can do a periodic analysis to deduce which instances are undersized or oversized.","title":"Node exporter size analysis"},{"location":"devops/prometheus/node_exporter/#node-exporter-alerts","text":"Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection.","title":"Node exporter alerts"},{"location":"devops/prometheus/node_exporter/#host-out-of-memory","text":"Node memory is filling up ( < 10% left). - alert : HostOutOfMemory expr : node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of memory (instance {{ $labels.instance }})\" message : \"Node memory is filling up (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of memory"},{"location":"devops/prometheus/node_exporter/#host-memory-under-memory-pressure","text":"The node is under heavy memory pressure. High rate of major page faults. - alert : HostMemoryUnderMemoryPressure expr : rate(node_vmstat_pgmajfault[1m]) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host memory under memory pressure (instance {{ $labels.instance }})\" message : \"The node is under heavy memory pressure. High rate of major page faults.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host memory under memory pressure"},{"location":"devops/prometheus/node_exporter/#host-unusual-network-throughput-in","text":"Host network interfaces are probably receiving too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputIn expr : sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput in (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably receiving too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual network throughput in"},{"location":"devops/prometheus/node_exporter/#host-unusual-network-throughput-out","text":"Host network interfaces are probably sending too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputOut expr : sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput out (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably sending too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual network throughput out"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-read-rate","text":"Disk is probably reading too much data (> 50 MB/s) - alert : HostUnusualDiskReadRate expr : sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read rate (instance {{ $labels.instance }})\" message : \"Disk is probably reading too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk read rate"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-write-rate","text":"Disk is probably writing too much data (> 50 MB/s) - alert : HostUnusualDiskWriteRate expr : sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write rate (instance {{ $labels.instance }})\" message : \"Disk is probably writing too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk write rate"},{"location":"devops/prometheus/node_exporter/#host-out-of-disk-space","text":"Disk is worryingly almost full ( < 10% left ). - alert : HostOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 10 for : 5m labels : severity : critical annotations : summary : \"Host out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Disk is almost full ( < 20% left ) - alert : HostReachingOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 20 for : 5m labels : severity : warning annotations : summary : \"Host reaching out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 20% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of disk space"},{"location":"devops/prometheus/node_exporter/#host-disk-will-fill-in-4-hours","text":"Disk will fill in 4 hours at current write rate - alert : HostDiskWillFillIn4Hours expr : predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) < 0 for : 5m labels : severity : critical annotations : summary : \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\" message : \"Disk will fill in 4 hours at current write rate\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host disk will fill in 4 hours"},{"location":"devops/prometheus/node_exporter/#host-out-of-inodes","text":"Disk is almost running out of available inodes ( < 10% left ). - alert : HostOutOfInodes expr : node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of inodes (instance {{ $labels.instance }})\" message : \"Disk is almost running out of available inodes (< 10% left)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of inodes"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-read-latency","text":"Disk latency is growing (read operations > 100ms). - alert : HostUnusualDiskReadLatency expr : rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (read operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk read latency"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-write-latency","text":"Disk latency is growing (write operations > 100ms) - alert : HostUnusualDiskWriteLatency expr : rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (write operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk write latency"},{"location":"devops/prometheus/node_exporter/#host-high-cpu-load","text":"CPU load is > 80% - alert : HostHighCpuLoad expr : 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80 for : 5m labels : severity : warning annotations : summary : \"Host high CPU load (instance {{ $labels.instance }})\" message : \"CPU load is > 80%\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host high CPU load"},{"location":"devops/prometheus/node_exporter/#host-context-switching","text":"Context switching is growing on node (> 1000 / s) # 1000 context switches is an arbitrary number. # Alert threshold depends on nature of application. # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58 - alert : HostContextSwitching expr : (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host context switching (instance {{ $labels.instance }})\" message : \"Context switching is growing on node (> 1000 / s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host context switching"},{"location":"devops/prometheus/node_exporter/#host-swap-is-filling-up","text":"Swap is filling up (>80%) - alert : HostSwapIsFillingUp expr : (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80 for : 5m labels : severity : warning annotations : summary : \"Host swap is filling up (instance {{ $labels.instance }})\" message : \"Swap is filling up (>80%)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host swap is filling up"},{"location":"devops/prometheus/node_exporter/#host-systemd-service-crashed","text":"SystemD service crashed - alert : HostSystemdServiceCrashed expr : node_systemd_unit_state{state=\"failed\"} == 1 for : 5m labels : severity : warning annotations : summary : \"Host SystemD service crashed (instance {{ $labels.instance }})\" message : \"SystemD service crashed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host SystemD service crashed"},{"location":"devops/prometheus/node_exporter/#host-physical-component-too-hot","text":"Physical hardware component too hot - alert : HostPhysicalComponentTooHot expr : node_hwmon_temp_celsius > 75 for : 5m labels : severity : warning annotations : summary : \"Host physical component too hot (instance {{ $labels.instance }})\" message : \"Physical hardware component too hot\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host physical component too hot"},{"location":"devops/prometheus/node_exporter/#host-node-overtemperature-alarm","text":"Physical node temperature alarm triggered - alert : HostNodeOvertemperatureAlarm expr : node_hwmon_temp_alarm == 1 for : 5m labels : severity : critical annotations : summary : \"Host node overtemperature alarm (instance {{ $labels.instance }})\" message : \"Physical node temperature alarm triggered\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host node overtemperature alarm"},{"location":"devops/prometheus/node_exporter/#host-raid-array-got-inactive","text":"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. - alert : HostRaidArrayGotInactive expr : node_md_state{state=\"inactive\"} > 0 for : 5m labels : severity : critical annotations : summary : \"Host RAID array got inactive (instance {{ $labels.instance }})\" message : \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host RAID array got inactive"},{"location":"devops/prometheus/node_exporter/#host-raid-disk-failure","text":"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap. - alert : HostRaidDiskFailure expr : node_md_disks{state=\"fail\"} > 0 for : 5m labels : severity : warning annotations : summary : \"Host RAID disk failure (instance {{ $labels.instance }})\" message : \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host RAID disk failure"},{"location":"devops/prometheus/node_exporter/#host-kernel-version-deviations","text":"Different kernel versions are running. - alert : HostKernelVersionDeviations expr : count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) > 1 for : 5m labels : severity : warning annotations : summary : \"Host kernel version deviations (instance {{ $labels.instance }})\" message : \"Different kernel versions are running\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host kernel version deviations"},{"location":"devops/prometheus/node_exporter/#host-oom-kill-detected","text":"OOM kill detected - alert : HostOomKillDetected expr : increase(node_vmstat_oom_kill[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host OOM kill detected (instance {{ $labels.instance }})\" message : \"OOM kill detected\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host OOM kill detected"},{"location":"devops/prometheus/node_exporter/#host-network-receive-errors","text":"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes. - alert : HostNetworkReceiveErrors expr : increase(node_network_receive_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Receive Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host Network Receive Errors"},{"location":"devops/prometheus/node_exporter/#host-network-transmit-errors","text":"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes. - alert : HostNetworkTransmitErrors expr : increase(node_network_transmit_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Transmit Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host Network Transmit Errors"},{"location":"devops/prometheus/node_exporter/#references","text":"Git Prometheus node exporter guide Node exporter alerts","title":"References"},{"location":"devops/prometheus/prometheus/","text":"Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. A quick overview of Prometheus would be, as stated in the coreos article : At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented. Go to the Prometheus architecture post for more details. We are living a shift to the DevOps culture, containers and Kubernetes. So nowadays: Developers need to integrate app and business related metrics as an organic part of the infrastructure. So monitoring needs to be democratized, made more accessible and cover additional layers of the stack. Container based infrastructures are changing how we monitor the resources. Now we have a huge number of volatile software entities, services, virtual network addresses, exposed metrics that suddenly appear or vanish. Traditional monitoring tools are not designed to handle this. These reasons pushed Soundcloud to build a new monitoring system that had the following features Multi-dimensional data model : The model is based on key-value pairs, similar to how Kubernetes itself organizes infrastructure metadata using labels. It allows for flexible and accurate time series data, powering its Prometheus query language. Accessible format and protocols : Exposing prometheus metrics is a pretty straightforward task. Metrics are human readable, are in a self-explanatory format, and are published using a standard HTTP transport. You can check that the metrics are correctly exposed just using your web browser. Service discovery : The Prometheus server is in charge of periodically scraping the targets, so that applications and services don\u2019t need to worry about emitting data (metrics are pulled, not pushed). These Prometheus servers have several methods to auto-discover scrape targets, some of them can be configured to filter and match container metadata, making it an excellent fit for ephemeral Kubernetes workloads. Modular and highly available components : Metric collection, alerting, graphical visualization, etc, are performed by different composable services. All these services are designed to support redundancy and sharding. Pull based metrics : Most monitoring systems are pushing metrics to a centralized collection platform. Prometheus flips this model on it's head with the following advantages: No need to install custom software in the physical servers or containers. Doesn't require applications to use CPU cycles pushing metrics. Handles service failure/unavailability gracefully. If a target goes down, Prometheus can record it was unable to retrieve data. You can use the Pushgateway if pulling metrics is not feasible. Installation \u2691 There are several ways to install prometheus , but I'd recommend using the Kubernetes or Docker Prometheus operator . Exposing your metrics \u2691 Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics . Metric types \u2691 There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately. Using labels \u2691 Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics. Prometheus rules \u2691 Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules. Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. A simple example rules file would be: groups : - name : example rules : - record : job:http_inprogress_requests:sum expr : sum by (job) (http_inprogress_requests) Regarding naming and aggregation conventions , Recording rules should be of the general form level:metric:operations . level represents the aggregation level and labels of the rule output. metric is the metric name and should be unchanged other than stripping _total off counters when using rate() or irate() . operations is a list of operations (splitted by : ) that were applied to the metric, newest operation first. If you want to add extra labels to the calculated rule use the labels tag like the following example: groups : - name : example rules : - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized Finding a metric \u2691 You can use {__name__=~\".*deploy.*\"} to find the metrics that have deploy somewhere in the name. Links \u2691 Homepage . Docs . Awesome Prometheus . Prometheus rules best practices and configuration . Diving deeper \u2691 Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Node Exporter Prometheus Troubleshooting Introduction posts \u2691 Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview Books \u2691 Prometheus Up & Running . Monitoring With Prometheus .","title":"Prometheus"},{"location":"devops/prometheus/prometheus/#installation","text":"There are several ways to install prometheus , but I'd recommend using the Kubernetes or Docker Prometheus operator .","title":"Installation"},{"location":"devops/prometheus/prometheus/#exposing-your-metrics","text":"Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics .","title":"Exposing your metrics"},{"location":"devops/prometheus/prometheus/#metric-types","text":"There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately.","title":"Metric types"},{"location":"devops/prometheus/prometheus/#using-labels","text":"Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics.","title":"Using labels"},{"location":"devops/prometheus/prometheus/#prometheus-rules","text":"Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules. Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. A simple example rules file would be: groups : - name : example rules : - record : job:http_inprogress_requests:sum expr : sum by (job) (http_inprogress_requests) Regarding naming and aggregation conventions , Recording rules should be of the general form level:metric:operations . level represents the aggregation level and labels of the rule output. metric is the metric name and should be unchanged other than stripping _total off counters when using rate() or irate() . operations is a list of operations (splitted by : ) that were applied to the metric, newest operation first. If you want to add extra labels to the calculated rule use the labels tag like the following example: groups : - name : example rules : - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized","title":"Prometheus rules"},{"location":"devops/prometheus/prometheus/#finding-a-metric","text":"You can use {__name__=~\".*deploy.*\"} to find the metrics that have deploy somewhere in the name.","title":"Finding a metric"},{"location":"devops/prometheus/prometheus/#links","text":"Homepage . Docs . Awesome Prometheus . Prometheus rules best practices and configuration .","title":"Links"},{"location":"devops/prometheus/prometheus/#diving-deeper","text":"Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Node Exporter Prometheus Troubleshooting","title":"Diving deeper"},{"location":"devops/prometheus/prometheus/#introduction-posts","text":"Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview","title":"Introduction posts"},{"location":"devops/prometheus/prometheus/#books","text":"Prometheus Up & Running . Monitoring With Prometheus .","title":"Books"},{"location":"devops/prometheus/prometheus_architecture/","text":"Prometheus Server \u2691 Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries. Prometheus Targets \u2691 Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX . Pushgateway \u2691 In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server. Service discovery \u2691 Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring. Alertmanager \u2691 The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. Data visualization and export \u2691 There are several ways to visualize or export data from Prometheus. Prometheus web UI \u2691 Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery. Grafana \u2691 Grafana is the best way to visually analyze the evolution of the metrics throughout time. API clients \u2691 Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too. Links \u2691 Prometheus Overview Open Source for U architecture overview","title":"Architecture"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-server","text":"Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries.","title":"Prometheus Server"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-targets","text":"Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX .","title":"Prometheus Targets"},{"location":"devops/prometheus/prometheus_architecture/#pushgateway","text":"In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server.","title":"Pushgateway"},{"location":"devops/prometheus/prometheus_architecture/#service-discovery","text":"Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring.","title":"Service discovery"},{"location":"devops/prometheus/prometheus_architecture/#alertmanager","text":"The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.","title":"Alertmanager"},{"location":"devops/prometheus/prometheus_architecture/#data-visualization-and-export","text":"There are several ways to visualize or export data from Prometheus.","title":"Data visualization and export"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-web-ui","text":"Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery.","title":"Prometheus web UI"},{"location":"devops/prometheus/prometheus_architecture/#grafana","text":"Grafana is the best way to visually analyze the evolution of the metrics throughout time.","title":"Grafana"},{"location":"devops/prometheus/prometheus_architecture/#api-clients","text":"Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too.","title":"API clients"},{"location":"devops/prometheus/prometheus_architecture/#links","text":"Prometheus Overview Open Source for U architecture overview","title":"Links"},{"location":"devops/prometheus/prometheus_installation/","text":"Kubernetes \u2691 To install the operator we'll use helmfile to install the stable/prometheus-operator chart . Add the following lines to your helmfile.yaml . - name : prometheus-operator namespace : monitoring chart : stable/prometheus-operator values : - prometheus-operator/values.yaml Edit the chart values. mkdir prometheus-operator helm inspect values stable/prometheus-operator > prometheus-operator/values.yaml vi prometheus-operator/values.yaml I've implemented the following changes: If you are using a managed solution like EKS, the provider will hide kube-scheduler and kube-controller-manager so those metrics will fail. Therefore you need to disable: defaultRules.rules.kubeScheduler: false . kubeScheduler.enabled: false . kubeControllerManager.enabled: false . Enabled the ingress of alertmanager , grafana and prometheus . Set up the storage of alertmanager and prometheus with storageClassName: gp2 (for AWS). Change additionalPrometheusRules to additionalPrometheusRulesMap as the former is going to be deprecated in future releases. For private clusters, disable the admission webhook . prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false And install. helmfile diff helmfile apply Once it's installed you can check everything is working by accessing the grafana dashboard. First of all get the pod name (we'll asume you've used the monitoring namespace). kubectl get pods -n monitoring | grep grafana Then set up the proxies kubectl port-forward {{ grafana_pod }} -n monitoring 3000 :3000 kubectl port-forward -n monitoring \\ prometheus-prometheus-operator-prometheus-0 9090 :9090 To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090 . If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved. Edit the 127.0.0.1 value to 0.0.0.0 for the key metricsBindAddress in kubectl -n kube-system edit cm kube-proxy-config And restart the DaemonSet: kubectl rollout restart -n kube-system daemonset.apps/kube-proxy Docker \u2691 To install it outside Kubernetes, use the cloudalchemy ansible role for host installations or the prom/prometheus docker with the following command: /usr/bin/docker run --rm \\ --name prometheus \\ -v /data/prometheus:/etc/prometheus \\ prom/prometheus:latest \\ --storage.tsdb.retention.time = 30d \\ --config.file = /etc/prometheus/prometheus.yml \\ With a basic prometheus configuration: File: /data/prometheus/prometheus.yml ```yaml \u2691 http://prometheus.io/docs/operating/configuration/ \u2691 global: evaluation_interval: 1m scrape_interval: 1m scrape_timeout: 10s external_labels: environment: helm rule_files: - /etc/prometheus/rules/*.rules scrape_configs: - job_name: prometheus metrics_path: /metrics static_configs: - targets: - prometheus:9090 ``` And some basic rules: File: /data/prometheus/rules/ groups : - name : ansible managed alert rules rules : - alert : Watchdog annotations : description : |- This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the \"DeadMansSnitch\" integration in PagerDuty. summary : Ensure entire alerting pipeline is functional expr : vector(1) for : 10m labels : severity : warning - alert : InstanceDown annotations : description : '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.' summary : Instance {{ $labels.instance }} down expr : up == 0 for : 5m labels : severity : critical - alert : RebootRequired annotations : description : '{{ $labels.instance }} requires a reboot.' summary : Instance {{ $labels.instance }} - reboot required expr : node_reboot_required > 0 labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up. summary : Filesystem is predicted to run out of space within the next 24 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast. summary : Filesystem is predicted to run out of space within the next 4 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 5% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 3% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up. summary : Filesystem is predicted to run out of inodes within the next 24 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast. summary : Filesystem is predicted to run out of inodes within the next 4 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 5% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 3% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeNetworkReceiveErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.' summary : Network interface is reporting many receive errors. expr : |- increase(node_network_receive_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeNetworkTransmitErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.' summary : Network interface is reporting many transmit errors. expr : |- increase(node_network_transmit_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeHighNumberConntrackEntriesUsed annotations : description : '{{ $value | humanizePercentage }} of conntrack entries are used' summary : Number of conntrack are getting close to the limit expr : |- (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75 labels : severity : warning - alert : NodeClockSkewDetected annotations : message : Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host. summary : Clock skew detected. expr : |- ( node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0 ) or ( node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0 ) for : 10m labels : severity : warning - alert : NodeClockNotSynchronising annotations : message : Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host. summary : Clock not synchronising. expr : |- min_over_time(node_timex_sync_status[5m]) == 0 for : 10m labels : severity : warning Next steps \u2691 Configure the alertmanager alerts . Configure the Blackbox Exporter . Configure the grafana dashboards.","title":"Prometheus Install"},{"location":"devops/prometheus/prometheus_installation/#kubernetes","text":"To install the operator we'll use helmfile to install the stable/prometheus-operator chart . Add the following lines to your helmfile.yaml . - name : prometheus-operator namespace : monitoring chart : stable/prometheus-operator values : - prometheus-operator/values.yaml Edit the chart values. mkdir prometheus-operator helm inspect values stable/prometheus-operator > prometheus-operator/values.yaml vi prometheus-operator/values.yaml I've implemented the following changes: If you are using a managed solution like EKS, the provider will hide kube-scheduler and kube-controller-manager so those metrics will fail. Therefore you need to disable: defaultRules.rules.kubeScheduler: false . kubeScheduler.enabled: false . kubeControllerManager.enabled: false . Enabled the ingress of alertmanager , grafana and prometheus . Set up the storage of alertmanager and prometheus with storageClassName: gp2 (for AWS). Change additionalPrometheusRules to additionalPrometheusRulesMap as the former is going to be deprecated in future releases. For private clusters, disable the admission webhook . prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false And install. helmfile diff helmfile apply Once it's installed you can check everything is working by accessing the grafana dashboard. First of all get the pod name (we'll asume you've used the monitoring namespace). kubectl get pods -n monitoring | grep grafana Then set up the proxies kubectl port-forward {{ grafana_pod }} -n monitoring 3000 :3000 kubectl port-forward -n monitoring \\ prometheus-prometheus-operator-prometheus-0 9090 :9090 To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090 . If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved. Edit the 127.0.0.1 value to 0.0.0.0 for the key metricsBindAddress in kubectl -n kube-system edit cm kube-proxy-config And restart the DaemonSet: kubectl rollout restart -n kube-system daemonset.apps/kube-proxy","title":"Kubernetes"},{"location":"devops/prometheus/prometheus_installation/#docker","text":"To install it outside Kubernetes, use the cloudalchemy ansible role for host installations or the prom/prometheus docker with the following command: /usr/bin/docker run --rm \\ --name prometheus \\ -v /data/prometheus:/etc/prometheus \\ prom/prometheus:latest \\ --storage.tsdb.retention.time = 30d \\ --config.file = /etc/prometheus/prometheus.yml \\ With a basic prometheus configuration: File: /data/prometheus/prometheus.yml","title":"Docker"},{"location":"devops/prometheus/prometheus_installation/#yaml","text":"","title":"```yaml"},{"location":"devops/prometheus/prometheus_installation/#httpprometheusiodocsoperatingconfiguration","text":"global: evaluation_interval: 1m scrape_interval: 1m scrape_timeout: 10s external_labels: environment: helm rule_files: - /etc/prometheus/rules/*.rules scrape_configs: - job_name: prometheus metrics_path: /metrics static_configs: - targets: - prometheus:9090 ``` And some basic rules: File: /data/prometheus/rules/ groups : - name : ansible managed alert rules rules : - alert : Watchdog annotations : description : |- This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the \"DeadMansSnitch\" integration in PagerDuty. summary : Ensure entire alerting pipeline is functional expr : vector(1) for : 10m labels : severity : warning - alert : InstanceDown annotations : description : '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.' summary : Instance {{ $labels.instance }} down expr : up == 0 for : 5m labels : severity : critical - alert : RebootRequired annotations : description : '{{ $labels.instance }} requires a reboot.' summary : Instance {{ $labels.instance }} - reboot required expr : node_reboot_required > 0 labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up. summary : Filesystem is predicted to run out of space within the next 24 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast. summary : Filesystem is predicted to run out of space within the next 4 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 5% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 3% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up. summary : Filesystem is predicted to run out of inodes within the next 24 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast. summary : Filesystem is predicted to run out of inodes within the next 4 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 5% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 3% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeNetworkReceiveErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.' summary : Network interface is reporting many receive errors. expr : |- increase(node_network_receive_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeNetworkTransmitErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.' summary : Network interface is reporting many transmit errors. expr : |- increase(node_network_transmit_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeHighNumberConntrackEntriesUsed annotations : description : '{{ $value | humanizePercentage }} of conntrack entries are used' summary : Number of conntrack are getting close to the limit expr : |- (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75 labels : severity : warning - alert : NodeClockSkewDetected annotations : message : Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host. summary : Clock skew detected. expr : |- ( node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0 ) or ( node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0 ) for : 10m labels : severity : warning - alert : NodeClockNotSynchronising annotations : message : Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host. summary : Clock not synchronising. expr : |- min_over_time(node_timex_sync_status[5m]) == 0 for : 10m labels : severity : warning","title":"http://prometheus.io/docs/operating/configuration/"},{"location":"devops/prometheus/prometheus_installation/#next-steps","text":"Configure the alertmanager alerts . Configure the Blackbox Exporter . Configure the grafana dashboards.","title":"Next steps"},{"location":"devops/prometheus/prometheus_operator/","text":"Prometheus has it's own kubernetes operator , which makes it simple to install with helm, and enables users to configure and manage instances of Prometheus using simple declarative configuration that will, in response, create, configure, and manage Prometheus monitoring instances. Once installed the Prometheus Operator provides the following features: Create/Destroy : Easily launch a Prometheus instance for your Kubernetes namespace, a specific application or team easily using the Operator. Simple Configuration : Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource. Target Services via Labels : Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. How it works \u2691 The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running. Links \u2691 Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3","title":"Prometheus Operator"},{"location":"devops/prometheus/prometheus_operator/#how-it-works","text":"The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running.","title":"How it works"},{"location":"devops/prometheus/prometheus_operator/#links","text":"Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3","title":"Links"},{"location":"devops/prometheus/prometheus_troubleshooting/","text":"Solutions for problems with Prometheus. Service monitor not being recognized \u2691 Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work. Failed calling webhook prometheusrulemutate.monitoring.coreos.com \u2691 Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.","title":"Prometheus Troubleshooting"},{"location":"devops/prometheus/prometheus_troubleshooting/#service-monitor-not-being-recognized","text":"Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work.","title":"Service monitor not being recognized"},{"location":"devops/prometheus/prometheus_troubleshooting/#failed-calling-webhook-prometheusrulemutatemonitoringcoreoscom","text":"Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.","title":"Failed calling webhook prometheusrulemutate.monitoring.coreos.com"},{"location":"drawing/drawing/","text":"It's really difficult to choose where to start if you want to learn how to draw from scratch as there are too many resources. I'm starting with Drawabox , a free course based on series of practical exercises to teach the basics. I'd probably go to Ctrl+Paint once I'm done. The basics \u2691 Changing the mindset \u2691 Start your drawing path with the following guidelines to make your progression smoother: Focus on the experience of drawing instead of the result. Understand that doing something badly does not define who you are. So tackle the I can't draw that feeling with I can't draw that well . If you're afraid the thing you want to draw is out of your reach, draw it anyway. At least half of the time spent drawing must be devoted to drawing purely for its own sake. If you don't have much time, alternate the purpose of your sessions. Don't over control your hand with your brain to try to be absolutely precise and accurate. Doing so will result in numerous course corrections making your strokes wobbly, stiff and erratic. Furthermore, spending all the focus resources in precision, will result in a lack to solve the other problems involved. Once muscle memory is gained, the strokes will be cleaner. Draw exactly what you see, while you see it . Don't trust you memory, as it will simplify things without you noticing it. Draw from your shoulder . We are used to pivot on the wrist as it makes stiff and accurate linework, suitable for writing. But falls apart when making smooth and consistent strokes. So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get yo u in the habit of taking the path of least resistance . So try to use the shoulder. This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow. If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder. Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should: Flow continuously : When making a line between two points, do it with a single continuous stroke even if you miss the end. Flow smoothly : Draw with a confident and persistent pace (enough to keep your brain from interfering and attempting to course correct as you go). Again we favor flow over accuracy , so expect to make your lines less accurate. Maintain a consistent trayectory : Split lines into derivable strokes. Otherwise, you'll make mindless zigzags. Drawing skills \u2691 The course focuses on these psychological skills and concepts : Confidence : The willingness to push forwards without hesitation once your preparations are complete. Control : The ability to decide ahead of time what kind of mark you wish to puto down on the page, and to execute as intended. Patience : The path is hard. Spatial Reasoning : To be able to understand the things we draw as being three dimensional forms that exist in and relate to one another within the three dimensional world. Construction : The ability to look at a complex object and break it down into simple components that can be drawn individually and combined to reconstruct our complex object on a page. Visual Communication : The ability to take a concept, idea, or amount of information, and to convey it clearly and directly to an audience using visual means. Ghosting \u2691 Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down. Planning : Lay out the terms of the line you want to draw, paint a dot for the start and another for the end. Rotating the page : Find the most comfortable angle of approach for the line you've planned. Usually it's roughly a 45 degree angle fom left to right (if you're right handed). Ghosting : Go through the motion of drawing your line, over and over, in one direction, without actually touching the page, so as to build muscle memory. Execution : Once you feel comfortable with the motion, without missing a beat or breaking the rhythm of repetition, lower your pen to the page and go through the motion one more time. Drawabox course guidelines \u2691 When doing the course exercises, try to: Read only the instruction pages that are assigned to each exercise. Don't redo the exercises until you achieve perfection, even when you don't feel satisfied with your results. Accept this now, and it will save you a lot of grief and wasted time in the future. Your only focus should be on following the instructions to the best of your current ability. Read and follow the instructions carefully to ensure you understand them. Each time you finish an exercise incorporate into a pool from which take two or three at the beginning of each session to do for 10 or 15 minutes. Tools to use \u2691 For the Drawabox course, you need a fineliner , also called felt tips or technical pens . The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5). When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink. For the paper, use the regular printer one. Links \u2691 Drawabox Ctrl+Paint Dive deeper \u2691 Pool of drawing exercises","title":"Drawing"},{"location":"drawing/drawing/#the-basics","text":"","title":"The basics"},{"location":"drawing/drawing/#changing-the-mindset","text":"Start your drawing path with the following guidelines to make your progression smoother: Focus on the experience of drawing instead of the result. Understand that doing something badly does not define who you are. So tackle the I can't draw that feeling with I can't draw that well . If you're afraid the thing you want to draw is out of your reach, draw it anyway. At least half of the time spent drawing must be devoted to drawing purely for its own sake. If you don't have much time, alternate the purpose of your sessions. Don't over control your hand with your brain to try to be absolutely precise and accurate. Doing so will result in numerous course corrections making your strokes wobbly, stiff and erratic. Furthermore, spending all the focus resources in precision, will result in a lack to solve the other problems involved. Once muscle memory is gained, the strokes will be cleaner. Draw exactly what you see, while you see it . Don't trust you memory, as it will simplify things without you noticing it. Draw from your shoulder . We are used to pivot on the wrist as it makes stiff and accurate linework, suitable for writing. But falls apart when making smooth and consistent strokes. So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get yo u in the habit of taking the path of least resistance . So try to use the shoulder. This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow. If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder. Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should: Flow continuously : When making a line between two points, do it with a single continuous stroke even if you miss the end. Flow smoothly : Draw with a confident and persistent pace (enough to keep your brain from interfering and attempting to course correct as you go). Again we favor flow over accuracy , so expect to make your lines less accurate. Maintain a consistent trayectory : Split lines into derivable strokes. Otherwise, you'll make mindless zigzags.","title":"Changing the mindset"},{"location":"drawing/drawing/#drawing-skills","text":"The course focuses on these psychological skills and concepts : Confidence : The willingness to push forwards without hesitation once your preparations are complete. Control : The ability to decide ahead of time what kind of mark you wish to puto down on the page, and to execute as intended. Patience : The path is hard. Spatial Reasoning : To be able to understand the things we draw as being three dimensional forms that exist in and relate to one another within the three dimensional world. Construction : The ability to look at a complex object and break it down into simple components that can be drawn individually and combined to reconstruct our complex object on a page. Visual Communication : The ability to take a concept, idea, or amount of information, and to convey it clearly and directly to an audience using visual means.","title":"Drawing skills"},{"location":"drawing/drawing/#ghosting","text":"Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down. Planning : Lay out the terms of the line you want to draw, paint a dot for the start and another for the end. Rotating the page : Find the most comfortable angle of approach for the line you've planned. Usually it's roughly a 45 degree angle fom left to right (if you're right handed). Ghosting : Go through the motion of drawing your line, over and over, in one direction, without actually touching the page, so as to build muscle memory. Execution : Once you feel comfortable with the motion, without missing a beat or breaking the rhythm of repetition, lower your pen to the page and go through the motion one more time.","title":"Ghosting"},{"location":"drawing/drawing/#drawabox-course-guidelines","text":"When doing the course exercises, try to: Read only the instruction pages that are assigned to each exercise. Don't redo the exercises until you achieve perfection, even when you don't feel satisfied with your results. Accept this now, and it will save you a lot of grief and wasted time in the future. Your only focus should be on following the instructions to the best of your current ability. Read and follow the instructions carefully to ensure you understand them. Each time you finish an exercise incorporate into a pool from which take two or three at the beginning of each session to do for 10 or 15 minutes.","title":"Drawabox course guidelines"},{"location":"drawing/drawing/#tools-to-use","text":"For the Drawabox course, you need a fineliner , also called felt tips or technical pens . The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5). When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink. For the paper, use the regular printer one.","title":"Tools to use"},{"location":"drawing/drawing/#links","text":"Drawabox Ctrl+Paint","title":"Links"},{"location":"drawing/drawing/#dive-deeper","text":"Pool of drawing exercises","title":"Dive deeper"},{"location":"drawing/exercise_pool/","text":"Set of exercises to maintain the fundamental skills required for drawing. Before doing a drawing session, spend 10-20 minutes doing one or several of these exercises. Superimposed lines : for different increasing lengths (4cm, 8cm, half the width and full width), draw a line with a ruler and repeat the stroke freehand eight times. Also try some arcing lines, and even some waves. Make sure you fray only at the end. Example: Ghosted lines : Fill up a page with straight lines following the ghosting method . Special things to avoid: Wobbly lines Arcing lines There are several levels of success with this exercise: Level 1 : Line is smooth and consistent without any visible wobbling, but doesn't quite pass through A or B, due to not following the right trajectory. It's a straight shot, but misses the mark a bit. Level 2 : Like level 1 and maintains the correct trajectory. It does however either fall short or overshot one or both points. Level 3 : Like level 2 and also starts at right at one point and ends exactly at the other. Example: Ghosted planes : Fill up a page with planes using the ghosting method . Start with 4 points, join them, then fill in the two diagonals, and then make a cross through the center of the X. Special things to avoid: Wobbly lines Arcing lines Example: As you repeat the exercise, you can start to envision these planes as being three dimensional rectilinear surfaces. The third and fourth steps, where we construct the diagonals and the cross can be treated as being a subdivision of the plane. The cross will require some estimation to find the center of each edge in space.","title":"Exercise Pool"},{"location":"life_automation/life_automation/","text":"Life Automation is the act of analyzing your interactions with the world to find ways to reduce the time or willpower spent on unwanted processes. Once you've covered some minimum life requirements (health, money or happiness), time is your most valued asset. It's sad to waste it doing stuff that we need but don't increase our happiness. So the idea is to identify which are those processes and find optimizations that allows us to do them in less time or using less willpower. I've also faced the problem of having so much stuff in my mind. Having background processes increase your brain load and are a constant sink of willpower. As a result, when you really need that CPU time, your brain is tired and doesn't work to it's full performance. Automating processes, like life logging and task management, allows you to delegate those worries. Life automation can lead to habit building, which reduces even more the willpower consumption of processes, at the same time it reduces the error rate. Automating life management \u2691 Week automation , or how to review and plan the week. Automating home chores \u2691 Using Grocy to maintain the house stock, shopping lists and meal plans.","title":"Life Automation"},{"location":"life_automation/life_automation/#automating-life-management","text":"Week automation , or how to review and plan the week.","title":"Automating life management"},{"location":"life_automation/life_automation/#automating-home-chores","text":"Using Grocy to maintain the house stock, shopping lists and meal plans.","title":"Automating home chores"},{"location":"life_automation/relationship_automation/","text":"I try to keep my mind as empty as possible of non relevant processes, that's why I use a task manager to handle my tasks and meetings. This system has a side effect, if there isn't something reminding you that you have to do something, you fail to do it. That principle applied to human relationships means that if you don't stumble that person in your daily life, it doesn't exist for you and you will probably not take enough care that the person deserves. To solve that problem I started creating periodic tasks to call these people or hang out. I've also used those tasks to keep a diary of the interactions. Recently I've found Monica a popular open source personal CRM that helps in the same direction. So I'm going to migrate all my information to the system and see how it goes.","title":"Relationship Automation"},{"location":"life_automation/vim_automation/","text":"Abbreviations \u2691 In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, ~/.vim/abbreviations.vim for abbreviations that can be used in every type of format and ~/.vim/markdown-abbreviations.vim for the ones that can interfere with programming typing. Those files are sourced in my .vimrc \" Abbreviations source ~ /.vim/ abbreviations. vim autocmd BufNewFile , BufReadPost *.md source ~ /.vim/ markdown - abbreviations. vim To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as teh . The process has it's inconveniences: You need different abbreviations for the capitalized versions, so you'd need two abbreviations for iab cant can't and iab Cant Can't It's not user friendly to add new words, as you need to open a file. The Vim Abolish plugin solves that. For example: \" Typing the following: Abolish seperate separate \" Is equivalent to: iabbrev seperate separate iabbrev Seperate Separate iabbrev SEPERATE SEPARATE Or create more complex rules, were each {} gets captured and expanded with different caps : Abolish {despa , sepe}rat{ e , es , ed , ing , ely , ion , ions , or} {despe , sepa}rat{} With a bang ( :Abolish! ) the abbreviation is also appended to the file in g:abolish_save_file . By default after/plugin/abolish.vim which is loaded by default. Typing :Abolish! im I'm will append the following to the end of this file: Abolish im I' m To make it quicker I've added a mapping for <leader>s . nnoremap < leader > s :Abolish !< Space > Check the README for more details. Troubleshooting \u2691 Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue . Abolish knobas knowledge - based Abolish w what Will yield KnowledgeBased if invoked with Knobas , and WHAT if invoked with W . Therefore the following definitions are preferred: Abolish Knobas Knowledge - based Abolish W What Vim Fugitive \u2691 Add portions of file to the index \u2691 To stage only part of the file to a commit, open it and launch :Gdiff . With diffput and diffobtain Vim's functionality you move to the index file (the one in the left) the changes you want to stage. Prepare environment to write the commit message \u2691 When I write the commit message I like to review what changes I'm commiting. To do it I find useful to close all windows and do a vertical split with the changes so I can write the commit message in one of the window while I scroll down in the other. As the changes are usually no the at the top of the file, I scroll the window of the right to the first change and then switch back to the left one in insert mode to start writing. I've also made some movement remappings: jj , kk , <C-d> and <C-u> in insert mode will insert normal mode and go to the window in the right to continue seeing the changes. i , a , o , O : if you are in the changes window it will go to the commit message window in insert mode. Once I've made the commit I want to only retain one buffer. Add the following snippet to do just that: \" Open commit message buffer in fullscreen with a vertical split, and close it with \" leader q au BufNewFile,BufRead *COMMIT_EDITMSG call CommitMessage() function! RestoreBindings() inoremap jj <esc>j inoremap kk <esc>k inoremap <C-d> <C-d> inoremap <C-u> <C-u> nnoremap i i nnoremap a a nnoremap o o nnoremap O O endfunction function! CommitMessage() \" Remap the saving mappings \" Close buffer when saving inoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> nnoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> inoremap jj <esc>:wincmd l<cr>j inoremap kk <esc>:wincmd l<cr>k inoremap <C-d> <esc>:wincmd l<cr><C-d> inoremap <C-u> <esc>:wincmd l<cr><C-u> nnoremap i :wincmd h<cr>i nnoremap a :wincmd h<cr>a nnoremap o :wincmd h<cr>o nnoremap O :wincmd h<cr>O \" Remove bad habits inoremap jk <nop> inoremap ZZ <nop> nnoremap ZZ <nop> \" Close all other windows only \" Create a vertical split vsplit \" Go to the right split wincmd l \" Go to the first change execute \"normal! /^diff\\<cr>8j\" \" Clear the search highlights nohl \" Go back to the left split wincmd h \" Enter insert mode execute \"startinsert\" endfunction I'm assuming that you save with <leader>w and that you're using Sayonara to close your buffers. Auto complete prose text \u2691 Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default. let g :ycm_filetype_blacklist = { \\ 'tagbar' : 1 , \\ 'qf' : 1 , \\ 'notes' : 1 , \\ 'unite' : 1 , \\ 'vimwiki' : 1 , \\ 'pandoc' : 1 , \\ 'infolog' : 1 \\} When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions. \" Limit the results for markdown files to 1 au FileType markdown let g :ycm_max_num_candidates = 1 au FileType markdown let g :ycm_max_num_identifier_candidates = 1 Find synonyms \u2691 Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config. File: ~/.vimrc Plugin 'ron89/thesaurus_query.vim' \" Thesaurus let g :tq_enabled_backends = [ \"mthesaur_txt\" ] let g :tq_mthesaur_file = \"~/.vim/thesaurus\" nnoremap < leader > r :ThesaurusQueryReplaceCurrentWord < CR > inoremap < leader > r < esc > :ThesaurusQueryReplaceCurrentWord < CR > Run :PluginInstall and download the thesaurus text from gutenberg.org Next time you find a word like therefore you can press :ThesaurusQueryReplaceCurrentWord and you'll get a window with the following: In line: ... therefore ... Candidates for therefore, found by backend: mthesaur_txt Synonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise (48)under the circumstances (49)whence (50)wherefore (51)wherefrom Type number and <Enter> (empty cancels; 'n': use next backend; 'p' use previous backend): If for example you type 45 and hit enter, it will change it for thus . Keep foldings \u2691 When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file augroup remember_folds autocmd ! autocmd BufWinLeave * mkview autocmd BufWinEnter * silent ! loadview augroup END Python folding done right \u2691 Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours... SimpylFold does the trick just fine.","title":"Vim Automation"},{"location":"life_automation/vim_automation/#abbreviations","text":"In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, ~/.vim/abbreviations.vim for abbreviations that can be used in every type of format and ~/.vim/markdown-abbreviations.vim for the ones that can interfere with programming typing. Those files are sourced in my .vimrc \" Abbreviations source ~ /.vim/ abbreviations. vim autocmd BufNewFile , BufReadPost *.md source ~ /.vim/ markdown - abbreviations. vim To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as teh . The process has it's inconveniences: You need different abbreviations for the capitalized versions, so you'd need two abbreviations for iab cant can't and iab Cant Can't It's not user friendly to add new words, as you need to open a file. The Vim Abolish plugin solves that. For example: \" Typing the following: Abolish seperate separate \" Is equivalent to: iabbrev seperate separate iabbrev Seperate Separate iabbrev SEPERATE SEPARATE Or create more complex rules, were each {} gets captured and expanded with different caps : Abolish {despa , sepe}rat{ e , es , ed , ing , ely , ion , ions , or} {despe , sepa}rat{} With a bang ( :Abolish! ) the abbreviation is also appended to the file in g:abolish_save_file . By default after/plugin/abolish.vim which is loaded by default. Typing :Abolish! im I'm will append the following to the end of this file: Abolish im I' m To make it quicker I've added a mapping for <leader>s . nnoremap < leader > s :Abolish !< Space > Check the README for more details.","title":"Abbreviations"},{"location":"life_automation/vim_automation/#troubleshooting","text":"Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue . Abolish knobas knowledge - based Abolish w what Will yield KnowledgeBased if invoked with Knobas , and WHAT if invoked with W . Therefore the following definitions are preferred: Abolish Knobas Knowledge - based Abolish W What","title":"Troubleshooting"},{"location":"life_automation/vim_automation/#vim-fugitive","text":"","title":"Vim Fugitive"},{"location":"life_automation/vim_automation/#add-portions-of-file-to-the-index","text":"To stage only part of the file to a commit, open it and launch :Gdiff . With diffput and diffobtain Vim's functionality you move to the index file (the one in the left) the changes you want to stage.","title":"Add portions of file to the index"},{"location":"life_automation/vim_automation/#prepare-environment-to-write-the-commit-message","text":"When I write the commit message I like to review what changes I'm commiting. To do it I find useful to close all windows and do a vertical split with the changes so I can write the commit message in one of the window while I scroll down in the other. As the changes are usually no the at the top of the file, I scroll the window of the right to the first change and then switch back to the left one in insert mode to start writing. I've also made some movement remappings: jj , kk , <C-d> and <C-u> in insert mode will insert normal mode and go to the window in the right to continue seeing the changes. i , a , o , O : if you are in the changes window it will go to the commit message window in insert mode. Once I've made the commit I want to only retain one buffer. Add the following snippet to do just that: \" Open commit message buffer in fullscreen with a vertical split, and close it with \" leader q au BufNewFile,BufRead *COMMIT_EDITMSG call CommitMessage() function! RestoreBindings() inoremap jj <esc>j inoremap kk <esc>k inoremap <C-d> <C-d> inoremap <C-u> <C-u> nnoremap i i nnoremap a a nnoremap o o nnoremap O O endfunction function! CommitMessage() \" Remap the saving mappings \" Close buffer when saving inoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> nnoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> inoremap jj <esc>:wincmd l<cr>j inoremap kk <esc>:wincmd l<cr>k inoremap <C-d> <esc>:wincmd l<cr><C-d> inoremap <C-u> <esc>:wincmd l<cr><C-u> nnoremap i :wincmd h<cr>i nnoremap a :wincmd h<cr>a nnoremap o :wincmd h<cr>o nnoremap O :wincmd h<cr>O \" Remove bad habits inoremap jk <nop> inoremap ZZ <nop> nnoremap ZZ <nop> \" Close all other windows only \" Create a vertical split vsplit \" Go to the right split wincmd l \" Go to the first change execute \"normal! /^diff\\<cr>8j\" \" Clear the search highlights nohl \" Go back to the left split wincmd h \" Enter insert mode execute \"startinsert\" endfunction I'm assuming that you save with <leader>w and that you're using Sayonara to close your buffers.","title":"Prepare environment to write the commit message"},{"location":"life_automation/vim_automation/#auto-complete-prose-text","text":"Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default. let g :ycm_filetype_blacklist = { \\ 'tagbar' : 1 , \\ 'qf' : 1 , \\ 'notes' : 1 , \\ 'unite' : 1 , \\ 'vimwiki' : 1 , \\ 'pandoc' : 1 , \\ 'infolog' : 1 \\} When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions. \" Limit the results for markdown files to 1 au FileType markdown let g :ycm_max_num_candidates = 1 au FileType markdown let g :ycm_max_num_identifier_candidates = 1","title":"Auto complete prose text"},{"location":"life_automation/vim_automation/#find-synonyms","text":"Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config. File: ~/.vimrc Plugin 'ron89/thesaurus_query.vim' \" Thesaurus let g :tq_enabled_backends = [ \"mthesaur_txt\" ] let g :tq_mthesaur_file = \"~/.vim/thesaurus\" nnoremap < leader > r :ThesaurusQueryReplaceCurrentWord < CR > inoremap < leader > r < esc > :ThesaurusQueryReplaceCurrentWord < CR > Run :PluginInstall and download the thesaurus text from gutenberg.org Next time you find a word like therefore you can press :ThesaurusQueryReplaceCurrentWord and you'll get a window with the following: In line: ... therefore ... Candidates for therefore, found by backend: mthesaur_txt Synonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise (48)under the circumstances (49)whence (50)wherefore (51)wherefrom Type number and <Enter> (empty cancels; 'n': use next backend; 'p' use previous backend): If for example you type 45 and hit enter, it will change it for thus .","title":"Find synonyms"},{"location":"life_automation/vim_automation/#keep-foldings","text":"When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file augroup remember_folds autocmd ! autocmd BufWinLeave * mkview autocmd BufWinEnter * silent ! loadview augroup END","title":"Keep foldings"},{"location":"life_automation/vim_automation/#python-folding-done-right","text":"Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours... SimpylFold does the trick just fine.","title":"Python folding done right"},{"location":"life_automation/week_automation/","text":"I've been polishing a week reviewing and planning method that suits my needs. I usually follow it on Wednesdays, as I'm too busy on Mondays and Tuesdays and it gives enough time to plan the weekend. Until I've got pydo ready to natively incorporate all this processes, I heavily use taskwarrior to manage my tasks and logs. To make the process faster and reproducible, I've written small python scripts using tasklib. Week review \u2691 Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date. Week planning \u2691 The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week Automation"},{"location":"life_automation/week_automation/#week-review","text":"Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date.","title":"Week review"},{"location":"life_automation/week_automation/#week-planning","text":"The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week planning"},{"location":"linux/brew/","text":"Complementary package manager to manage the programs that aren't in the Debian repositories. Usage \u2691 TBC References \u2691 Homebrew formula for a Go app","title":"brew"},{"location":"linux/brew/#usage","text":"TBC","title":"Usage"},{"location":"linux/brew/#references","text":"Homebrew formula for a Go app","title":"References"},{"location":"linux/cookiecutter/","text":"Cookiecutter is a command-line utility that creates projects from cookiecutters (project templates). Install \u2691 pip install cookiecutter Use \u2691 DEPRECATION: use cruft instead You may want to use cruft to generate your templates instead, as it will help you maintain the project with the template updates. Something that it's not easy with cookiecutter alone cookiecutter {{ path_or_url_to_cookiecutter_template }} User config \u2691 If you use Cookiecutter a lot, you\u2019ll find it useful to have a user config file. By default Cookiecutter tries to retrieve settings from a .cookiecutterrc file in your home directory. Example user config: default_context : full_name : \"Audrey Roy\" email : \"audreyr@example.com\" github_username : \"audreyr\" cookiecutters_dir : \"/home/audreyr/my-custom-cookiecutters-dir/\" replay_dir : \"/home/audreyr/my-custom-replay-dir/\" abbreviations : python : https://github.com/audreyr/cookiecutter-pypackage.git gh : https://github.com/{0}.git bb : https://bitbucket.org/{0} Possible settings are: default_context A list of key/value pairs that you want injected as context whenever you generate a project with Cookiecutter. These values are treated like the defaults in cookiecutter.json , upon generation of any project. cookiecutters_dir Directory where your cookiecutters are cloned to when you use Cookiecutter with a repo argument. replay_dir Directory where Cookiecutter dumps context data to, which you can fetch later on when using the replay feature. abbreviations A list of abbreviations for cookiecutters. Abbreviations can be simple aliases for a repo name, or can be used as a prefix, in the form abbr:suffix . Any suffix will be inserted into the expansion in place of the text {0} , using standard Python string formatting. With the above aliases, you could use the cookiecutter-pypackage template simply by saying cookiecutter python . Write your own cookietemplates \u2691 Create files or directories with conditions \u2691 For files use a filename like '{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}' . For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information. File: post_gen_project.py import os import sys REMOVE_PATHS = [ '{ % i f cookiecutter.packaging != \"pip\" %} requirements.txt { % e ndif %}' , '{ % i f cookiecutter.packaging != \"poetry\" %} poetry.lock { % e ndif %}' , ] for path in REMOVE_PATHS : path = path . strip () if path and os . path . exists ( path ): if os . path . isdir ( path ): os . rmdir ( path ) else : os . unlink ( path ) Add some text to a file if a condition is met \u2691 Use jinja2 conditionals. Note the - at the end of the conditional opening, play with {%- ... -%} and {% ... %} for different results on line appending. { % if cookiecutter.install_docker == 'yes' -% } - src : git+ssh://mywebpage.org/ansible-roles/docker.git version : 1.0.3 { % - else -% } - src : git+ssh://mywebpage.org/ansible-roles/other-role.git version : 1.0.2 { % - endif % } Initialize git repository on the created cookiecutter \u2691 Added the following to the post generation hooks. File: hooks/post_gen_project.py import subprocess subprocess . call ([ 'git' , 'init' ]) subprocess . call ([ 'git' , 'add' , '*' ]) subprocess . call ([ 'git' , 'commit' , '-m' , 'Initial commit' ]) Prevent cookiecutter from processing some files \u2691 By default cookiecutter will try to process every file as a Jinja template. This behaviour produces wrong results if you have Jinja templates that are meant to be taken as literal. Starting with cookiecutter 1.1, you can tell cookiecutter to only copy some files without interpreting them as Jinja templates . Add a _copy_without_render key in the cookiecutter config file ( cookiecutter.json ). It takes a list of regular expressions. If a filename matches the regular expressions it will be copied and not processed as a Jinja template. { \"project_slug\" : \"sample\" , \"_copy_without_render\" : [ \"*.js\" , \"not_rendered_dir/*\" , \"rendered_dir/not_rendered_file.ini\" ] } Prevent additional whitespaces when jinja condition is not met. \u2691 Jinja2 has a whitespace control that can be used to manage the whitelines existent between the Jinja blocks. The problem comes when a condition is not met in an if block, in that case, Jinja adds a whitespace which will break most linters. This is the solution I've found out that works as expected. ### Multienvironment This playbook has support for the following environments: {% if cookiecutter.production_environment == \"True\" -%} * Production {% endif %} {%- if cookiecutter.staging_environment == \"True\" -%} * Staging {% endif %} {%- if cookiecutter.development_environment == \"True\" -%} * Development {% endif %} ### Tags Testing your own cookiecutter templates \u2691 The pytest-cookies plugin comes with a cookies fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests. Install \u2691 pip install pytest-cookies Usage \u2691 @pytest.fixture def context(): return { \"playbook_name\": \"My Test Playbook\", } The cookies.bake() method generates a new project from your template based on the default values specified in cookiecutter.json: def test_bake_project ( cookies ): result = cookies . bake ( extra_context = { 'repo_name' : 'hello world' }) assert result . exit_code == 0 assert result . exception is None assert result . project . basename == 'hello world' assert result . project . isdir () It accepts the extra_context keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data. The cookiecutter-django has a nice test file using this fixture. Mocking the contents of the cookiecutter hooks \u2691 Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline. If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the cookies.bake() code. Instead it delegates in cookiecutter to run them, which opens a subprocess to run them , so the mocks don't work. The alternative is setting an environmental variable in your tests to skip those steps: File: tests/conftest.py import os os . environ [ \"COOKIECUTTER_TESTING\" ] = \"true\" File: hooks/pre_gen_project.py def main (): # ... pre_hook content ... if __name__ == \"__main__\" : if os . environ . get ( \"COOKIECUTTER_TESTING\" ) != \"true\" : main () If you want to test the content of main , you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors. Debug failing template generation \u2691 Sometimes the generation of the templates will fail in the tests, I've found that the easier way to debug why is to inspect the result object of the result = cookies.bake() statement with pdb. It has an exception method with lineno argument and source . With that information I've been able to locate the failing line. It also has a filename attribute but it doesn't seem to work for me. References \u2691 Git Docs","title":"cookiecutter"},{"location":"linux/cookiecutter/#install","text":"pip install cookiecutter","title":"Install"},{"location":"linux/cookiecutter/#use","text":"DEPRECATION: use cruft instead You may want to use cruft to generate your templates instead, as it will help you maintain the project with the template updates. Something that it's not easy with cookiecutter alone cookiecutter {{ path_or_url_to_cookiecutter_template }}","title":"Use"},{"location":"linux/cookiecutter/#user-config","text":"If you use Cookiecutter a lot, you\u2019ll find it useful to have a user config file. By default Cookiecutter tries to retrieve settings from a .cookiecutterrc file in your home directory. Example user config: default_context : full_name : \"Audrey Roy\" email : \"audreyr@example.com\" github_username : \"audreyr\" cookiecutters_dir : \"/home/audreyr/my-custom-cookiecutters-dir/\" replay_dir : \"/home/audreyr/my-custom-replay-dir/\" abbreviations : python : https://github.com/audreyr/cookiecutter-pypackage.git gh : https://github.com/{0}.git bb : https://bitbucket.org/{0} Possible settings are: default_context A list of key/value pairs that you want injected as context whenever you generate a project with Cookiecutter. These values are treated like the defaults in cookiecutter.json , upon generation of any project. cookiecutters_dir Directory where your cookiecutters are cloned to when you use Cookiecutter with a repo argument. replay_dir Directory where Cookiecutter dumps context data to, which you can fetch later on when using the replay feature. abbreviations A list of abbreviations for cookiecutters. Abbreviations can be simple aliases for a repo name, or can be used as a prefix, in the form abbr:suffix . Any suffix will be inserted into the expansion in place of the text {0} , using standard Python string formatting. With the above aliases, you could use the cookiecutter-pypackage template simply by saying cookiecutter python .","title":"User config"},{"location":"linux/cookiecutter/#write-your-own-cookietemplates","text":"","title":"Write your own cookietemplates"},{"location":"linux/cookiecutter/#create-files-or-directories-with-conditions","text":"For files use a filename like '{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}' . For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information. File: post_gen_project.py import os import sys REMOVE_PATHS = [ '{ % i f cookiecutter.packaging != \"pip\" %} requirements.txt { % e ndif %}' , '{ % i f cookiecutter.packaging != \"poetry\" %} poetry.lock { % e ndif %}' , ] for path in REMOVE_PATHS : path = path . strip () if path and os . path . exists ( path ): if os . path . isdir ( path ): os . rmdir ( path ) else : os . unlink ( path )","title":"Create files or directories with conditions"},{"location":"linux/cookiecutter/#add-some-text-to-a-file-if-a-condition-is-met","text":"Use jinja2 conditionals. Note the - at the end of the conditional opening, play with {%- ... -%} and {% ... %} for different results on line appending. { % if cookiecutter.install_docker == 'yes' -% } - src : git+ssh://mywebpage.org/ansible-roles/docker.git version : 1.0.3 { % - else -% } - src : git+ssh://mywebpage.org/ansible-roles/other-role.git version : 1.0.2 { % - endif % }","title":"Add some text to a file if a condition is met"},{"location":"linux/cookiecutter/#initialize-git-repository-on-the-created-cookiecutter","text":"Added the following to the post generation hooks. File: hooks/post_gen_project.py import subprocess subprocess . call ([ 'git' , 'init' ]) subprocess . call ([ 'git' , 'add' , '*' ]) subprocess . call ([ 'git' , 'commit' , '-m' , 'Initial commit' ])","title":"Initialize git repository on the created cookiecutter"},{"location":"linux/cookiecutter/#prevent-cookiecutter-from-processing-some-files","text":"By default cookiecutter will try to process every file as a Jinja template. This behaviour produces wrong results if you have Jinja templates that are meant to be taken as literal. Starting with cookiecutter 1.1, you can tell cookiecutter to only copy some files without interpreting them as Jinja templates . Add a _copy_without_render key in the cookiecutter config file ( cookiecutter.json ). It takes a list of regular expressions. If a filename matches the regular expressions it will be copied and not processed as a Jinja template. { \"project_slug\" : \"sample\" , \"_copy_without_render\" : [ \"*.js\" , \"not_rendered_dir/*\" , \"rendered_dir/not_rendered_file.ini\" ] }","title":"Prevent cookiecutter from processing some files"},{"location":"linux/cookiecutter/#prevent-additional-whitespaces-when-jinja-condition-is-not-met","text":"Jinja2 has a whitespace control that can be used to manage the whitelines existent between the Jinja blocks. The problem comes when a condition is not met in an if block, in that case, Jinja adds a whitespace which will break most linters. This is the solution I've found out that works as expected. ### Multienvironment This playbook has support for the following environments: {% if cookiecutter.production_environment == \"True\" -%} * Production {% endif %} {%- if cookiecutter.staging_environment == \"True\" -%} * Staging {% endif %} {%- if cookiecutter.development_environment == \"True\" -%} * Development {% endif %} ### Tags","title":"Prevent additional whitespaces when jinja condition is not met."},{"location":"linux/cookiecutter/#testing-your-own-cookiecutter-templates","text":"The pytest-cookies plugin comes with a cookies fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests.","title":"Testing your own cookiecutter templates"},{"location":"linux/cookiecutter/#install_1","text":"pip install pytest-cookies","title":"Install"},{"location":"linux/cookiecutter/#usage","text":"@pytest.fixture def context(): return { \"playbook_name\": \"My Test Playbook\", } The cookies.bake() method generates a new project from your template based on the default values specified in cookiecutter.json: def test_bake_project ( cookies ): result = cookies . bake ( extra_context = { 'repo_name' : 'hello world' }) assert result . exit_code == 0 assert result . exception is None assert result . project . basename == 'hello world' assert result . project . isdir () It accepts the extra_context keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data. The cookiecutter-django has a nice test file using this fixture.","title":"Usage"},{"location":"linux/cookiecutter/#mocking-the-contents-of-the-cookiecutter-hooks","text":"Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline. If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the cookies.bake() code. Instead it delegates in cookiecutter to run them, which opens a subprocess to run them , so the mocks don't work. The alternative is setting an environmental variable in your tests to skip those steps: File: tests/conftest.py import os os . environ [ \"COOKIECUTTER_TESTING\" ] = \"true\" File: hooks/pre_gen_project.py def main (): # ... pre_hook content ... if __name__ == \"__main__\" : if os . environ . get ( \"COOKIECUTTER_TESTING\" ) != \"true\" : main () If you want to test the content of main , you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors.","title":"Mocking the contents of the cookiecutter hooks"},{"location":"linux/cookiecutter/#debug-failing-template-generation","text":"Sometimes the generation of the templates will fail in the tests, I've found that the easier way to debug why is to inspect the result object of the result = cookies.bake() statement with pdb. It has an exception method with lineno argument and source . With that information I've been able to locate the failing line. It also has a filename attribute but it doesn't seem to work for me.","title":"Debug failing template generation"},{"location":"linux/cookiecutter/#references","text":"Git Docs","title":"References"},{"location":"linux/cruft/","text":"cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates. Many project template utilities exist that automate the copying and pasting of code to create new projects. This seems great! However, once created, most leave you with that copy-and-pasted code to manage through the life of your project. Key Features \u2691 Cookiecutter Compatible cruft utilizes Cookiecutter as its template expansion engine. Meaning it retains full compatibility with all existing Cookiecutter templates. Template Validation cruft can quickly validate whether or not a project is using the latest version of a template using cruft check . This check can easily be added to CI pipelines to ensure your projects stay in-sync. Automatic Template Updates cruft automates the process of updating code to match the latest version of a template, making it easy to utilize template improvements across many projects. Installation \u2691 pip install cruft Usage \u2691 Creating a New Project \u2691 To create a new project using cruft run cruft create PROJECT_URL from the command line. cruft will then ask you any necessary questions to create your new project. It will use your answers to expand the provided template, and then return the directory it placed the expanded project. Behind the scenes, cruft uses Cookiecutter to do the project expansion. The only difference in the resulting output is a .cruft.json file that contains the git hash of the template used as well as the parameters specified. Updating a Project \u2691 To update an existing project, that was created using cruft, run cruft update in the root of the project. If there are any updates, cruft will have you review them before applying. If you accept the changes cruft will apply them to your project and update the .cruft.json file for you. Sometimes certain files just aren't good fits for updating. Such as test cases or __init__ files. You can tell cruft to always skip updating these files on a project by project basis by added them to a skip section within your .cruft.json file: { \"template\" : \"https://github.com/timothycrosley/cookiecutter-python\" , \"commit\" : \"8a65a360d51250221193ed0ec5ed292e72b32b0b\" , \"skip\" : [ \"cruft/__init__.py\" , \"tests\" ], ... } Or, if you have toml installed, you can add skip files directly to a tool.cruft section of your pyproject.toml file: [tool.cruft] skip = [\"cruft/__init__.py\", \"tests\"] Checking a Project \u2691 Checking to see if a project is missing a template update is as easy as running cruft check . If the project is out-of-date an error and exit code 1 will be returned. cruft check can be added to CI pipelines to ensure projects don't unintentionally drift. Linking an Existing Project \u2691 Have an existing project that you created from a template in the past using Cookiecutter directly? You can link it to the template that was used to create it using: cruft link TEMPLATE_REPOSITORY . You can then specify the last commit of the template the project has been updated to be consistent with, or accept the default of using the latest commit from the template. Compute the diff \u2691 With time, your boilerplate may end up being very different from the actual cookiecutter template. Cruft allows you to quickly see what changed in your local project compared to the template. It is as easy as running cruft diff . If any local file differs from the template, the diff will appear in your terminal in a similar fashion to git diff . The cruft diff command optionally accepts an --exit-code flag that will make cruft exit with a non-0 code should any diff is found. You can combine this flag with the skip section of your .cruft.json to make stricter CI checks that ensures any improvement to the template is always submitted upstream. References \u2691 Docs","title":"cruft"},{"location":"linux/cruft/#key-features","text":"Cookiecutter Compatible cruft utilizes Cookiecutter as its template expansion engine. Meaning it retains full compatibility with all existing Cookiecutter templates. Template Validation cruft can quickly validate whether or not a project is using the latest version of a template using cruft check . This check can easily be added to CI pipelines to ensure your projects stay in-sync. Automatic Template Updates cruft automates the process of updating code to match the latest version of a template, making it easy to utilize template improvements across many projects.","title":"Key Features"},{"location":"linux/cruft/#installation","text":"pip install cruft","title":"Installation"},{"location":"linux/cruft/#usage","text":"","title":"Usage"},{"location":"linux/cruft/#creating-a-new-project","text":"To create a new project using cruft run cruft create PROJECT_URL from the command line. cruft will then ask you any necessary questions to create your new project. It will use your answers to expand the provided template, and then return the directory it placed the expanded project. Behind the scenes, cruft uses Cookiecutter to do the project expansion. The only difference in the resulting output is a .cruft.json file that contains the git hash of the template used as well as the parameters specified.","title":"Creating a New Project"},{"location":"linux/cruft/#updating-a-project","text":"To update an existing project, that was created using cruft, run cruft update in the root of the project. If there are any updates, cruft will have you review them before applying. If you accept the changes cruft will apply them to your project and update the .cruft.json file for you. Sometimes certain files just aren't good fits for updating. Such as test cases or __init__ files. You can tell cruft to always skip updating these files on a project by project basis by added them to a skip section within your .cruft.json file: { \"template\" : \"https://github.com/timothycrosley/cookiecutter-python\" , \"commit\" : \"8a65a360d51250221193ed0ec5ed292e72b32b0b\" , \"skip\" : [ \"cruft/__init__.py\" , \"tests\" ], ... } Or, if you have toml installed, you can add skip files directly to a tool.cruft section of your pyproject.toml file: [tool.cruft] skip = [\"cruft/__init__.py\", \"tests\"]","title":"Updating a Project"},{"location":"linux/cruft/#checking-a-project","text":"Checking to see if a project is missing a template update is as easy as running cruft check . If the project is out-of-date an error and exit code 1 will be returned. cruft check can be added to CI pipelines to ensure projects don't unintentionally drift.","title":"Checking a Project"},{"location":"linux/cruft/#linking-an-existing-project","text":"Have an existing project that you created from a template in the past using Cookiecutter directly? You can link it to the template that was used to create it using: cruft link TEMPLATE_REPOSITORY . You can then specify the last commit of the template the project has been updated to be consistent with, or accept the default of using the latest commit from the template.","title":"Linking an Existing Project"},{"location":"linux/cruft/#compute-the-diff","text":"With time, your boilerplate may end up being very different from the actual cookiecutter template. Cruft allows you to quickly see what changed in your local project compared to the template. It is as easy as running cruft diff . If any local file differs from the template, the diff will appear in your terminal in a similar fashion to git diff . The cruft diff command optionally accepts an --exit-code flag that will make cruft exit with a non-0 code should any diff is found. You can combine this flag with the skip section of your .cruft.json to make stricter CI checks that ensures any improvement to the template is always submitted upstream.","title":"Compute the diff"},{"location":"linux/cruft/#references","text":"Docs","title":"References"},{"location":"linux/elasticsearch/","text":"Backup \u2691 It's better to use the curator tool Create snapshot \u2691 curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true Create snapshot of selected indices \u2691 curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true curl -XPUT 'localhost:9200/_snapshot/my_backup/snapshot_1?pretty' -H 'Content-Type: application/json' -d ' { \"indices\": \"index_1,index_2\", \"ignore_unavailable\": true, \"include_global_state\": false } ' List all backups \u2691 Check for my-snapshot-repo curl {{ url }} /_snapshot/ {{ backup_path }} /*?pretty Restore backup \u2691 First you need to close the selected indices curl {{ url }} / {{ indice_name }} /_close Then restore curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} /_restore?wait_for_completion = true Create snapshot of selected indices \u2691 curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true Delete snapshot \u2691 curl -XDELETE {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} Delete snapshots older than X \u2691 File: curator.yml client : hosts : - 'a data node' port : 9200 url_prefix : use_ssl : False certificate : client_cert : client_key : ssl_no_validate : False http_auth : timeout : 30 master_only : False logging : loglevel : INFO logfile : D:\\CuratorLogs\\logs.txt logformat : default blacklist : [ 'elasticsearch' , 'urllib3' ] File: delete_old_snapshots.yml actions : 1 : action : delete_snapshots description : >- Delete snapshots from the selected repository older than 100 days (based on creation_date), for everything but 'citydirectory-' prefixed snapshots. options : repository : 'dcs-elastic-snapshot' disable_action : False filters : - filtertype : pattern kind : prefix value : citydirectory- exclude : True - filtertype : age source : creation_date direction : older unit : days unit_count : 100 Information gathering \u2691 Get status of cluster \u2691 curl {{ url }} /_cluster/health?pretty curl {{ url }} /_cat/nodes?v curl {{ url }} /_cat/indices?v curl {{ url }} /_cat/shards If you've got red status, use the following command to choose the first unassigned shard that it finds and explains why it cannot be allocated to a node. curl {{ url }} /_cluster/allocation/explain?v Get settings \u2691 curl {{ url }} /_settings Get space left \u2691 curl {{ url }} /_nodes/stats/fs?pretty List plugins \u2691 curl {{ url }} /_nodes/plugins?pretty Upload \u2691 Single data upload \u2691 curl -XPOST '{{ url }}/{{ path_to_table }}' -d '{{ json_input }}' where json_input can be { \"field\" : \"value\" } Bulk upload of data \u2691 curl -H 'Content-Type: application/x-ndjson' -XPOST \\ '{{ url }}/{{ path_to_table }}/_bulk?pretty' --data-binary @ {{ json_file }} Delete \u2691 Delete data \u2691 curl -XDELETE {{ url }} / {{ path_to_ddbb }} Troubleshooting \u2691 Recover from yellow state \u2691 A yellow cluster represents that some of the replica shards in the cluster are unassigned. I can see that around 14 replica shards are unassigned. You can confirm the state of the cluster with the following commands curl <domain-endpoint>_cluster/health?pretty curl -X GET <domain-endpoint>/_cat/shards | grep UNASSIGNED curl -X GET <domain-endpoint>/_cat/indices | grep yellow If you have metrics of the JVMMemoryPressure of the nodes, check if the memory of a node reached 100% around the time the cluster reached yellow state. One can generally confirm the reason for a cluster going yellow by looking at the output of the following API call: curl -X GET <domain-endpoint>/_cluster/allocation/explain | jq If it shows a CircuitBreakerException , it confirms that a spike in the JVM metric caused the node to go down. The JVM memory pressure specifies the percentage of the Java heap in a cluster node. It's determined by the following factors: The amount of data on the cluster in proportion to the amount of resources. The query load on the cluster. Here's what happens as JVM memory pressure increases: At 75%: Amazon ES triggers the Concurrent Mark Sweep (CMS) garbage collector. The CMS collector runs alongside other processes to keep pauses and disruptions to a minimum. The garbage collection is a CPU-intensive process. If JVM memory pressure stays at this percentage for a few minutes, then you could encounter ClusterBlockException, JVM OutOfMemoryError, or other cluster performance issues. Above 75%: If the CMS collector fails to reclaim enough memory and usage remains above 75%, Amazon ES triggers a different garbage collection algorithm. This algorithm tries to free up memory and prevent a JVM OutOfMemoryError (OOM) exception by slowing or stopping processes. Above 92% for 30 minutes: Amazon ES blocks all write operations. Around 95%: Amazon ES kills processes that try to allocate memory. If a critical process is killed, one or more cluster nodes might fail. At 100%: Amazon ES JVM is configured to exit and eventually restarts on OutOfMemory (OOM). To prevent high JVM memory pressure: Avoid queries on wide ranges, such as aggregations, wildcard or big time range queries. Avoid sending a large number of requests at the same time. Be sure that you have the appropriate number of shards. Be sure that your shards are distributed evenly between nodes. When possible, avoid aggregating on text fields. This helps prevent increases in field data. The more field data you have, the more heap space is consumed. Use the GET _cluster/stats API operation to check field data. If you must aggregate on text fields, change the mapping type to keyword. If JVM memory pressure gets too high, use the following API operations to clear the field data cache: POST /index_name/_cache/clear (index-level cache) and POST /_cache/clear (cluster-level cache). Note: Clearing the cache can disrupt queries that are in progress. Reallocate unassigned shards \u2691 Elasticsearch makes 5 attempts to assign the shard but if it fails to be assigned after 5 attempts, the shards will remain unassigned. There is a solution to this issue in order to bring the cluster to green state. You can disable the replicas on the failing index and then enable replicas back. Disable Replica curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 0 } }' * Enable the Replica back: curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 1 } }' Please note that it will take some time for the shards to be completely assigned and hence you might see intermittent cluster status as YELLOW.","title":"elasticsearch"},{"location":"linux/elasticsearch/#backup","text":"It's better to use the curator tool","title":"Backup"},{"location":"linux/elasticsearch/#create-snapshot","text":"curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true","title":"Create snapshot"},{"location":"linux/elasticsearch/#create-snapshot-of-selected-indices","text":"curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true curl -XPUT 'localhost:9200/_snapshot/my_backup/snapshot_1?pretty' -H 'Content-Type: application/json' -d ' { \"indices\": \"index_1,index_2\", \"ignore_unavailable\": true, \"include_global_state\": false } '","title":"Create snapshot of selected indices"},{"location":"linux/elasticsearch/#list-all-backups","text":"Check for my-snapshot-repo curl {{ url }} /_snapshot/ {{ backup_path }} /*?pretty","title":"List all backups"},{"location":"linux/elasticsearch/#restore-backup","text":"First you need to close the selected indices curl {{ url }} / {{ indice_name }} /_close Then restore curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} /_restore?wait_for_completion = true","title":"Restore backup"},{"location":"linux/elasticsearch/#create-snapshot-of-selected-indices_1","text":"curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true","title":"Create snapshot of selected indices"},{"location":"linux/elasticsearch/#delete-snapshot","text":"curl -XDELETE {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }}","title":"Delete snapshot"},{"location":"linux/elasticsearch/#delete-snapshots-older-than-x","text":"File: curator.yml client : hosts : - 'a data node' port : 9200 url_prefix : use_ssl : False certificate : client_cert : client_key : ssl_no_validate : False http_auth : timeout : 30 master_only : False logging : loglevel : INFO logfile : D:\\CuratorLogs\\logs.txt logformat : default blacklist : [ 'elasticsearch' , 'urllib3' ] File: delete_old_snapshots.yml actions : 1 : action : delete_snapshots description : >- Delete snapshots from the selected repository older than 100 days (based on creation_date), for everything but 'citydirectory-' prefixed snapshots. options : repository : 'dcs-elastic-snapshot' disable_action : False filters : - filtertype : pattern kind : prefix value : citydirectory- exclude : True - filtertype : age source : creation_date direction : older unit : days unit_count : 100","title":"Delete snapshots older than X"},{"location":"linux/elasticsearch/#information-gathering","text":"","title":"Information gathering"},{"location":"linux/elasticsearch/#get-status-of-cluster","text":"curl {{ url }} /_cluster/health?pretty curl {{ url }} /_cat/nodes?v curl {{ url }} /_cat/indices?v curl {{ url }} /_cat/shards If you've got red status, use the following command to choose the first unassigned shard that it finds and explains why it cannot be allocated to a node. curl {{ url }} /_cluster/allocation/explain?v","title":"Get status of cluster"},{"location":"linux/elasticsearch/#get-settings","text":"curl {{ url }} /_settings","title":"Get settings"},{"location":"linux/elasticsearch/#get-space-left","text":"curl {{ url }} /_nodes/stats/fs?pretty","title":"Get space left"},{"location":"linux/elasticsearch/#list-plugins","text":"curl {{ url }} /_nodes/plugins?pretty","title":"List plugins"},{"location":"linux/elasticsearch/#upload","text":"","title":"Upload"},{"location":"linux/elasticsearch/#single-data-upload","text":"curl -XPOST '{{ url }}/{{ path_to_table }}' -d '{{ json_input }}' where json_input can be { \"field\" : \"value\" }","title":"Single data upload"},{"location":"linux/elasticsearch/#bulk-upload-of-data","text":"curl -H 'Content-Type: application/x-ndjson' -XPOST \\ '{{ url }}/{{ path_to_table }}/_bulk?pretty' --data-binary @ {{ json_file }}","title":"Bulk upload of data"},{"location":"linux/elasticsearch/#delete","text":"","title":"Delete"},{"location":"linux/elasticsearch/#delete-data","text":"curl -XDELETE {{ url }} / {{ path_to_ddbb }}","title":"Delete data"},{"location":"linux/elasticsearch/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"linux/elasticsearch/#recover-from-yellow-state","text":"A yellow cluster represents that some of the replica shards in the cluster are unassigned. I can see that around 14 replica shards are unassigned. You can confirm the state of the cluster with the following commands curl <domain-endpoint>_cluster/health?pretty curl -X GET <domain-endpoint>/_cat/shards | grep UNASSIGNED curl -X GET <domain-endpoint>/_cat/indices | grep yellow If you have metrics of the JVMMemoryPressure of the nodes, check if the memory of a node reached 100% around the time the cluster reached yellow state. One can generally confirm the reason for a cluster going yellow by looking at the output of the following API call: curl -X GET <domain-endpoint>/_cluster/allocation/explain | jq If it shows a CircuitBreakerException , it confirms that a spike in the JVM metric caused the node to go down. The JVM memory pressure specifies the percentage of the Java heap in a cluster node. It's determined by the following factors: The amount of data on the cluster in proportion to the amount of resources. The query load on the cluster. Here's what happens as JVM memory pressure increases: At 75%: Amazon ES triggers the Concurrent Mark Sweep (CMS) garbage collector. The CMS collector runs alongside other processes to keep pauses and disruptions to a minimum. The garbage collection is a CPU-intensive process. If JVM memory pressure stays at this percentage for a few minutes, then you could encounter ClusterBlockException, JVM OutOfMemoryError, or other cluster performance issues. Above 75%: If the CMS collector fails to reclaim enough memory and usage remains above 75%, Amazon ES triggers a different garbage collection algorithm. This algorithm tries to free up memory and prevent a JVM OutOfMemoryError (OOM) exception by slowing or stopping processes. Above 92% for 30 minutes: Amazon ES blocks all write operations. Around 95%: Amazon ES kills processes that try to allocate memory. If a critical process is killed, one or more cluster nodes might fail. At 100%: Amazon ES JVM is configured to exit and eventually restarts on OutOfMemory (OOM). To prevent high JVM memory pressure: Avoid queries on wide ranges, such as aggregations, wildcard or big time range queries. Avoid sending a large number of requests at the same time. Be sure that you have the appropriate number of shards. Be sure that your shards are distributed evenly between nodes. When possible, avoid aggregating on text fields. This helps prevent increases in field data. The more field data you have, the more heap space is consumed. Use the GET _cluster/stats API operation to check field data. If you must aggregate on text fields, change the mapping type to keyword. If JVM memory pressure gets too high, use the following API operations to clear the field data cache: POST /index_name/_cache/clear (index-level cache) and POST /_cache/clear (cluster-level cache). Note: Clearing the cache can disrupt queries that are in progress.","title":"Recover from yellow state"},{"location":"linux/elasticsearch/#reallocate-unassigned-shards","text":"Elasticsearch makes 5 attempts to assign the shard but if it fails to be assigned after 5 attempts, the shards will remain unassigned. There is a solution to this issue in order to bring the cluster to green state. You can disable the replicas on the failing index and then enable replicas back. Disable Replica curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 0 } }' * Enable the Replica back: curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 1 } }' Please note that it will take some time for the shards to be completely assigned and hence you might see intermittent cluster status as YELLOW.","title":"Reallocate unassigned shards"},{"location":"linux/fail2ban/","text":"Usage \u2691 Unban IP \u2691 fail2ban-client set {{ jail }} unbanip {{ ip }} Where jail can be ssh .","title":"fail2ban"},{"location":"linux/fail2ban/#usage","text":"","title":"Usage"},{"location":"linux/fail2ban/#unban-ip","text":"fail2ban-client set {{ jail }} unbanip {{ ip }} Where jail can be ssh .","title":"Unban IP"},{"location":"linux/google_chrome/","text":"Although I hate it, there are web pages that don't work on Firefox or Chromium. In those cases I install google-chrome and uninstall as soon as I don't need to use that service. Installation \u2691 Debian \u2691 wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb","title":"google chrome"},{"location":"linux/google_chrome/#installation","text":"","title":"Installation"},{"location":"linux/google_chrome/#debian","text":"wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb","title":"Debian"},{"location":"linux/haproxy/","text":"HAProxy is free, open source software that provides a high availability load balancer and proxy server for TCP and HTTP-based applications that spreads requests across multiple servers. It is written in C and has a reputation for being fast and efficient (in terms of processor and memory usage). Use HAProxy as a reverse proxy \u2691 reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the server itself. Unlike a forward proxy, which is an intermediary for its associated clients to contact any server, a reverse proxy is an intermediary for its associated servers to be contacted by any client. In other words, a proxy is associated with the client(s), while a reverse proxy is associated with the server(s); a reverse proxy is usually an internal-facing proxy used as a 'front-end' to control and protect access to a server on a private network. It can be done at Web server level (Nginx, Apache, ...) or at load balancer level. This HAProxy post shows how to translate Apache's proxy pass directives to the HAProxy configuration. frontend ft_global acl host_dom.com req.hdr(Host) dom.com acl path_mirror_foo path -m beg /mirror/foo/ use_backend bk_myapp if host_dom.com path_mirror_foo backend bk_myapp [...] # external URL => internal URL # http://dom.com/mirror/foo/bar => http://bk.dom.com/bar # ProxyPass /mirror/foo/ http://bk.dom.com/bar http-request set-header Host bk.dom.com reqirep ^([^ :]*)\\ /mirror/foo/(.*) \\1\\ /\\2 # ProxyPassReverse /mirror/foo/ http://bk.dom.com/bar # Note: we turn the urls into absolute in the mean time acl hdr_location res.hdr(Location) -m found rspirep ^Location:\\ (https?://bk.dom.com(:[0-9]+)?)?(/.*) Location:\\ /mirror/foo3 if hdr_location # ProxyPassReverseCookieDomain bk.dom.com dom.com acl hdr_set_cookie_dom res.hdr(Set-cookie) -m sub Domain= bk.dom.com rspirep ^(Set-Cookie:.*)\\ Domain=bk.dom.com(.*) \\1\\ Domain=dom.com\\2 if hdr_set_cookie_dom # ProxyPassReverseCookieDomain / /mirror/foo/ acl hdr_set_cookie_path res.hdr(Set-cookie) -m sub Path= rspirep ^(Set-Cookie:.*)\\ Path=(.*) \\1\\ Path=/mirror/foo2 if hdr_set_cookie_path Other useful examples can be retrieved from drmalex07 or ferdinandosimonetti gists.","title":"HAProxy"},{"location":"linux/haproxy/#use-haproxy-as-a-reverse-proxy","text":"reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the server itself. Unlike a forward proxy, which is an intermediary for its associated clients to contact any server, a reverse proxy is an intermediary for its associated servers to be contacted by any client. In other words, a proxy is associated with the client(s), while a reverse proxy is associated with the server(s); a reverse proxy is usually an internal-facing proxy used as a 'front-end' to control and protect access to a server on a private network. It can be done at Web server level (Nginx, Apache, ...) or at load balancer level. This HAProxy post shows how to translate Apache's proxy pass directives to the HAProxy configuration. frontend ft_global acl host_dom.com req.hdr(Host) dom.com acl path_mirror_foo path -m beg /mirror/foo/ use_backend bk_myapp if host_dom.com path_mirror_foo backend bk_myapp [...] # external URL => internal URL # http://dom.com/mirror/foo/bar => http://bk.dom.com/bar # ProxyPass /mirror/foo/ http://bk.dom.com/bar http-request set-header Host bk.dom.com reqirep ^([^ :]*)\\ /mirror/foo/(.*) \\1\\ /\\2 # ProxyPassReverse /mirror/foo/ http://bk.dom.com/bar # Note: we turn the urls into absolute in the mean time acl hdr_location res.hdr(Location) -m found rspirep ^Location:\\ (https?://bk.dom.com(:[0-9]+)?)?(/.*) Location:\\ /mirror/foo3 if hdr_location # ProxyPassReverseCookieDomain bk.dom.com dom.com acl hdr_set_cookie_dom res.hdr(Set-cookie) -m sub Domain= bk.dom.com rspirep ^(Set-Cookie:.*)\\ Domain=bk.dom.com(.*) \\1\\ Domain=dom.com\\2 if hdr_set_cookie_dom # ProxyPassReverseCookieDomain / /mirror/foo/ acl hdr_set_cookie_path res.hdr(Set-cookie) -m sub Path= rspirep ^(Set-Cookie:.*)\\ Path=(.*) \\1\\ Path=/mirror/foo2 if hdr_set_cookie_path Other useful examples can be retrieved from drmalex07 or ferdinandosimonetti gists.","title":"Use HAProxy as a reverse proxy"},{"location":"linux/hypothesis/","text":"Hypothesis is an open-source software project that aims to collect comments about statements made in any web-accessible content, and filter and rank those comments to assess each statement's credibility. It offers an online web application where registered users share highlights and annotations over any webpage. As of 2020-06-11, although the service can be self-hosted, it's not yet easy to do so. Install \u2691 Client \u2691 If you're using Chrome or any derivative there is an official extension. Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine . Alternatively you can use the Hypothesis bookmarklet . The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings . Though there is yet no documentation on this topic. I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me . Server \u2691 The infrastructure can be deployed with Docker-compose. version : '3' services : postgres : image : postgres:11.5-alpine ports : - 5432 # - '5432:5432' elasticsearch : image : hypothesis/elasticsearch:latest ports : - 9200 #- '9200:9200' environment : - discovery.type=single-node rabbit : image : rabbitmq:3.6-management-alpine ports : - 5672 - 15672 #- '5672:5672' #- '15672:15672' web : image : hypothesis/hypothesis:latest environment : - APP_URL=http://localhost:5000 - AUTHORITY=localhost - BROKER_URL=amqp://guest:guest@rabbit:5672// - CLIENT_OAUTH_ID - CLIENT_URL=http://localhost:3001/hypothesis - DATABASE_URL=postgresql://postgres@postgres/postgres - ELASTICSEARCH_URL=http://elasticsearch:9200 - NEW_RELIC_APP_NAME=h (dev) - NEW_RELIC_LICENSE_KEY - SECRET_KEY=notasecret ports : - '5000:5000' depends_on : - postgres - elasticsearch - rabbit docker-compose up Initialize the database and create the admin user. docker-compose exec web /bin/sh hypothesis init hypothesis user add hypothesis user admin <username> The service is available at http://localhost:5000 . To check the latest developments of the Docker compose deployment follow the issue #4899 . They also provide the tools they use to deploy the production service into AWS. References \u2691 Homepage FAQ Bug tracker Feature request tracker Server deployment open issues \u2691 Self-hosting Docker compose Create admin user when using Docker compose Steps required to run both h and serve the client from internal server How to deploy h on VM","title":"hypothesis"},{"location":"linux/hypothesis/#install","text":"","title":"Install"},{"location":"linux/hypothesis/#client","text":"If you're using Chrome or any derivative there is an official extension. Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine . Alternatively you can use the Hypothesis bookmarklet . The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings . Though there is yet no documentation on this topic. I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me .","title":"Client"},{"location":"linux/hypothesis/#server","text":"The infrastructure can be deployed with Docker-compose. version : '3' services : postgres : image : postgres:11.5-alpine ports : - 5432 # - '5432:5432' elasticsearch : image : hypothesis/elasticsearch:latest ports : - 9200 #- '9200:9200' environment : - discovery.type=single-node rabbit : image : rabbitmq:3.6-management-alpine ports : - 5672 - 15672 #- '5672:5672' #- '15672:15672' web : image : hypothesis/hypothesis:latest environment : - APP_URL=http://localhost:5000 - AUTHORITY=localhost - BROKER_URL=amqp://guest:guest@rabbit:5672// - CLIENT_OAUTH_ID - CLIENT_URL=http://localhost:3001/hypothesis - DATABASE_URL=postgresql://postgres@postgres/postgres - ELASTICSEARCH_URL=http://elasticsearch:9200 - NEW_RELIC_APP_NAME=h (dev) - NEW_RELIC_LICENSE_KEY - SECRET_KEY=notasecret ports : - '5000:5000' depends_on : - postgres - elasticsearch - rabbit docker-compose up Initialize the database and create the admin user. docker-compose exec web /bin/sh hypothesis init hypothesis user add hypothesis user admin <username> The service is available at http://localhost:5000 . To check the latest developments of the Docker compose deployment follow the issue #4899 . They also provide the tools they use to deploy the production service into AWS.","title":"Server"},{"location":"linux/hypothesis/#references","text":"Homepage FAQ Bug tracker Feature request tracker","title":"References"},{"location":"linux/hypothesis/#server-deployment-open-issues","text":"Self-hosting Docker compose Create admin user when using Docker compose Steps required to run both h and serve the client from internal server How to deploy h on VM","title":"Server deployment open issues"},{"location":"linux/linux_miscellaneous/","text":"List all process swap usage \u2691 for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file ; done | sort -k 2 -n -r | less","title":"Miscellaneous Linux Commands"},{"location":"linux/linux_miscellaneous/#list-all-process-swap-usage","text":"for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file ; done | sort -k 2 -n -r | less","title":"List all process swap usage"},{"location":"linux/mkdocs/","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. I've automated the creation of the mkdocs site in this cookiecutter template . Installation \u2691 Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name }} site_author : {{ your_name }} site_url : {{ site_url }} nav : - Introduction : 'index.md' - Basic Usage : 'basic_usage.md' - Configuration : 'configuration.md' - Update : 'update.md' - Advanced Usage : - Projects : \"projects.md\" - Tags : \"tags.md\" plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : \"theme\" logo : \"images/logo.png\" palette : primary : 'blue grey' accent : 'light blue' extra_css : - 'stylesheets/extra.css' - 'stylesheets/links.css' repo_name : {{ repository_name }} # for example: 'lyz-code/pydo' repo_url : {{ repository_url }} # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve Add a github pages hook. \u2691 Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : 'x64' - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working. Links \u2691 Homepage . Material theme configuration guide","title":"mkdocs"},{"location":"linux/mkdocs/#installation","text":"Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name }} site_author : {{ your_name }} site_url : {{ site_url }} nav : - Introduction : 'index.md' - Basic Usage : 'basic_usage.md' - Configuration : 'configuration.md' - Update : 'update.md' - Advanced Usage : - Projects : \"projects.md\" - Tags : \"tags.md\" plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : \"theme\" logo : \"images/logo.png\" palette : primary : 'blue grey' accent : 'light blue' extra_css : - 'stylesheets/extra.css' - 'stylesheets/links.css' repo_name : {{ repository_name }} # for example: 'lyz-code/pydo' repo_url : {{ repository_url }} # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve","title":"Installation"},{"location":"linux/mkdocs/#add-a-github-pages-hook","text":"Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : 'x64' - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working.","title":"Add a github pages hook."},{"location":"linux/mkdocs/#links","text":"Homepage . Material theme configuration guide","title":"Links"},{"location":"linux/monica/","text":"Monica is an open-source web application to organize the interactions with your loved ones. They call it a PRM, or Personal Relationship Management. Think of it as a CRM (a popular tool used by sales teams in the corporate world) for your friends or family. Monica allows people to keep track of everything that's important about their friends and family. Like the activities done with them. When you last called someone. What you talked about. It will help you remember the name and the age of the kids. It can also remind you to call someone you haven't talked to in a while. They have pricing plans for their hosted service, but the self-hosted solution has all the features. It also has a nice API to interact with. Install \u2691 They provide a very throughout documented Docker installation . If you just want to test it, use this docker compose File: docker-compose.yml version: \"3.4\" services: app: image: monicahq/monicahq depends_on: - db ports: - 8080:80 environment: # generate with `pwgen -s 32 1` for instance: - APP_KEY=DoKMvhGu795QcMBP1I5sw8uk85MMAPS9 - DB_HOST=db volumes: - data:/var/www/monica/storage restart: always db: image: mysql:5.7 environment: - MYSQL_RANDOM_ROOT_PASSWORD=true - MYSQL_DATABASE=monica - MYSQL_USER=homestead - MYSQL_PASSWORD=secret volumes: - mysql:/var/lib/mysql restart: always volumes: data: name: data mysql: name: mysql Once you install your own, you may want to: Change the APP_KEY Change the database credentials. In the application docker are loaded as DB_USERNAME , DB_HOST and DB_PASSWORD . Set up the environment and the application url with APP_ENV=production and APP_URL . Set up the email configuration MAIL_MAILER : smtp MAIL_HOST : smtp.service.com # ex: smtp.sendgrid.net MAIL_PORT : 587 # is using tls, as you should MAIL_USERNAME : my_service_username # ex: apikey MAIL_PASSWORD : my_service_password # ex: SG.Psuoc6NZTrGHAF9fdsgsdgsbvjQ.JuxNWVYmJ8LE0 MAIL_ENCRYPTION : tls MAIL_FROM_ADDRESS : no-reply@xxx.com # ex: email you want the email to be FROM MAIL_FROM_NAME : Monica # ex: name of the sender Here is an example of all the possible configurations. They also share other configuration examples where you can take ideas of alternate setups. If you don't want to use docker, check the other installation documentation . References \u2691 Homepage Git Docs Blog","title":"monica"},{"location":"linux/monica/#install","text":"They provide a very throughout documented Docker installation . If you just want to test it, use this docker compose File: docker-compose.yml version: \"3.4\" services: app: image: monicahq/monicahq depends_on: - db ports: - 8080:80 environment: # generate with `pwgen -s 32 1` for instance: - APP_KEY=DoKMvhGu795QcMBP1I5sw8uk85MMAPS9 - DB_HOST=db volumes: - data:/var/www/monica/storage restart: always db: image: mysql:5.7 environment: - MYSQL_RANDOM_ROOT_PASSWORD=true - MYSQL_DATABASE=monica - MYSQL_USER=homestead - MYSQL_PASSWORD=secret volumes: - mysql:/var/lib/mysql restart: always volumes: data: name: data mysql: name: mysql Once you install your own, you may want to: Change the APP_KEY Change the database credentials. In the application docker are loaded as DB_USERNAME , DB_HOST and DB_PASSWORD . Set up the environment and the application url with APP_ENV=production and APP_URL . Set up the email configuration MAIL_MAILER : smtp MAIL_HOST : smtp.service.com # ex: smtp.sendgrid.net MAIL_PORT : 587 # is using tls, as you should MAIL_USERNAME : my_service_username # ex: apikey MAIL_PASSWORD : my_service_password # ex: SG.Psuoc6NZTrGHAF9fdsgsdgsbvjQ.JuxNWVYmJ8LE0 MAIL_ENCRYPTION : tls MAIL_FROM_ADDRESS : no-reply@xxx.com # ex: email you want the email to be FROM MAIL_FROM_NAME : Monica # ex: name of the sender Here is an example of all the possible configurations. They also share other configuration examples where you can take ideas of alternate setups. If you don't want to use docker, check the other installation documentation .","title":"Install"},{"location":"linux/monica/#references","text":"Homepage Git Docs Blog","title":"References"},{"location":"linux/nodejs/","text":"Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. Install \u2691 The debian base repositories are really outdated, so add the NodeSource repository curl -sL https://deb.nodesource.com/setup_12.x | sudo bash - sudo apt-get update sudo apt-get install nodejs npm nodejs --version Links \u2691 Home","title":"nodejs"},{"location":"linux/nodejs/#install","text":"The debian base repositories are really outdated, so add the NodeSource repository curl -sL https://deb.nodesource.com/setup_12.x | sudo bash - sudo apt-get update sudo apt-get install nodejs npm nodejs --version","title":"Install"},{"location":"linux/nodejs/#links","text":"Home","title":"Links"},{"location":"linux/rm/","text":"rm definition In computing, rm (short for remove) is a basic command on Unix and Unix-like operating systems used to remove objects such as computer files, directories and symbolic links from file systems and also special files such as device nodes, pipes and sockets Debugging \u2691 Cannot remove file: \u201cStructure needs cleaning\u201d \u2691 From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"rm"},{"location":"linux/rm/#debugging","text":"","title":"Debugging"},{"location":"linux/rm/#cannot-remove-file-structure-needs-cleaning","text":"From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"Cannot remove file: \u201cStructure needs cleaning\u201d"},{"location":"linux/syncthing/","text":"Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it's transmitted over the internet. Installation \u2691 Debian or Ubuntu \u2691 # Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing Docker \u2691 Use Linuxserver Docker Links \u2691 Home Getting Started","title":"Syncthing"},{"location":"linux/syncthing/#installation","text":"","title":"Installation"},{"location":"linux/syncthing/#debian-or-ubuntu","text":"# Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing","title":"Debian or Ubuntu"},{"location":"linux/syncthing/#docker","text":"Use Linuxserver Docker","title":"Docker"},{"location":"linux/syncthing/#links","text":"Home Getting Started","title":"Links"},{"location":"linux/wireguard/","text":"Wireguard is an simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is a general purpose VPN for running on embedded interfaces and super computers alike. Initially released for the Linux kernel, it's now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. Although it's under heavy development, it already might be the most secure, easiest to use, and simplest VPN solution in the industry. Features: Simple and easy to use: WireGuard aims to be as easy to configure and deploy as SSH. A VPN connection is made by exchanging public keys \u2013 exactly like exchanging SSH keys \u2013 and all the rest is transparently handled by WireGuard. It's even capable of roaming between IP addresses, like Mosh. There is no need to manage connections, worry about state, manage daemons, or worry about what's under the hood. WireGuard presents a basic yet powerful interface. Cryptographically Sound: WireGuard uses state-of-the-art cryptography, such as the Noise protocol framework, Curve25519, ChaCha20, Poly1305, BLAKE2, SipHash24, HKDF, and secure trusted constructions. It makes conservative and reasonable choices and has been reviewed by cryptographers. Minimal Attack Surface: WireGuard is designed with ease-of-implementation and simplicity in mind. It's meant to be implemented in very few lines of code, and auditable for security vulnerabilities. Compared to behemoths like *Swan/IPsec or OpenVPN/OpenSSL, in which auditing the gigantic codebases is an overwhelming task even for large teams of security experts, WireGuard is meant to be comprehensively reviewable by single individuals. High Performance: A combination of extremely high-speed cryptographic primitives and the fact that WireGuard lives inside the Linux kernel means that secure networking can be very high-speed. It is suitable for both small embedded devices like smartphones and fully loaded backbone routers. Well Defined & Thoroughly Considered: WireGuard is the result of a lengthy and thoroughly considered academic process, resulting in the technical whitepaper, an academic research paper which clearly defines the protocol and the intense considerations that went into each decision. Plus it's created by the same guy as pass , which uses Gentoo, I like this guy. Conceptual Overview \u2691 WireGuard securely encapsulates IP packets over UDP. You add a WireGuard interface, configure it with your private key and your peers' public keys, and then you send packets across it. All issues of key distribution and pushed configurations are out of scope of WireGuard; these are issues much better left for other layers. It mimics the model of SSH and Mosh; both parties have each other's public keys, and then they're simply able to begin exchanging packets through the interface. Simple Network Interface \u2691 WireGuard works by adding network interfaces, called wg0 (or wg1, wg2, wg3, etc). This network interface can then be configured normally using the ordinary networking utilities. The specific WireGuard aspects of the interface are configured using the wg tool. This interface acts as a tunnel interface. WireGuard associates tunnel IP addresses with public keys and remote endpoints. When the interface sends a packet to a peer, it does the following: This packet is meant for 192.168.30.8. Which peer is that? Let me look... Okay, it's for peer ABCDEFGH. (Or if it's not for any configured peer, drop the packet.) Encrypt entire IP packet using peer ABCDEFGH's public key. What is the remote endpoint of peer ABCDEFGH? Let me look... Okay, the endpoint is UDP port 53133 on host 216.58.211.110. Send encrypted bytes from step 2 over the Internet to 216.58.211.110:53133 using UDP. When the interface receives a packet, this happens: I just got a packet from UDP port 7361 on host 98.139.183.24. Let's decrypt it! It decrypted and authenticated properly for peer LMNOPQRS. Okay, let's remember that peer LMNOPQRS's most recent Internet endpoint is 98.139.183.24:7361 using UDP. Once decrypted, the plain-text packet is from 192.168.43.89. Is peer LMNOPQRS allowed to be sending us packets as 192.168.43.89? If so, accept the packet on the interface. If not, drop it. Behind the scenes there is much happening to provide proper privacy, authenticity, and perfect forward secrecy, using state-of-the-art cryptography. Cryptokey Routing \u2691 At the heart of WireGuard is a concept called Cryptokey Routing, which works by associating public keys with a list of tunnel IP addresses that are allowed inside the tunnel. Each network interface has a private key and a list of peers. Each peer has a public key. For example, a server computer might have this configuration: [Interface] PrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk= ListenPort = 51820 [Peer] PublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg= AllowedIPs = 10.192.122.3/32, 10.192.124.1/24 [Peer] PublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0= AllowedIPs = 10.192.122.4/32, 192.168.0.0/16 [Peer] PublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA= AllowedIPs = 10.10.10.230/32 And a client computer might have this simpler configuration: [Interface] PrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE= ListenPort = 21841 [Peer] PublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw= Endpoint = 192.95.5.69:51820 AllowedIPs = 0.0.0.0/0 In the server configuration, each peer (a client) will be able to send packets to the network interface with a source IP matching his corresponding list of allowed IPs. For example, when a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it's allowed onto the interface; otherwise it's dropped. In the server configuration, when the network interface wants to send a packet to a peer (a client), it looks at that packet's destination IP and compares it to each peer's list of allowed IPs to see which peer to send it to. For example, if the network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it to that peer's most recent Internet endpoint. In the client configuration, its single peer (the server) will be able to send packets to the network interface with any source IP (since 0.0.0.0/0 is a wildcard). For example, when a packet is received from peer HIgo9xNz..., if it decrypts and authenticates correctly, with any source IP, then it's allowed onto the interface; otherwise it's dropped. In the client configuration, when the network interface wants to send a packet to its single peer (the server), it will encrypt packets for the single peer with any destination IP address (since 0.0.0.0/0 is a wildcard). For example, if the network interface is asked to send a packet with any destination IP, it will encrypt it using the public key of the single peer HIgo9xNz..., and then send it to the single peer's most recent Internet endpoint. In other words, when sending packets, the list of allowed IPs behaves as a sort of routing table, and when receiving packets, the list of allowed IPs behaves as a sort of access control list. This is what we call a Cryptokey Routing Table: the simple association of public keys and allowed IPs. Because all packets sent on the WireGuard interface are encrypted and authenticated, and because there is such a tight coupling between the identity of a peer and the allowed IP address of a peer, system administrators do not need complicated firewall extensions, such as in the case of IPsec, but rather they can simply match on \"is it from this IP? on this interface?\", and be assured that it is a secure and authentic packet. This greatly simplifies network management and access control, and provides a great deal more assurance that your iptables rules are actually doing what you intended for them to do. Built-in Roaming \u2691 The client configuration contains an initial endpoint of its single peer (the server), so that it knows where to send encrypted data before it has received encrypted data. The server configuration doesn't have any initial endpoints of its peers (the clients). This is because the server discovers the endpoint of its peers by examining from where correctly authenticated data originates. If the server itself changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update the configuration just the same. Both client and server send encrypted data to the most recent IP endpoint for which they authentically decrypted data. Thus, there is full IP roaming on both ends.","title":"Wireguard"},{"location":"linux/wireguard/#conceptual-overview","text":"WireGuard securely encapsulates IP packets over UDP. You add a WireGuard interface, configure it with your private key and your peers' public keys, and then you send packets across it. All issues of key distribution and pushed configurations are out of scope of WireGuard; these are issues much better left for other layers. It mimics the model of SSH and Mosh; both parties have each other's public keys, and then they're simply able to begin exchanging packets through the interface.","title":"Conceptual Overview"},{"location":"linux/wireguard/#simple-network-interface","text":"WireGuard works by adding network interfaces, called wg0 (or wg1, wg2, wg3, etc). This network interface can then be configured normally using the ordinary networking utilities. The specific WireGuard aspects of the interface are configured using the wg tool. This interface acts as a tunnel interface. WireGuard associates tunnel IP addresses with public keys and remote endpoints. When the interface sends a packet to a peer, it does the following: This packet is meant for 192.168.30.8. Which peer is that? Let me look... Okay, it's for peer ABCDEFGH. (Or if it's not for any configured peer, drop the packet.) Encrypt entire IP packet using peer ABCDEFGH's public key. What is the remote endpoint of peer ABCDEFGH? Let me look... Okay, the endpoint is UDP port 53133 on host 216.58.211.110. Send encrypted bytes from step 2 over the Internet to 216.58.211.110:53133 using UDP. When the interface receives a packet, this happens: I just got a packet from UDP port 7361 on host 98.139.183.24. Let's decrypt it! It decrypted and authenticated properly for peer LMNOPQRS. Okay, let's remember that peer LMNOPQRS's most recent Internet endpoint is 98.139.183.24:7361 using UDP. Once decrypted, the plain-text packet is from 192.168.43.89. Is peer LMNOPQRS allowed to be sending us packets as 192.168.43.89? If so, accept the packet on the interface. If not, drop it. Behind the scenes there is much happening to provide proper privacy, authenticity, and perfect forward secrecy, using state-of-the-art cryptography.","title":"Simple Network Interface"},{"location":"linux/wireguard/#cryptokey-routing","text":"At the heart of WireGuard is a concept called Cryptokey Routing, which works by associating public keys with a list of tunnel IP addresses that are allowed inside the tunnel. Each network interface has a private key and a list of peers. Each peer has a public key. For example, a server computer might have this configuration: [Interface] PrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk= ListenPort = 51820 [Peer] PublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg= AllowedIPs = 10.192.122.3/32, 10.192.124.1/24 [Peer] PublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0= AllowedIPs = 10.192.122.4/32, 192.168.0.0/16 [Peer] PublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA= AllowedIPs = 10.10.10.230/32 And a client computer might have this simpler configuration: [Interface] PrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE= ListenPort = 21841 [Peer] PublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw= Endpoint = 192.95.5.69:51820 AllowedIPs = 0.0.0.0/0 In the server configuration, each peer (a client) will be able to send packets to the network interface with a source IP matching his corresponding list of allowed IPs. For example, when a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it's allowed onto the interface; otherwise it's dropped. In the server configuration, when the network interface wants to send a packet to a peer (a client), it looks at that packet's destination IP and compares it to each peer's list of allowed IPs to see which peer to send it to. For example, if the network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it to that peer's most recent Internet endpoint. In the client configuration, its single peer (the server) will be able to send packets to the network interface with any source IP (since 0.0.0.0/0 is a wildcard). For example, when a packet is received from peer HIgo9xNz..., if it decrypts and authenticates correctly, with any source IP, then it's allowed onto the interface; otherwise it's dropped. In the client configuration, when the network interface wants to send a packet to its single peer (the server), it will encrypt packets for the single peer with any destination IP address (since 0.0.0.0/0 is a wildcard). For example, if the network interface is asked to send a packet with any destination IP, it will encrypt it using the public key of the single peer HIgo9xNz..., and then send it to the single peer's most recent Internet endpoint. In other words, when sending packets, the list of allowed IPs behaves as a sort of routing table, and when receiving packets, the list of allowed IPs behaves as a sort of access control list. This is what we call a Cryptokey Routing Table: the simple association of public keys and allowed IPs. Because all packets sent on the WireGuard interface are encrypted and authenticated, and because there is such a tight coupling between the identity of a peer and the allowed IP address of a peer, system administrators do not need complicated firewall extensions, such as in the case of IPsec, but rather they can simply match on \"is it from this IP? on this interface?\", and be assured that it is a secure and authentic packet. This greatly simplifies network management and access control, and provides a great deal more assurance that your iptables rules are actually doing what you intended for them to do.","title":"Cryptokey Routing"},{"location":"linux/wireguard/#built-in-roaming","text":"The client configuration contains an initial endpoint of its single peer (the server), so that it knows where to send encrypted data before it has received encrypted data. The server configuration doesn't have any initial endpoints of its peers (the clients). This is because the server discovers the endpoint of its peers by examining from where correctly authenticated data originates. If the server itself changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update the configuration just the same. Both client and server send encrypted data to the most recent IP endpoint for which they authentically decrypted data. Thus, there is full IP roaming on both ends.","title":"Built-in Roaming"},{"location":"linux/zfs/","text":"ZFS combines a file system with a volume manager. Usage \u2691 Mount a pool as readonly \u2691 zpool import -o readonly = on {{ pool_name }} Mount a ZFS snapshot in a directory as readonly \u2691 mount -t zfs {{ pool_name }} / {{ snapshot_name }} {{ mount_path }} -o ro List volumes \u2691 zpool list List snapshots \u2691 zfs list -t snapshot Get read and write stats from pool \u2691 zpool iostat {{ pool_name }} {{ refresh_time_in_seconds }}","title":"ZFS"},{"location":"linux/zfs/#usage","text":"","title":"Usage"},{"location":"linux/zfs/#mount-a-pool-as-readonly","text":"zpool import -o readonly = on {{ pool_name }}","title":"Mount a pool as readonly"},{"location":"linux/zfs/#mount-a-zfs-snapshot-in-a-directory-as-readonly","text":"mount -t zfs {{ pool_name }} / {{ snapshot_name }} {{ mount_path }} -o ro","title":"Mount a ZFS snapshot in a directory as readonly"},{"location":"linux/zfs/#list-volumes","text":"zpool list","title":"List volumes"},{"location":"linux/zfs/#list-snapshots","text":"zfs list -t snapshot","title":"List snapshots"},{"location":"linux/zfs/#get-read-and-write-stats-from-pool","text":"zpool iostat {{ pool_name }} {{ refresh_time_in_seconds }}","title":"Get read and write stats from pool"},{"location":"linux/zip/","text":"zip is an UNIX command line tool to package and compress files. Usage \u2691 Create a zip file \u2691 zip -r {{ zip_file }} {{ files_to_save }} Split files to a specific size \u2691 zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m Compress with password \u2691 zip -er {{ zip_file }} {{ files_to_save }} Read files to compress from a file \u2691 cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@ Uncompress a zip file \u2691 unzip {{ zip_file }}","title":"zip"},{"location":"linux/zip/#usage","text":"","title":"Usage"},{"location":"linux/zip/#create-a-zip-file","text":"zip -r {{ zip_file }} {{ files_to_save }}","title":"Create a zip file"},{"location":"linux/zip/#split-files-to-a-specific-size","text":"zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m","title":"Split files to a specific size"},{"location":"linux/zip/#compress-with-password","text":"zip -er {{ zip_file }} {{ files_to_save }}","title":"Compress with password"},{"location":"linux/zip/#read-files-to-compress-from-a-file","text":"cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@","title":"Read files to compress from a file"},{"location":"linux/zip/#uncompress-a-zip-file","text":"unzip {{ zip_file }}","title":"Uncompress a zip file"},{"location":"linux/luks/luks/","text":"LUKS definition The Linux Unified Key Setup (LUKS) is a disk encryption specification created by Clemens Fruhwirth in 2004 and was originally intended for Linux. While most disk encryption software implements different, incompatible, and undocumented formats, LUKS implements a platform-independent standard on-disk format for use in various tools. This not only facilitates compatibility and interoperability among different programs, but also assures that they all implement password management in a secure and documented manner. The reference implementation for LUKS operates on Linux and is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend. LUKS is designed to conform to the TKS1 secure key setup scheme. LUKS Commands \u2691 We use the cryptsetup command to interact with LUKS partitions. Header management \u2691 Get the disk header \u2691 cryptsetup luksDump /dev/sda3 Backup header \u2691 cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }} Key management \u2691 Add a key \u2691 cryptsetup luksAddKey --key-slot 1 {{ luks_device }} Test if you remember the key \u2691 Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }} Delete some keys \u2691 cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }} Delete all keys \u2691 cryptsetup luksErase {{ device }} Encrypt hard drive \u2691 Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt LUKS debugging \u2691 Resource busy \u2691 Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"LUKS"},{"location":"linux/luks/luks/#luks-commands","text":"We use the cryptsetup command to interact with LUKS partitions.","title":"LUKS Commands"},{"location":"linux/luks/luks/#header-management","text":"","title":"Header management"},{"location":"linux/luks/luks/#get-the-disk-header","text":"cryptsetup luksDump /dev/sda3","title":"Get the disk header"},{"location":"linux/luks/luks/#backup-header","text":"cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }}","title":"Backup header"},{"location":"linux/luks/luks/#key-management","text":"","title":"Key management"},{"location":"linux/luks/luks/#add-a-key","text":"cryptsetup luksAddKey --key-slot 1 {{ luks_device }}","title":"Add a key"},{"location":"linux/luks/luks/#test-if-you-remember-the-key","text":"Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }}","title":"Test if you remember the key"},{"location":"linux/luks/luks/#delete-some-keys","text":"cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }}","title":"Delete some keys"},{"location":"linux/luks/luks/#delete-all-keys","text":"cryptsetup luksErase {{ device }}","title":"Delete all keys"},{"location":"linux/luks/luks/#encrypt-hard-drive","text":"Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt","title":"Encrypt hard drive"},{"location":"linux/luks/luks/#luks-debugging","text":"","title":"LUKS debugging"},{"location":"linux/luks/luks/#resource-busy","text":"Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"Resource busy"},{"location":"linux/vim/vim_plugins/","text":"Black \u2691 To install Black you first need python3-venv . sudo apt-get install python3-venv Add the plugin and configure it so vim runs it each time you save. File ~/.vimrc Plugin 'psf/black' \" Black autocmd BufWritePre *.py execute ':Black' A configuration issue exists for neovim. If you encounter the error AttributeError: module 'black' has no attribute 'find_pyproject_toml' , do the following: cd ~/.vim/bundle/black git checkout 19 .10b0 As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well \"\" python indent autocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab \" python-mode let g:pymode_options_max_line_length = 88 let g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length} ALE \u2691 ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client. ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem. In other words, this plugin allows you to lint while you type. ALE offers support for fixing code with command line tools in a non-blocking manner with the :ALEFix feature, supporting tools in many languages , like prettier, eslint, autopep8, and more. Installation \u2691 Install with Vundle: Plugin 'dense-analysis/ale' Configuration \u2691 let g :ale_sign_error = '\u2718' let g :ale_sign_warning = '\u26a0' highlight ALEErrorSign ctermbg = NONE ctermfg = red highlight ALEWarningSign ctermbg = NONE ctermfg = yellow let g :ale_linters_explicit = 1 let g :ale_lint_on_text_changed = 'normal' \" let g:ale_lint_on_text_changed = 'never' let g :ale_lint_on_enter = 0 let g :ale_lint_on_save = 1 let g :ale_fix_on_save = 1 let g :ale_linters = { \\ 'markdown' : [ 'markdownlint' , 'writegood' , 'alex' , 'proselint' ] , \\ 'json' : [ 'jsonlint' ] , \\ 'python' : [ 'flake8' , 'mypy' , 'pylint' , 'alex' ] , \\ 'yaml' : [ 'yamllint' , 'alex' ] , \\ '*' : [ 'alex' , 'writegood' ] , \\} let g :ale_fixers = { \\ '*' : [ 'remove_trailing_lines' , 'trim_whitespace' ] , \\ 'json' : [ 'jq' ] , \\ 'python' : [ 'isort' ] \\ 'terraform' : [ 'terraform' ] , \\} inoremap < leader > e < esc > :ALENext < cr > nnoremap < leader > e :ALENext < cr > inoremap < leader > p < esc > :ALEPrevious < cr > nnoremap < leader > p :ALEPrevious < cr > Where: let g:ale_linters_explicit : Prevent ALE load only the selected linters. use <leader>e and <leader>p to navigate through the warnings. If you feel that it's too heavy, use ale_lint_on_enter or increase the ale_lint_delay . Use :ALEInfo to see the ALE configuration for the specific buffer. Flakehell \u2691 Flakehell is not supported yet . Until that issue is closed we need the following configuration: let g:ale_python_flake8_executable = flake8helled let g:ale_python_flake8_use_global = 1 Toggle fixers on save \u2691 There are cases when you don't want to run the fixers in your code. Ale doesn't have an option to do it , but zArubaru showed how to do it. If you add to your configuration command! ALEToggleFixer execute \"let g:ale_fix_on_save = get(g:, 'ale_fix_on_save', 0) ? 0 : 1\" You can then use :ALEToggleFixer to activate an deactivate them. References \u2691 ALE supported tools","title":"Vim Plugins"},{"location":"linux/vim/vim_plugins/#black","text":"To install Black you first need python3-venv . sudo apt-get install python3-venv Add the plugin and configure it so vim runs it each time you save. File ~/.vimrc Plugin 'psf/black' \" Black autocmd BufWritePre *.py execute ':Black' A configuration issue exists for neovim. If you encounter the error AttributeError: module 'black' has no attribute 'find_pyproject_toml' , do the following: cd ~/.vim/bundle/black git checkout 19 .10b0 As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well \"\" python indent autocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab \" python-mode let g:pymode_options_max_line_length = 88 let g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length}","title":"Black"},{"location":"linux/vim/vim_plugins/#ale","text":"ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client. ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem. In other words, this plugin allows you to lint while you type. ALE offers support for fixing code with command line tools in a non-blocking manner with the :ALEFix feature, supporting tools in many languages , like prettier, eslint, autopep8, and more.","title":"ALE"},{"location":"linux/vim/vim_plugins/#installation","text":"Install with Vundle: Plugin 'dense-analysis/ale'","title":"Installation"},{"location":"linux/vim/vim_plugins/#configuration","text":"let g :ale_sign_error = '\u2718' let g :ale_sign_warning = '\u26a0' highlight ALEErrorSign ctermbg = NONE ctermfg = red highlight ALEWarningSign ctermbg = NONE ctermfg = yellow let g :ale_linters_explicit = 1 let g :ale_lint_on_text_changed = 'normal' \" let g:ale_lint_on_text_changed = 'never' let g :ale_lint_on_enter = 0 let g :ale_lint_on_save = 1 let g :ale_fix_on_save = 1 let g :ale_linters = { \\ 'markdown' : [ 'markdownlint' , 'writegood' , 'alex' , 'proselint' ] , \\ 'json' : [ 'jsonlint' ] , \\ 'python' : [ 'flake8' , 'mypy' , 'pylint' , 'alex' ] , \\ 'yaml' : [ 'yamllint' , 'alex' ] , \\ '*' : [ 'alex' , 'writegood' ] , \\} let g :ale_fixers = { \\ '*' : [ 'remove_trailing_lines' , 'trim_whitespace' ] , \\ 'json' : [ 'jq' ] , \\ 'python' : [ 'isort' ] \\ 'terraform' : [ 'terraform' ] , \\} inoremap < leader > e < esc > :ALENext < cr > nnoremap < leader > e :ALENext < cr > inoremap < leader > p < esc > :ALEPrevious < cr > nnoremap < leader > p :ALEPrevious < cr > Where: let g:ale_linters_explicit : Prevent ALE load only the selected linters. use <leader>e and <leader>p to navigate through the warnings. If you feel that it's too heavy, use ale_lint_on_enter or increase the ale_lint_delay . Use :ALEInfo to see the ALE configuration for the specific buffer.","title":"Configuration"},{"location":"linux/vim/vim_plugins/#flakehell","text":"Flakehell is not supported yet . Until that issue is closed we need the following configuration: let g:ale_python_flake8_executable = flake8helled let g:ale_python_flake8_use_global = 1","title":"Flakehell"},{"location":"linux/vim/vim_plugins/#toggle-fixers-on-save","text":"There are cases when you don't want to run the fixers in your code. Ale doesn't have an option to do it , but zArubaru showed how to do it. If you add to your configuration command! ALEToggleFixer execute \"let g:ale_fix_on_save = get(g:, 'ale_fix_on_save', 0) ? 0 : 1\" You can then use :ALEToggleFixer to activate an deactivate them.","title":"Toggle fixers on save"},{"location":"linux/vim/vim_plugins/#references","text":"ALE supported tools","title":"References"},{"location":"meta/meta/","text":"In this book you'll find, in a wiki format, all the notes I made on a huge variety of topics, such as, Linux, DevOps , DevSecOps, feminism, rationalism, life automation , productivity or programming. The main goal is to store all the knowledge gathered throughout my life in a way that everyone can benefit from reading it or referencing in an easy and quickly way. I will be updating this wiki quite often as I use it myself daily both to keep an account of things I know as well as things I want to know and everything in between. History \u2691 I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. Furthermore, as I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that I won't have an interesting RSS feed. You could go through the git log but it doesn't make any sense. That's why I'm thinking of generating a monthly newsletter similar to Gwern's Newsletters or Changelog . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born. Book structure \u2691 Each directory is a topic that can include other subtopics under it related to the parent topic. As sometimes the strict hierarchical structure of the categories doesn't work, I also use tags to link articles. If this is your first time visiting this wiki, you can just start reading from the top entry down and see what sparks your interest. Content Structure \u2691 Each topic will have a title, some description of it, usually my own thoughts and knowledge on it as well as referencing some resources or links I have liked or used that helped me either understand the topic or gain appreciation of it. The structure of each of the posts will often look roughly like this: Title Description - My thoughts on the topic. Subtopics - Various subtopics related to the main topic. Notes - My own personal notes on the matter as well as things I found interesting on the internet regarding the topic. I often give a link of where I got things from. Links - Links related to the topic. Links \u2691 My blue book is heavily inspired in this two other second brains: Gwern's website Nikita's wiki","title":"Meta"},{"location":"meta/meta/#history","text":"I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. Furthermore, as I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that I won't have an interesting RSS feed. You could go through the git log but it doesn't make any sense. That's why I'm thinking of generating a monthly newsletter similar to Gwern's Newsletters or Changelog . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born.","title":"History"},{"location":"meta/meta/#book-structure","text":"Each directory is a topic that can include other subtopics under it related to the parent topic. As sometimes the strict hierarchical structure of the categories doesn't work, I also use tags to link articles. If this is your first time visiting this wiki, you can just start reading from the top entry down and see what sparks your interest.","title":"Book structure"},{"location":"meta/meta/#content-structure","text":"Each topic will have a title, some description of it, usually my own thoughts and knowledge on it as well as referencing some resources or links I have liked or used that helped me either understand the topic or gain appreciation of it. The structure of each of the posts will often look roughly like this: Title Description - My thoughts on the topic. Subtopics - Various subtopics related to the main topic. Notes - My own personal notes on the matter as well as things I found interesting on the internet regarding the topic. I often give a link of where I got things from. Links - Links related to the topic.","title":"Content Structure"},{"location":"meta/meta/#links","text":"My blue book is heavily inspired in this two other second brains: Gwern's website Nikita's wiki","title":"Links"},{"location":"projects/projects/","text":"Also known as where I'm spending my spare time. Active projects \u2691 Projects under active development. Pydo \u2691 I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo . I'm now doing a full rewrite of the codebase following the repository pattern which led me to create a Python library . Blue book \u2691 I'm refactoring all the knowledge gathered in the past in my cheat sheet repository into the blue book. This means migrating 7422 articles, almost 50 million lines, to the new structure. It's going to be a slow and painful process \u1559(\u21c0\u2038\u21bc\u2036)\u1557 . Clinv \u2691 As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. As you can see in how do you document your infrastructure? , there is still a void on how to maintain an inventory of dynamic resources with a DevSecOps point of view. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv , Repository pattern \u2691 I'm creating a Python library so that implementing the repository pattern in new projects is easier. I usually spin up new ideas for programs monthly, and managing the storage of the information is cumbersome and repeating. My idea is to refactor that common codebase into a generic library that anyone can use. Cookiecutter Python template \u2691 Following the same reasoning as the previous section, I've spent a lot of time investigating quality measures for python projects, such as project structure, ci testing, ci building, dependency management, beautiful docs or pre-commits. With the cookiecutter template , it is easy to create a new project with the best quality measures with zero effort. Furthermore, with cruft I can keep all the projects generated with the template updated with the best practices. Autoimport \u2691 Throughout the development of a python program you continuously need to manage the python import statements either because you need one new object or because you no longer need it. This means that you need to stop writing whatever you were writing, go to the top of the file, create or remove the import statement and then resume coding. This workflow break is annoying and almost always unnecessary. autoimport solves this problem if you execute it whenever you have an import error, for example by configuring your editor to run it when saving the file. yamlfix \u2691 A simple opinionated yaml formatter that keeps your comments Media indexation \u2691 I've got a music collection of 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system. Maintained projects \u2691 Projects where I don't have my focus on, but I still give support and eventually develop new features. Drode \u2691 drode is a wrapper over the Drone and AWS APIs to make deployments more user friendly. It assumes that the projects are configured to continuous deliver all master commits to staging. Then those commits can be promoted to production or to staging for upgrades and rollbacks. It has the following features: Prevent failed jobs to be promoted to production. Promote jobs with less arguments than the drone command line. Wait for a drone build to end, then raise the terminal bell. Home Stock inventory \u2691 I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I've integrated Grocy in my life. Mediarss \u2691 I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service. So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor. Playlist_generator \u2691 When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^). mep \u2691 I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"Projects"},{"location":"projects/projects/#active-projects","text":"Projects under active development.","title":"Active projects"},{"location":"projects/projects/#pydo","text":"I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo . I'm now doing a full rewrite of the codebase following the repository pattern which led me to create a Python library .","title":"Pydo"},{"location":"projects/projects/#blue-book","text":"I'm refactoring all the knowledge gathered in the past in my cheat sheet repository into the blue book. This means migrating 7422 articles, almost 50 million lines, to the new structure. It's going to be a slow and painful process \u1559(\u21c0\u2038\u21bc\u2036)\u1557 .","title":"Blue book"},{"location":"projects/projects/#clinv","text":"As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. As you can see in how do you document your infrastructure? , there is still a void on how to maintain an inventory of dynamic resources with a DevSecOps point of view. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv ,","title":"Clinv"},{"location":"projects/projects/#repository-pattern","text":"I'm creating a Python library so that implementing the repository pattern in new projects is easier. I usually spin up new ideas for programs monthly, and managing the storage of the information is cumbersome and repeating. My idea is to refactor that common codebase into a generic library that anyone can use.","title":"Repository pattern"},{"location":"projects/projects/#cookiecutter-python-template","text":"Following the same reasoning as the previous section, I've spent a lot of time investigating quality measures for python projects, such as project structure, ci testing, ci building, dependency management, beautiful docs or pre-commits. With the cookiecutter template , it is easy to create a new project with the best quality measures with zero effort. Furthermore, with cruft I can keep all the projects generated with the template updated with the best practices.","title":"Cookiecutter Python template"},{"location":"projects/projects/#autoimport","text":"Throughout the development of a python program you continuously need to manage the python import statements either because you need one new object or because you no longer need it. This means that you need to stop writing whatever you were writing, go to the top of the file, create or remove the import statement and then resume coding. This workflow break is annoying and almost always unnecessary. autoimport solves this problem if you execute it whenever you have an import error, for example by configuring your editor to run it when saving the file.","title":"Autoimport"},{"location":"projects/projects/#yamlfix","text":"A simple opinionated yaml formatter that keeps your comments","title":"yamlfix"},{"location":"projects/projects/#media-indexation","text":"I've got a music collection of 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system.","title":"Media indexation"},{"location":"projects/projects/#maintained-projects","text":"Projects where I don't have my focus on, but I still give support and eventually develop new features.","title":"Maintained projects"},{"location":"projects/projects/#drode","text":"drode is a wrapper over the Drone and AWS APIs to make deployments more user friendly. It assumes that the projects are configured to continuous deliver all master commits to staging. Then those commits can be promoted to production or to staging for upgrades and rollbacks. It has the following features: Prevent failed jobs to be promoted to production. Promote jobs with less arguments than the drone command line. Wait for a drone build to end, then raise the terminal bell.","title":"Drode"},{"location":"projects/projects/#home-stock-inventory","text":"I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I've integrated Grocy in my life.","title":"Home Stock inventory"},{"location":"projects/projects/#mediarss","text":"I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service. So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor.","title":"Mediarss"},{"location":"projects/projects/#playlist_generator","text":"When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^).","title":"Playlist_generator"},{"location":"projects/projects/#mep","text":"I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"mep"},{"location":"psychology/the_xy_problem/","text":"The XY problem is a communication or thinking problem encountered in situations where the real issue, X , of the human asking for help is obscured, because instead of asking directly about issue X , they ask how to solve a secondary issue, Y , which they believe will allow them to resolve issue X . However, resolving issue Y often does not resolve issue X , or is a poor way to resolve it, and the obscuring of the real issue and the introduction of the potentially strange secondary issue can lead to the person trying to help having unnecessary difficulties in communication and offering poor solutions. How to avoid it \u2691 Always include information about a broader picture along with any attempted solution. If someone asks for more information, do provide details. If there are other solutions you've already ruled out, share why you've ruled them out. This gives more information about your requirements.","title":"The XY Problem"},{"location":"psychology/the_xy_problem/#how-to-avoid-it","text":"Always include information about a broader picture along with any attempted solution. If someone asks for more information, do provide details. If there are other solutions you've already ruled out, share why you've ruled them out. This gives more information about your requirements.","title":"How to avoid it"},{"location":"writing/orthography/","text":"My english writing is not so good, this article is an effort to gather all my common pitfalls. When to write Apostrophes before an s \u2691 For most singular nouns, add apostrophe + s : The writer's desk . For most plural nouns, add apostrophe : The writers' desk (multiple writers). For plural nouns that do not end in s, add apostrophe + s : The geese's migration route . For singular proper nouns both apostrophe and apostrophe + s is accepted, but as the plural proper nouns ending in s, the correct form is apostrophe I'd use that for both, so: Charles Dickens' novels and The Smiths' vacation . The personal pronouns, do not have apostrophes to form possessives, this means that your , yours , hers , its , ours , their , whose , and theirs . In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive. Who vs Whom \u2691 If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition. A vs An \u2691 We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a . Comma before and \u2691 There are two cases: It's required to put a comma before and when it\u2019s connecting two independent clauses. It\u2019s almost always optional the use of comma before and in lists. This case is also known as serial commas or Oxford commas . Avoid there is at the start of the sentence \u2691 Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point.","title":"English Grammar and Ortography"},{"location":"writing/orthography/#when-to-write-apostrophes-before-an-s","text":"For most singular nouns, add apostrophe + s : The writer's desk . For most plural nouns, add apostrophe : The writers' desk (multiple writers). For plural nouns that do not end in s, add apostrophe + s : The geese's migration route . For singular proper nouns both apostrophe and apostrophe + s is accepted, but as the plural proper nouns ending in s, the correct form is apostrophe I'd use that for both, so: Charles Dickens' novels and The Smiths' vacation . The personal pronouns, do not have apostrophes to form possessives, this means that your , yours , hers , its , ours , their , whose , and theirs . In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive.","title":"When to write Apostrophes before an s"},{"location":"writing/orthography/#who-vs-whom","text":"If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition.","title":"Who vs Whom"},{"location":"writing/orthography/#a-vs-an","text":"We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a .","title":"A vs An"},{"location":"writing/orthography/#comma-before-and","text":"There are two cases: It's required to put a comma before and when it\u2019s connecting two independent clauses. It\u2019s almost always optional the use of comma before and in lists. This case is also known as serial commas or Oxford commas .","title":"Comma before and"},{"location":"writing/orthography/#avoid-there-is-at-the-start-of-the-sentence","text":"Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point.","title":"Avoid there is at the start of the sentence"},{"location":"writing/writing/","text":"Writing is difficult, at least for me. Even more if you aren't using your native tongue. Principles \u2691 Make it pleasant to the reader \u2691 Writing is a medium of communication, so avoid introducing elements that push away the reader, such as: Spelling mistakes. Gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing. Ugly environment: Present your texts through a pleasant medium such as a mkdocs webpage. Write like you talk: Ask yourself, is this the way I'd say this if I were talking to a friend? . If it isn't, imagine what you would say, and use that instead. Format errors: If you're writing in markdown, make sure that the result has no display bugs. Write short articles: Even though I love Gwern site , I find it daunting most of times. Instead of a big post, I'd rather use multiple well connected articles. Saying more with less \u2691 Never use a long word where a short one will do. Replace words like really like with love or other more appropriate words that save space writing and are more meaningful. Don't use filler words like really . Be aware of pacing \u2691 Be aware of pacing between words and sentences. The sentences ideally should flow into one another. Breaks in form of commas and full steps are important, as they allow for the reader to take a break and absorb the point that you tried to deliver. Try to use less tan 30 words per sentence. For example, change Due to the fact that to because . One purpose \u2691 A good piece of writing has a single, sharp, overriding purpose. Every part of the writing, even the digressions, should serve that purpose. Put another way, clarity of the general purpose is an absolute requirement in a good piece of writing. This observation matters because it's often tempting to let your purpose expand and become vague. Writing a piece about gardens? Hey, why not include that important related thought you had about rainforests? Now you have a piece that's sort of about gardens and sort of about rainforests, and not really about anything. The reader can no longer bond to it. A complicating factor is that sometimes you need to explore beyond the boundaries of your current purpose. You're writing for purpose A, but your instinct says that you need to explore subject B. Unfortunately, you're not yet sure how subject B fits in. If that's the case then you must take time to explore, and to understand how, if at all, subject B fits in, and whether you need to revise your purpose. This is emotionally difficult. It creates uncertainty, and you may feel as though your work on subject B is wasted effort. These doubts must be resisted. Avoid using clich\u00e9s \u2691 Clich\u00e9s prevent readers from visualization , making them an obstacle to creating memorable writing. Citing the sources \u2691 If it's a small phrase or a refactor, link the source inside the phrase or at the header of the section. If it's a big refactor, add it to a references section. If it's a big block without editing use admonition quotes Take all the guidelines as suggestions \u2691 All the sections above are guidelines, not rules to follow blindly, I try to adhere to them as much as possible, but if I feel it doesn't apply I ignore them. Unconnected thoughts \u2691 Replace adjectives with data. Nearly all of -> 84% of . Remove weasel words . Most adverbs are superfluous. When you say \"generally\" or \"usually\" you're probably undermining your point and the use of \"very\" or \"extremely\" are hyperbolic and breathless and make it easier to regard what you're writing as not serious. Examine every word: a surprising number don't serve any purpose. While wrapping your content into a story you may find yourself talking about your achievements more than giving actionable advice. If that happens, try to get to the bottom of how you achieved these achievements and break this process down, then focus on the process more than on your personal achievement. Set up a system that prompts people to review the material. Don't be egocentric, limit the use of I , use the implied subject instead: It's where I go to -> It's the place to go. I take different actions -> Taking different actions . Don't be possessive, use the instead of my . If you don't know how to express something use services like deepl . Use synonyms instead of repeating the same word over and over. Think who are you writing to. Use active voice : Active voice ensures that the actors are identified and it generally leaves less open questions. The exception is if you want to emphasize the object of the sentence. Tests \u2691 Using automatic tools that highlight the violations of the previous principles may help you to internalize all the measures, even more with the new ones. Configure your editor to: Run a spell checker that you can check as you write. Alert you on new orthography rules you want to adopt. Use linters to raise your awareness on the rest of issues. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. markdownlint : style checker and lint tool for Markdown/CommonMark files. proselint : Is another linter for prose. write-good is a naive linter for English prose. Use formatters to make your writing experience more pleasant. mdformat : I haven't tested it yet, but looks promising. There are some checks that I wasn't able to adopt: Try to use less tan 30 words per sentence. Check that every sentence is ended with a dot. Be consistent across document structures, use References instead of Links , or Installation instead of Install . gwern markdown-lint.sh script file . Avoid the use of here , use descriptive link text. Rotten links: use linkchecker (I think there was a mkdocs plugin to do this). Also read how to archive urls . check for use of the word \"significant\"/\"significance\" and insert \"[statistically]\" as appropriate (to disambiguate between effect sizes and statistical significance; this common confusion is one reason for \"statistical-significance considered harmful\" ) Writing workflow \u2691 Start with a template. Use synoptical reading to gather ideas in an unconnected thoughts section. Once you've got several refactor them in sections with markdown headers. Ideally you'll want to wrap your whole blog post into a story or timeline. Add an abstract so the reader may decide if she wants to read it. Publication \u2691 Think how to publicize: Hacker News Reddit LessWrong (and further sites as appropriate) References \u2691 Awesome: Nikita's writing notes Gwern's writing checklist Good: Long Naomi pen post with some key ideas Doing \u2691 https://github.com/mnielsen/notes-on-writing/blob/master/notes_on_writing.md#readme Todo \u2691 https://www.scottadamssays.com/2015/08/22/the-day-you-became-a-better-writer-2nd-look/ https://blog.stephsmith.io/learning-to-write-with-confidence/ https://styleguide.mailchimp.com/tldr/ https://content-guide.18f.gov/inclusive-language/ https://performancejs.com/post/31b361c/13-Tips-for-Writing-a-Technical-Book https://github.com/RacheltheEditor/ProductionGuide#readme https://mkaz.blog/misc/notes-on-technical-writing/ https://www.swyx.io/writing/cfp-advice/ https://sivers.org/d22 https://homes.cs.washington.edu/~mernst/advice/write-technical-paper.html Investigate on readability tests: Definition Introduction on Readability List of readability tests and formulas An example of a formula Books \u2691 https://www.amazon.de/dp/0060891548/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=39f2ab8ab47769b2a106e9667149df30&creativeASIN=0060891548&tag=gregdoesit03-21&creative=9325&camp=1789 https://www.amazon.de/dp/0143127799/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=ff9322c17ca288b1d9d6b5fb8d6df619&creativeASIN=0143127799&tag=gregdoesit03-21&creative=9325&camp=1789","title":"Writing"},{"location":"writing/writing/#principles","text":"","title":"Principles"},{"location":"writing/writing/#make-it-pleasant-to-the-reader","text":"Writing is a medium of communication, so avoid introducing elements that push away the reader, such as: Spelling mistakes. Gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing. Ugly environment: Present your texts through a pleasant medium such as a mkdocs webpage. Write like you talk: Ask yourself, is this the way I'd say this if I were talking to a friend? . If it isn't, imagine what you would say, and use that instead. Format errors: If you're writing in markdown, make sure that the result has no display bugs. Write short articles: Even though I love Gwern site , I find it daunting most of times. Instead of a big post, I'd rather use multiple well connected articles.","title":"Make it pleasant to the reader"},{"location":"writing/writing/#saying-more-with-less","text":"Never use a long word where a short one will do. Replace words like really like with love or other more appropriate words that save space writing and are more meaningful. Don't use filler words like really .","title":"Saying more with less"},{"location":"writing/writing/#be-aware-of-pacing","text":"Be aware of pacing between words and sentences. The sentences ideally should flow into one another. Breaks in form of commas and full steps are important, as they allow for the reader to take a break and absorb the point that you tried to deliver. Try to use less tan 30 words per sentence. For example, change Due to the fact that to because .","title":"Be aware of pacing"},{"location":"writing/writing/#one-purpose","text":"A good piece of writing has a single, sharp, overriding purpose. Every part of the writing, even the digressions, should serve that purpose. Put another way, clarity of the general purpose is an absolute requirement in a good piece of writing. This observation matters because it's often tempting to let your purpose expand and become vague. Writing a piece about gardens? Hey, why not include that important related thought you had about rainforests? Now you have a piece that's sort of about gardens and sort of about rainforests, and not really about anything. The reader can no longer bond to it. A complicating factor is that sometimes you need to explore beyond the boundaries of your current purpose. You're writing for purpose A, but your instinct says that you need to explore subject B. Unfortunately, you're not yet sure how subject B fits in. If that's the case then you must take time to explore, and to understand how, if at all, subject B fits in, and whether you need to revise your purpose. This is emotionally difficult. It creates uncertainty, and you may feel as though your work on subject B is wasted effort. These doubts must be resisted.","title":"One purpose"},{"location":"writing/writing/#avoid-using-cliches","text":"Clich\u00e9s prevent readers from visualization , making them an obstacle to creating memorable writing.","title":"Avoid using clich\u00e9s"},{"location":"writing/writing/#citing-the-sources","text":"If it's a small phrase or a refactor, link the source inside the phrase or at the header of the section. If it's a big refactor, add it to a references section. If it's a big block without editing use admonition quotes","title":"Citing the sources"},{"location":"writing/writing/#take-all-the-guidelines-as-suggestions","text":"All the sections above are guidelines, not rules to follow blindly, I try to adhere to them as much as possible, but if I feel it doesn't apply I ignore them.","title":"Take all the guidelines as suggestions"},{"location":"writing/writing/#unconnected-thoughts","text":"Replace adjectives with data. Nearly all of -> 84% of . Remove weasel words . Most adverbs are superfluous. When you say \"generally\" or \"usually\" you're probably undermining your point and the use of \"very\" or \"extremely\" are hyperbolic and breathless and make it easier to regard what you're writing as not serious. Examine every word: a surprising number don't serve any purpose. While wrapping your content into a story you may find yourself talking about your achievements more than giving actionable advice. If that happens, try to get to the bottom of how you achieved these achievements and break this process down, then focus on the process more than on your personal achievement. Set up a system that prompts people to review the material. Don't be egocentric, limit the use of I , use the implied subject instead: It's where I go to -> It's the place to go. I take different actions -> Taking different actions . Don't be possessive, use the instead of my . If you don't know how to express something use services like deepl . Use synonyms instead of repeating the same word over and over. Think who are you writing to. Use active voice : Active voice ensures that the actors are identified and it generally leaves less open questions. The exception is if you want to emphasize the object of the sentence.","title":"Unconnected thoughts"},{"location":"writing/writing/#tests","text":"Using automatic tools that highlight the violations of the previous principles may help you to internalize all the measures, even more with the new ones. Configure your editor to: Run a spell checker that you can check as you write. Alert you on new orthography rules you want to adopt. Use linters to raise your awareness on the rest of issues. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. markdownlint : style checker and lint tool for Markdown/CommonMark files. proselint : Is another linter for prose. write-good is a naive linter for English prose. Use formatters to make your writing experience more pleasant. mdformat : I haven't tested it yet, but looks promising. There are some checks that I wasn't able to adopt: Try to use less tan 30 words per sentence. Check that every sentence is ended with a dot. Be consistent across document structures, use References instead of Links , or Installation instead of Install . gwern markdown-lint.sh script file . Avoid the use of here , use descriptive link text. Rotten links: use linkchecker (I think there was a mkdocs plugin to do this). Also read how to archive urls . check for use of the word \"significant\"/\"significance\" and insert \"[statistically]\" as appropriate (to disambiguate between effect sizes and statistical significance; this common confusion is one reason for \"statistical-significance considered harmful\" )","title":"Tests"},{"location":"writing/writing/#writing-workflow","text":"Start with a template. Use synoptical reading to gather ideas in an unconnected thoughts section. Once you've got several refactor them in sections with markdown headers. Ideally you'll want to wrap your whole blog post into a story or timeline. Add an abstract so the reader may decide if she wants to read it.","title":"Writing workflow"},{"location":"writing/writing/#publication","text":"Think how to publicize: Hacker News Reddit LessWrong (and further sites as appropriate)","title":"Publication"},{"location":"writing/writing/#references","text":"Awesome: Nikita's writing notes Gwern's writing checklist Good: Long Naomi pen post with some key ideas","title":"References"},{"location":"writing/writing/#doing","text":"https://github.com/mnielsen/notes-on-writing/blob/master/notes_on_writing.md#readme","title":"Doing"},{"location":"writing/writing/#todo","text":"https://www.scottadamssays.com/2015/08/22/the-day-you-became-a-better-writer-2nd-look/ https://blog.stephsmith.io/learning-to-write-with-confidence/ https://styleguide.mailchimp.com/tldr/ https://content-guide.18f.gov/inclusive-language/ https://performancejs.com/post/31b361c/13-Tips-for-Writing-a-Technical-Book https://github.com/RacheltheEditor/ProductionGuide#readme https://mkaz.blog/misc/notes-on-technical-writing/ https://www.swyx.io/writing/cfp-advice/ https://sivers.org/d22 https://homes.cs.washington.edu/~mernst/advice/write-technical-paper.html Investigate on readability tests: Definition Introduction on Readability List of readability tests and formulas An example of a formula","title":"Todo"},{"location":"writing/writing/#books","text":"https://www.amazon.de/dp/0060891548/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=39f2ab8ab47769b2a106e9667149df30&creativeASIN=0060891548&tag=gregdoesit03-21&creative=9325&camp=1789 https://www.amazon.de/dp/0143127799/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=ff9322c17ca288b1d9d6b5fb8d6df619&creativeASIN=0143127799&tag=gregdoesit03-21&creative=9325&camp=1789","title":"Books"}]}